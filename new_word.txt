Morph Examples and Motivations Distributions of Morph Examples Overview of Morph Resolution Crosssource Comparable Data Example each morph and target pair is shown in the same color Network Schema of MorphRelated Het erogeneous Information Network Example of MorphRelated Heteroge neous Information Network The System Performance Based on Com binations of Surface and Semantic Features The System Performance Based on Each Single Feature Set Description of feature sets Glob only uses the same set of similarity measures when combined with other semantic features The Effects of Temporal Constraint Accuracy of Target Candidate Detection The Effects of Social Features The System Performance of Integrating Cross Source and Cross Genre Information Effects of Popularity of Morphs Performance of Two Categories Architecture of the translation approach based on a loglinear modeling approach Example of a symmetrized word alignment Verbmobil task Algorithm phraseextract for extracting phrases from a wordaligned sentence pair Here quasiconsecutiveTP is a predicate that tests whether the set of words TP is consecutive with the possible exception of words that are not aligned Examples of alignment templates obtained in training Dependencies in the alignment template model Example of segmentation of German sentence and its English translation into alignment templates Algorithm for breadthfirst search with pruning Algorithm minjumps to compute the minimum number of needed jumps DcJ1 j to complete the translation Statistics for Verbmobil task training corpus Train conventional dictionary Lex development corpus Dev test corpus Test Words words without punctuation marks Effect of alignment template length on translation quality Effect of pruning parameter tp and heuristic function on search efficiency for directtranslation model Np 50000 Effect of pruning parameter tp and heuristic function on error rate for directtranslation model Np 50000 Effect of pruning parameter Np and heuristic function on search efficiency for directtranslation model tp 1012 Effect of pruning parameter Np and heuristic function on error rate for directtranslation model tp 1012 Corpus statistics for Hansards task Words words without punctuation marks Corpus statistics for ChineseEnglish corporalarge data track Words words without punctuation marks Translation results on the Hansards task Example translations for ChineseEnglish MT 1 Feature set for coreference resolution Feature 22 23 and features involving Cj are not used in the singlecandidate model Results for the nonpronoun resolution Results for the pronoun resolution Results for the coreference resolution List of semantic labels Examples of aggregated instances Results of semantic classification Confusion matrix of acquired nouns Results of ablation experiments Linking FrameNet frames and VerbNet classes Mapping algorithm refining step Results of the mapping algorithm F1s of some individual FN role classifiers and the overall multiclassifier accuracy 454 roles F1s of some individual ILC classifiers and the overall multiclassifier accuracy 180 classes on PB and 133 on FN Semantic role learning curve Graph representing transliteration pairs and cooccurence relations Ten highestscoring matches for the Xin hua corpus for 81301 The final column is the log P estimate for the transliteration Starred entries are incorrect MRRs of the frequency correlation meth ods MRRs on the augmented candidate list Effectiveness of combining the two scor ing methods Propagation Core items Effectiveness of score propagation Propagation All items An example of MOD feature extraction An oval in the dependency tree denotes a bunsetsu of features Precision for 200 candidates EvRec Precision for each phrase type EvLing Anaphora resolution preferences Description of the unrestricted corpora used in the evaluation Results obtained in the detection of zeropronouns Classification of third person zero pronouns Syntactic tree kernel STK Integrating Brown cluster information Overview of the ACE 2005 data Distribution of relations in ACE 2005 For each domain the percentage of target domain words types that are unseen in the source together with the most frequent OOV words Comparison to previous work on the 7 re lations of ACE 2004 K kernelbased F feature based yesno models argument order explicitly Brown clusters in tree kernels cf Fig 2 F1 per coarse relation type ACE 2005 SYS is the final model ie last row PETPET WCPET LSA of Table 5 Indomain first column and outofdomain performance columns two to four on ACE 2005 PET and BOW are abbreviated by P and B respectively If not specified BOW is marked Bell tree representation for three mentions numbers in denote a partial entity Infocus entities are marked on the solid arrows and active mentions are marked by Solid arrows signify that a mention is linked with an infocus partial entity while dashed arrows indicate starting of a new entity Basic features used in the maximum entropy model Statistics of three test sets Table 4 Impact of feature categories Numbers after are the standard deviations indicates that the result is significantly pairwise ttest different from the line above at Performance vs log start penalty Results on the MUC6 formal test set Entityconstrained Mention Fmeasure MP uses Table 3 Coreference results on true mentions MP mentionpair model EM entityme features wh features None of the ECMF differences between MP and EM is statistically significant at The generic beamsearch algorithm Feature templates for the word segmentor Feature templates of a typical characterbased word segmentor Speedaccuracy tradeoff of the segmentor Training development and test data for word segmentation on CTB5 The accuracies of various word segmentors over the first SIGHAN bakeoff data The accuracies of various word segmentors over the second SIGHAN bakeoff data The extended generic beamsearch algorithm with multiple beams Comparison between three different decoders for word segmentation POS feature templates for the joint segmentor and POS tagger The influence of beamsizes and the convergence of the perceptron for the joint segmentor and POS tagger The speeds of joint word segmentation and POStagging by 10fold cross validation The accuracies of joint segmentation and POStagging by 10fold cross validation The comparison of overall accuracies of various joint segmentor and POStaggers by 10fold cross validation using CTB Training development and test data from CTB5 for joint word segmentation and POStagging Accuracy comparisons between various joint segmentors and POStaggers on CTB5 The accuracyspeed tradeoff graph for the joint segmentor and POStaggers and the twostage baseline An example Chinese dependency tree Transitionbased feature templates for the dependency parser Transitionbased feature context for the dependency parser Graphbased feature templates for the dependency parser The training development and test data for English dependency parsing The accuracyspeed tradeoff graph for the transitionbased and combined dependency parsers Accuracy comparisons between various dependency parsers on English data Training development and test data for Chinese dependency parsing Test accuracies of various dependency parsers on CTB5 data The combined segmentation POStagging and dependency parsing Fscores using different pipelined systems An example Chinese lexicalized phrasestructure parse tree Feature templates for the phrasestructure parser The standard split of CTB2 data for phrasestructure parsing Accuracies of various phrasestructure parsers on CTB2 with goldstandard POStags Accuracies of various phrasestructure parsers on CTB2 with automatically assigned tags Accuracies of our phrasestructure parser on CTB5 using goldstandard and automatically assigned POStags The accuracyspeed tradeoff graph for the phrasestructure parser Comparison of dependency accuracies between phrasestructure parsing and dependency parsing using CTB5 data Composition of two FSTs maintaining separate transitions Finitestate cascades for five natural language problems Changing a decision in the derivation lattice All paths generate the observed data The bold path rep resents the current sample and the dotted path represents a sidetrack in which one decision is changed Multiple EM restarts for POS tagging Each point represents one random restart the yaxis is tag ging accuracy and the xaxis is EMs objective function log Pdata Multiple Bayesian learning runs using anneal ing with temperature decreasing from 2 to 008 for POS tagging Each point represents one run the yaxis is tag ging accuracy and the xaxis is the log Pderivation of the final sample Multiple Bayesian learning runs using averag ing for POS tagging Each point represents one run the yaxis is tagging accuracy and the xaxis is the average log Pderivation over all samples after burnin Gibbs sampling for Bayesian inference outperforms both EM and Variational Bayesian EM The output of EM alignment was used as the gold standard Comparison of Min Simple Fulland DynamicExpansions More Examples DynamicExpansion Tree Span Scheme Comparison of different contextsensitive Comparison of tree span schemes with antecedents in different sentences apart Feature templates used in Rphase Ex ample used is 32 ddd Tags used in LMR Tagging scheme Example of LMR Tagging Additional feature templates used in C phase Example used is 32 ddd with tagging results after Rphase as SSLMR Official BakeOff2005 results Keys F Regular Tagging only all training data are used P1 Regular Tagging only 90 of training data are used P2 Regular Tagging only 70 of training data are used S Regular and Correctional Tagging Separated Mode I Regular and Correctional Tagging Integrated Mode Experimental results of CityU corpus measured in Fmeasure Dependency representation of example 2 from Talbanken05 Animacy classification scheme Zaenen et al 2004 The animacy data set from Talbanken05 number of noun lemmas Types and tokens in each class Accuracy for MBL and SVM classifiers on Talbanken05 nouns in accumulated frequency bins by Parole frequency Precision recall and Fscores for the two classes in MBLexperiments with a general feature space Confusion matrix for the MBLclassifier with a general feature space on the 10 data set on Talbanken05 nouns Overall results in experiments with au tomatic features compared to gold standard fea tures expressed as unlabeled and labeled attach ment scores System overview Table Construction Usefulness evaluation result Correctness evaluation result Example of a prediction for English to French translation s is the source sentence h is the part of its translation that has already been typed x is what the translator wants to type and x is the prediction Probability that a prediction will be ac cepted versus its gain Time to read and accept or reject proposals versus their length Approximate times in seconds to generate predictions of maximum word sequence length M on a 12GHz processor for the MEMD model Results for different user simulations Numbers give reductions in keystrokes Results for different predictor configura tions Numbers give reductions in keystrokes Snapshot of the supersenseannotated data The 7 article titles translated in each domain with total counts of sentences tokens and supersense mentions Overall there are 2219 sentences with 65452 tokens and 23239 mentions 13 tokensmention on average Counts exclude sentences marked as problematic and mentions marked Illustration of the alignment of steps Statistics of datasets F1 scores in of SegTagDep on CTB 5c1 wrt the training epoch xaxis and parsing feature weights in legend Performance of baseline and joint models wrt the average processing time in sec per sen tence Each point corresponds to the beam size of 4 8 16 32 64 The beam size of 16 is used for SegTag in SegTagDep and SegTagTagDep Final results on CTB5j F1 scores and speed in sentences per sec of SegTagDep on CTB5c1 wrt the beam size Final results on CTB6 and CTB7 Segmentation POS tagging and unlabeled attachment dependency F1 scores averaged over five trials on CTB5c Figures in parentheses show the differences over SegTagDep p 001 Examples of the semantic role features Decoding algorithm for the standard TreetoString transducer lef twrightw denote the leftright boundary word of s c1 c2 denote the descendants of v ordered based on RHS of t An example showing the combination of the se mantic role sequences of the states Abovemiddle is the state information beforeafter applying the TTS template and bot tom is the used TTS template and the triggered SRFs during the combination An example showing how to compute the target side position of a semantic role by using the median of its aligning points Decoding algorithm using semantic role features Semac1 role c2 role t denotes the triggered semantic role features when combining two children states and ex amples can be found in Figure 3 Computing the partition function of the conditional probability P rST Semas1 s2 t denotes all the seman tic role features generated by combining s1 and s2 using t Examples of the MT outputs with and without SRFs The first and second example shows that SRFs improve the completeness and the ordering of the MT outputs respectively the third example shows that SRFs improve both properties The subscripts of each Chinese phrase show their aligned words in English Distribution of the sentences where the semantic role features give nopositivenegative impact to the sentence fluency in terms of the completeness and ordering of the semantic roles BLEU4 scores of different systems Abridged grammatical representation for the example sentence 9 a An undirected graph G representing the similarity matrix b The bipartite graph showing three clusters on G c The induced clusters U d The new graph G1 over clusters U e The new bipartite graph over G1 Performance on T3 using a predefined tree structure Performance on T2 using a predefined tree structure Comparison against Stevenson and Joanis 2003s result on T1 using similar features NMI of unconstrained HGFC when trees for T2 and T3 are inferred automatically Effect of Factors Sentence Recency Performance of Algorithms Notation and signatures for our framework Additional notation and signatures for CAM CoreLexs basic types with their corresponding WordNet anchors CAM adopts these as meta senses Sample of experimental items for the meta alternation anmfod Abbreviations are listed in Table 2 Sample targets for meta alternations with high AP and midcoherence values Average Precision and Coherence for each meta alternation Correlation r 0743 p 0001 Meta alternations and their average precision values for the task The random baseline performs at 0313 while the frequency baseline ranges from 0255 to 0369 with a mean of 0291 Alternations for which the model outperforms the frequency baseline are in boldface mean AP 0399 standard deviation 0119 Illustration of dictionary based segmenta tion finite state transducer Performance of the mention detection sys tem using lexical features only Performance of the mention detection sys tem using lexical syntactic gazetteer features as well as features obtained by running other namedentity classifiers Performance of the mention detection sys tem including all ACE04 subtasks Effect of Arabic stemming features on coref erence resolution The row marked with Truth represents the results with true mentions while the row marked with System represents that mentions are detected by the system Numbers under ECM F are EntityConstrainedMention Fmeasure and numbers under ACEVal are ACEvalues Organisation of the hierarchical graph of concepts a An undirected graph G representing the similarity matrix b The bipartite graph showing three clusters on G c The induced clusters U d The new graph G1 over clusters U e The new bipartite graph over G1 Salient features for fire and the violence cluster Discovered metaphorical associations Identified metaphorical expressions for the mappings FEELING IS FIRE and CRIME IS A DISEASE Metaphors tagged by the system in bold Outline of word segmentation process Segmentation results by a pure subwordbased IOB tagging The upper numbers are of the character based and the lower ones the subwordbased Our segmentation results by the dictionary based approach for the closed test of Bakeoff 2005 very low Roov rates due to no OOV recognition applied Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of Fscore Effects of combination using the confidence measure The upper numbers and the lower numbers are of the characterbased and the subwordbased respec tively Feature set for the baseline pronoun res olution system structuredfeatures for the instance ihim the man Results of the syntactic structured fea tures Results using different parsers Comparison of the structured feature and the flat features extracted from parse trees Learning curves of systems with different features Feature templates used for CRF in our system performance each step of our system achieves A framework for jointly identifying and aligning bilingual NEs Initial typesensitive ChineseEnglish NER performance NEA typeinsensitive typesensitive performance on the test set NEA typeinsensitive typesensitive performance with the same Chinese NE recognizer Wus system and different English NE recognizers NER typeinsensitive typesensitive performance of different English NE recognizers NER typeinsensitive typesensitive performance of different Chinese NE recognizers NEA typeinsensitive typesensitive performance with the same English NE recognizer Mallet system and different Chinese NE recognizers NEA typeinsensitive typesensitive performance with a different English NE recognizer and another Chinese NE recognizer Initial NE recognition typeinsensitive typesensitive performance across various domains The superiority of our joint model on three different domains indicated by typeinsensitive typesensitive performance those signicant entries are marked in comparison with baseline Comparison between a ME framework and the derived model on the same test set Distribution of various error categories typeinsensitive Distribution of Category VI error classes typeinsensitive Top four worstcase statistics of features for NE boundary errors Distribution of the NE type errors MERTW Effect of adjacent contextual nonNE bigrams on the test set Typesensitive improvement for ChineseEnglish NER Typeinsensitive improvement for ChineseEnglish NER English NE recognition on test data after semisupervised learning NE alignment on test data after semisupervised learning Syntactical Variations of activate Extraction of Raw Pattern Division of Raw Pattern into Combina tion Pattern Components EntityMainEntity Example Demonstrating Advantages of Full Parsing Features for SVM Learning of Prediction Model Results of IE Experiment Effect of Training Corpus Size 2 Effect of Training Corpus Size 1 Causes of Error for FPs Comparison for Head and Tail datasets Illustration of entityrelationship graphs Summary for graphs and test datasets obtained from each seed pair How to get the ordered set B t i j MRR of baseline and reinforced matrices Distribution over number of hits Matched translations over Ve and Vc Parameter setup for and Precision Recall and F1score of Engkoo Google and Ours with head and tail datasets Precision Recall and F1score of Baseline Engkoo Google and Ours over test sets Ti The first set of features in our model All of them are binary The final feature set includes two sets the set here and a set obtained by its conjunction with the verbs lemma An example parse tree for the second head word feature Accuracy and error reduction ER results in percents for our model and the MF baseline Error reduction is computed as M ODELM 100M F F Results are given for the WSJ and GENIA corpora test sets The top table is for a model receiving gold standard parses of the test data The bottom is for a model using Charniak and Johnson 2005 stateoftheart parses of the test data In the main scenario left instances were always mapped to VN classes while in the OIP one right it was possible during both training and test to map instances as not belonging to any existing class For the latter no results are displayed for polysemous verbs since each verb can be mapped both to other and to at least one class Taxonomy of Chinese words used in developing MSRSeg a A Chinese sentence Slashes indicate word boundaries b An output of our word segmentation system Square brackets indicate word boundaries indicates a morpheme boundary Taxonomy of morphologically derived words MDWs in MSRSeg Words in the MSR gold test set Domainstyle distribution in the MSR test corpus Standards and corpora Evaluation measures for Chinese word segmenter Context model word classes class models and feature functions The perceptron training algorithm for Chinese word segmentation Overall architecture of MSRSeg Generative patterns of ONA where sij denotes the jth character of the ith word of ON Sun Zhou and Gao 2003 FT detection results on the MSR gold test set The All column shows the results of detecting all 10 types of factoids as described in Table 1 which amount to 6630 factoids as shown in Table 3 NWI results on HK and AS corpora NWI as postprocessor versus unified approach NW 11 identification results on PK test set NW 21 identification results on PK test set NWI results on PK and CTB corpora NWI as postprocessor versus unified approach Word internal structure and classtype transformation templates Comparison scores for PK open and CTB open Size of training data set and the adaptation results on AS open Comparison scores for HK open and AS open Methods of resolving OAs in word segmentation on the MSR test set Figure 7 a A Chinese OAS b Two sentences in the training set which contain t whose OASs have been replaced with the single tokens OAS Li et al 2003 Results of 70 highfrequency twocharacter CASs Voting indicates the accuracy of the baseline method that always chooses the more frequent case of a given CAS ME indicates the accuracy of the maximumentropy classifier VSM indicates the accuracy of the method of using VSM for disambiguation Comparison of performance of MSRSeg The versions that are trained using semisupervised iterative training with different initial training sets Rows 1 to 8 versus the version that is trained on annotated corpus of 20 million words Row 9 Precision of organization name recognition on the MSR test set using Viterbi iterative training initialized by four seed sets with different sizes Precision of location name recognition on the MSR test set using Viterbi iterative training initialized by four seed sets with different sizes Precision of person name recognition on the MSR test set using Viterbi iterative training initialized by four seed sets with different sizes MSRSeg system results for the MSR test set Comparisons against other segmenters In Column 1 SXX indicates participating sites in the 1st SIGHAN International Chinese Word Segmentation Bakeoff and CRFs indicates the word segmenter reported in Peng et al 2004 In Columns 2 to 5 entries contain the Fmeasure of each segmenter on different open runs with the best performance in bold Column SiteAvg is the average Fmeasure over the data sets on which a segmenter reported results of open runs where a bolded entry indicates the segmenter outperforms MSRSeg Column OurAvg is the average Fmeasure of MSRSeg over the same data sets where a bolded entry indicates that MSRSeg outperforms the other segmenter Crosssystem comparison results Results for the acquisition of subcategori sation frames Clustering evaluation for the experiment with Named Entities Clustering evaluation for the experiment without Named Entities Entity detection and tracking system flow The procedure of TBL entity track ingcoreference model EDT and mention detection results Statistics of the ACE corpus Examples of transformation rules of Templates for feedback Examples of zero anaphora Examples of zero anaphora a A Chinese sentence Slashes indicate word boundaries b An output of our word segmentation system Square brackets indicate word boundaries indicates a morpheme boundary Class models system results Comparison results Subtypes of the ArgM modifier tag Split constituents In this case a single semantic role label points to multiple nodes in the original treebank tree Interannotator agreement Confusion matrix among subtypes of ArgM defined in Table 1 Entries are fraction of all ArgM labels Entries are a fraction of all ArgM labels true zeros are omitted while other entries are rounded to zero Confusion matrix for argument labels with ArgM labels collapsed into one category Entries are a fraction of total annotations true zeros are omitted while other entries are rounded to zero Comparison of frames Most frequent semantic roles for each syntactic position Most frequent syntactic positions for each semantic role Semantic roles of verbs subjects for the verb classes of Merlo and Stevenson 2001 cont Semantic roles for different frame sets of kick In this example the path from the predicate ate to the argument NP He can be represented as VBjVPjSNP with j indicating upward movement in the parse tree and downward movement Backoff lattice with more specific distributions towards the top Accuracy of semanticrole prediction in percentages for known boundaries the system is given the constituents to classify Accuracy of semanticrole prediction in percentages for unknown boundaries the system must identify the correct constituents as arguments and give them the correct roles Common values in percentages for parse tree path in PropBank data using goldstandard parses Accuracy of semanticrole prediction for unknown boundaries the system must identify the correct constituents as arguments and give them the correct roles Summary of results for unknownboundary condition The directional matching relationships between a hypothesis h an entailment rule r and a text t in the Contextual Preferences framework Recall R Precision P and Mean Average Pre cision MAP when also using rules for matching Recall R Precision P and Mean Average Pre cision MAP when only matching template hypotheses directly RecallPrecision curves for ranking using a only the prior baseline b allCP c allCPpr MAP under the 50 rules All setup when adding component match scores to Precision P or prior only MAP baselines and when ranking with allCP or allCPpr methods but ignoring that component scores Unigram bigram and trigram counts of the ligature corpus Unigram bigram and trigram counts of the word corpus Results changing beam width k of the tree Sample analysis of an English sentence Input Do we have to reserve rooms Resolution of ambiguity on the Verbmobil corpus Candidates for equivalence classes Training and test with hierarchical lexicon Inverse restructuring analyze and annotation all require morphosyntactic analysis of the transformed sentences Disambiguation of conventional dictionaries Learn phrases analyze and annotation require morphosyntactic analysis of the transformed sentences Training with scarce resources Restructuring learn phrases and annotation all require morphosyntactic analysis of the transformed sentences Statistics of corpora for training Verbmobil and Nespole Singletons are types occurring only once in training The official vocabularies in Verbmobil Statistics for the test sets for German to English translation Verbmobil Eval2000 Test and Develop and Nespole Conventional dictionary used to complement the training corpus Impact of corpus size measured in number of running words in the corpus on vocabulary size measured in number of different fullform words found in the corpus for the German part of the Verbmobil corpus Results for hierarchical lexicon model Nespole Restructuring entails treatment of question inversion and separated verb prefixes as well as merging of phrases in both languages The same conventional dictionary was used as in the experiments the Verbmobil The language model was trained on a combination of the English parts of the Nespole corpus and the Verbmobil corpus Examples of the effect of the hierarchical lexicon The performance on the set of unknown Experimental result of total unknown The performance on the set of unknown Example of a socalled semiformal text where one can see that here more time points are available and that those can be complemen tary to the time points to be extracted from formal texts So already at this level a unification or merging of extracted time points is necessary The value of the penalized loss based on the number of iterations DPLVMs vs CRFs on the MSR data Details of the corpora WT represents word types CT represents character types SC represents simplied Chinese TC represents traditional Chinese Error analysis on the latent variable seg menter The errors are grouped into four types over generalization errors on named entities errors on idioms and errors from datainconsistency Selfbootstrapping algorithm Numbers of relations on the ACE RDC 2004 break down by relation types and subtypes Stratefied Sampling for initial seeds The initial performance of applying various sampling strategies to selecting the initial The highest performance of applying various sampling strategies in selecting the initial seed set on the ACE RDC 2004 corpus Bootstrapping time for different p values Performance for different p values Comparison of two augmentation strategies over different sampling strategies in selecting the initial seed set Comparison of semisupervised relation classification systems on the ACE RDC 2003 corpus A pruned phrase tokenization lattice Edges are tokenizations of phrases eg e5 represents tokenizing question into a word and e7 represents tokenizing doubt him into a partial word doubt followed by a word him The pseudo code of Algorithm 1 The Riv over the bakeoff2 data The Roov over the bakeoff2 data The Fscore over the bakeoff2 data A focused entailment graph For clarity edges that can be inferred by transitivity are omitted The single strongly connected component is surrounded by a dashed line Positive and negative examples for entailment in the training set The direction of entailment is from the left template to the right template The similarity score features used to represent pairs of templates The columns specify the corpus over which the similarity score was computed the template representation the similarity measure employed and the feature representation as described in Section 41 Scenarios in which we added hard constraints to the ILP Results when tuning for performance over the development set Results when the development set is not used to estimate and K Results with prior estimated on the development set that is 01 which is equivalent to 23 Recallprecision curve comparing ILPGlobal with GreedyGlobal and ILPLocal Results per concept for the ILPGlobal Results of all distributional similarity measures when tuning K over the development set We encode the description of the measures presented in Table 2 in the following manner h healthcare corpus R RCV1 corpus b binary templates u unary templates L Lin similarity measure B BInc similarity measure pCt pair of CUI tuples representation pC pair of CUIs representation Ct CUI tuple representation C CUI representation Lin Pantel similarity lists learned by Lin and Pantel Comparing disagreements between ILPGlobal and ILPLocal against the goldstandard graphs A comparison between ILPGlobal and ILPLocal for two fragments of the testset concept seizure A comparison between ILPGlobal and ILPlocal for two fragments of the testset concept diarrhea Comparing disagreements between ILPGlobal and GreedyGlobal against the goldstandard graphs A comparison between ILPGlobal and GreedyGlobal Parts A1A3 depict the incremental progress of Greedy Global for a fragment of the headache graph Part B depicts the corresponding fragment in ILPGlobal Nodes surrounded by a bold oval shape are strongly connected components Distribution of probabilities given by the classier over all node pairs of the testset graphs Error analysis for false positives and false negatives A scenario where ILPGlobal makes a mistake but ILPLocal is correct The set of new features The last two columns denote the number and percentage of examples for which the value of the feature is nonzero in examples generated from the 23 goldstandard graphs Macroaverage recall precision and F1 on the development set and test set using the parameters that maximize F1 of the learned edges over the development set Results of feature analysis The second column denotes the proportion of manually annotated examples for which the feature value is nonzero A detailed explanation of the other columns is provided in the body of the article A hierarchical summary of propositions involving nausea as an argument such as headache is related to nausea acupuncture helps with nausea and Lorazepam treats nausea Example distributions of German verbs Data similarity measures kmeans experiment baseline and upper bound Comparing distributions on D1 and D2 Comparing similarity measures on D1 and D2 Comparing clustering initializations on D2 Comparing clustering initializations on D1 Comparing feature descriptions Comparing selectional preference frame definitions Comparing selectional preference slot definitions Varying the number of clusters evaluation Randadj Largescale clustering on D1 Largescale clustering on D3 with nnandnadnsdass Largescale clustering on D2 LMR Tagging Learning curves on the development dataset of the Beijing Univ corpus Learning curves on the development dataset of the HK City Univ corpus Official Bakeoff Outcome Fscore on development data Sample Embedded Answer Summariser and VPA Architecture Positional sentence weight for varying Precision by Named Entity Class Recall by Named Entity Class Average Precision and Recall Utility Score Comparison Summaries Recall and Precision Relation Feature Spaces of the Example Sentence to stop the merger of an estimated Performance of seven relation feature spaces over the 5 ACE major types using parse tree information only Performance comparison the numbers in parentheses report the performance over the 24 ACE subtypes while the numbers outside paren theses is for the 5 ACE major types Error Distribution Distribution of antecedent NP types in the otheranaphora data set Overview of the results for all baselines for otheranaphora Descriptive statistics for WordNet hypsyn relations for otheranaphora Patterns and instantiations for otheranaphora Descriptive statistics for Web scores and BNC scores for otheranaphora Properties of the variations for the corpusbased algorithms for otheranaphora Web results for otheranaphora BNC results for otheranaphora Overview of the results for the best algorithms for otheranaphora Occurrences of error types for the best otheranaphora algorithm algoWebv4 Distribution of antecedent NP types for definite NP anaphora Overview of the results for all baselines for coreference Descriptive statistics for WordNet hypsyn relations on the coreference data set Overview of the results for all WordNet algorithms for coreference Overview of the results for all Web algorithms for coreference Overview of the results for all BNC algorithms for coreference Occurrences of error types for the best coreference algorithm algoWebv4n Word segmentation on NIST data sets Example of 1ton word alignments be tween English words and Chinese characters Word segmentation on IWSLT data sets Example of a word lattice BS on IWSLT 2007 task BS on IWSLT 2006 task Corpus statistics for Chinese Zh character segmentation and English En BS on NIST task Scalability of BS on NIST task Vocabulary size of NIST task 40K Scaleup to 160K on IWSLT data sets Vocabulary size of IWSLT task 40K The search graph on development set of IWSLT task BS on IWSLT data sets using MTTK Word alignment based translation model PJ AE IBM Model 4 Example of word alignment Example of chunkbased alignment Chunkbased translation model The words in bold are head words Basic Travel Expression Corpus Experimental results for JapaneseEnglish Examples of viterbi chunking and chunk alignment for EnglishtoJapanese translation model Chunks are bracketed and the words with to the left are head words An example of English Chinese and French terms consisting of the same morphemes Example of first and second order features using a predefined ngram size of 2 Example of a term construction rule as a branch in a decision tree FScore of the RF and SVM GIZA and Levenshtein distancebased classifier on the first order dataset Best observed performance of RF SVM and GIZA and Levenshtein Distance FScore of the RF and SVM GIZA and Levenshtein distancebased classifier on the second order dataset 1 OBI vs BI where the lost of F 1 such as SCB is caused by incorrect English segments that will be discussed in the section 4 Baseline vs Submitted Results An example of annotation projection for relation detection of a bitext in English and Korean Numbers of projected instances Experimental Results Rules and patterns for the four syntacticosemantic structures Regular expression notations matches the preceding element zero or more times matches the preceding element one or more times indicates that the preceding element is optional indicates or Abbreviations Ec m coarsegrained entity type of mention m Ld labels in dependency path between the headword of two mentions We use square brackets and to denote mention boundaries The in the Formulaic row denotes the occurrence of a lexical in text Additional RE features Microaveraged across the 5 folds RE results using predicted mentions Microaveraged across the 5 folds RE results using gold mentions Recall and precision of the patterns Improvement in predicted mention RE Improvement in gold mention RE Comparison of our system with the bestreported systems on MUC6 and MUC7 A second example of disagreement in segmentation guidelines Examples of disagreement in segmentation guidelines Analysis of results of segmentation on LDC training and test data for all CWS schemes BLEU scores for CWS schemes Correlation between Fscore and BLEU Feature blending of translation models Feature interpolation of translation models AICTCLAS Bdicthybrid CdictPKULDC DdictCITYU ECRFAS Common grammatical relations of Minipar involving nouns The top 20 most similar words for country and their ranks in the similarity list of LIN followed by the next four words in the similarity list that were judged as entailing at least in one direction The top 10 ranked features for country produced by MI the weighting function employed in the LIN method Lexical entailment precision values for topn similar words by the Bootstrapped LIN and the original LIN method Percentage of correct entailments within the top 40 candidate pairs of each of the methods LIN and Bootstrapped LIN denoted as LINB in the gure when using varying numbers of topranked features in the feature vector The value of All corresponds to the full size of vectors and is typically in the range of 300400 features Comparative precision values for the top 20 similarity lists of the three selected similarity measures with MI and Bootstrapped feature weighting for each Top 30 features of town by bootstrapped weighting based on LIN WJ and COS as initial similarities The three sets of words are almost identical with relatively minor ranking differences Top 10 features of country by the Bootstrapped feature weighting LIN MI weighting The top 10 common features for countrystate and countryparty along with their corresponding ranks in each of the two feature vectors The features are sorted by the sum of their feature weights with both words Top 20 most similar words for country and their ranks in the similarity list by the Bootstrapped LIN measure Bootstrapped weighting top 10 common features for countrystate and countryparty along with their corresponding ranks in the two sorted feature vectors Comparison between the acfrratio for MI and Bootstrapped LIN methods when using varying numbers of common topranked features in the words feature vectors The comparative error rates of the pseudodisambiguation task for the three examined similarity measures with and without applying the bootstrapped weighting for each of them Statistical results Evaluation results Wrong assignment due to missing sense from the Hound of the Baskervilles Ch 14 Sample Minipar parse and extracted gram matical function features Experiment 1 Results for label unknown sense WSD confidence level approach confi dence threshold std dev Outlier detection by comparing distances between nearest neighbors Experiment 2 Results for label unknown sense NNbased outlier detection 10 stan dard deviation Experiment 2 Results by training set size 10 Experiment 3 Results for label unknown sense NNbased outlier detection 10 stan dard deviation Extending training sets an example Acceptance radius of an outlier within the training set left and a more normal training set object right Experiment 3 Results by training set size 10 Experiments 2 and 3 Results by the num ber of senses of a lemma condition All 10 Example entries for the Transfer of a Message levels 1 and 2 classes Clusters for transitive unaccusative and ditransitive Outcome of clustering procedure Average Precision Recall and F1 at dif ferent top K rule cutoff points Distribution of reasons for false negatives missed argument mentions by BInc at K20 Distribution of reasons for false positives incorrect argument extractions by BInc at K20 Rule type distribution of a sample of 200 rules that extracted incorrect mentions The corre sponding numbers of incorrect mentions extracted by the sampled rules is shown in parentheses The Maytag interface Translation extraction from comparable corpora using crosslingual WSI and WSD Entries from the EnglishSlovene sense cluster inventory Disambiguation results Comparison of different configurations Results of the experiment GC examples Devtest Set Statistics by Language Fmeasure Breakdown by Mention Type NAMe NOMinal PREmodifier and PROnoun Chinese data does not have the PRE type Impact of Syntactic Features on English Sys tem After Taking out Distance Features Numbers are Fmeasures An example where syntactic features help to link the PRO mention hm with its antecedent the NAM Distribution of Pronoun Mentions and Fre quency of ccommand Features Summary Results on the 2004 ACE Evaluation Data A Portion of the Syntactic Tree An example of NE and nonNE Possibility combination of neighboring tokens within the corpus for PER A NE detection window 2 All possible NEs identified in a test article c Unification When the tokens of two NEs are NEs after agentsbased modification Results of MET2 under different configurations Priority Order for Second Person ADs Priority Order for Third Person ADs Priority Order for First Person ADs Basic Features for CRFbased Segmenter Performance of our system in the compe tition Effectiveness of postprocessing rules Overview of the WEBRE algorithm Illustrated with examples sampled from experiment results The tables and rec tangles with a database sign show knowledge sources shaded rectangles show the 2 phases and the dotted shapes show the sys tem output a set of Type A relations and a set of Type B relations The orange arrows denote resources used in phase 1 and the green arrows show the resources used in phase 2 Pairwise precisionrecallF1 of WEBRE and SNE Overall results by Vieira and Poesio Discoursenew prediction results by Bean and Riloff Evaluation of the three anaphoric resolvers discussed by Ng and Cardie Results of Uryupinas discourse new clas sifier Results of Uryupinas uniqueness classifier Using an oracle Evaluation of the GUITAR system without DN detection off raw text Evaluation of the GUITAR system without DN detection over a handannotated treebank Relation types and subtypes in the ACE training data Contribution of different features over 43 relation subtypes in the test data Distribution of errors Comparison of our system with other bestreported systems on the ACE corpus Performance of different relation types and major subtypes in the test data Distribution of relations over words and other mentions in between in the training data Results of the baseline model best guess Word distribution in the extended Cilin Results of the baseline model best 5 guesses Results of combining the charactercategory association and rulebased models best guess Results of the charactercategory association model best guess Results of the charactercategory association model best 5 guesses Results of the rulebased model best guess Results of the corpusbased model on words with different frequency Results of the corpusbased model Parameter settings of the corpusbased model Results of the combined model for classify ing unknown words into major and medium catego ries best guess Reordering for the German verbgroup DP algorithm for statistical machine translation Coverage set hypothesis extensions for the IBM reordering Multireference word error rate mWER Example Translations for the Verbmobil task Examples of nonphonetic translations Dissimilarity of temporal distributions of WTO in English and Chinese corpora Framework overview Evidence cardinality in the corpora Network of relations Edges indicate that the relations have a nonempty support inter section and edge labels show the size of the inter section Relation clusters and a few individual relations Edge labels show the size of the inter section MRR with decreasing comparability Example translations from the different methods Boldface indicates correct translations Evaluation results of the methods Example of similar document pairs Automatically generated training set examples Impact of scaling techinques ILP ILPscale microaverage F1 and AUC for the algorithms Precisionrecall curve for the algorithms A pair of comparable nonparallel documents A pair of comparable sentences A Parallel Fragment Extraction System Translated fragments according to the lexicon Our approach for detecting parallel fragments The lower part of the figure shows the source and target sentence together with their alignment Above are displayed the initial signal and the filtered signal The circles indicate which fragments of the target sentence are selected by the procedure Sizes of the extracted datasets Sizes of our comparable corpora SMT performance results Incremental evaluations by incrementally adding new features word features and high dimensional edge features new word detection and ADF training replacing SGD training with ADF training Number of passes is decided by empirical convergence of the training methods Fscore curves on the MSR CU and PKU datasets ADF learning vs SGD and LBFGS training methods Comparing our method with the stateoftheart CWS systems Test corpora details Evaluation closed results on all data sets Evaluation open results on all test sets Comparison our closed results with the top three in all test sets Architecture of the translation approach based on Bayes decision rule Some training events for the English word which The symbol is the placeholder of the English word which in the English context In the German part the placeholder corresponds to the word aligned to which in the first example the German word die the word das in the second and the word was in the third The considered English and German contexts are separated by the double bar p The last number in the rightmost position is the number of occurrences of the event in the whole corpus Meaning of different feature categories where s represents a specific target word and t repre sents a specific source word The 10 most important features and their respective category and values for the English word which f Number of features used according to different cutoff threshold In the second column of the table are shown the number of features used when only the English context is considered The third column correspond to English German and WordClasses contexts Corpus characteristics for translation task Training and Test perplexities us ing different contextual information and different thresholds The reference perplexities obtained with the basic translation model 5 are TrainPP 1038 and TestPP 1322 Corpus characteristics for perplexity quality experiments Preliminary translation results for the Verbmobil Test147 for different contextual infor mation and different thresholds using the top10 translations The baseline translation results for model 4 are WER5480 and PER4307 Four examples showing the translation obtained with the Model 4 and the ME model for a given German source sentence Monolingual and Crosslingual Baseline Slot Filling Pipelines Baseline Pipeline Results Distribution of Spurious Errors Validation Features for Crosslingual Slot Filling Using Basic Features to Filter Answers Fact vs Statistical CrossDoc Features The count of the types of anaphora per corpus Bibliome system scores at Bacteria Biotope Task in BioNLP shared tasks 2011 Architecture of the translation approach based on Bayes decision rule Candidates for equivalence classes Corpus statistics Verbmobil training Singletons are types occurring only once in train ing Statistics of the Verbmobil test corpus for GermantoEnglish translation Unknowns are word forms not contained in the training corpus Effect of the introduction of equivalence classes For the baseline we used the original in flected word forms Examples for the effect of the combined lexica Effect of twolevel lexicon combination For the baseline we used the conventional onelevel full form lexicon Examples for the effect of equivalence classes resulting from dropping morphosyntactic tags not relevant for translation First the translation using the original representation then the new representation its reduced form and the resulting translation The incompleteness of Freebase are must have attributes for a person Plate diagram of our model False negative matches on the Riedel Riedel et al 2010 and KBP dataset Surdeanu et al 2012 All numbers are on bag pairs of entities level BD are the numbers before downsampling the negative set to 10 and 5 in Riedel and KBP dataset respectively Performance on the KBP dataset The figures on the left middle and right show MIML Hoffmann and Mintz compared to the same MIMLSemi curve respectively MIMLSemi is shown in red curves lighter curves in black and white while other algorithms are shown in black curves darker curves in black and white Dependency parse tree for the sentence in the ACE corpus Toujan Faisal 54 said she was informed of the refusal by an Interior Min istry committee overseeing election preparations Undersampled system for the task of rela tion detection The proportion of positive examples in the training and test corpus is 500 and 206 respectively System for the task of relation classifica tion The two classes are INR and COG and we evaluate using accuracy Acc The proportion of INR relations in training and test set is 497 and 4963 respectively Selfbootstrapping algorithm Clusteringbased stratified seed sampling Performance of various clusteringbased seed sampling strategies on the heldout test data with the optimal cluster number for each clustering algorithm Performance in F1score over different cluster numbers with intrastratum sampling on the develop ment data Step 4 Step 2 Step 5 Step 3 50document corpora averages LO Dice configuration scores Best LO and LL configurations scores Best MAP in Experiment 1 LO sentence configuration scores LO cosine sentence configuration scores Average rank of correct translation according to average source term frequency LO cosine sentence configuration scores Feature set for our pronoun resolution systemed feature is only for the singlecandidate model while ed feature is only for the twincandidate mode The performance of different resolution systems Results of different feature groups under the TC model for Npron resolution Relationship types and their argument type con straints Count of relationships in 77 gold standard documents The relationship extraction system Feature sets used for learning relationships The size of a set is the number of features in that set Variation in performance by feature set Features sets are abbreviated as in Table 3 For the first seven columns features were added cumulatively to each other The next two columns allgen and notok are as de scribed in Table 3 The final two columns give inter annotator agreement and corrected inter annotator agreement for comparison Variation in performance by number of sentence boundaries n and by training corpus size Mechanical evaluation of translation JapanesetoEnglish Display of NICT ATR SpeechtoSpeech Translation System Evaluation of speech recognition Human Evaluation of translation A fragment of an entailment graph a its SCC graph b and its reduced graph c Nodes are predicates with typed variables see Section 5 which are omitted in b and c for compactness Three types of transitivity constraint violations Runtime in seconds for various values Predictive power of admissible and almost admissible heuristic functions Training corpus statistics without punctuation marks Test corpora statistics Average search time s per sentence Search Success Rate 1 million hypothe ses Search errors Effect of observation pruning on the translation quality average over all test sets A E Success Rate for 12 and 14word sentences Translation quality Proposed method data flow Feature set used in the Stage 2 classifier and their number for the causal relation experiments Precision of acquired relations causality L and S denote lenient and strict evaluation Precision of acquired relations prevention L and S denote lenient and strict evaluation Precision of acquired relations material L and S denote lenient and strict evaluation Frequencies of patterns in the evaluation data causation Contribution of feature sets material Contribution of feature sets prevention Contribution of feature sets causality Example of the context of in Eat fruits and the context of in Play basketball An example for the BMES representa tion The sentence is I love Bei jing Tiananmen square which consists of 4 Chi nese words I love Beijing and Tiananmen square Algorithm description Comparison of fscores when changing the size of labeled data 110 14 12 and all labeled data The size of unlabeled data is xed as 5 million characters Details of the PKU data Details of the unlabeled data Comparison of our approach with the stateofart systems Comparison of our approach with using only the Gigaword corpus A Motivating Example Processed Data Statistics Sample Is Values Human Assessment of Errors Slot Value Translation Assessment from Ran dom Sample of 1000 Performance of Unsupervised Name Mining Example of Learned Name Pairs with Gloss Translations in Parentheses Name Pairs Mined Using Previous Methods Average word accuracy for transduced sentences Fraction of the sentences that were transduced Sizes of the automata Time consumption of transduction Ten relation instances extracted by our system that did not appear in Freebase The 23 largest Freebase relations we use with their size and an instance of each relation Dependency parse with dependency path from Edwin Hubble to Marshfield highlighted in boldface Features for Astronomer Edwin Hubble was born in Marshfield Missouri Examples of highweight features for several relations Key SYN syntactic feature LEX lexical feature x reversed NE named entity tag of entity Automatic evaluation with 50 of Freebase relation data held out and 50 used in training on the 102 largest relations we use Precision for three different feature sets lexical features syntactic features and both is reported at recall levels from 10 to 100000 At the 100000 recall level we classify most of the instances into three relations 60 as locationcontains 13 as personplaceofbirth and 10 as personnationality Estimated precision on humanevaluation experiments of the highestranked 100 and 1000 results per relation using stratified samples Average gives the mean precision of the 10 relations Key Syn syntactic features only Lex lexical features only We use stratified samples because of the overabundance of locationcontains instances among our highconfidence results Linking FrameNet frames and VerbNet classes Results of the mapping algorithm Mapping algorithm refining step F1 and accuracy of the argument classifiers and the overall multiclassifier for FrameNet semantic roles Semantic Role learning curve Examples of pseudo features Sources of the training data Number of candidates for each target language Examples of the top3 candidates in the transliteration of English Chinese Examples of the top3 candidates in the transliteration of EnglishKorean Number of evaluated English Name MRRs of the phonetic transliteration Size of the test data MRRs for the phonetic transliteration 2 MRRs of the phonetic transliteration Dependency tree for the sentence PROT1 contains a sequence motif binds to PROT2 Comparison with other PPI extraction systems in the AIMed corpus Comparison of contributions of different features to relation detection across multiple domains Comparison of performance across the five PPI corpora MEDLDA Mixed Membership MEDLDA Relation types for ACE 05 corpus Overall performance of the 3 systems Multiclass Classification Results with PlusCOMP for SVM LLDA and MEDLDA for the six ACE 05 categories and NOREL LLDA Fmeausres for 3 feature conditions SVM Fmeausres for 3 feature conditions MEDLDA Fmeausres for 3 feature conditions Fmeasures for every kernel in Khayyamian et al 2009 and MEDLDA Features used by paraphrase classifier Illustration of features f812 Bidirectional checking of entailment relation of p1 p2 and p2 p1 p1 is reduces bone mass in s1 and p2 is decreases the quantity of bone in s2 p1 and p2 are exchanged between s1 and s2 to generate corresponding paraphrased sentences s01 and s02 p1 p2 p2 p1 is verified if s1 s01 s2 s02 holds In this case both of them hold English is used for ease of explanation Number of extracted paraphrases Examples of correct and incorrect paraphrases extracted by our supervised method with their rank Precision curves of paraphrase extraction Top7 Chinese longform candidates for the En glish acronym TAA according to the LH score The performances of the transliteration models and their comparison on EMatch The BLEU score of selftrained h4 translitera tion models under four selection strategies nt n15 stands for the nth iteration The BLEU score of selftrained cascaded trans lation model under five initial training sets 25 noun lexicographer files in W ORD N ET Example nouns and their supersenses 2 billion word corpus statistics Grammatical relations from S EXTANT Handcoded rules for supersense guessing Breakdown of results by supersense Summary of supersense tagging accuracies The best two performing systems of each type according to finegrained recall in Senseval2 and 3 Polysemous word types in the Senseval2 and 3 English allwords tasks test documents with no data in SemCor 0 columns or with very little data 1 and 5 occurrences Note that there are no annotations for adverbs in the Senseval3 documents Words excluding multiwords in WordNet 171 and the BNC without any data in SemCor Most frequent sense analysis for Senseval2 and 3 polysemous lemmas occurring more than once in a document adverb data is only from Senseval2 Most frequent sense analysis for all polysemous lemmas in the Senseval2 and 3 test data broken down by their frequencies of occurrence in SemCor adverb data is only from Senseval2 The prevalence ranking process for the noun star Example dss and sss scores for star and its neighbors Grammatical contexts used for acquiring the BNC thesaurus Thesaurus coverage of polysemous words excluding multiwords in WordNet 16 Evaluation on SemCor polysemous words only Simplified prevalence score evaluation on SemCor polysemous words only Results of the error analysis for the sample of 80 words SemCor results for Nouns using jcn Evaluating predominant sense information for polysemous nouns on the Senseval2 allwords task data Senseval2 results polysemous nouns only broken down by their frequencies of occurrence in SemCor TYPE precision on finding the predominant sense for the Senseval2 English allwords test data for nouns having a frequency less than or equal to various thresholds WSD precision on the Senseval2 English allwords test data for nouns having a frequency less than or equal to various thresholds Most frequent SFC labels for all senses of polysemous words in WordNet by part of speech Distribution of domain labels of predominant senses for 38 polysemous nouns ranked using the SPORTS and FINANCE corpora Distribution of domain labels of predominant senses for 20 polysemous verbs ranked using the SPORTS and FINANCE corpora WSD using predominant senses training and testing on all domain combinations handclassified corpora WSD using predominant senses training and testing on all domain combinations automatically classified corpora Example of a word with internal structure Example of telescopic compound a and sepa rable word b Structure of the outofvocabulary word English People Two words that differ only in one character but have different internal structures The character people is part of a personal name in tree a but is a suffix in b An example word which has very complex structures The actual output of our parser trained with a fully annotated treebank Proposed output for the new Chinese word seg mentation paradigm Difference between our output a of parsing the word olive oil and the output b of Luo 2003 In c we have a true flat word namely the loca tion name Los Angeles Example word structure annotation We add an f to the POS tags of words with no further structures Example of parser error Tree a is correct and b is the wrong result by our parser Labeled precision and recall for the three types of labels The line labeled Flat is for unlabeled met rics of flat words which is effectively the ordinary word segmentation accuracy List of keywords used in WordNet search for generating WN CLASS features Distribution of SCs in the ACE corpus SC classification accuracies of different methods for the ACE training set and test set Results for feature ablation experiments Accuracies of singlefeature classifiers Resolution accuracies for the ACE test set Coreference results obtained via the MUC scoring program for the ACE test set 1 Statistics of relation types and subtypes in the training data of the ACE RDC 2003 corpus Note According to frequency all the subtypes are divided into three bins large middle small with 400 as the lower threshold for the large bin and 200 as the upper threshold for the small bin Learning curve of the hierarchical strategy and its comparison with the flat strategy for some major relation subtypes Note FS for the flat strategy and HS for the hierarchical strategy Comparison of the hierarchical and flat learning strategies on the relation subtypes of differ ent training data sizes Notes the figures in the parentheses indicate the cosine similarities between the weight vectors of the linear discriminative functions learned using the two strategies Comparison of our system with other bestreported systems The combined sequence and parse tree representation of the relation instance leader of a minority government The nonessential nodes for a and for minority are removed based on the algorithm from Qian et al 2008 Examples of similar syntactic structures across different relation types The head words of the first and the second arguments are shown in italic and bold respectively Examples of unigram and bigram features extracted from Figure 1 Comparison of different methods on ACE 2004 data set P R and F stand for precision recall and F1 respectively The average performance of TLcomb with different T k 104 and 1 Performance of TLcomb and TLauto as H changes Performance of TLNE BL and BLA as the number of seed instances S of the target type increases H 500 T was set to 104 and 102 Average F1 using different hypothesized typespecific features General architecture of LINGUA Success rate of anaphora resolution Complexity of the evaluation data Summary of LINGUA performance An example of words and their bit string representations obtained in this paper Words in bold are head words that appeared in Table 1 Lexical features for relation extraction Cluster features ordered by importance Performance comparison on the ACE 2004 data over the 7 relation types Performance 12 of the baseline and using different cluster features with PC4 over the 7 types Performance of each individual relation type based on 5fold crossvalidation Performance over the 7 relation types with different sizes of training data Prefix10 uses the single prefix length 10 to generate word clusters as used by Chan and Roth 2010 The F1Measure value is shown for every kernel on each ACE2005 main relation type For every relation type the best result is shown in bold font Example of DIRT algorithm output Most confident paraphrases of X put emphasis on Y Example of inference rules needed in RTE Lexical variations creating new rules based on DIRT rule X face threat of Y X at risk of Y Dependency structure of text Tree skeleton in bold Precision on full RTE data Coverageprecision with various rule collections Precision on the covered RTE data Error analysis Frequency of Relation SubTypes in the ACE training and devtest corpus The Performance of SVM and LP algorithm with different sizes of labeled data for relation detection on relation subtypes The LP algorithm is run with two similarity measures cosine similarity and JS divergence The performance of SVM and LP algorithm with different sizes of labeled data for relation detection and classification on relation subtypes The LP algorithm is run with two similarity measures cosine similarity and JS divergence Comparison of the performance of previous methods on ACE RDC task Comparison of the performance of the bootstrapped SVM method from Zhang 2004 and LP method with 100 seed labeled examples for relation type classification task Growing Algorithm for Language Model Pruning Language Model Pruning Algorithm Calculation of Importance of Bigrams Stepbystep Growing Algorithm Comparison of Number of Bigrams at FMeasure 9633 Performance Comparison of Different Pruning Methods Performance Comparison of Combined Model and KLD Model Correlation between Perplexity Perplexity Comparison of Different Pruning Methods Overview of the method Evaluation results within sets Evaluation results for links Extracted NE pair instances and context Figure 3 High TFITF words in ComCom Numbers are TFITF score frequency in the collec tion TF frequency in the corpus TF and word Examples and number of them in Semcor for sense approach and for class approach BLC for WN16 using all or hyponym relations Average polysemy on SE2 and SE3 Results for nouns Learning curve of BLC20 on SE3 Learning curve of BLC20 on SE2 Results for verbs Learning curve of SuperSense on SE3 Learning curve of SuperSense on SE2 Architecture of Nameaware Machine Translation System Translation Performance Statistics and Name Distribution of Test Data Sets A u to m a tic M e tr ic s H u m a n E v a lu a tio n Figure 2 Scores based on Automatic Metrics and Human Evaluation Impact of Joint Bilingual Name Tagging on Word Alignment name tokensall tokens Figure 3 Word alignment gains according to the percentage of name words in each sentence feature templates accuracy using nonaveraged and averaged perceptron learning curves of the averaged and non averaged perceptron algorithms the influence of agenda size the accuracies over the second SIGHAN bakeoff data the accuracies over the first SIGHAN bake off data the influence of features F Fmeasure Feature numbers are from Table 1 Different representations of a relation instance in the example sentence provide bene five different tree kernel setups on the ACE 2003 five major types using the parse tree structure information only regardless of any entityrelated information Performance comparison on the ACE 2004 data over both 7 major types the numbers outside parentheses and 23 subtypes the num bers in parentheses Performance comparison on the ACE 20032003 data over both 5 major types the numbers outside parentheses and 24 subtypes the numbers in parentheses Error distribution of major types on both the 2003 and 2004 data for the compos ite kernel by polynomial expansion Nouns and verbs supersense labels and short description from the Wordnet documentation The noun box in Wordnet each line lists one synset the set of synonyms a definition an optional example sentence and the supersense label Statistics of the datasets The row Super senses lists the number of instances of supersense labels partitioned in the following two rows between verb and noun supersense labels The lowest four rows summarize average polysemy figures at the synset and supersense level for both nouns and verbs Summary of results for random and first sense baselines and supersense tagger is the standard error computed on the five trials results Summary of results of baseline and tagger on selected subsets of labels NER categories evaluated on Semcor upper section and 5 most frequent verb middle and noun bottom categories evaluated on Senseval Context Clustering with Spectralbased Clustering technique Frequency of Major Relation SubTypes in the ACE training and devtest corpus Performance of our proposed method Spectral based clustering compared with other unsupervised methods Hasegawa et al 2004s clustering method and Kmeans clustering Different Context Window Size Setting System architecture overview Procedure to mine key lexicons for each semantic type Some key lexicons and verbs for two semantic types Salience grading for candidate antecedents Procedure to find semantic types for antecedent candidates Statistics of anaphor and antecedent pairs Feature impact experiments FScore of Medstract and 100Medlines Comparisons among different strategies on Medstract Impacts of the mined semantic lexicons and the use of PubMed Context Clustering with Spectralbased Clustering technique Frequency of Major Relation SubTypes in the ACE training and devtest corpus Different Context Window Size Setting Comparison of the existing efforts on ACE RDC task Outline of word segmentation process Three different vocabulary sizes used in subword based tagging s1 contains all the characters s2 and s3 contains some common words Corpus statistics in Sighan Bakeoff 2005 Segmentation results of dictionarybased segmentation in closed test of Bakeoff 2005 A separates the results of unigram bigram and trigram Segmentation results by the pure subwordbased IOB tagging The separator divides the results by three lexicon sizes as illustrated in Table 3 The first is characterbased s1 while the other two are subwordbased with different lexicons s2s3 Riv and Roov varing as the confidence threshold t Effects of combination using the confidence measure Here we used 08 and confidence threshold t 07 The separator divides the results of s1 s2 and s3 Effects of using CRF The separator divides the results of s1 and s3 List of results in Sighan Bakeoff 2005 NPs in a sample from the Catalan training data left and the English translation right Evolution of A means relative to the length of the nbest sequence The MCPG algorithm Comparison of paraphrase generators Top the MOSES baseline middle and bold the truescore MCPG down the translator MCPG The use of truescore improves the MCPG per formances MCPG reaches MOSES performance level Verb classes see Section 31 their Levin class numbers and the number of experimental verbs in each see Section 32 Experimental Results C50 is supervised accuracy Base is on random clusters set Ling is manually selected subset Seed is seedverbselected set See text for further description Feature counts for Ling and Seed feature sets Relation extraction results on the JDPA Corpus test set broken down by document source Selected document statistics for three JDPA Corpus document sources Size of Seed Lexicons Performance on Bilingual Lexicon Extraction Seeds with the Highest Weight Translation Candidates for manic depression Examples of templates suggested by DIRT and TEASE as having an entailment relation in some direction with the input template X change Y The entailment direction arrows were judged manually and added for readability Rule evaluation examples and their judgment Average Precision P and Yield Y at the rule and template levels Examples for disagreement between the two judges Entity type constraints BasicRE gives the performance of our basic RE system on predicting finegrained relations obtained by performing 5fold cross validation on only the news wire corpus of ACE2004 Each sub sequent row Hier HierrelEntC Coref Wiki and Cluster gives the individual contribution from using each knowledge The bottom row ALL gives the performance improvements from adding HierrelEntCCorefWikiCluster indicates no change in score A general architecture for paraphrasing approaches leveraging the distributional similarity hypothesis Two different dependency tree paths a and b that are considered paraphrastic because the same words John and problem are used to ll the corresponding slots shown coindexed in both the paths The implied meaning of each dependency path is also shown Using Chinese translations as the distributional elements to extract a set of English paraphrastic patterns from a large English corpus A bootstrapping algorithm to extract phrasal paraphrase pairs from monolingual parallel corpora The merging algorithm a How the merging algorithm works for two simple parse trees to produce a shared forest Note that for clarity not all constituents are expanded fully Leaf nodes with two entries represent paraphrases b The word lattice generated by linearizing the forest in a A paraphrase generation lattice for the sentence He ate lunch at a cafe near Paris Alternate paths between various nodes represent phrasal replacements The probability values associated with each edge are not shown for the sake of clarity An example showing the generalization of the word lattice a into a slotted lattice b The word lattice is produced by aligning seven sentences Nodes having indegrees 1 occur in more than one sentence Nodes with thick incoming edges occur in all sentences Extracting consistent bilingual phrasal correspondences from the shown sentence pairs i1 j1 i2 j2 denotes the correspondence fi1 fj1 ei2 ej2 Not all extracted correspondences are shown An example of syntactically motivated paraphrastic patterns that can be extracted from the paraphrase corpus constructed by Cohn CallisonBurch and Lapata 2008 Test verbs and their monosemouspolysemic gold standard senses Connected components nearest neighbour NN clustering D is the KullbackLeibler distance Information Bottleneck IB iterative clustering D is the KullbackLeibler distance Clustering performance on the predominant senses with and without prepositions The last entry presents the per formance of random clustering with K 25 which yielded the best results among the three values K25 35 and 42 The fraction of verb pairs clustered together as a function of the number of shared senses results of the NN algo rithm Evaluation against the monosemous Pred and pol ysemous Multiple gold standards The figures in parentheses are results of evaluation on randomly polysemous data sig nificance of the actual figure Results were obtained with fine grained SCFs including prepositions The fraction of verb pairs clustered together as a function of the number of different senses between pair mem bers results of the NN algorithm Addition method Evaluation of the manual annotation improvement summarization ratio 30 Evaluation of the manual annotation improvement summarization ratio 15 Evaluation of the GUITAR improvement summarization ratio 30 Evaluation of the GUITAR improvement summarization ratio 15 Performance of our system versus a baseline The greedy binding problem a The correct binding b the greedy binding c the result The Lattice of the 8 Patterns Segmentation accuracy of different seg menters Results on three query categories MAP of different IR systems with differ ent segmenters The feature set for coreference resolution Nonrelational features describe a mention and in most cases take on a value of YES or NO Relational features describe the relationship between the two mentions and indicate whether they are COMPATIBLE INCOMPATIBLE or NOT APPLICABLE Statistics for the ACE 2005 corpus MUC CEAF and B3 coreference results using system mentions MUC CEAF and B3 coreference results using true mentions Two characteristic topics for the Y slot of acquire along with their topicbiased Lin sim ilarities scores Lint compared with the original Lin similarity for two rules The relevance of each topic to different arguments of acquire is illus trated by showing the top 5 words in the argument y vector vacquire for which the illustrated topic is the most likely one Contextsensitive similarity scores in bold for the Y slots of four rule applications The components of the score calculation are shown for the topics of Table 1 For each rule application the table shows a couple of the topicbiased scores Lint of the rule as in Table 1 along with the topic relevance for the given context ptdv w which weighs the topicbiased scores in the LinW T cal culation The contextinsensitive Lin score is shown for comparison MAP values on corresponding test set ob tained by each method Figures in parentheses in dicate optimal number of LDA topics Sizes of rule application test set for each learned ruleset MAP results for the two split Lin test sets How the IBM models model the translation process This is a hypothetical example and not taken from any actual training or decoding logs Runtimes for sentences of length 1080 The graph shows the average runtimes of 10 different sample sentences of the respective length with swap op erations restricted to a maximum swap segment size of 5 and a maximum swap distance of 2 A decoding trace using improvement caching and tiling ICT The search in the second and later iterations is limited to areas where a change has been applied marked in bold print note that the number of alignment checked goes down over time The higher number of alignments checked in the second iteration is due to the insertion of an additional word which increases the number of possible swap and insertion operations Decoding without ICT results in the same translation but requires 11 iterations and checks a total of 17701 alignments as opposed to 5 iterations with a total of 4464 alignments with caching Number of search iterations left and total number of alignments considered right during search in depen dence of input length The data is taken from the translation of the Chinese testset from the TIDES MT evaluation in June 2002 Translations were performed with a maximum swap distance of 2 and a maximum swap segment size of 5 BLEUscores for the Chinese test set de coding in dependence of maximum swap distance and maximum swap segment size Decoder performance on the June 2002 TIDES MT evluation test set with multiple searches from randomized starting points MSD2 MSSS5 sentence length Figure 6 Time consumption of the various change types in Parsing tree FDG Analysers output example Evaluation results from DSOWSJ PATTree Instantiation for Figure 1 In the extraction process the PATtree is Results for feature combination Results for value setting Results for initial ranking manner Figure 5 Results for webpage snippet number 73 Experiment on Multiple Feature Fusion To verify the effectiveness for multiple feature fusion the test on the feature combination for OOV term translation is implemented As shown in Table 1 the highest accuracy the percentage of the correct translations in all the extracted translations of 831367 can be ac OOV term translation examples Results for EnglishChinese CLIR com bining our OOV term translation model Results for 4fold sitewise crossvalidation us ing the DP corpus DP corpus comparison for OPUS features based on frequent vs domainrelevant verbs Results on the Bitter Lemons corpus Architecture of the translation approach based on Bayes decision rule Wordtoword alignment Example of a word alignment and of ex tracted alignment templates Bilingual training corpus recognition lex icon and translation lexicon PM punctuation mark Illustration of search in statistical trans lation Illustration of bottomtotop search Comparison of three statistical translation approaches test on text input 251 sentences 2197 words 430 punctuation marks Sentence error rates of endtoend evalua tion speech recognizer with WER25 corpus of 5069 and 4136 dialogue turns for translation Ger man to English and English to German respec tively Disambiguation examples using morphosyntactic analysis Accuracy of our system in each period M 10 Precision and recall for different values of Rank of correct translation for period Dec 01 Dec 15 and Dec 16 Dec 31 Cont rank is the context rank Trans Rank is the transliteration rank NA means the word cannot be transliterated insuff means the correct translation appears less than 10 times in the English part of the comparable corpus comm means the correct translation is a word ap pearing in the dictionary we used or is a stop word phrase means the correct translation contains multi ple English words Dataset Statistics Pronouns as Opinion Targets Results of AR for Opinion Targets Op Target Op Word Pair Extraction An example sequence representation The subgraph on the left represents a bigram feature The subgraph on the right represents a unigram feature that states the entity type of arg 2 An example dependency parse tree rep resentation The subgraph represents a dependency relation feature between arg 1 Palestinians and of Comparison among the three feature sub spaces and the effect of including larger features CTB 10fold CV word segmentation F measure for our word segmenter Comparison of word segmentation F measure for SIGHAN bakeoff3 tasks POS tagging accuracy using oneata time wordbased POS tagger POS tagging accuracy using oneata time characterbased POS tagger Summary table on the various methods investigated for POS tagging CTB 10fold CV word segmentation F measure using an allatonce approach CTB 10fold CV POS tagging accuracy using an allatonce approach Some of the words extracted from the small corpus Experiments on the thresholdprecision relationship of the small corpus Experiments on the word lengthprecision relationship of the small corpus Experiments on the thresholdpartial recall relationship of the small corpus Some words extracted from the large corpus Experiments on the thresholdprecision relationship of the large corpus Experiments on the thresholdpartial recall relationship of the large corpus Experiments on the word lengthprecision relationship of the large corpus with threshold nine Experiments on the word lengthprecision relationship of the large corpus with threshold three Numerictype compounds extracted Precision and partial recall of word lengths two to seven of the second experiment on IT and AV Precision and partial recall of word lengths two to four of the first experiment on IT and AV Semantic expansion example Note that the expanded queries that were generated in the first two retrieved texts listed under matched query do not contain the original query Examples for correct templates that were learned by TEASE for input templates Analysis of the number of relevant documents out of the top 10 and the total number of retrieved documents up to 100 for a sample of queries Translation example Architecture of the statistical translation approach based on Bayes decision rule Regular alignment example for the translation direction German to English For each German source word there is exactly one English target word on the alignment path Illustration of the transitions in the regular and in the inverted alignment model The regular alignment model left figure is used to generate the sentence from left to right the inverted alignment model right figure is used to generate the sentence from bottom to top DPbased algorithm for solving travelingsalesman problems due to Held and Karp The outermost loop is over the cardinality of subsets of already visited cities Illustration of the algorithm by Held and Karp for a traveling salesman problem with J 5 cities Not all permutations of cities have to be evaluated explicitly For a given subset of cities the order in which the cities have been visited can be ignored DPbased algorithm for statistical MT that consecutively processes subsets C of source sentence positions of increasing cardinality Word reordering for the translation direction German to English The reordering is restricted to the German verb group Order in which the German source positions are covered for the GermantoEnglish reordering example given in Figure 5 Word reordering for the translation direction English to German The reordering is restricted to the English verb group Order in which the English source positions are covered for the EnglishtoGerman reordering example given in Figure 7 Procedural description to compute the set Succ of successor hypotheses by which to extend a partial hypothesis S C j Illustration of the IBMstyle reordering constraint Number of processed arcs for the pseudotranslation task as a function of the input sentence length J yaxis is given in log scale The complexity for the four different reordering constraints MON GE EG and S3 is given The complexity of the S3 constraint is close to J4 Twolist implementation of a DPbased search algorithm for statistical MT Training and test conditions for the GermantoEnglish Verbmobil corpus number of words without punctuation Example translations for the translation direction German to English using three different reordering constraints MON GE and S3 Translation results for the translation direction English to German on the TEST331 test set The results are given in terms of computing time WER and PER for three different reordering constraints MON EG and S3 Demonstration of the combination of the two pruning thresholds tC 50 and tc 125 to speed up the search process for the two reordering constraints GE and S3 no 50 The translation performance is shown in terms of mWER on the TEST331 test set Training and test conditions for the Hansards task number of words without punctuation Example translations for the translation direction English to German using three different reordering constraints MON EG and S3 Example translations for the translation direction French to English using the S3 reordering constraint Outline of the segmentation process Scores for UPUC corpus Scores for MSRA corpus Scores for CityU corpus An example graph modeling relations between mentions Number of clustering decisions made ac cording to mention type rows anaphor columns antecedent and percentage of wrong decisions Results of different systems on the CoNLL12 English data sets Number of recall errors according to mention type rows anaphor columns antecedent Precision statistics for pronouns Rows are pronoun surfaces columns number of cluster ing decisions and percentage of wrong decisions for all and only anaphoric pronouns respectively Removal and reduction of constituents using dependencies Different setups for entityrelated se mantic tree EST Contribution of constituent dependen cies in respective mode inside parentheses and accumulative mode outside parentheses Improvements of different tree setups Combinatorial Search Problems in Decoding Average decoding time P r f ae for IBM Models Log score NIST scores 5fold crossvalidation results on training data Results obtained on the official test set of the 2011 DDI Extraction challenge LII filtering refers to the techniques proposed in Chowdhury and Lavelli 2012b for reducing skewness in RE data distribution stat sig in dicates that the improvement of Fscore due to usage of Stage 1 classifier is statistically significant verified using Approximate Randomization Procedure Noreen 1989 number of iterations 1000 confidence level 001 Examples of phrase meaningfulness Note that the comments are not presented to Turkers Examples given in the description of Task 2 Left An entailment graph For clarity edges that can be inferred by transitivity are omitted Right A hierarchical summary of propositions involving nausea as an argument such as headache is related to nausea acupuncture helps with nausea and Lorazepam treats nausea Results for all experiments Subgraph of Local1 output forheadache Subgraph of tunedLP output for headache Comparing disagreements between the best local and global algorithms against the gold standard Relation between number of classes and alternations Estimation of model parameters Estimation of Fc f v and Fv c Ten most frequent classes using equal distribution of verb frequencies Estimation of Fv c for the verb feed Ten most frequent classes using unequal distribution of verb frequencies Smoothed estimates Model accuracy using equal distribution of verb frequencies for the estimation of Pc Model accuracy using unequal distribution of verb frequencies for the estimation of Pc Model accuracy using unequal distribution of verb frequencies for the estimation of Pc Model accuracy using equal distribution of verb frequencies for the estimation of Pc Semantic preferences for verbs with the doubleobject frame Features for collocations Word sense disambiguation accuracy for NP1 V NP2 to NP3 frame Word sense disambiguation accuracy for NP1 V NP2 frame Word sense disambiguation accuracy for NP1 V NP2 NP3 frame Word sense disambiguation accuracy for NP1 V NP2 for NP3 frame Word sense disambiguation accuracy for NP1 V NP2 NP3 frame Word sense disambiguation accuracy for NP1 V NP2 frame Word sense disambiguation accuracy for NP1 V to NP2 NP3 frame Word sense disambiguation accuracy for NP1 V for NP2 NP3 frame Examples of context free and contextsensitive sub trees related with Figure 1b Note the bold node is the root for a subtree Different tree span categories with SPT dotted circle and an ex ample of the dynamic contextsensitive tree span solid circle Evaluation of contextsensitive convolution tree kernels using SPT on the major relation types of the ACE RDC 2003 inside the parentheses and 2004 outside the parentheses corpora Comparison of dynamic contextsensitive tree span with SPT using our contextsensitive convolution tree kernel on the major relation types of the ACE RDC 2003 inside the parentheses and 2004 outside the parentheses corpora 18 of positive instances in the ACE RDC 2003 test data belong to the predicatelinked category Comparison of difference systems on the ACE RDC 2004 corpus over both 7 types outside the parentheses and 23 subtypes inside the parentheses Comparison of difference systems on the performs the stateoftheart Collins and Duffys con ACE RDC 2003 corpus over both 5 types outside the volution tree kernel It also shows that featurebased parentheses and 24 subtypes inside the parentheses Possible Relations between ARG1 and ARG2 Evaluation on Structure Features Evaluation of Correction and Inference Mechanisms Evaluation of Feature and Their Combinations Evaluation of Two Detection and Classification Modes Imbalance Training Class Problem IBM Model 3 Linguistic levels as feature sets Semantic features Accuracy results for binary decisions Accuracy results for combined decisions Comparison of accuracy scores across linguistic levels Levels all and morph against the Gold Standard Results for ensemble classifier Example of BLC selection Most frequent monosemic words in BG Most frequent BLC20 semantic classes on WordNet 30 Number of training examples Results of task17 The B I E S Tag Set Example of Lattice Used in the Markov ModelBased Method Example of the Character Tagging Method Word boundaries are indicated by vertical lines Example of the Hybrid Method Character Types Calculated Values of i Statistical Information of Corpora Performance of Japanese Word Segmentation Performance of Chinese Word Segmentation GETARUNS AR algorithm Expletive it compared results GETARUNS pronouns collapsed at structural level Overall results CoverageAccuracy Examples of new alternations New Verb Classes Average results for 35 verbs Association frequencies for target verb Association overlap for target verbs Coverage of verb association features by grammarwindow resources Accuracy for induced verb classes Latent Dependency coupling for the RE task The DC ONNECT factor expresses ternary connection re lations because the shared head word of the proposed re lation is unknown As is convention variables are repre sented by circles factors by rectangles Relation Extraction Results Models using hidden constituency syntax provide significant gains over the syntacticallyuniformed baseline model in both languages but the advantages of the latent syntax were mitigated on the smaller Chinese data set A tiered graphic representing the three different SRL model configurations The baseline system is described in the bottom c d the separate panels highlighting the independent predictions of this model sense labels are assigned in an entirely separate process from argument prediction Pruning in the model takes place primarily in this tier since we observe true predicates we only instantiate over these indices The middle tier b illustrates the syntactic representation layer and the connective factors between syntax and SRL In the observed syntax model the Link variables are clamped to their correct values with no need for a factor to coordinate them to form a valid tree Finally the hidden model comprises all layers including a combinatorial syntactic constraint a over syntactic variables In this scenario all labels in b are hidden at both training and test time Examining the learned hidden representation for SRL In this example the syntactic dependency arcs derived from gold standard syntactic annotations left are entirely disjoint from the correct predicatearguments pairs shown in the heatmaps by the squares outlined in black and the observed syntax model fails to recover any of the correct predictions In contrast the hidden model structure right learns a representation that closely parallels the desired end task predictions helping it recover three of the four correct SRL predictions shaded arcs red corresponds to a correct prediction with true labels GA KARA etc and providing some evidence towards the fourth The dependency tree corresponding to the hidden structure is derived by edgefactored decoding dependency variables whose beliefs 05 are classified as true though some arcs not relevant to the SRL predictions are omitted for clarity SRL Results The hidden model excels on the unlabeled prediction results often besting the scores obtained using the parses distributed with the CoNLL data sets These gains did not always translate to the labeled task where poor sense prediction hindered absolute performance Illustration on temporality Symmetry of window size Optimality of window size The translation examples where shaded cells indicate the correctly translated pairs MRR Precision Recall and F1score Supersense evaluation results Values are the percentage of correctly assigned supersenses k indicates the number of nearest neighbours considered Pseudodisambiguation Percentage of correct choices made Lbound denotes the Web1T lower bound on the a1 n bigram size the number of decisions made Results on the unseen plausibility dataset Results Chinese character usage in 3 corpora The numbers in brackets indicate the percentage of characters that are shared by at least 2 corpora Number of entries in 3 corpora Number of unique entries in training and test sets categorized by semantic attributes Language detection accuracies using a 4gram language model for the letter sequence of the source name in Latin script The effect of language and gender in MRR performance of phonetic translit eration for 3 corpora using unigram and bigram language models Gender detection accuracies using a 4gram language model for the letter sequence of the source name in Latin script Overall transliteration performance The effect of language detection The effect of gender detection schemes An example of alignment for Japanese and English sentences Translation Model IBM Model 4 string insertion operator for lefttoright decoding method A string e0 was appended after the partial output string e and the last word in e 0 was aligned from f j string insertion operation for righttoleft decoding method A string e0 was prepended before the partial output string e and the first word in e 0 was aligned from f j Merging lefttoright and righttoleft hypotheses ef and eb in bidirectional decoding method Figure 5a merge two open hypotheses while Figure 5b merge them with inserted zero fer tility words Comparison of the three decoders by the ratio each decoder produced search errors Statistics on a travel conversation corpus Consistently formatted sentence trans lation pairs Consistently formatted term translation pairs The framework of our approach Example segmentations indicates the separator between adjacent snippets Character classes Performance of different settings Contribution of every feature Missing argument examples of biological interactions A protein domainreferring phrase example Statistics of anaphoric expressions A subjective pronoun resolution example Nonanaphoric DNP examples Possessive pronoun resolution examples Example patterns for parallelism An example antecedent of a nominal in teraction keyword Example patterns of nominal interaction keywords Example patterns of proteins and their do mains An annotation example for the necessity of species information Term variation examples Protein name grounding examples Experimental results of test corpus An example result of BioAR Incorrect resolution example of pronoun resolution module The templates for generating potentially deter ministic constraints of English POS tagging Morph features of frequent words and rare words as computed from the WSJ Corpus of Penn Treebank Comparison of raw input and constrained input Comparison of raw input and constrained input Character and wordbased features of a possi ble word wi over the input character sequence c Suppose that wi ci0 ci1 ci2 and its preceding and following char acters are cl and cr respectively Deterministic constraints for POS tagging POS tagging with deterministic constraints The maximum in each column is bold Character tagging with deterministic constraints ILP problem size and segmentation speed Substitutioninsertiondeletion patterns for phonemes based on English secondlanguage learners data reported in Swan and Smith 2002 Each row shows an input phoneme class possi ble output phonemes including null and the positions where the substitution or deletion is likely to occur Examples of features and associated costs Pseudofeatures are shown in boldface Exceptional denotes a situation such as the semivowel j substituting for the affricate dZ Substitutions between these two sounds actually occur frequently in secondlanguage error data Substitutiondeletioninsertion costs for g Examples of the three top candidates in the transliteration of EnglishArabic EnglishHindi and EnglishChinese The second column is the rank Languagepair datasets Number of evaluated English NEs CoreMRR scores with different values using score combination A higher puts more weight on the phonetic model MRRs and CorrRate for the pronunciation method top and time correlation method middle The bottom table shows the scores for the combination CoreMRR Partofspeech tags of the Penn Chinese Treebank that are referenced in this paper Please see Xia 2000 for the full list Categories of multicharacter words that are considered strings without internal structures see Section 41 Each category is illustrated with one example from our corpus Categories of multicharacter words that are considered strings with internal structures see Section 42 Each category is illustrated with an example from our corpus Both the individual characters and the compound they form receive a POS tag POS annotations of a couplet ie a pair of two verses in a classical Chinese poem See Table 1 for the meaning of the POS tags POS annotations of an example sentence with a string wan lai evening that has internal structure See Section 42 for two possible translations and Table 1 for the meaning of the POS tags Partofspeech annotations of the three character strings xi liu ying Little Willow military camp and xin feng shi Xinfeng city Both are strings with internal structures with nested structures that perfectly match at all three levels They are the noun phrases that end both verses in the couplet The semantic roles of cases beside C3 verb cluster The semantic roles of cases beside C1 verb cluster Rule expansion with minimal context Example 3 Sample of extracted entailment rules Accuracy of the extracted sets of rules Resulting sets of entailment rules Segmentation recall relative to gold word frequency Baseline performance Segmentation precisionrecall relative to gold word length in training data Word length statistics on test sets Fscore of two segmenters with and without word tokentype features Upper bound for combination The error reduction ER rate is a comparison between the Fscore produced by the oracle combination sys tem and the characterbased system see Tab 1 Segmentation performance presented in previous work and of our combination model PrecisionRecallFscore of different models Accuracy of maximum entropy system using different subsets of features for S ENSEVAL 2 verbs Overall accuracy of maximum entropy sys tem using different subsets of features for Penn Chi nese Treebank words manually segmented partof speechtagged parsed Overall accuracy of maximum entropy sys tem using different subsets of features for Peoples Daily News words manually segmented partof speechtagged Overall accuracy of maximum entropy sys tem using different subsets of features for Peoples Daily News words automatically segmented part ofspeechtagged parsed Overview of the system The pool of features for all languages Experiment results as F1 scores where IM is identification of mentions and S Setting Final system results as F1 scores where IM is identification of mentions and S Setting For more details cf Recasens et al 2010 Interannotator agreement of ACE 2005 relation annotation Numbers are the distinct relation mentions whose both arguments are in the list of adjudicated entity mentions Percentage of examples of major syntactic classes cumulative distribution of frequency CDF of the relative ranking of modelpredicted probability of being positive for false negatives in a pool mixed of false negatives and true negatives and the CDF of the relative ranking of modelpredicted probability of being negative for false positives in a pool mixed of false positives and true positives Categories of spurious relation mentions in fp1 on a sample of 10 of relation mentions ranked by the percentage of the examples in each category In the sample text red text also marked with dotted underlines shows head words of the first arguments and the underlined text shows head words of the second arguments Performance of RDC trained on fp1fp2adj and tested on adj TSVM optimization function for nonseparable case Joachims 1999 Performance with SVM trained on a fraction of adj It shows 5 fold cross validation results 5fold crossvalidation results All are trained on fp1 except the last row showing the unchanged algorithm trained on adj for comparison and tested on adj McNemars test show that the improvement from purify to tSVM and from tSVM to ADJ are statistically significant with p005 ESA and inputoutput data A character sequence and its subsequence pairs The path of Selection The binary tree of Selection The initial frequencies of character sequences The adjusted frequencies of character sequences The hierarchical form of a result The scales of corpora The results of setting 1 Punctuation and other encoding information are not used the maximum length is 30 The results of setting 2 Punctuation and other encoding information are not used the maximum length is 10 The results of setting 3 Punctuation is used the maximum length is 30 The results of setting 4 Punctuation and other encoding information are used the maximum length is 30 The difference between the results of four settings The results brought by different maximum lengths The empirical formulae for the prediction linear model The correlation between the scales and the proper exponents The four types of changes Convergence of results The time complexity in practice 4 samples 10 30 50 and 100 The comparison between NPYLM and ESA The comparison between DLG AV BE and ESA The comparison between IWSLRRI SSS TONGOO THT and ESA Probabilistic Approaches Outputs from each algorithm at different sorted ranks WordNetbased scores Performance on Internet data Equation 1 settings Precisionrecall curve for rescoring Syntactic frames for VerbNet classes VerbNet classes An excerpt from SemLink Training instances obtained from Verb Net upper and VerbNetSemLink lower Example of features for sway 14 classes used in Joanis et al 2008 and their corresponding Levin class numbers Corpus size vs accuracy Accuracy and KLdivergence for the all class task the VerbNetSemLink setting Accuracy and KLdivergence for the all class task the VerbNet only setting Accuracy for the 14class task Corpus size vs KLdivergence Contribution of features Results of paraphrases evaluation for 100 sentences in French using English as the pivot lan guage Comparison between the baseline system MOSES and our algorithm MCPG Table 2 Chunk feature templates i f jFk is the chunk label plus the tag of its right most child if the j2r t tree is a chunk Otherwise i f is the con stituent label of the j r t tree Extend feature templates G fA k l is the root constituent label of th Check feature templates G fj Learning curves wordsegmentation F measure and parsing label Fmeasure vs percentage of training data Languagedependent lexical features A word list can be collected to encode different WS wordsegmentation Baseline languageindependent features LexFeat plus lex ical features Numbers are averaged over the 10 ex periments in Figure 2 Parsing and word segmentation F measures vs the experiment numbers Lines with triangles segmentation Lines with circles label Dottedlines languageindependent features only Solid lines plus lexical features Usefulness of syntactic information black dashdotted line word boundaries only red dashed line POS info and blue solid line full parse trees IV and OOV recall in Zhang et al 2006a Corpora statistics of Bakeoff 2005 Feature templates used for CRF in our experiments Error analysis of confidence measure with and without EIV tag Results of CT when MP is less than 0875 Results of different approach used in our experiments White background lines are the results we repeat Zhangs methods and they have some trivial difference with Table 1 A text segment from MUC6 data set Feature set for the baseline pronoun resolution system Backward features used to capture the coreferential information of a candidate Results of different systems for pronoun resolution on MUC6 and MUC7 Here we only list backward feature assigner for pronominal candidates In RealResolve1 to RealResolve4 the backward features for nonpronominal candidates are all found by DTnonpron The pronoun resolution algorithm by incorporating coreferential information of can didates The classifier refining algorithm New training and testing procedures A Generic Morphological Analyzer as a Black Box User Interface with Arabic Script Dis play in Java Mouse clicks on the virtual keyboard or key presses on the physical keyboard are inter cepted converted to Arabic Unicode characters and stored in a buffer which has a start and an end but no inherent ordering The Arabic Canvas Object observes the buffer and contains an Ara bic Scribe object that renders the string of Uni code characters righttoleft as connected Arabic glyphs Information Flow in the Xerox Arabic Demo Input words from the user interface are transmitted across the Internet dotted lines and analyzed by a server typically producing multi ple analysis strings Each analysis string is then generated in fully voweled form combined with English glosses and then reformatted as HTML before being sent back across the Internet to the users browser for display The analyzer and gen erator finitestate transducers FSTs are identical except that the lower side language of the genera tor is limited to contain only fullyvoweled words A morphological analyzergenerator can be implemented elegantly and efficiently as a finitestate transducer By Xerox convention the lowerside language consists of surface strings words and the upperside language consists of strings representing analyses of the lowerside words Such a transducer is a data structure rather than code and the runtime code that applies such a transducer to input strings in either direction is completely languageindependent Creation of a Lexical Transducer The o operator represents the composition operation Morph Examples and Motivations Distributions of Morph Examples Overview of Morph Resolution Crosssource Comparable Data Example each morph and target pair is shown in the same color Network Schema of MorphRelated Het erogeneous Information Network Example of MorphRelated Heteroge neous Information Network The System Performance Based on Com binations of Surface and Semantic Features The System Performance Based on Each Single Feature Set Description of feature sets Glob only uses the same set of similarity measures when combined with other semantic features The Effects of Temporal Constraint Accuracy of Target Candidate Detection The Effects of Social Features The System Performance of Integrating Cross Source and Cross Genre Information Effects of Popularity of Morphs Performance of Two Categories Confusion Sets An Initial Learning Curve for Confusable Disambiguation Learning Curves for Confusable Disambiguation Distribution of Error Types Turning distributional similarity into a weighted inference rule Results on the RTE1 Test Set Results on the STS video dataset Architecture of the translation approach based on a loglinear modeling approach Example of a symmetrized word alignment Verbmobil task Algorithm phraseextract for extracting phrases from a wordaligned sentence pair Here quasiconsecutiveTP is a predicate that tests whether the set of words TP is consecutive with the possible exception of words that are not aligned Examples of alignment templates obtained in training Dependencies in the alignment template model Example of segmentation of German sentence and its English translation into alignment templates Algorithm for breadthfirst search with pruning Algorithm minjumps to compute the minimum number of needed jumps DcJ1 j to complete the translation Statistics for Verbmobil task training corpus Train conventional dictionary Lex development corpus Dev test corpus Test Words words without punctuation marks Effect of alignment template length on translation quality Effect of pruning parameter tp and heuristic function on search efficiency for directtranslation model Np 50000 Effect of pruning parameter tp and heuristic function on error rate for directtranslation model Np 50000 Effect of pruning parameter Np and heuristic function on search efficiency for directtranslation model tp 1012 Effect of pruning parameter Np and heuristic function on error rate for directtranslation model tp 1012 Corpus statistics for Hansards task Words words without punctuation marks Corpus statistics for ChineseEnglish corporalarge data track Words words without punctuation marks Translation results on the Hansards task Example translations for ChineseEnglish MT A Morphophonological Rule Auxiliary Denitions Features with Set Values Subject and Object Agreement Features Verb Lexicon Specic Subject and Object Agreement Rules A Cascade of Compositions Shared Agreement Rules Rules of Referral Denition of Lingala Verbal Morphology 1 Feature set for coreference resolution Feature 22 23 and features involving Cj are not used in the singlecandidate model Results for the nonpronoun resolution Results for the pronoun resolution Results for the coreference resolution Distribution of dialogue acts in our dataset The 7 speakers from ICSIMRDA dataset used in our experiments The table lists the Speaker ID orig inal speaker tag the type of meeting selected for this speaker the number of meetings this speaker participated and the total number of dialogue acts by this speaker Effect of samespeaker data on dialogue act recognition We compare two approaches 1 when a recognizer is trained on the same person and tested on new utterances from the same person and 2 when the recognizer was trained on another speaker same test set We vary the amount of training data to be 200 500 1000 1500 and 2000 dialogue acts In all cases using speakerspecific recognizer outperforms recognizer from other speakers The average results among all 7 speakers when train with different combinations of speaker specific data and other speakers data are displayed In both Constant adaptation and Reweighted adaptation models the num ber of speaker specific data are varied from 200 500 1000 1500 to 2000 In Generic model only all other speakers data are used for training data Average results of Reweighting among all 7 speakers when the amount of speaker specific data is 0 500 2000 Beam search algorithm for joint tagging and de pendency parsing of input sentence x with weight vector w and beam parameters b1 and b2 The symbols hc hs and hf denote respectively the configuration score and feature representation of a hypothesis h hcA denotes the arc set of hc Transitions for joint tagging and dependency parsing extending the system of Nivre 2009 The stack is represented as a list with its head to the right and tail and the buffer B as a list with its head to the left and tail The notation f a b is used to denote the function that is exactly like f except that it maps a to b Specialized feature templates for tagging We use i and Bi to denote the ith token in the stack and buffer B respectively with indexing starting at 0 and we use the following functors to extract properties of a token i ith best tag si score of ith best tag finally predicted tag w word form pi word prefix of i characters si word suffix of i characters Score differences are binned in discrete steps of 005 Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and the score threshold Beam parameters fixed at b1 40 b2 4 Accuracy scores for the CoNLL 2009 shared task test sets Rows 12 Top performing systems in the shared CoNLL Shared Task 2009 Gesmundo et al 2009 was placed first in the shared task for Bohnet 2010 we include the updated scores later reported due to some improvements of the parser Rows 34 Baseline k 1 and best settings for k and on development set Rows 56 Wider beam b1 80 and added graph features G and cluster features C Second beam parameter b2 fixed at 4 in all cases Accuracy scores for WSJPTB converted with head rules of Yamada and Matsumoto 2003 and labeling rules of Nivre 2006 Best dev setting k 3 04 Results marked with use additional information sources and are not directly comparable to the others Selected entries from the confusion matrix for parts of speech in German with Fscores for the lefthand side category ADJ ADJD or ADJA adjective ADV adverb ART determiner APPR preposition NE proper noun NN common noun PRELS relative pronoun VVFIN finite verb VVINF nonfinite verb VAFIN finite auxiliary verb VAINF nonfinite auxil iary verb VVPP participle XY not a word We use to denote the set of categories with as a prefix Accuracy scores for Penn Chinese Treebank converted with the head rules of Zhang and Clark 2008 Best dev setting k 3 01 MSTParser results from Li et al 2011 UAS scores from Li et al 2011 and Ha tori et al 2011 recalculated from the separate accuracy scores for root words and nonroot words reported in the original papers Selected entries from the confusion matrix for parts of speech in English with Fscores for the lefthand side category DT determiner IN preposition or sub ordinating conjunction JJ adjective JJR compara tive adjective NN singular or mass noun NNS plural noun POS possessive clitic RB adverb RBR com parative adverb RP particle UH interjection VB base form verb VBD past tense verb VBG gerund or present participle VBN past participle VBP present tense verb not 3rd person singular VBZ present tense verb 3rd person singular We use to denote the set of categories with as a prefix Pseudo code of our clustering algorithm The predicates of two sentences white The company has said it plans to restate its earnings for 2000 through 2002 grey The company had announced in January that it would have to restate earnings from the Microsoft Research Paragraph Corpus are aligned by computing clusters with minimum cuts Results for sentencebased predicte alignment in the three benchmark settings MTC Leagues and MSR all numbers in results that significantly differ from Full are marked with asterisks p005 p001 Impact of removing individual measures and us ing a tuned weighting scheme all numbers in results that significantly differ from Full are marked with aster isks p005 p001 Results for GigaPairs all numbers in re sults that significantly differ from Full are marked with asterisks p005 p001 Example of the transfer of a verbal chunk Examples of translations An example of the label consistency problem excerpted from a document in the CoNLL 2003 English dataset Features used by the CRF for the two tasks named entity recognition NER and template filling TF Counts of the number of times multiple occurrences of a token sequence is labeled as different entity types in the same document Taken from the CoNLL training set F1 scores of the local CRF and nonlocal models on the CMU Seminar Announcements dataset We also provide the results from Sutton and McCallum 2004 for comparison F1 scores of the local CRF and nonlocal models on the CoNLL 2003 named entity recognition dataset We also provide the results from Bunescu and Mooney 2004 for comparison Summary of the previous work on coreference resolution that employs the learning algorithms the clustering algorithms the feature sets and the training instance creation methods discussed in Section 31 Statistics for the ACE corpus Results for the three ACE data sets obtained via the MUC scoring program Results for the three ACE data sets obtained via the BCUBED scoring program The coreference systems that achieved the highest Fmeasure scores for each test set and scorer combination The average rank of the candidate partitions produced by each system for the corresponding test set is also shown List of semantic labels Examples of aggregated instances Results of semantic classification Confusion matrix of acquired nouns Results of ablation experiments Overview of the tasks investigated in this paper n size of ngram POS parts of speech Ling linguistic knowledge Type type of task Meaning of diacritics indicating statistical sig nificance 2 tests Performance comparison with the literature for candidate selection for MT Performance of Altavista counts and BNC counts for candidate selection for MT data from Prescher et al 2000 Performance comparison with the literature for context sensitive spelling correction Performance of Altavista counts and BNC counts for context sensitive spelling correction data from Cucerzan and Yarowsky 2002 Performance comparison with the literature for compound bracketing Performance of Altavista counts and BNC counts for compound bracketing data from Lauer 1995 Performance of Altavista counts and BNC counts for adjective ordering data from Malouf 2000 Performance comparison with the literature for compound interpretation Performance of Altavista counts and BNC counts for compound interpretation data from Lauer 1995 Performance comparison with the literature for noun countability detection Performance of Altavista counts and BNC counts for noun countability detection data from Bald win and Bond 2003 Syntactic Seeding Heuristics Caseframe Network Examples Lexical Caseframe Expectations Semantic Caseframe Expectations General Knowledge Sources Individual Performance of KSs for Disasters Individual Performance of KSs for Terrorism General Contextual Role Knowledge Sources General Knowledge Sources Linking FrameNet frames and VerbNet classes Mapping algorithm refining step Results of the mapping algorithm F1s of some individual FN role classifiers and the overall multiclassifier accuracy 454 roles F1s of some individual ILC classifiers and the overall multiclassifier accuracy 180 classes on PB and 133 on FN Semantic role learning curve Graph representing transliteration pairs and cooccurence relations Ten highestscoring matches for the Xin hua corpus for 81301 The final column is the log P estimate for the transliteration Starred entries are incorrect MRRs of the frequency correlation meth ods MRRs on the augmented candidate list Effectiveness of combining the two scor ing methods Propagation Core items Effectiveness of score propagation Propagation All items An example of MOD feature extraction An oval in the dependency tree denotes a bunsetsu of features Precision for 200 candidates EvRec Precision for each phrase type EvLing Anaphora resolution preferences Description of the unrestricted corpora used in the evaluation Results obtained in the detection of zeropronouns Classification of third person zero pronouns a A related work section extracted from Wu and Oard 2008 b An associated topic hierar chy tree of a c An associated topic tree annotated with key wordsphrases The demographics of RWSData No RW RA SbL WbL TS and TD are labeled as Number of Related Works Referenced Articles Sentencebased Length of Word based Length of Tree Size and Tree Depth respectively A context modeling example Evaluation results for ReWoS variants and baselines CCG and LTAG supertag sequences Experimental results with individual features compared against Moses and the moseschart baseline Experimental results with combined features Numbers of rules in Hiero or phrasepairs in Moses Syntactic tree kernel STK Integrating Brown cluster information Overview of the ACE 2005 data Distribution of relations in ACE 2005 For each domain the percentage of target domain words types that are unseen in the source together with the most frequent OOV words Comparison to previous work on the 7 re lations of ACE 2004 K kernelbased F feature based yesno models argument order explicitly Brown clusters in tree kernels cf Fig 2 F1 per coarse relation type ACE 2005 SYS is the final model ie last row PETPET WCPET LSA of Table 5 Indomain first column and outofdomain performance columns two to four on ACE 2005 PET and BOW are abbreviated by P and B respectively If not specified BOW is marked Bell tree representation for three mentions numbers in denote a partial entity Infocus entities are marked on the solid arrows and active mentions are marked by Solid arrows signify that a mention is linked with an infocus partial entity while dashed arrows indicate starting of a new entity Basic features used in the maximum entropy model Statistics of three test sets Table 4 Impact of feature categories Numbers after are the standard deviations indicates that the result is significantly pairwise ttest different from the line above at Performance vs log start penalty Results on the MUC6 formal test set Entityconstrained Mention Fmeasure MP uses Table 3 Coreference results on true mentions MP mentionpair model EM entityme features wh features None of the ECMF differences between MP and EM is statistically significant at A verse written in the BAD web application The response of the rhyme search engine The BAD application before entering a verse showing two possible rhyme patterns The generic beamsearch algorithm Feature templates for the word segmentor Feature templates of a typical characterbased word segmentor Speedaccuracy tradeoff of the segmentor Training development and test data for word segmentation on CTB5 The accuracies of various word segmentors over the first SIGHAN bakeoff data The accuracies of various word segmentors over the second SIGHAN bakeoff data The extended generic beamsearch algorithm with multiple beams Comparison between three different decoders for word segmentation POS feature templates for the joint segmentor and POS tagger The influence of beamsizes and the convergence of the perceptron for the joint segmentor and POS tagger The speeds of joint word segmentation and POStagging by 10fold cross validation The accuracies of joint segmentation and POStagging by 10fold cross validation The comparison of overall accuracies of various joint segmentor and POStaggers by 10fold cross validation using CTB Training development and test data from CTB5 for joint word segmentation and POStagging Accuracy comparisons between various joint segmentors and POStaggers on CTB5 The accuracyspeed tradeoff graph for the joint segmentor and POStaggers and the twostage baseline An example Chinese dependency tree Transitionbased feature templates for the dependency parser Transitionbased feature context for the dependency parser Graphbased feature templates for the dependency parser The training development and test data for English dependency parsing The accuracyspeed tradeoff graph for the transitionbased and combined dependency parsers Accuracy comparisons between various dependency parsers on English data Training development and test data for Chinese dependency parsing Test accuracies of various dependency parsers on CTB5 data The combined segmentation POStagging and dependency parsing Fscores using different pipelined systems An example Chinese lexicalized phrasestructure parse tree Feature templates for the phrasestructure parser The standard split of CTB2 data for phrasestructure parsing Accuracies of various phrasestructure parsers on CTB2 with goldstandard POStags Accuracies of various phrasestructure parsers on CTB2 with automatically assigned tags Accuracies of our phrasestructure parser on CTB5 using goldstandard and automatically assigned POStags The accuracyspeed tradeoff graph for the phrasestructure parser Comparison of dependency accuracies between phrasestructure parsing and dependency parsing using CTB5 data Composition of two FSTs maintaining separate transitions Finitestate cascades for five natural language problems Changing a decision in the derivation lattice All paths generate the observed data The bold path rep resents the current sample and the dotted path represents a sidetrack in which one decision is changed Multiple EM restarts for POS tagging Each point represents one random restart the yaxis is tag ging accuracy and the xaxis is EMs objective function log Pdata Multiple Bayesian learning runs using anneal ing with temperature decreasing from 2 to 008 for POS tagging Each point represents one run the yaxis is tag ging accuracy and the xaxis is the log Pderivation of the final sample Multiple Bayesian learning runs using averag ing for POS tagging Each point represents one run the yaxis is tagging accuracy and the xaxis is the average log Pderivation over all samples after burnin Gibbs sampling for Bayesian inference outperforms both EM and Variational Bayesian EM The output of EM alignment was used as the gold standard Extract of a FrenchEnglish sentence pair segmented into bilingual units The original org French sentence appears at the top of the figure just above the reordered source s and target t The pair s t decomposes into a sequence of L bilingual units tuples u1 uL Each tuple ui contains a source and a target phrase si and ti Experimental results in terms of BLEU scores measured on the newstest2011 and newstest2012 For newstest2012 the scores are provided by the organizers Source transformed and extracted trees given headline British soldier killed in Afghanistan Types of features extracted for edge e from h to n Filters applied to candidate pair H S Results for the systems and original headline and stand for significantly better than Unsupervised and Our system at 95 confidence respectively Results for two kinds of headlines Results for the unsupervised baseline and the supervised system trained on three kinds of feature sets Percentage of major punctuation marks in the Chinese corpus 4 Syntax of Chinese Dash Rhetorical pattern of CQuestion Rhetorical pattern of CExclamation Rhetorical pattern of CEllipses Rhetorical pattern of CSemicolon Rhetorical pattern of CDash Rhetorical pattern of CColon Rhetorical Function of Exclamation Mark in Chinese and German corpora POSmorphological feature accuracies on the development sets PARSEVAL scores on the development sets Architecture of the dependency ranking system Baseline performance and nbest oracle scores UASLAS on the development sets mate uses the prepro cessing provided by the organizers the other parsers use the preprocessing described in Section 2 Unlabeled TedEval scores accuracyexact match for the test sets in the predicted segmentation set ting Only sentences of length 70 are evaluated Feature sets for the dependency ranker for each language default denotes the default ranker feature set Performance UASLAS of the reranker on the development sets Baseline denotes our baseline Rankeddflt and Ranked denote the default and optimized ranker feature sets respectively Oracle denotes the oracle scores Final PARSEVAL F1 scores for constituents on the test set for the predicted setting ST Baseline denotes the best baseline out of 2 provided by the Shared Task organizers Our submission is underlined Final UASLAS scores for dependencies on the test sets for the predicted setting Other denotes the highest scoring other participant in the Shared Task ST Baseline denotes the MaltParser baseline provided by the Shared Task organizers Comparison of Min Simple Fulland DynamicExpansions More Examples DynamicExpansion Tree Span Scheme Comparison of different contextsensitive Comparison of tree span schemes with antecedents in different sentences apart Feature templates used in Rphase Ex ample used is 32 ddd Tags used in LMR Tagging scheme Example of LMR Tagging Additional feature templates used in C phase Example used is 32 ddd with tagging results after Rphase as SSLMR Official BakeOff2005 results Keys F Regular Tagging only all training data are used P1 Regular Tagging only 90 of training data are used P2 Regular Tagging only 70 of training data are used S Regular and Correctional Tagging Separated Mode I Regular and Correctional Tagging Integrated Mode Experimental results of CityU corpus measured in Fmeasure Clustering an 11nodes graph with CW in two iterations The middle node gets the grey or the black class Small numbers denote edge weights Percentage of obtaining two clusters when applying CW on nbipartite cliques oscillating states in matrix CW for an unweighted graph The 10bipartite clique Rate of obtaining two clusters for mix tures of SWgraphs dependent on merge rate r normalized Mutual Information values for three graphs and different iterations in Bipartite neighboring cooccurrence graph a and secondorder graph on neighboring cooccurrences b clustered with CW Disambiguation results in dependent on frequency Disambiguation results in dependent on word class nouns verbs adjectives the largest clusters from partitioning the second order graph with CW Graphical model for the Bayesian QueryFocused Summarization Model Empirical results for the baseline models as well as BAYE S UM when all query fields are used Performance with noisy relevance judg ments The Xaxis is the Rprecision of the IR engine and the Yaxis is the summarization per formance in MAP Solid lines are BAYE S UM dot ted lines are KLRel Bluestars indicate title only redcircles indicated titledescriptionsummary and blackpluses indicate all fields Empirical results for the positionbased model the KLbased models and BAYE S UM with different inputs Translation performance of EM Gibbs sampling and variational Bayes and lower translation performance Section VA after applying alignment combination within and across methods EMCo Arabic English BLEU and TER scores of various a methods EMCo GSCo EMCoGSCo and VBCo Czech English BLEU scores of various al EMCo GSCo EMCoGSCo and VBCo German English BLEU scores of various al EMCo GSCo EMCoGSCo and VBCo Distribution of alignment fertilities for source language tokens Arabic English BLEU scores o schemes in the 1Msentence translation task Intrinsic and extrinsic evaluation of alignments in the small data experiments a Alignment dictionary size normalized by the average of source and target vocabulary sizes b Average alignment fertility of aligned singletons c Percentage of unaligned singletons d Number of symmetric alignments normalized by the average of source and target tokens e Percentage of training set vocabulary covered by singleword phrases in the phrase table f Decodetime rate of input words that are in the training vocabulary but without a translation in the phrase table g Phrase table size normalized by the average of source and target tokens BLEU scores of alignments estimated at different iterations Left EM middle samples from the Gibbs chain right GS viterbi estimates with Dependency representation of example 2 from Talbanken05 Animacy classification scheme Zaenen et al 2004 The animacy data set from Talbanken05 number of noun lemmas Types and tokens in each class Accuracy for MBL and SVM classifiers on Talbanken05 nouns in accumulated frequency bins by Parole frequency Precision recall and Fscores for the two classes in MBLexperiments with a general feature space Confusion matrix for the MBLclassifier with a general feature space on the 10 data set on Talbanken05 nouns Overall results in experiments with au tomatic features compared to gold standard fea tures expressed as unlabeled and labeled attach ment scores System overview Table Construction Usefulness evaluation result Correctness evaluation result Example of a prediction for English to French translation s is the source sentence h is the part of its translation that has already been typed x is what the translator wants to type and x is the prediction Probability that a prediction will be ac cepted versus its gain Time to read and accept or reject proposals versus their length Approximate times in seconds to generate predictions of maximum word sequence length M on a 12GHz processor for the MEMD model Results for different user simulations Numbers give reductions in keystrokes Results for different predictor configura tions Numbers give reductions in keystrokes Snapshot of the supersenseannotated data The 7 article titles translated in each domain with total counts of sentences tokens and supersense mentions Overall there are 2219 sentences with 65452 tokens and 23239 mentions 13 tokensmention on average Counts exclude sentences marked as problematic and mentions marked Illustration of the alignment of steps Statistics of datasets F1 scores in of SegTagDep on CTB 5c1 wrt the training epoch xaxis and parsing feature weights in legend Performance of baseline and joint models wrt the average processing time in sec per sen tence Each point corresponds to the beam size of 4 8 16 32 64 The beam size of 16 is used for SegTag in SegTagDep and SegTagTagDep Final results on CTB5j F1 scores and speed in sentences per sec of SegTagDep on CTB5c1 wrt the beam size Final results on CTB6 and CTB7 Segmentation POS tagging and unlabeled attachment dependency F1 scores averaged over five trials on CTB5c Figures in parentheses show the differences over SegTagDep p 001 Examples of the semantic role features Decoding algorithm for the standard TreetoString transducer lef twrightw denote the leftright boundary word of s c1 c2 denote the descendants of v ordered based on RHS of t An example showing the combination of the se mantic role sequences of the states Abovemiddle is the state information beforeafter applying the TTS template and bot tom is the used TTS template and the triggered SRFs during the combination An example showing how to compute the target side position of a semantic role by using the median of its aligning points Decoding algorithm using semantic role features Semac1 role c2 role t denotes the triggered semantic role features when combining two children states and ex amples can be found in Figure 3 Computing the partition function of the conditional probability P rST Semas1 s2 t denotes all the seman tic role features generated by combining s1 and s2 using t Examples of the MT outputs with and without SRFs The first and second example shows that SRFs improve the completeness and the ordering of the MT outputs respectively the third example shows that SRFs improve both properties The subscripts of each Chinese phrase show their aligned words in English Distribution of the sentences where the semantic role features give nopositivenegative impact to the sentence fluency in terms of the completeness and ordering of the semantic roles BLEU4 scores of different systems The results of different systems for coreference resolution Top patterns chosen under different scoring schemes The decision tree Nwire for the system using the single semantic relatedness feature Abridged grammatical representation for the example sentence 9 Sequence of POStagged units used to estimate the bilingual ngram LM Statistics for the training tune and test data sets Size in words of reorderings ob served in training bitexts Size of translation unit ngrams seen in test for different ngram models Reordering accuracy BLEU results Translation accuracy BLEU results Example context for the spelling confusion set piecepeace and extracted features Example context for WSD S ENSEVAL2 target word bar inventory of 21 senses and extracted features The increase in performance for successive variants of Bayes and Mixture Model as evaluated by 5fold cross vali dation on S ENSEVAL 2 English data A WSD example that shows the influence of syntactic collocational and longdistance context features the probability estimates used by Nave Bayes and MM and their associated weights and the posterior probabilities of the true sense as computed by the two models 2 Figure 4 WSD example showing the utility of the MVC method A sense with a high variational coefficient is preferred to the mode of the MM distribution the fields corresponding to the true sense are highlighted MM and MMVC performance by performing 5 fold cross validation on S ENSEVAL 2 data for 4 languages Results using 5fold cross validation on S ENSEVAL 2 English lexicalsample training data The contribution of MMVC in a rankbased classi fier combination on S ENSEVAL 1 and S ENSEVAL 2 English as computed by 5fold cross validation over training data Results on the standard 14 CSSC data sets Results using 5fold cross validation on S ENSEVAL 1 training data English Learning Curve for MM and MMVC on S ENSEVAL 2 English crossvalidated on heldout data ings of the 13th International Conference pages 182190 Table 6 Accuracy on S ENSEVAL1 and S ENSEVAL2 En A R Golding and D Roth 1999 A winnowbased appro glish test data only the supervised systems with a coverage of to contextsensitive spelling correction Machine Learni at least 97 were used to compute the mean and variance 3413107130 a An undirected graph G representing the similarity matrix b The bipartite graph showing three clusters on G c The induced clusters U d The new graph G1 over clusters U e The new bipartite graph over G1 Performance on T3 using a predefined tree structure Performance on T2 using a predefined tree structure Comparison against Stevenson and Joanis 2003s result on T1 using similar features NMI of unconstrained HGFC when trees for T2 and T3 are inferred automatically Effect of Factors Sentence Recency Performance of Algorithms Notation and signatures for our framework Additional notation and signatures for CAM CoreLexs basic types with their corresponding WordNet anchors CAM adopts these as meta senses Sample of experimental items for the meta alternation anmfod Abbreviations are listed in Table 2 Sample targets for meta alternations with high AP and midcoherence values Average Precision and Coherence for each meta alternation Correlation r 0743 p 0001 Meta alternations and their average precision values for the task The random baseline performs at 0313 while the frequency baseline ranges from 0255 to 0369 with a mean of 0291 Alternations for which the model outperforms the frequency baseline are in boldface mean AP 0399 standard deviation 0119 Illustration of dictionary based segmenta tion finite state transducer Performance of the mention detection sys tem using lexical features only Performance of the mention detection sys tem using lexical syntactic gazetteer features as well as features obtained by running other namedentity classifiers Performance of the mention detection sys tem including all ACE04 subtasks Effect of Arabic stemming features on coref erence resolution The row marked with Truth represents the results with true mentions while the row marked with System represents that mentions are detected by the system Numbers under ECM F are EntityConstrainedMention Fmeasure and numbers under ACEVal are ACEvalues Organisation of the hierarchical graph of concepts a An undirected graph G representing the similarity matrix b The bipartite graph showing three clusters on G c The induced clusters U d The new graph G1 over clusters U e The new bipartite graph over G1 Salient features for fire and the violence cluster Discovered metaphorical associations Identified metaphorical expressions for the mappings FEELING IS FIRE and CRIME IS A DISEASE Metaphors tagged by the system in bold 1 Sources of conict in crosslingual subjectivity transfer Definitions and synonyms of the fourth sense of the noun argument the fourth sense of verb decide and the first sense of adjective free as provided by the English and Romanian WordNets for Romanian we also provide the manual translation into English Crosslingual bootstrapping Multilingual bootstrapping Macroaccuracy for crosslingual bootstrapping Fmeasure for the objective and subjective classes for crosslingual bootstrapping Macroaccuracy for multilingual bootstrapping versus crosslingual framework Fmeasure for the objective and subjective classes for multilingual bootstrapping versus crosslingual framework Sample of subjective features appearing in the top 100 discriminant attributes selected with Information Gain on the 3rd fold training data at iteration 10 The words in italics in the multilingual features represent equivalent translations in English and Romanian Structure and data preprocessing of the initial dataset and the cleaned one after preprocessing Schematic owchart of the workow we followed regarding the datasets the training techniques and the operations Number of tweets mentioning party leaders Example tweet assigned with feature scores Performance of the training algorithms supervised against semisupervised techniques The semisupervised precision is evaluated indirectly by using the predicted dataset as trainset against the humanannotated manual small dataset Ironic tweets that received every party and their election results The uctuation describes the difference between the May 2012 election results and the previous Performance measures of the training algorithms Green indicates the best performance while red the worst A portion of the local cooccurrence graph for mouse from the SemEval2010 Task 14 corpus VMeasure and paired FScore results for different partitionings of the dendrogram The dashed vertical line indicates SP D Performance results on the SemEval2010 WSI Task with rank shown in parentheses Refer ence scores of the best submitted systems are shown in the bottom Manual analysis of suggested corrections on CLC data Precision and recall for articles Precision and recall for prepositions Using different amounts of annotated training data for the article metaclassifier Using different amounts of annotated training The Stanford parser Klein and Manning 2002 is unable to recover the verbal reading of the unvocalized surface form an Table 1 Diacritized particles and pseudoverbs that after orthographic normalization have the equivalent surface form an The distinctions in the ATB are linguistically justified but complicate parsing Table 8a shows that the best model recovers SBAR at only 710 F1 Frequency distribution for sentence lengths in the WSJ sections 223 and the ATB p13 English parsing evaluations usually report results on sentences up to length 40 Arabic sentences of up to length 63 would need to be evaluated to account for the same fraction of the data We propose a limit of 70 words for Arabic parsing evaluations Gross statistics for several different treebanks Test set OOV rate is computed using the following splits ATB Chiang et al 2006 CTB6 Huang and Harper 2009 Ne gra Dubey and Keller 2003 English sections 221 train and section 23 test An ATB sample from the human evaluation The ATB annotation guidelines specify that proper nouns should be specified with a flat NP a But the city name Sharm Al Sheikh is also iDafa hence the possibility for the incorrect annotation in b Evaluation of 100 randomly sampled variation nu clei types The samples from each corpus were indepen dently evaluated The ATB has a much higher fraction of nuclei per tree and a higher typelevel error rate Dev set learning curves for sentence lengths 70 All three curves remain steep at the maximum training set size of 18818 trees Test set results Maamouri et al 2009b evaluated the Bikel parser using the same ATB split but only reported dev set results with gold POS tags for sentences of length 40 The Bikel GoldPOS configuration only supplies the gold POS tags it does not force the parser to use them We are unaware of prior results for the Stanford parser The constituent Restoring of its constructive and effective role parsed by the three different models gold segmen tation The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals Like verbs maSdar takes arguments and assigns case to its objects whereas it also demonstrates nominal characteristics by eg taking determiners and heading iDafa Fassi Fehri 1993 In the ATB 2 3 astaadah is tagged 48 times as a noun and 9 times as verbal noun Consequently all three parsers prefer the nominal reading Table 8b shows that verbal nouns are the hardest preterminal categories to identify None of the models attach the attributive adjectives correctly Dev set results for sentences of length 70 Cov erage indicates the fraction of hypotheses in which the char acter yield exactly matched the reference Each model was able to produce hypotheses for all input sentences In these experiments the input lacks segmentation markers hence the slightly different dev set baseline than in Table 6 Per category performance of the Berkeley parser on sentence lengths 70 dev set gold segmentation a Of the high frequency phrasal categories ADJP and SBAR are the hardest to parse We showed in 2 that lexical ambiguity explains the underperformance of these categories b POS tagging accuracy is lowest for maSdar verbal nouns VBGVN and adjectives eg JJ Richer tag sets have been suggested for modeling morphologically complex distinctions Diab 2007 but we find that linguistically rich tag sets do not help parsing c Coordination ambiguity is shown in dependency scores by eg S S S R and NP NP NP R NP NP PP R and NP NP ADJP R are both iDafa attachment Examples of ambiguity for the English word play together with different translations depending on the context Related research integrating context into wordbased SMT WBSMT models Related research integrating context into alternative SMT models Related research using English as source language Example of CCG supertags CCG supertags are combined under the operations of forward and backward applications into a parse tree Example of LTAG supertags LTAG supertags are combined under the operations of substitution and adjunction into a parse tree The dependency parse tree of the English sentence Can you play my favourite old record and the dependency features extracted from it for the SMT phrase play my favourite The semantic graph of an English sentence and the semantic features extracted from it for an SMT phrase Some of the possible Spanish translations of the English phrase make with their memorybased con textdependent translation probabilities rightmost column compared against contextindependent transla tion probabilities of the baseline system Weights of different loglinear features of the CCG1 system Experiments with words and partsofspeech as contextual features Experiments with dependency relations Experiments combining dependency relations words and partofspeech Distances found between phrase boundaries with linked modifier words and with parent words Experiments applying individual features in EnglishtoHindi translation Experiments applying combinations of features in EnglishtoHindi translation Experimental results on the WMT 2010 test set Experimental results on the WMT 2009 test set Results on largescale DutchtoEnglish translation Results on EnglishtoDutch translation employing homogeneous features Experimental results for largescale EnglishtoChinese translation Experimental results for largescale EnglishtoJapanese translation BLEU learning curves left and difference curves right comparing the Moses baseline against two IGTree LTAG1 and PR and TRIBL SuperPair1 and PR classifiers Average number of target phrase distribution sizes for source phrases for TRIBL and IGTree com pared to the Moses baseline BLEU difference curves of four contextinformed models using TRIBL DutchtoEnglish Learning curves lefthand side graphs and difference curves righthand side graphs comparing the Moses baseline against four contextinformed models PR OE POS2 and Word2 These curves are plotted with scores obtained using three evaluation metrics BLEU top METEOR centre and TER bottom EnglishtoDutch Learning curves lefthand side graphs and difference curves righthand side graphs comparing the Moses baseline against four contextinformed models CCG1 LTAG1 PR PSAL POS2 and Word2 These curves are plotted with scores obtained using three evaluation met rics BLEU top METEOR centre and TER bottom BiTAM models for Bilingual document and sentencepairs A node in the graph represents a random variable and a hexagon denotes a parameter Unshaded nodes are hidden variables All the plates represent replicates The outmost plate M plate represents M bilingual documentpairs while the inner N plate represents the N repeated choice of topics for each sentencepairs in the document the inner Jplate represents J wordpairs within each sentencepair a BiTAM1 samples one topic denoted by z per sentencepair b BiTAM2 utilizes the sentencelevel topics for both the translation model ie pf e z and the monolingual word distribution ie pez c BiTAM3 samples one topic per wordpair Training and Test Data Statistics Topicspecific translation lexicons are learned by a 3topic BiTAM1 The third lexicon Topic3 prefers to translate the word Korean into ChaoXian mNorth Korean The cooccurrence Cooc IBM14 and HMM only prefer to translate into HanGuo ISouth Korean The two candidate translations may both fade out in the learned translation lexicons Three most distinctive topics are displayed The English words for each topic are ranked according to pez estimated from the topicspecific English sentences weighted by dnk 33 functional words were removed to highlight the main content of each topic Topic A is about UsChina economic relationships Topic B relates to Chinese companies merging Topic C shows the sports of handicapped people performances over eight Variational EM itera tions of BiTAM1 using both the Null word and the laplace smoothing IBM1 is shown over eight EM iterations for comparison Word Alignment Accuracy Fmeasure and Machine Translation Quality for BiTAM Models comparing with IBM Models and HMMs with a training scheme of 18 h7 43 on the Treebank data listed in Table 1 For each column the highlighted alignment the best one under that model setting is picked up to further evaluate the translation quality Evaluating Word Alignment Accuracies and Machine Translation Qualities for BiTAM Models IBM Models HMMs and boosted BiTAMs using all the training data listed in Table 1 Other experimental conditions are similar to Table 4 Sample seeds used for each semantic relation and sample outputs from Espresso The number in the parentheses for each relation denotes the total number of seeds System performance on the production relation on the CHEM dataset System performance on the reaction relation on the CHEM dataset System performance on the isa relation on the TREC9 dataset System performance on the isa relation on the CHEM dataset System performance on the partof relation on the TREC9 dataset System performance on the partof relation on the CHEM dataset System performance on the succession relation on the TREC9 dataset Outline of word segmentation process Segmentation results by a pure subwordbased IOB tagging The upper numbers are of the character based and the lower ones the subwordbased Our segmentation results by the dictionary based approach for the closed test of Bakeoff 2005 very low Roov rates due to no OOV recognition applied Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of Fscore Effects of combination using the confidence measure The upper numbers and the lower numbers are of the characterbased and the subwordbased respec tively An underspecified discourse structure and its five configurations A wRTG modelling Fig 1 Runtime Comparison Feature set for the baseline pronoun res olution system structuredfeatures for the instance ihim the man Results of the syntactic structured fea tures Results using different parsers Comparison of the structured feature and the flat features extracted from parse trees Learning curves of systems with different features Feature templates used for CRF in our system performance each step of our system achieves Experimental data sets Precision at top 200 Results of 1000 sentences Precision at top 50 Results of 3000 sentences Precision at top 100 Results of 2000 sentences A framework for jointly identifying and aligning bilingual NEs Initial typesensitive ChineseEnglish NER performance NEA typeinsensitive typesensitive performance on the test set NEA typeinsensitive typesensitive performance with the same Chinese NE recognizer Wus system and different English NE recognizers NER typeinsensitive typesensitive performance of different English NE recognizers NER typeinsensitive typesensitive performance of different Chinese NE recognizers NEA typeinsensitive typesensitive performance with the same English NE recognizer Mallet system and different Chinese NE recognizers NEA typeinsensitive typesensitive performance with a different English NE recognizer and another Chinese NE recognizer Initial NE recognition typeinsensitive typesensitive performance across various domains The superiority of our joint model on three different domains indicated by typeinsensitive typesensitive performance those signicant entries are marked in comparison with baseline Comparison between a ME framework and the derived model on the same test set Distribution of various error categories typeinsensitive Distribution of Category VI error classes typeinsensitive Top four worstcase statistics of features for NE boundary errors Distribution of the NE type errors MERTW Effect of adjacent contextual nonNE bigrams on the test set Typesensitive improvement for ChineseEnglish NER Typeinsensitive improvement for ChineseEnglish NER English NE recognition on test data after semisupervised learning NE alignment on test data after semisupervised learning Syntactical Variations of activate Extraction of Raw Pattern Division of Raw Pattern into Combina tion Pattern Components EntityMainEntity Example Demonstrating Advantages of Full Parsing Features for SVM Learning of Prediction Model Results of IE Experiment Effect of Training Corpus Size 2 Effect of Training Corpus Size 1 Causes of Error for FPs Overall steps of proposed method The result of preprocessing Pseudocode to extract UW Weightmeasure of cooccurring words Pseudocode to extract UW Extracted UW and noun set Result of normalization Related noun group and sum of Bayesian Weight of cooccurring words TercomTERs of invWERoracles and in paren theses oracle BLEU scores of confusion networks gen erated with tercom and ITG alignments The best results per row are shown in bold Comparison of average perdocument ter comTER with invWER on the EVAL07 GALE Newswire NW and Weblogs WB data sets Nave FSA with duplicated paths FSA for the pattern hit a e Overgenerating FSA 4tape representation for the Hebrew word htpqdut FSRA for the pattern hit a e FSRA2 for Arabic nominative definite and indefinite nouns FSRA for a given CNF formula Participleforming combinations in German Interdigitation FSRA general Reduplication for n 4 Reduplication general case FSRA for Arabic nominative definite nouns Time comparison between FSAs and FSRAs Space comparison between FSAs and FSRAs Four of the five logically possible schemes for annotating coordination show up in humanproduced depen dency treebanks The other possibility is a reverse Melcuk scheme These treebanks also differ on other conventions With the English tree and alignment provided by a parser and aligner at test time the Chinese parser finds the correct dependencies see 6 A monolingual parsers incor rect edges are shown with dashed lines Precision and recall of direct dependency projection via onetoone links alone Adapting a parser to a new annotation style We learn to parse in a target style wide column label given some number narrow column label of supervised targetstyle training sentences As a font of additional features all training and test sentences have already been augmented with parses in some source style row label either goldstandard parses an oracle experiment or else the output of a parser trained on 18k source trees more realistic If we have 0 training sentences we simply output the sourcestyle parse But with 10 or 100 targetstyle training sentences each offdiagonal block learns to adapt mostly closing the gap with the diagonal block in the same column In the diagonal blocks source and target styles match and the QG parser degrades performance when acting as a stacked parser Test accuracy with unsupervised training methods Parser projection with target trees Using the true or 1best parse trees in the source language is equivalent to having twice as much data in the target language Note that the penalty for using automatic alignments instead of gold alignments is negligible in fact using Source text alone is often higher than Gold alignments Using gold source trees however significantly outperforms using 1best source trees Plate diagram representation of the trigram HMM The indexes i and j range over the set of tags and k ranges over the set of characters Hyperparameters have been omitted from the figure for clarity The conditioning structure of the hierarchical PYP with an embedded character language models Simulation comparing the expected table count solid lines versus the approximation under Eq 3 dashed lines for various values of a This data was gen erated from a single PYP with b 1 P0 i 14 and n 100 customers which all share the same tag WSJ performance comparing previous work to our own model The columns display the manyto1 accuracy and the V measure both averaged over 5 inde pendent runs Our model was run with the local sampler HMM the typelevel sampler 1HMM and also with the character LM 1HMMLM Also shown are results using Dirichlet Process DP priors by fixing a 0 The system abbreviations are CGS10 Christodoulopoulos et al 2010 BBDK10 BergKirkpatrick et al 2010 and GG07 Goldwater and Griffiths 2007 Starred entries denote results reported in CGS10 Sorted frequency of tags for WSJ The gold standard distribution follows a steep exponential curve while the induced model distributions are more uniform Cooccurence between frequent gold yaxis and predicted xaxis tags comparing mkcls top and PYP1HMMLM bottom Both axes are sorted in terms of frequency Darker shades indicate more frequent cooc curence and columns represent the induced tags M1 accuracy vs number of samples Manyto1 accuracy across a range of languages comparing our model with mkcls and the best published result BergKirkpatrick et al 2010 and Lee et al 2010 This data was taken from the CoNLLX shared task training sets resulting in listed corpus sizes Fine PoS tags were used for evaluation except for items marked with c which used the coarse tags For each language the systems were trained to produce the same number of tags as the gold standard Pipeline architecture for dialogue act recognition and reranking component Here the input is a list of dialogue acts with confidence scores and the output is the same list of dialogue acts but with recomputed confidence scores A dialogue act is represented as DialogueActTypeattributevalue pairs Bayesian network for probabilistic rea soning of locations variable from desc which incorporates ASR Nbest information in the vari ablefrom desc nbest and dialogue history in formation in the remaining random variables Bayesian dialogue act recognisers show ing the impact of ASR Nbest information Comparison for Head and Tail datasets Illustration of entityrelationship graphs Summary for graphs and test datasets obtained from each seed pair How to get the ordered set B t i j MRR of baseline and reinforced matrices Distribution over number of hits Matched translations over Ve and Vc Parameter setup for and Precision Recall and F1score of Engkoo Google and Ours with head and tail datasets Precision Recall and F1score of Baseline Engkoo Google and Ours over test sets Ti The first set of features in our model All of them are binary The final feature set includes two sets the set here and a set obtained by its conjunction with the verbs lemma An example parse tree for the second head word feature Accuracy and error reduction ER results in percents for our model and the MF baseline Error reduction is computed as M ODELM 100M F F Results are given for the WSJ and GENIA corpora test sets The top table is for a model receiving gold standard parses of the test data The bottom is for a model using Charniak and Johnson 2005 stateoftheart parses of the test data In the main scenario left instances were always mapped to VN classes while in the OIP one right it was possible during both training and test to map instances as not belonging to any existing class For the latter no results are displayed for polysemous verbs since each verb can be mapped both to other and to at least one class Selected morphosyntactic categories in the OLiA Reference Model Attributive demonstrative pronouns PDAT in the STTS Annotation Model Individuals for accusative and sin gular in the TIGER Annotation Model Selected morphological features in the OLiA Reference Model The STTS tags PDAT and ART their rep resentation in the Annotation Model and linking with the Reference Model Evaluation setup Confidence scores for diese in ex 1 Recall for morphological hasXY descriptions Examples of DTs and their ICDcodes The dataset shuffled and divided into 3 sets The dataset of DT ICDcode pairs A simplified version in Foma source code of the regular expressions and transducers used to bypass several sources of distortion within the DTs in order to parse variations of unseen input DTs Performance of different FS machines in terms of the percentage of unclassified entries All the classified entries were correctly classified yielding as a result a precision of 100 BLEU scores for each translation direction trained on directional condition on target and generate source and symmetrised alignments growdiagfinaland Observe that the plots are on different scales This means that results cannot directly be compared across plots Taxonomy of Chinese words used in developing MSRSeg a A Chinese sentence Slashes indicate word boundaries b An output of our word segmentation system Square brackets indicate word boundaries indicates a morpheme boundary Taxonomy of morphologically derived words MDWs in MSRSeg Words in the MSR gold test set Domainstyle distribution in the MSR test corpus Standards and corpora Evaluation measures for Chinese word segmenter Context model word classes class models and feature functions The perceptron training algorithm for Chinese word segmentation Overall architecture of MSRSeg Generative patterns of ONA where sij denotes the jth character of the ith word of ON Sun Zhou and Gao 2003 FT detection results on the MSR gold test set The All column shows the results of detecting all 10 types of factoids as described in Table 1 which amount to 6630 factoids as shown in Table 3 NWI results on HK and AS corpora NWI as postprocessor versus unified approach NW 11 identification results on PK test set NW 21 identification results on PK test set NWI results on PK and CTB corpora NWI as postprocessor versus unified approach Word internal structure and classtype transformation templates Comparison scores for PK open and CTB open Size of training data set and the adaptation results on AS open Comparison scores for HK open and AS open Methods of resolving OAs in word segmentation on the MSR test set Figure 7 a A Chinese OAS b Two sentences in the training set which contain t whose OASs have been replaced with the single tokens OAS Li et al 2003 Results of 70 highfrequency twocharacter CASs Voting indicates the accuracy of the baseline method that always chooses the more frequent case of a given CAS ME indicates the accuracy of the maximumentropy classifier VSM indicates the accuracy of the method of using VSM for disambiguation Comparison of performance of MSRSeg The versions that are trained using semisupervised iterative training with different initial training sets Rows 1 to 8 versus the version that is trained on annotated corpus of 20 million words Row 9 Precision of organization name recognition on the MSR test set using Viterbi iterative training initialized by four seed sets with different sizes Precision of location name recognition on the MSR test set using Viterbi iterative training initialized by four seed sets with different sizes Precision of person name recognition on the MSR test set using Viterbi iterative training initialized by four seed sets with different sizes MSRSeg system results for the MSR test set Comparisons against other segmenters In Column 1 SXX indicates participating sites in the 1st SIGHAN International Chinese Word Segmentation Bakeoff and CRFs indicates the word segmenter reported in Peng et al 2004 In Columns 2 to 5 entries contain the Fmeasure of each segmenter on different open runs with the best performance in bold Column SiteAvg is the average Fmeasure over the data sets on which a segmenter reported results of open runs where a bolded entry indicates the segmenter outperforms MSRSeg Column OurAvg is the average Fmeasure of MSRSeg over the same data sets where a bolded entry indicates that MSRSeg outperforms the other segmenter Crosssystem comparison results The screenshot of our webbased system shows a simple quantitative analysis of the frequency of two terms in news articles over time While in the 90s the term Friedensmission peace operation was predominant a reverse tendency can be observed since 2001 with Auslandseinsatz foreign intervention being now frequently used Overview of the complete processing chain Dependency parse of a sentence that contains indirect speech see Sentence 2 10 most used verbs lemma in indirect speech Results of a 10fold crossvalidation for various machine learning algorithms Learning Curves for Confusion Set Disambiguation Representation Size vs Training Corpus Size Complementarity Voting Among Classifiers Active Learning with Large Corpora CommitteeBased Unsupervised Learning Committee Agreement vs Accuracy Comparison of Unsupervised Learning Methods Results for the acquisition of subcategori sation frames Clustering evaluation for the experiment with Named Entities Clustering evaluation for the experiment without Named Entities ELUS Segmenter Open test in percentages Closed test in percentages Graphical model of HMBiTAM Graphical model of synonym pair gen erative process The number of vocabularies in the 10k 50k and 100k data sets Comparison of word alignment accuracy The best results are indicated in bold type The additional data set sizes are a 10k b 50k c 100k Entity detection and tracking system flow The procedure of TBL entity track ingcoreference model EDT and mention detection results Statistics of the ACE corpus Examples of transformation rules of Templates for feedback Three narrative events and the six most likely events to include in the same chain One of the 69 test documents containing 10 narrative events The protagonist is President Bush Results with varying sizes of training data Year 2003 is not explicitly shown because it has an unusually small number of documents compared to other years A narrative chain and its reverse order Results for choosing the correct ordered chain 10 means there were at least 10 pairs of ordered events in the chain An Employment Chain Dotted lines indicate incorrect before relations An automatically learned Prosecution Chain Arrows indicate the before relation Examples of zero anaphora Examples of zero anaphora Annotation statistics Results for baseline BAS system standard multiclass SVM Results for basic DAC system perclass feature optimization followed by maximum confidence based choice ER refers to error reduction in percent over standard multiclass SVM Table 2 Results for cascading minoritypreference DAC system DACCMP consult classifiers in reverse order of frequency of class ER refers to error reduction in percent over standard multiclass SVM Table 2 Results for ODP system using various sources of DA tags Posthoc analysis on the models built by the DAC system some of the top features with corresponding feature weights in parentheses for each individual tagger POS tags are capitalized BOS stands for Beginning Of Sentence a A Chinese sentence Slashes indicate word boundaries b An output of our word segmentation system Square brackets indicate word boundaries indicates a morpheme boundary Class models system results Comparison results Probability of of boundaries f10 m 3 Results of segmentation of entry titles Fscore precisionrecall Acceptance rates for a noun phrase in the course of iteration All models were with backoff mix ing BM Diffs in the course of iteration All models were with backoff mixing BM Effect of matching skip Fscore precisionrecall Features based on the token string Sources of Dictionaries The whole process of retraining the upper case NER Q signifies that the text is converted to upper case before processing Fmeasure on MUC6 and MUC7 test data Improvements in Fmeasure on MUC6 plotted against amount of selected unlabeled data used Improvements in Fmeasure on MUC7 plotted against amount of selected unlabeled data used Subtypes of the ArgM modifier tag Split constituents In this case a single semantic role label points to multiple nodes in the original treebank tree Interannotator agreement Confusion matrix among subtypes of ArgM defined in Table 1 Entries are fraction of all ArgM labels Entries are a fraction of all ArgM labels true zeros are omitted while other entries are rounded to zero Confusion matrix for argument labels with ArgM labels collapsed into one category Entries are a fraction of total annotations true zeros are omitted while other entries are rounded to zero Comparison of frames Most frequent semantic roles for each syntactic position Most frequent syntactic positions for each semantic role Semantic roles of verbs subjects for the verb classes of Merlo and Stevenson 2001 cont Semantic roles for different frame sets of kick In this example the path from the predicate ate to the argument NP He can be represented as VBjVPjSNP with j indicating upward movement in the parse tree and downward movement Backoff lattice with more specific distributions towards the top Accuracy of semanticrole prediction in percentages for known boundaries the system is given the constituents to classify Accuracy of semanticrole prediction in percentages for unknown boundaries the system must identify the correct constituents as arguments and give them the correct roles Common values in percentages for parse tree path in PropBank data using goldstandard parses Accuracy of semanticrole prediction for unknown boundaries the system must identify the correct constituents as arguments and give them the correct roles Summary of results for unknownboundary condition The directional matching relationships between a hypothesis h an entailment rule r and a text t in the Contextual Preferences framework Recall R Precision P and Mean Average Pre cision MAP when also using rules for matching Recall R Precision P and Mean Average Pre cision MAP when only matching template hypotheses directly RecallPrecision curves for ranking using a only the prior baseline b allCP c allCPpr MAP under the 50 rules All setup when adding component match scores to Precision P or prior only MAP baselines and when ranking with allCP or allCPpr methods but ignoring that component scores Properties of the manually aligned corpus Examples of output of the phrasebased and syntaxbased systems Distribution of generated paraphrases per Lev enshtein distance NIST scores per Levenshtein distance Trivial and singlefeature baselines using SVM acc unless noted otherwise Combination results using SVMacc Results by relation Unigram bigram and trigram counts of the ligature corpus Unigram bigram and trigram counts of the word corpus Results changing beam width k of the tree Lattice representation of the sentence bclm hneim Doublecircles denote token boundaries Lattice arcs correspond to different segments of the token each lattice path encodes a possible reading of the sentence Notice how the token bclm have analyses which include segments which are not directly present in the unsegmented form such as the definite article h 13 and the pronominal suffix which is expanded to the sequence fl hm of them 24 45 Parsing scores of the various systems Sample analysis of an English sentence Input Do we have to reserve rooms Resolution of ambiguity on the Verbmobil corpus Candidates for equivalence classes Training and test with hierarchical lexicon Inverse restructuring analyze and annotation all require morphosyntactic analysis of the transformed sentences Disambiguation of conventional dictionaries Learn phrases analyze and annotation require morphosyntactic analysis of the transformed sentences Training with scarce resources Restructuring learn phrases and annotation all require morphosyntactic analysis of the transformed sentences Statistics of corpora for training Verbmobil and Nespole Singletons are types occurring only once in training The official vocabularies in Verbmobil Statistics for the test sets for German to English translation Verbmobil Eval2000 Test and Develop and Nespole Conventional dictionary used to complement the training corpus Impact of corpus size measured in number of running words in the corpus on vocabulary size measured in number of different fullform words found in the corpus for the German part of the Verbmobil corpus Results for hierarchical lexicon model Nespole Restructuring entails treatment of question inversion and separated verb prefixes as well as merging of phrases in both languages The same conventional dictionary was used as in the experiments the Verbmobil The language model was trained on a combination of the English parts of the Nespole corpus and the Verbmobil corpus Examples of the effect of the hierarchical lexicon The performance on the set of unknown Experimental result of total unknown The performance on the set of unknown Speech Act Categories and Kappa values An example discussion thread Statistics for each Speech Act Category Some of the top selected features by Infor mation Gain SA classification results Thread Classification Results Example patterns in student discussion threads Results from Direct Thread Classification Two fragments of a hierarchy over word class distributions Error reduction as a function of vocabulary size Evaluation of coarsegrained POS tagging on test data Evaluation of coarsegrained POS tagging on test data Example of a socalled semiformal text where one can see that here more time points are available and that those can be complemen tary to the time points to be extracted from formal texts So already at this level a unification or merging of extracted time points is necessary Example of a German noun phrase First and last word agree in number gender and case value Syntactic features h and ld mark features from the head and the leftmost daughter dir is a binary fea ture marking the direction of the head with respect to the current token The effect of syntactic features when predicting morphological information mark statistically signifi cantly better models compared to our baseline sentence based ttest with 005 The effect of syntactic features when predicting morphology using lexicons mark statistically signifi cantly better models compared to our baseline sentence based ttest with 005 Syntactic features for featurama Czech mark statistically significantly better models compared to feat urama sentencebased ttest with 005 Agreement counts in morphological annotation compared between the baseline system and the oracle system using gold syntax Simple parser vs full parser syntactic quality Trained on first 5000 sentences of the training set Dependency between amount of training data for syntactic parser and quality of morphological prediction Impact of the improved morphology on the qual ity of the dependency parser for Czech and German Simple parser vs full parser morphological quality The parsing models were trained on the first 5000 sentences of the training data the morphological tagger was trained on the full training set Baseline Outofthebox BerkeleyParser performance on the devset Number of learned splits per POS category after five splitmerge cycles Number of learned splits per NTcategory after five splitmerge cycles A layered POS tag representation A latent layered POS tag representation The lattice for the Hebrew sequence see footnote 19 Devset results when using lattice parsing on top of an external lexiconanalyzer Devset results when incorporating an external lexicon Devset results of using the agreementfilter on top of the lexiconenhanced lattice parser parser does both segmentation and parsing Devset results of using the agreementfilter on top of the lexiconenhanced parser starting from gold segmentation Numbers of parsetree nodes in the 1best parses of the development set that triggered gender or number agreement checks and the results of these checks Testset results of the bestperforming models NP agreement violations that were caught by the agreement filter system a Nouncompound case that was correctly handled b Case involving conjunction that was correctly handled c A case where fixing the agreement violation introduces a PPattachment mistake The NLMWSD test set and some of its sub sets Note that the test set used by Joshi et al 2005 comprises the set union of the terms used by Liu et al 2004 and Leroy and Rindflesch 2005 while the com mon subset is formed from their intersection Results from WSD system applied to various sections of the NLMWSD data set using a variety of fea tures and machine learning algorithms Results from baseline and previously published approaches are included for comparison Perword performance of best reported systems The value of the penalized loss based on the number of iterations DPLVMs vs CRFs on the MSR data Details of the corpora WT represents word types CT represents character types SC represents simplied Chinese TC represents traditional Chinese Error analysis on the latent variable seg menter The errors are grouped into four types over generalization errors on named entities errors on idioms and errors from datainconsistency Example of semantic trees Two STs composing a STN ROUGE Fscores for different systems LDA Phrase pairs extracted from a document pair with an economic topic Topic words extracted from targetside doc uments Corpus statistics 4 Contribution of various caches in our cache based documentlevel SMT system Note that signific ance tests are done against Moses Contribution of combining the three caches Contribution of combining the dynamic Contribution of combining the dynamic and Contribution of employing the dynamic cache on different test documents Impact of the topic cache size Contribution of the static cache on the first sentence of each test document ie with empty dynamic cache Positive and negative examples Descriptions of the 10 corpora Average precisions over the 10 corpora of different window size 3 seeds Precision top N with 3 seeds and window size w 3 Selfbootstrapping algorithm Numbers of relations on the ACE RDC 2004 break down by relation types and subtypes Stratefied Sampling for initial seeds The initial performance of applying various sampling strategies to selecting the initial The highest performance of applying various sampling strategies in selecting the initial seed set on the ACE RDC 2004 corpus Bootstrapping time for different p values Performance for different p values Comparison of two augmentation strategies over different sampling strategies in selecting the initial seed set Comparison of semisupervised relation classification systems on the ACE RDC 2003 corpus A pruned phrase tokenization lattice Edges are tokenizations of phrases eg e5 represents tokenizing question into a word and e7 represents tokenizing doubt him into a partial word doubt followed by a word him The pseudo code of Algorithm 1 The Riv over the bakeoff2 data The Roov over the bakeoff2 data The Fscore over the bakeoff2 data Denition of NE in IREX Example of morphological analyses Case frame of haken dispatch Experimental results Fmeasure Comparison with previous work The set of types and subtypes of relations used in the 2004 ACE evaluation The Division of LDC annotated data into training and development test sets Comparing Fmeasure precision and recall of different voting schemes for English relation extraction Comparing Fmeasure precision and recall of different voting schemes for Chinese relation extraction Comparing Fmeasure precision and recall of different voting schemes for Arabic relation ex traction Comparing the best Fmeasure obtained by AtLeastN Voting with Majority Voting Summing and the single best classifier A focused entailment graph For clarity edges that can be inferred by transitivity are omitted The single strongly connected component is surrounded by a dashed line Positive and negative examples for entailment in the training set The direction of entailment is from the left template to the right template The similarity score features used to represent pairs of templates The columns specify the corpus over which the similarity score was computed the template representation the similarity measure employed and the feature representation as described in Section 41 Scenarios in which we added hard constraints to the ILP Results when tuning for performance over the development set Results when the development set is not used to estimate and K Results with prior estimated on the development set that is 01 which is equivalent to 23 Recallprecision curve comparing ILPGlobal with GreedyGlobal and ILPLocal Results per concept for the ILPGlobal Results of all distributional similarity measures when tuning K over the development set We encode the description of the measures presented in Table 2 in the following manner h healthcare corpus R RCV1 corpus b binary templates u unary templates L Lin similarity measure B BInc similarity measure pCt pair of CUI tuples representation pC pair of CUIs representation Ct CUI tuple representation C CUI representation Lin Pantel similarity lists learned by Lin and Pantel Comparing disagreements between ILPGlobal and ILPLocal against the goldstandard graphs A comparison between ILPGlobal and ILPLocal for two fragments of the testset concept seizure A comparison between ILPGlobal and ILPlocal for two fragments of the testset concept diarrhea Comparing disagreements between ILPGlobal and GreedyGlobal against the goldstandard graphs A comparison between ILPGlobal and GreedyGlobal Parts A1A3 depict the incremental progress of Greedy Global for a fragment of the headache graph Part B depicts the corresponding fragment in ILPGlobal Nodes surrounded by a bold oval shape are strongly connected components Distribution of probabilities given by the classier over all node pairs of the testset graphs Error analysis for false positives and false negatives A scenario where ILPGlobal makes a mistake but ILPLocal is correct The set of new features The last two columns denote the number and percentage of examples for which the value of the feature is nonzero in examples generated from the 23 goldstandard graphs Macroaverage recall precision and F1 on the development set and test set using the parameters that maximize F1 of the learned edges over the development set Results of feature analysis The second column denotes the proportion of manually annotated examples for which the feature value is nonzero A detailed explanation of the other columns is provided in the body of the article A hierarchical summary of propositions involving nausea as an argument such as headache is related to nausea acupuncture helps with nausea and Lorazepam treats nausea Example distributions of German verbs Data similarity measures kmeans experiment baseline and upper bound Comparing distributions on D1 and D2 Comparing similarity measures on D1 and D2 Comparing clustering initializations on D2 Comparing clustering initializations on D1 Comparing feature descriptions Comparing selectional preference frame definitions Comparing selectional preference slot definitions Varying the number of clusters evaluation Randadj Largescale clustering on D1 Largescale clustering on D3 with nnandnadnsdass Largescale clustering on D2 Illustration of Pareto Frontier Ten hypotheses are plotted by their scores in two metrics Hypotheses indicated by a circle o are paretooptimal while those indicated by a plus are not The line shows the convex hull which attains only a subset of paretooptimal points The triangle 4 is a point that is weakly paretooptimal but not paretooptimal Task characteristics sentences in TrainDev of features and metrics used Our MT models are trained with standard phrasebased Moses software Koehn and others 2007 with IBM M4 alignments 4gram SRILM lexical ordering for PubMed and distance ordering for the NIST system The decoder generates 50best lists each iteration We use SVMRank Joachims 2006 as opti mization subroutine for PRO which efficiently handle all pairwise samples without the need for sampling PubMed Results The curve represents the Pareto Frontier of all results collected after multiple runs NIST Results Average number of Pareto points Training time usage in PMOPRO Algo 2 Avg runtime per sentence of FindPareto Learning Curve on RIBES comparing single objective optimization and PMO SyntSem tagged corpus extract Optimal context size S and criteria space distribution by partofspeech of Precision P and usage proportion optimal context size using both precision with and without feature precision decrease when omitting non space distribution of most reliable target word frequency F average LMR Tagging Learning curves on the development dataset of the Beijing Univ corpus Learning curves on the development dataset of the HK City Univ corpus Official Bakeoff Outcome Fscore on development data Evaluation measures tp true positives fp false positives tn true negatives fn false negatives pr precision re recall Classification results with XLE starredness parser exceptions and zero parses Method 1 Classification results with 5gram and fre quency threshold 4 Method 2 Classification results with decision tree on XLE output Method 3 Classification results with decision tree on vectors of frequency of rarest ngrams Method 4 Classification results with decision tree on joined feature set Method 5 Accuracy by sentence length for Method 5 measured on separate grammatical and ungrammat ical data Gr Grammatical AG Agreement RW RealWord EW Extra Word MW Missing Word Sample Embedded Answer Summariser and VPA Architecture Positional sentence weight for varying Precision by Named Entity Class Recall by Named Entity Class Average Precision and Recall Utility Score Comparison Summaries Recall and Precision Fuzzy hierarchical clustering for Paraphrase Extraction Collecting paraphrases using a Paraphrase Recognizer Algorithm for Fuzzy Agglomerative Clustering based on verbs a Binary merging of clusters b Merging of multiple clusters Simplied Lesk algorithm 21 Example of fuzzy divisive clustering Algorithm for fuzzy divisive clustering based on nouns Performance of Paraphrase Recognition system on MSRPC 25 Dependency parse and triples for the sentence Mr Burke said it was a textbook landing considering the circumstances Statistics of paraphrase pairs retrieved from MSRPC Precision of existing and proposed approaches Relative recall evaluation Comparative performance on MSRPC Performance of proposed system on MSRVDC Dataset 1 Performance evaluation of proposed and existing systems Performance of existing approaches on MSRVDC Dataset 1 Performance evaluation on MSRVDC Dataset 2 Training and test set sources genres sizes in terms of numbers of tokens and unigram and bi gram coverage of the training set on the test sets Learning curves of word prediction accu racies of IGT REE trained on TRAIN REUTERS and tested on REUTERS ALICE and BROWN Word prediction speed in terms of the number of classified test examples per second mea sured on the three test sets with increasing training examples Both axes have a logarithmic scale Learning curves in terms of word predic tion accuracy on deciding between the confusable pair there their and theyre by IGT REE trained on TRAIN REUTERS and tested on REUTERS AL ICE and BROWN The top graphs are accuracies at tained by the confusable expert the bottom graphs are attained by the allwords predictor trained on TRAIN REUTERS until 130 million examples and on TRAIN NYT beyond marked by the vertical bar Disambiguation scores on nine confusable set attained by confusable experts trained on ex amples extracted from 1 billion words of text from TRAIN REUTERS plus TRAIN NYT on the three test sets Disambiguation scores on nine confusable set attained by the allwords prediction classifier trained on 30 million examples of TRAIN REUTERS and by confusable experts on the same training set The second column displays the number of exam ples of each confusable set in the 30million word training set the list is ordered on this column Relation Feature Spaces of the Example Sentence to stop the merger of an estimated Performance of seven relation feature spaces over the 5 ACE major types using parse tree information only Performance comparison the numbers in parentheses report the performance over the 24 ACE subtypes while the numbers outside paren theses is for the 5 ACE major types Error Distribution Kleene RegularExpression Assignment Examples Four synchronous rules with topic distributions Each subgraph shows a rule with its topic distribution where the Xaxis means topic index and the Yaxis means the topic probability Notably the rule b and rule c shares the same source Chinese string but they have different topic distributions due to the different English translations Example of topictotopic correspondence The last line shows the correspondence probability Each col umn means a topic represented by its top10 topical word s The first column is a targetside topic while the rest three columns are sourceside topics Result of our topic similarity model in terms of BLEU and speed words per second comparing with the traditional hierarchical system Baseline and the topicspecific lexicon translation method TopicLex SimSrc and SimTgt denote similarity by sourceside and targetside ruledistribution respectively while SimSen acti vates the two similarity and two sensitivity features Avg is the average B LEU score on the two test sets Scores marked in bold mean significantly Koehn 2004 better than Baseline p 001 Effects of onetoone and onetomany topic pro jection Distribution of antecedent NP types in the otheranaphora data set Overview of the results for all baselines for otheranaphora Descriptive statistics for WordNet hypsyn relations for otheranaphora Patterns and instantiations for otheranaphora Descriptive statistics for Web scores and BNC scores for otheranaphora Properties of the variations for the corpusbased algorithms for otheranaphora Web results for otheranaphora BNC results for otheranaphora Overview of the results for the best algorithms for otheranaphora Occurrences of error types for the best otheranaphora algorithm algoWebv4 Distribution of antecedent NP types for definite NP anaphora Overview of the results for all baselines for coreference Descriptive statistics for WordNet hypsyn relations on the coreference data set Overview of the results for all WordNet algorithms for coreference Overview of the results for all Web algorithms for coreference Overview of the results for all BNC algorithms for coreference Occurrences of error types for the best coreference algorithm algoWebv4n Word segmentation on NIST data sets Example of 1ton word alignments be tween English words and Chinese characters Word segmentation on IWSLT data sets Example of a word lattice BS on IWSLT 2007 task BS on IWSLT 2006 task Corpus statistics for Chinese Zh character segmentation and English En BS on NIST task Scalability of BS on NIST task Vocabulary size of NIST task 40K Scaleup to 160K on IWSLT data sets Vocabulary size of IWSLT task 40K The search graph on development set of IWSLT task BS on IWSLT data sets using MTTK Illustration of similarities in POS tag statistics across languages a The unigram frequency statistics on five tags for two close languages English and German b Sample sentences in English and German Verbs are shown in blue prepositions in red and noun phrases in green It can be seen that noun phrases follow prepositions An iterative algorithm for minimizing our ob jective in Eq 7 For simplicity we assume that all the weights i and are equal to one It can be shown that the objective monotonically decreases in every iteration The set of typological features that we use for source language selection The first column gives the ID of the feature as listed in WALS The second column describes the feature and the last column enumerates the allowable values for each feature besides these values each feature can also have a value of No dominant order Directed dependency accuracy of our model and the baselines using gold POS tags for the target language The first section of the table is for the direct transfer of the MST parser McDonald et al 2011 The second section is for the weighted mixture parsing model Cohen et al 2011 The first two columns Random and Greedy of each section present the parsing performance with a random or a greedy mapping The third column Petrov shows the results when the mapping of Petrov et al 2011 is used The fourth column Model shows the results when our mapping is used and the fifth column in the first section Best Pair shows the performance of our model when the best source language is selected for every target language The last column Tag Diff presents the difference between our mapping and the mapping of Petrov et al 2011 by showing the percentage of target language tokens for which the two mappings select a different universal tag Objective values for the different mappings used in our experiments for four languages Note that the goal of the optimization procedure is to minimize the objective value Word alignment based translation model PJ AE IBM Model 4 Example of word alignment Example of chunkbased alignment Chunkbased translation model The words in bold are head words Basic Travel Expression Corpus Experimental results for JapaneseEnglish Examples of viterbi chunking and chunk alignment for EnglishtoJapanese translation model Chunks are bracketed and the words with to the left are head words Opinion PageRank Opinion HITS model Opinion Question Answering System Sentiment lexicon description Opinion PageRank Performance with varying parameter 02 Sentiment Lexicon Performance Opinion PageRank Performance with varying parameter 05 Questionspecific popular topic words and opinion words generated by Opinion HITS Opinion HITS Performance with vary ing parameter Comparison results with TAC 2008 Three Top Ranked Systems system 13 demonstrate top 3 systems in TAC Screenshot of ConAno Overview of annotation environment An example of English Chinese and French terms consisting of the same morphemes Example of first and second order features using a predefined ngram size of 2 Example of a term construction rule as a branch in a decision tree FScore of the RF and SVM GIZA and Levenshtein distancebased classifier on the first order dataset Best observed performance of RF SVM and GIZA and Levenshtein Distance FScore of the RF and SVM GIZA and Levenshtein distancebased classifier on the second order dataset Correlation between cohesiondriven functions 1 OBI vs BI where the lost of F 1 such as SCB is caused by incorrect English segments that will be discussed in the section 4 Baseline vs Submitted Results An example of annotation projection for relation detection of a bitext in English and Korean Numbers of projected instances Experimental Results Possible interpretations for the text wAlY Habash and Rambow 2005 Syntactic dependency scheme used in this work Labels that arent selfexplanatory or similar to the labels used by Tratz and Hovy 2011 for English or CATiB for Arabic Habash and Roth 2009 are in bold for completely new relations or italics for similarly named but semantically different relations Counts of the number of files sentences Sent original spacedelimited tokens Tok ATB tree tokens Tree Toks and affixes in the experimental data Counts for the POS tags mentioned in Table 5 Top 10 POS mistakes made more often by either the CTFTM with parsing or the CTFTM without on the ATB part 1 2 and 3 development set Results for the various experiments Exp for both the development and test portions of the data including per token clitic separation tokenization accuracy partofspeech tagging F1 affix boundary detection F1 affix labeling F1 and both unlabeled and labeled attachment scores Gibbs sampling for word alignment Performance of Final Translation BLEU4 Performance of Word Alignment Distribution of annotated data Results of UniGraph BiGraph and Bi Graph Results of the gloss classifier Comparison to onlySL and onlyGraph Results of an iterative approach Accuracy and frequency of the top 5 for each iteration Part of a sample headline cluster with subclusters Examples of correct above and incorrect below alignments Precision and Recall for both methods Chinese parse tree with empty elements marked The meaning of the sentence is Implementation of the law is temporarily suspended English parse tree with empty elements marked a As annotated in the Penn Treebank b With empty elements recongured and slash categories added Recall on different types of empty categories YX Yang and Xue 2010 Ours split 6 Results on Penn Chinese Treebank Results on Penn English Treebank Wall Street Journal sentences with 100 words or fewer Length distribution of entities in the train ing set of the shared task in 2004 JNLPBA Modification of O other labels to transfer information on a preceding named entity The framework of our system We first enumerate all possible candidate states and then filter out low probability states by using a lightweight classifier and represent them by using feature forest Features used in the naive Bayes Classi fier for the entity candidate ws ws1 we spi is the result of shallow parsing at wi Example of feature forest representation of linear chain CRFs Feature functions are as signed to and nodes Example of packed representation of semiCRFs The states that have the same end po sition and preventity label are packed Filtering results using the naive Bayes classifier The number of entity candidates for the training set was 4179662 and that of the develop ment set was 418628 Feature templates used for the chunk s ws ws1 we where ws and we represent the words at the beginning and ending of the target chunk respectively pi is the part of speech tag of wi and sci is the shallow parse result of wi Comparison with other systems Performance with filtering on the development data 10 1012 means the threshold probability of the filtering is 10 1012 Performance of our system on the evalu ation set Overall performance on the evaluation set L is the upper bound of the length of possible chunks in semiCRFs Basic Statistics of DUC2007 Update Data Set System Comparison Experiment Results Rules and patterns for the four syntacticosemantic structures Regular expression notations matches the preceding element zero or more times matches the preceding element one or more times indicates that the preceding element is optional indicates or Abbreviations Ec m coarsegrained entity type of mention m Ld labels in dependency path between the headword of two mentions We use square brackets and to denote mention boundaries The in the Formulaic row denotes the occurrence of a lexical in text Additional RE features Microaveraged across the 5 folds RE results using predicted mentions Microaveraged across the 5 folds RE results using gold mentions Recall and precision of the patterns Improvement in predicted mention RE Improvement in gold mention RE Bahktins characterization of dialogue Bahktin 1986 describes a discourse along the three major properties style situation and topic Current information retrieval systems focus on the topical as pect which might be crucial in written documents Furthermore since throughout text analysis is still a hard problem information retrieval has mostly used keywords to characterize topic Many features that could be extracted are therefore ignored in a tradi tional keyword based approach Information access hierarchy Oral com munications take place in very different formats and the first step in the search is to determine the database or subdatabase of the rejoinder The next step is to find the specific rejoinder Since re joinders can be very long the rejoinder has to seg mented and a segment has to be selected Distribution of activity types Both databases contain a lot of discussing informing and storytelling activities however the meeting data contains a lot more planning and advising Intercoder agreement for activities The meeting dialogues and Santa Barbara corpus have been annotated by a seminaive coder and the first author of the paper The coefficient is determined as in Carletta et al 1997 and mutual information measures how much one label informs the other see Sec 3 For CallHome Spanish 3 dialogues were coded for activities by two coders and the result seems to indicate that the task was easier Activity detection Activities are detected on the Santa Barbara Corpus SBC and the meet ing database meet either without clustering the activities all or clustering them according to their interactivity interactive see Sec 2 for details TV show types The distribution of show types in a large database of TV shows 1067 shows that has been recorded over the period of a couple of months until April 2000 in Pittsburgh PA Detection accuracy summary The detec tion of highlevel genre as exemplified by the differ entiation of corpora can be done with high accuracy using simple features Ries 1999 Similar it was fairly easy to discriminate between male and female speakers on Switchboard Ries 1999 Discrimi nating between subgenre such as TVshow types Sec 4 can be done with reasonable accuracy How ever it is a lot harder to discriminate between ac tivities within one conversation for personal phone calls CallHome Ries et al 2000 or for general rejoinders Santa and meetings Sec 2 Show type detection Using the neural net work described in Sec 2 the show type was detected If there is a number in the word column the word feature is being used The number indicates how many wordpart of speech pairs are in the vocabu lary additionally to the parts of speech Crossdomain B3 Bagga and Baldwin 1998 results for Reconcile with its general feature set The Paired Permutation test Pesarin 2001 was used for statistical significance testing and gray cells represent results that are not significantly different from the best result B3 results for baselines and lexicalized feature sets across four domains B3 results for baselines and lexicalized feature sets on the broadcoverage ACE 2004 data set Crossdomain B3 and MUC results for Reconcile and Sieve with lexical features Gray cells represent results that are not significantly different from the best results in the column at the 005 plevel System Architecture Automatic Evaluations Biography Text Evaluations Weather Text Evaluations Weather Sentence Evaluations Biography Sentence Evaluations Structure of a threepass machine translation system with the new regeneration pass The original Nbest translations list N best1 is expanded to generate a new Nbest translations list Nbest2 before the rescoring pass Structure of a typical twopass ma chine translation system Nbest translations are generated by the decoder and the 1best transla tion is returned after rescored with additional feature functions Example of original hypotheses and 3 grams collected from them New generated hypotheses through n gram expansion and one reference Expanding a partial hypothesis via a matching ngram Example of creating a confusion net work from the word alignments and new hy potheses generated through the confusion net work The sentence in bold is the alignment ref erence Statistics of training development and test data for NIST task Statistics of training development and test data for IWSLT task Translations output by system RESC2 and COMB on IWSLT task caseinsensitive Translation performances BLEU and NIST scores of NIST task decoder 1best rescoring on original 2400 Nbest RESC1 and 4000 Nbest hypotheses RESC2 redecoding RD ngram expansion NE confusion network CN and combination of all hypotheses COMB Number of translations generated by each method in the final translation output of system COMB decoder Orig redecoding RD ngram expansion NE and confusion network CN Tot is the size of the devtest set Comparison of our system with the bestreported systems on MUC6 and MUC7 Similarity graph after its sparsification Data about our evaluation corpora Evaluation of topic segmentation for the French corpus Pk and WD as percentages Evaluation of topic identification Evaluation of topic segmentation for the English corpus Pk and WD as percentages Counts of different misspellings of Albert Einsteins name in a web query log Modified Viterbi search stopword treatment Example of trellis of the modified Viterbi search Accuracy and recall as functions of the number of monthly query logs used to train the language model Accuracy of the proposed system on a set which contains misspelled queries that the users had reformulated Accuracy of various instantiations of the system A graphical representation of the HMM ap proach for speaker role labeling This is a simple first order HMM Automatic role labeling results using the HMM and Maxent classifiers Impact of role sequence information on the HMM and Maxent classifiers The combination results of the HMM and Maxent are also provided Plate diagram of the basic model with a single feature per token the observed variable f M Z and nj are the number of word types syntactic classes z and features tokens per word type respectively Plate diagram of the extended model with T kinds of tokenlevel features f t variables and a single kind of typelevel feature morphology m Vmeasure VM and manytoone M1 results on the languages in the MULTEXTEast corpus using the gold standard number of classes shown in Table 4 BASE results use 1word context features alone or with morphology ALIGNMENTS adds alignment features reporting the average score across all possible choices of paired language and the scores under the best performing paired language in parens alone or with morphology features Final results on 25 corpora in 20 languages with the number of induced classes equal to the number of gold standard tags in all cases kmeans and SVD2 models could not produce a clustering in the Czech CoNLL corpus due its size Best published results are from Christodoulopoulos et al 2010 BergKirkpatrick et al 2010 and Lee et al 2010 The latter two papers do not report VM scores No best published results are shown for the MULTEXT languages Christodoulopoulos et al 2010 report results based on 45 tags suggesting that clark performs best on these corpora Examples of sentences x domainindependent underspecified logical forms l0 fully specified logical forms y and answers a drawn from the Freebase domain A sample CCG parse Definition of the operations used to transform the structure of the underspecified logical form l0 to match the ontology O The function typec calculates a constant cs type The function freevlf returns the set of variables that are free in lf not bound by a lambda term or quantifier The function subexpslf generates the set of all subexpressions of the lambda calculus expression lf Example derivation for the query how many people visit the public library of new york annu ally Underspecified constants are labelled with the words from the query that they are associated with for readability Constants from O written in typeset are introduced in step c Parameter estimation from QA pairs GeoQuery Results Ablation Results Results on the FQ dataset Example error cases with associated frequencies illustrating system output and gold standard references 5 of the cases were miscellaneous or otherwise difficult to categorize A second example of disagreement in segmentation guidelines Examples of disagreement in segmentation guidelines Analysis of results of segmentation on LDC training and test data for all CWS schemes BLEU scores for CWS schemes Correlation between Fscore and BLEU Feature blending of translation models Feature interpolation of translation models AICTCLAS Bdicthybrid CdictPKULDC DdictCITYU ECRFAS Morphological AnalysisGeneration as a Relation between Analyses and Words Aymara utamankapxasamachiwa it appears that they are in your house Inuktitut Parimunngaujumaniralauqsimanngittunga I never said I wanted to go to Paris Multiple Analyses for suis Compilation of a Regular Expression into an fst that Maps between Two Regular Languages Creation of a Lexical Transducer A Path in a Transducer for English After the Application of Compile Replace Networks Illustrating Steps 2 and 3 of the CompileReplace Algorithm A Network with a RegularExpression Substring on the Lower Side Two Paths in the Initial Malay Transducer Defined via Concatenation The Malay fst After the Application of CompileReplace to the LowerSide Language A Template Network and Two Filler Networks Intermediate Result Initial paths After Applying CompileReplace to the Lower Side A translation forest which is the running example throughout this paper The reference translation is the gunman was killed by the police 1 Solid hyperedges denote a reference derivation tree t1 which exactly yields the reference translation 2 Replacing e3 in t1 with e4 results a competing nonreference derivation t2 which fails to swap the order of X34 3 Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 Generally this is done by deleting a node X01 Lexicalize and generalize operators over t1 part in Figure 1 Although here only shows the nodes we also need to change relative edges actually 1 Applying lexicalize operator on the nonterminal node X01 in a results a new derivation shown in b 2 When visiting bei in b the generalize operator changes the derivation into c Corpus statistics of Chinese side where Sent Avg Lon and Len are short for sentence longest average and length respectively RT RAIN denotes the reachable given rule table without added rules subset of T RAIN data Effect of our method comparing with MERT and perceptron in terms of B LEU We also compare our fast generation method with different data only reachable or full data Data is the size of data for training the feature weights means significantly Koehn 2004 better than MERT p 001 Common grammatical relations of Minipar involving nouns The top 20 most similar words for country and their ranks in the similarity list of LIN followed by the next four words in the similarity list that were judged as entailing at least in one direction The top 10 ranked features for country produced by MI the weighting function employed in the LIN method Lexical entailment precision values for topn similar words by the Bootstrapped LIN and the original LIN method Percentage of correct entailments within the top 40 candidate pairs of each of the methods LIN and Bootstrapped LIN denoted as LINB in the gure when using varying numbers of topranked features in the feature vector The value of All corresponds to the full size of vectors and is typically in the range of 300400 features Comparative precision values for the top 20 similarity lists of the three selected similarity measures with MI and Bootstrapped feature weighting for each Top 30 features of town by bootstrapped weighting based on LIN WJ and COS as initial similarities The three sets of words are almost identical with relatively minor ranking differences Top 10 features of country by the Bootstrapped feature weighting LIN MI weighting The top 10 common features for countrystate and countryparty along with their corresponding ranks in each of the two feature vectors The features are sorted by the sum of their feature weights with both words Top 20 most similar words for country and their ranks in the similarity list by the Bootstrapped LIN measure Bootstrapped weighting top 10 common features for countrystate and countryparty along with their corresponding ranks in the two sorted feature vectors Comparison between the acfrratio for MI and Bootstrapped LIN methods when using varying numbers of common topranked features in the words feature vectors The comparative error rates of the pseudodisambiguation task for the three examined similarity measures with and without applying the bootstrapped weighting for each of them gooda15 gloss and examples gooda15 SentiWN scores Jumping POS in WordNet Total number of synsets classified by sentiment Results for Positive and Negative Classes Accuracy Trends on MicroWnOp Corpus Measuring Accuracy Tradeoff between Margin Threshold and name recognition performance Accuracy of obscure name recognition Coreference factors for name recognition System Flow Results for Mutiple Document System with additional retrieved texts Contributions of features Results for Mutiple Document System Results for Single Document System Results with Coref Rules Alone Baseline Name Tagger Comparison with voted cache Statistical results Evaluation results Wrong assignment due to missing sense from the Hound of the Baskervilles Ch 14 Sample Minipar parse and extracted gram matical function features Experiment 1 Results for label unknown sense WSD confidence level approach confi dence threshold std dev Outlier detection by comparing distances between nearest neighbors Experiment 2 Results for label unknown sense NNbased outlier detection 10 stan dard deviation Experiment 2 Results by training set size 10 Experiment 3 Results for label unknown sense NNbased outlier detection 10 stan dard deviation Extending training sets an example Acceptance radius of an outlier within the training set left and a more normal training set object right Experiment 3 Results by training set size 10 Experiments 2 and 3 Results by the num ber of senses of a lemma condition All 10 Example entries for the Transfer of a Message levels 1 and 2 classes Clusters for transitive unaccusative and ditransitive Outcome of clustering procedure Logical form graphs aligned with sur face forms in two languages Logical form graph Encoding local word order Graph for a neoDavidsonian structure DRS and corresponding DRG in tuples and in graph format for A customer did not pay From DRS to DRG labelling Wordaligned DRG for A customer did not pay All alignment information including surface tuples is highlighted Surface composition of embedded structures Wordaligned DRG for the sentence Michelle thinks that Obama smokes Analysis of NP coordination in a distributive left and a collective interpretation right Average ratings and Pearson correlation for rules from the personal stories corpus Lower ratings are better see Fig 2 Instructions for judging of unsharpened factoids Average Precision Recall and F1 at dif ferent top K rule cutoff points Distribution of reasons for false negatives missed argument mentions by BInc at K20 Distribution of reasons for false positives incorrect argument extractions by BInc at K20 Rule type distribution of a sample of 200 rules that extracted incorrect mentions The corre sponding numbers of incorrect mentions extracted by the sampled rules is shown in parentheses The Maytag interface Translation extraction from comparable corpora using crosslingual WSI and WSD Entries from the EnglishSlovene sense cluster inventory Disambiguation results Comparison of different configurations Results of the experiment GC examples Devtest Set Statistics by Language Fmeasure Breakdown by Mention Type NAMe NOMinal PREmodifier and PROnoun Chinese data does not have the PRE type Impact of Syntactic Features on English Sys tem After Taking out Distance Features Numbers are Fmeasures An example where syntactic features help to link the PRO mention hm with its antecedent the NAM Distribution of Pronoun Mentions and Fre quency of ccommand Features Summary Results on the 2004 ACE Evaluation Data A Portion of the Syntactic Tree Ungrammatical Arabic output of Google Trans late for the English input The car goes quickly The subject should agree with the verb in both gender and number but the verb has masculine inflection For clarity the Arabic tokens are arranged lefttoright Segmentation and tagging of the Arabic token AEJ JJ and they will write it This token has four seg ments with conflicting grammatical features For example the number feature is singular for the pronominal object and plural for the verb Our model segments the raw to ken tags each segment with a morphosyntactic class eg PronFemSg and then scores the class sequences Notation used in this paper The convention eIi indicates a subsequence of a length I sequence Breadthfirst beam search algorithm of Och and Ney 2004 Typically a hypothesis stack H is maintained for each unique source coverage set Procedure for scoring agreement for each hy pothesis generated during the search algorithm of Fig 4 In the extended hypothesis eI1 the index n 1 indicates the start of the new attachment Intrinsic evaluation accuracy development set for Arabic segmentation and tagging Translation quality results BLEU4 for newswire nw sets Avg is the weighted averaged by number of sentences of the individual test set gains All improvements are statistically significant at p 001 An example of NE and nonNE Possibility combination of neighboring tokens within the corpus for PER A NE detection window 2 All possible NEs identified in a test article c Unification When the tokens of two NEs are NEs after agentsbased modification Results of MET2 under different configurations Weights learned for discount features Nega tive weights indicate bonuses positive weights indicate penalties Adding new features with MIRA significantly improves translation accuracy Scores are caseinsensitive IBM B scores or significantly better than MERT baseline p 005 or 001 respectively Weights learned for inserting target English words with rules that lack Chinese words Weights learned for employing rules whose En glish sides are rooted at particular syntactic categories Weights learned for generating syntactic nodes of various types anywhere in the English translation Using over 10000 wordcontext features leads to overfitting but its detrimental effects are modest Scores on the tuning set were obtained from the 1best output of the online learning algorithm whereas scores on the test set were obtained using averaged weights Weights learned for wordcontext features which fire when English word e is generated aligned to Chinese word f with Chinese word f1 to the left or f1 to the right Glosses for Chinese words are not part of features Improved syntaxbased translations due to MIRAtrained weights Features based on the token string Sources of Dictionaries Comparison of results for MUC6 Training Data Fmeasure after successive addition of each global feature group Comparison of a confusion network and a lat tice An example of alignment units Different cases of null insertion A toy instance of lattice construction Results on the MT02 and MT05 test sets Results on the MT06 and MT08 test sets Effect of dictionary scale A real translation example Effect of semantic alignments Priority Order for Second Person ADs Priority Order for Third Person ADs Priority Order for First Person ADs Basic Features for CRFbased Segmenter Performance of our system in the compe tition Effectiveness of postprocessing rules Overview of the WEBRE algorithm Illustrated with examples sampled from experiment results The tables and rec tangles with a database sign show knowledge sources shaded rectangles show the 2 phases and the dotted shapes show the sys tem output a set of Type A relations and a set of Type B relations The orange arrows denote resources used in phase 1 and the green arrows show the resources used in phase 2 Pairwise precisionrecallF1 of WEBRE and SNE Distribution of Class Labels in the WSJ Section of the Penn TreeBank Accuracies for Different Context Types and Sizes Accuracies for WordExtraction Us Litkowski and Hargraves 2007 selected exam ing MALT Parser or Heuristics ples based on a search for governors8 most anno Accuracies for LeaveOne Out LOO and OnlyOne WordExtractionRule Evaluation none includes all words and serves for comparison Important words reduce accuracy for LOO but rank high when used as only rule Accuracies for Coarse and FineGrained PSD Using MALT and Heuristics Sorted by preposition Precision Recall and F1 Results for CoarseGrained Classification Comparison to OHara and Wiebe 2009 Classes ordered by frequency Graphical model representing M L SLDA Shaded nodes represent observations plates denote repli cation and lines show probabilistic dependencies Two methods for constructing multilingual distributions over words On the left paths to the German word wunsch in GermaNet are shown On the right paths to the English word room are shown Both English and German words are shown some internal nodes in GermaNet have been omitted for space represented by dashed lines Note that different senses are denoted by different internal paths and that internal paths are distinct from the perlanguage expression Topics along with associated regression coefficient from a learned 25topic model on GermanEnglish left and GermanChinese right documents Notice that themerelated topics have regression parameter near zero topics discussing the number of pages have negative regression parameters topics with good great hao good and uberzeugt convinced have positive regression parameters For the GermanChinese corpus note the presence of gut good in one of the negative sentiment topics showing the difficulty of learning collocations Terminologies Illustration of the paraphrase degree calculation Illustration of the coordinate degree calculation Another example of some discovered paraphrases Performance of our method for paraphrase acquisition An example of some discovered paraphrases Input sentences Overall results by Vieira and Poesio Discoursenew prediction results by Bean and Riloff Evaluation of the three anaphoric resolvers discussed by Ng and Cardie Results of Uryupinas discourse new clas sifier Results of Uryupinas uniqueness classifier Using an oracle Evaluation of the GUITAR system without DN detection off raw text Evaluation of the GUITAR system without DN detection over a handannotated treebank Relation types and subtypes in the ACE training data Contribution of different features over 43 relation subtypes in the test data Distribution of errors Comparison of our system with other bestreported systems on the ACE corpus Performance of different relation types and major subtypes in the test data Distribution of relations over words and other mentions in between in the training data Feature templates for POS tagging wi is the ith word in the sentence ti is its POS tag For a word w cj w is its j th character cj w is the last j th character and lw is its length Parse tree binarization Unary rule normalization Nonterminalyield unary chains are collapsed to single unary rules Identity unary rules are added to spans that have no unary rule Feature templates for parsing where X can be word first and last character of word first and last character bigram of word POS tag Xla Xra denotes the firstlast ath X in the span while Xla Xra denotes the ath X leftright to span Xm is the first X of right child and Xm1 is the last X of the left child len lenl lenr denote the length of the span left child and right child respectively wl is the length of word ROOTLEAF means the template can only generate the features for the rootinitial span Complexity Analysis of Algorithm 1 Boundary information is added to states to cal culate the bracket scores in the face of word segmentation errors Left the original parse tree Right the converted parse tree The numbers in the brackets are the indices of the character boundaries based on word segmentation Training development and test data of CTB 5 Results for the joint word segmentation and POS tagging task Word segmentation results Parameters used in our system Results for the joint segmentation tagging and parsing task using pipeline and joint models POS tagging error patterns means the error number of the corresponding pattern made by the pipeline tagging model and mean the error number reduced or increased by the joint model Summary of the results obtained by our algorithm in comparison to Word 2007 Results of the fillintheblank exercise Results of the inflection exercise Solution of the multiple choice exercise Replication of the experiment with a corpus of nonnative speakers CEDEL2 Lozano 2009 The number of OAS types CAS types LUW types and EIW types for our CWS The differences of Fmeasure and ROOV between nearby steps of our CWS The scored results of our CWS in the MSRC track OOV is 0034 for 3rd bakeoff The Fmeasure improvement between the BMMbased CWS and it with WSM in the MSRC track OOV is 0034 using a b and c system dictionary Subparts and features extracts from the Akkadian project Results of the baseline model best guess Word distribution in the extended Cilin Results of the baseline model best 5 guesses Results of combining the charactercategory association and rulebased models best guess Results of the charactercategory association model best guess Results of the charactercategory association model best 5 guesses Results of the rulebased model best guess Results of the corpusbased model on words with different frequency Results of the corpusbased model Parameter settings of the corpusbased model Results of the combined model for classify ing unknown words into major and medium catego ries best guess Performance of the statistical approach using a trigram model based on Google Web1T Influence of the ngram model on the perfor mance of the statistical approach Performance of the knowledgebased ap proach using the JiangConrath semantic relatedness measure Performance of knowledgebased approach using different relatedness measures Results obtained by a combination of the best statistical and knowledgebased configuration Best Single is the best precision or recall obtained by a sin gle measure Union merges the detections of both approaches Intersection only detects an error if both methods agree on a detection Reordering for the German verbgroup DP algorithm for statistical machine translation Coverage set hypothesis extensions for the IBM reordering Multireference word error rate mWER Example Translations for the Verbmobil task Statistics about the results of our word sense discovery algorithm Average precision of discovered senses for English in relation with WordNet Senses found by our algorithm from first order cooccurrences LM1 and LAT1 Examples of learned pronoun probabilities EM input for our example sentence jvalues follow each lexical candidate Comparison to SVM Weights set by maximum entropy Accuracy for all cases all excluding sen tences with quotes and only sentences with quotes Examples of nonphonetic translations Dissimilarity of temporal distributions of WTO in English and Chinese corpora Framework overview Evidence cardinality in the corpora Network of relations Edges indicate that the relations have a nonempty support inter section and edge labels show the size of the inter section Relation clusters and a few individual relations Edge labels show the size of the inter section MRR with decreasing comparability Example translations from the different methods Boldface indicates correct translations Evaluation results of the methods Example of similar document pairs Automatically generated training set examples Impact of scaling techinques ILP ILPscale microaverage F1 and AUC for the algorithms Precisionrecall curve for the algorithms A pair of comparable nonparallel documents A pair of comparable sentences A Parallel Fragment Extraction System Translated fragments according to the lexicon Our approach for detecting parallel fragments The lower part of the figure shows the source and target sentence together with their alignment Above are displayed the initial signal and the filtered signal The circles indicate which fragments of the target sentence are selected by the procedure Sizes of the extracted datasets Sizes of our comparable corpora SMT performance results Language families in our data set The Other category includes 9 language isolates and 21 language family singletons Average accuracy for EM baseline and model variants across 503 languages First panel results on all languages Second panel results for 30 isolate and singleton languages Third panel results for 27 nonLatin alphabet languages Cyril lic and Greek Standard Deviations across lan guages are about 2 Plurality language families across 20 clusters The columns indicate portion of lan guages in the plurality family number of lan guages and entropy over families Inferred Dirichlet transition hyperparameters for bigram CLUST on threeway classification task with four latent clusters Row gives starting state column gives target state Size of red blobs are proportional to magnitude of corresponding hyperparameters Incremental evaluations by incrementally adding new features word features and high dimensional edge features new word detection and ADF training replacing SGD training with ADF training Number of passes is decided by empirical convergence of the training methods Fscore curves on the MSR CU and PKU datasets ADF learning vs SGD and LBFGS training methods Comparing our method with the stateoftheart CWS systems Test corpora details Evaluation closed results on all data sets Evaluation open results on all test sets Comparison our closed results with the top three in all test sets Architecture of the translation approach based on Bayes decision rule Some training events for the English word which The symbol is the placeholder of the English word which in the English context In the German part the placeholder corresponds to the word aligned to which in the first example the German word die the word das in the second and the word was in the third The considered English and German contexts are separated by the double bar p The last number in the rightmost position is the number of occurrences of the event in the whole corpus Meaning of different feature categories where s represents a specific target word and t repre sents a specific source word The 10 most important features and their respective category and values for the English word which f Number of features used according to different cutoff threshold In the second column of the table are shown the number of features used when only the English context is considered The third column correspond to English German and WordClasses contexts Corpus characteristics for translation task Training and Test perplexities us ing different contextual information and different thresholds The reference perplexities obtained with the basic translation model 5 are TrainPP 1038 and TestPP 1322 Corpus characteristics for perplexity quality experiments Preliminary translation results for the Verbmobil Test147 for different contextual infor mation and different thresholds using the top10 translations The baseline translation results for model 4 are WER5480 and PER4307 Four examples showing the translation obtained with the Model 4 and the ME model for a given German source sentence Monolingual and Crosslingual Baseline Slot Filling Pipelines Baseline Pipeline Results Distribution of Spurious Errors Validation Features for Crosslingual Slot Filling Using Basic Features to Filter Answers Fact vs Statistical CrossDoc Features The count of the types of anaphora per corpus Bibliome system scores at Bacteria Biotope Task in BioNLP shared tasks 2011 Etrees and Derivation Trees for 2abc Data examined by the two systems for the ATB A correct tree tree1 and an incorrect tree tree2 for BCLM HNEIM indexed by terminal boundaries Erroneous nodes in the parse hypothesis are marked in italics Missing nodes from the hypothesis are marked in bold The morphological segmentation possibilities of BCLM HNEIM Doublecircles are word boundaries Architecture of the translation approach based on Bayes decision rule Candidates for equivalence classes Corpus statistics Verbmobil training Singletons are types occurring only once in train ing Statistics of the Verbmobil test corpus for GermantoEnglish translation Unknowns are word forms not contained in the training corpus Effect of the introduction of equivalence classes For the baseline we used the original in flected word forms Examples for the effect of the combined lexica Effect of twolevel lexicon combination For the baseline we used the conventional onelevel full form lexicon Examples for the effect of equivalence classes resulting from dropping morphosyntactic tags not relevant for translation First the translation using the original representation then the new representation its reduced form and the resulting translation The incompleteness of Freebase are must have attributes for a person Plate diagram of our model False negative matches on the Riedel Riedel et al 2010 and KBP dataset Surdeanu et al 2012 All numbers are on bag pairs of entities level BD are the numbers before downsampling the negative set to 10 and 5 in Riedel and KBP dataset respectively Performance on the KBP dataset The figures on the left middle and right show MIML Hoffmann and Mintz compared to the same MIMLSemi curve respectively MIMLSemi is shown in red curves lighter curves in black and white while other algorithms are shown in black curves darker curves in black and white Coarse overview From multilingual in put to typed relations and instances Some examples for MEDLINE tagset Number of lex entries per tag and sample words Quasimorphological operations Translation results for EnglishGerman Translation results for GermanEnglish Translation results for FrenchEnglish Translation results for EnglishFrench Plan to the experiments described in this paper A multiword expression in HeiST Filtering out objective phrases HeiST baseline crosslingual projection SVM Comparison figures on subsets of the Stanford Sentiment Treebank Lexiconbased phrase labeling Incorporating additional information Rule types in SSTb and HeiST Precision of rules with nonneutral parent label ID daughters and parent have identical labels BLEU scores of English to Russian ma chine translation system evaluated on tst2012 us ing baseline GIZA alignment and translitera tion augmentedGIZA OOVTI presents the score of the system trained using TAGIZA af ter transliterating OOVs BLEU scores of English to Russian ma chine translation system evaluated on tst2012 and tst2013 using baseline GIZA alignment and transliteration augmentedGIZA alignment and postprocessed the output by transliterating OOVs Human evaluation in WMT13 is performed on TAGIZA tested on tst2013 marked with Russian to English machine translation system evaluated on tst2012 and tst2013 Human evaluation in WMT13 is performed on the system trained using the original corpus with TAGIZA for alignment marked with This loglog plot shows that there are many rare features and few common features The probability that a feature occurs in x number of N best lists behaves according to the powerlaw x where 228 Feature growth rate For Nbest list i in the table we have NewFt number of new fea tures introduced since Nbest i 1 SoFar Total number of features defined so far and Ac tive number of active features for Nbest i Eg we extracted 7535 new features from Nbest 2 combined with the 3900 from Nbest 1 the total features so far is 11435 BLEU difference of 1000 bootstrap sam ples 95 confidence interval is 15 90 The proposed approach therefore seems to be a stable method Results for different feature sets with corresponding feature size and traintest BLEUPER All multitask features give statistically significant improvements over the baselines boldfaced eg Shared Subspace 291 BLEU vs Baseline 286 BLEU Combinations of multitask features with high frequency features also give significant improvements over the high frequency features alone TreeTagger and RFTagger outputs Starred word forms are modified during preprocessing Sequence of units built from surface word forms top and POStags bottom Results for FrenchEnglish Results for GermanEnglish Example dependency tree Example coreferent paths Italicized entities generally corefer Example noncoreferent paths Italicized entities do not generally corefer Gender classification performance Example gendernumber probability ANC pronoun resolution accuracy for varying SVMthresholds Resolution accuracy Dependency parse tree for the sentence in the ACE corpus Toujan Faisal 54 said she was informed of the refusal by an Interior Min istry committee overseeing election preparations Undersampled system for the task of rela tion detection The proportion of positive examples in the training and test corpus is 500 and 206 respectively System for the task of relation classifica tion The two classes are INR and COG and we evaluate using accuracy Acc The proportion of INR relations in training and test set is 497 and 4963 respectively A Dictionary based Word Graph Synthetic Data Set from Xinhua News Quantitative Evaluation of Common Topic Finding crosscollection loglikelihood Effectiveness of Extracting Common Topics Qualitative Evaluation Effectiveness of Latent Topic Extraction from MultiLanguage Corpus Selfbootstrapping algorithm Clusteringbased stratified seed sampling Performance of various clusteringbased seed sampling strategies on the heldout test data with the optimal cluster number for each clustering algorithm Performance in F1score over different cluster numbers with intrastratum sampling on the develop ment data Misspellings of receive Classification of corpus token by type Contextsensitive spelling correction denotes also using 60 WSJ 5 corrupted Memorybased learner results Synonyms for chain Synonyms for home Average I NV R for 300 headwords InvR scores ranked by difference Giga word to Web Corpus Step 4 Step 2 Step 5 Step 3 50document corpora averages LO Dice configuration scores Best LO and LL configurations scores Best MAP in Experiment 1 LO sentence configuration scores LO cosine sentence configuration scores Average rank of correct translation according to average source term frequency LO cosine sentence configuration scores Feature set for our pronoun resolution systemed feature is only for the singlecandidate model while ed feature is only for the twincandidate mode The performance of different resolution systems Results of different feature groups under the TC model for Npron resolution Relationship types and their argument type con straints Count of relationships in 77 gold standard documents The relationship extraction system Feature sets used for learning relationships The size of a set is the number of features in that set Variation in performance by feature set Features sets are abbreviated as in Table 3 For the first seven columns features were added cumulatively to each other The next two columns allgen and notok are as de scribed in Table 3 The final two columns give inter annotator agreement and corrected inter annotator agreement for comparison Variation in performance by number of sentence boundaries n and by training corpus size Additional features designed to improve model of longrange reordering Effect of different sets of reference translations used during tuning Effect of discriminatively learned penalties for OOV words Effect of supplementing recasing model training data with the test set source Mechanical evaluation of translation JapanesetoEnglish Display of NICT ATR SpeechtoSpeech Translation System Evaluation of speech recognition Human Evaluation of translation A fragment of an entailment graph a its SCC graph b and its reduced graph c Nodes are predicates with typed variables see Section 5 which are omitted in b and c for compactness Three types of transitivity constraint violations Runtime in seconds for various values Computation of probabilities using the language model During training a classified instance in this case for the confusible pair then than are generated from a sentence During testing a similar instance is generated The classifier decides what the corresponding class and hence which word should be the focus word This table shows the performance achieved by the different systems shown in accuracy The Number of cases denotes the number of instances in the testset The chunking results for the six systems associated with the project shared task CoNLL 2000 The baseline results have been obtained by selecting the most frequent chunk tag associ ated with each partofspeech tag The best results at CoNLL2000 were obtained by Support Vector Machines A majority vote of the six LCG sys tems does not perform much worse than this best result A majority vote of the five best systems outperforms the best result slightly error re duction The NP chunking results for six sys tems associated with the project The baseline results have been obtained by selecting the most frequent chunk tag associated with each partof speech tag The best results for this task have been obtained with a combination of seven learn ers five of which were operated by project mem bers The combination of these five performances is not far off these best results The results for three systems associ ated with the project for the NP bracketing task the shared task at CoNLL99 The baseline re sults have been obtained by finding NP chunks in the text with an algorithm which selects the most frequent chunk tag associated with each partof speech tag The best results at CoNLL99 was obtained with a bottomup memorybased learner An improved version of that system MBL deliv ered the best project result The MDL results have been obtained on a different data set and therefore combination of the three systems was not feasible Predictive power of admissible and almost admissible heuristic functions Training corpus statistics without punctuation marks Test corpora statistics Average search time s per sentence Search Success Rate 1 million hypothe ses Search errors Effect of observation pruning on the translation quality average over all test sets A E Success Rate for 12 and 14word sentences Translation quality Part of a sample headline cluster with aligned paraphrases Examples of generated paraphrased head lines Correlations between human judge ments and automatic evaluation metrics for vari ous edit distances Automatic evaluation and sentence Levenshtein scores Proposed method data flow Feature set used in the Stage 2 classifier and their number for the causal relation experiments Precision of acquired relations causality L and S denote lenient and strict evaluation Precision of acquired relations prevention L and S denote lenient and strict evaluation Precision of acquired relations material L and S denote lenient and strict evaluation Frequencies of patterns in the evaluation data causation Contribution of feature sets material Contribution of feature sets prevention Contribution of feature sets causality Stemmed results on 3138utterance test set Asterisked results are significantly better than the baseline p 005 using 1000 iterations of paired bootstrap resampling Koehn 2004 Rank trajectories of 4 LDA inferred topics with incremental topic inference The xaxis indicates the utterance number The yaxis indicates a topics rank at each utterance Example of the context of in Eat fruits and the context of in Play basketball An example for the BMES representa tion The sentence is I love Bei jing Tiananmen square which consists of 4 Chi nese words I love Beijing and Tiananmen square Algorithm description Comparison of fscores when changing the size of labeled data 110 14 12 and all labeled data The size of unlabeled data is xed as 5 million characters Details of the PKU data Details of the unlabeled data Comparison of our approach with the stateofart systems Comparison of our approach with using only the Gigaword corpus Derivational entropy of Gq and cross entropies for three different corpora A Sample Network MUC7 Level Distribution of the Facts Combined MUC7 Level Distribution of Each of the Facts MUC7 Level Distribution of Each of the Facts MUC7 Level Distribution of Each of the Facts MUC7 Level Distribution of Each of the Facts MUC4 Level Distribution of the Five Facts Combined MUC4 Level Distribution of Each of the Five Facts MUC5 Level Distribution of the Five Facts Combined MUC5 Level Distribution of Each of the Five Facts MUC6 Level Distribution of Each of the Six Facts Domain Numbers of MUC4 MUC5 MUC6 and MUC7 MUC6 Level Distribution of the Six Facts Combined GermanEnglish translation results Results are cumulative EnglishGerman translation results results are cumulative except for the three alternative PAL configurations Examples of parallel phrases used in word alignment Summary of devtest results and shared task test results for submitted systems and LIU baseline with hier archical reordering A Motivating Example Processed Data Statistics Sample Is Values Human Assessment of Errors Slot Value Translation Assessment from Ran dom Sample of 1000 Performance of Unsupervised Name Mining Example of Learned Name Pairs with Gloss Translations in Parentheses Name Pairs Mined Using Previous Methods Types of message speech acts in corpus Categories of Message Speech Act Thread length distribution Frequency of speech acts Gold standard length distribution Sample poster scores System Performance Comparison SA strength scores Example queries for abbreviation BSA Properties of abbreviations corpus retrieved from Medline Performance of WSD system using various combinations of learning algorithms and features Performance of WSD system over individual ab breviations in three reduced corpora Average word accuracy for transduced sentences Fraction of the sentences that were transduced Sizes of the automata Time consumption of transduction confirms that names participating in re Baseline Word Clustering by Relation Reranking by Coreference Reranking by Relation Baseline Word Clustering by Relation Reranking by Coreference System Flow Plate diagram representation of the model ti s wi s and si s denote the tags words and segmentations respectively Gs are various DPs in the model Ej s and j s are the tagspecific emission distributions and their respective Dirichlet prior parameters H is Gamma base distribution S is the base distribution over segments Coupled DP concetrations parameters have been omitted for clarity Loglikelihood of samples plotted against iter ations Dark lines show the average over five runs grey lines in the back show the real samples Tagging part of loglikelihood plotted against Vmeasure Segmentation results on different languages Results are calculated based on word types For each language we report precision recall and F1 measure number of word types in the corpus and number of word types with gold standard segmentation available For each language we report the segmentation result without and with emission likelihood scaling without LLS and with LLS respectively Tagging results for different languages For each language we report median onetoone 11 manytoone m1 and Vmeasure Vm together with standard deviation from five runs where median is taken over Vmeasure Types is the number of word types in each corpus True is the number of gold tags and Induced reports the median number of tags induced by the model together with standard deviation Best Pub lists the best published results so far also 11 m1 and Vm in Christodoulopoulos et al 2011 Blunsom and Cohn 2011 and Lee et al 2010 Tagging and segmentation results on Estonian MultextEast corpus Learned seg and Learned tag com pared to the semisupervised setting where segmentations are fixed to gold standard Fixed seg and tags are fixed to gold standard Fixed tag Finally the segmentatation results from Morfessor system for comparison are pre sented Ten relation instances extracted by our system that did not appear in Freebase The 23 largest Freebase relations we use with their size and an instance of each relation Dependency parse with dependency path from Edwin Hubble to Marshfield highlighted in boldface Features for Astronomer Edwin Hubble was born in Marshfield Missouri Examples of highweight features for several relations Key SYN syntactic feature LEX lexical feature x reversed NE named entity tag of entity Automatic evaluation with 50 of Freebase relation data held out and 50 used in training on the 102 largest relations we use Precision for three different feature sets lexical features syntactic features and both is reported at recall levels from 10 to 100000 At the 100000 recall level we classify most of the instances into three relations 60 as locationcontains 13 as personplaceofbirth and 10 as personnationality Estimated precision on humanevaluation experiments of the highestranked 100 and 1000 results per relation using stratified samples Average gives the mean precision of the 10 relations Key Syn syntactic features only Lex lexical features only We use stratified samples because of the overabundance of locationcontains instances among our highconfidence results Linking FrameNet frames and VerbNet classes Results of the mapping algorithm Mapping algorithm refining step F1 and accuracy of the argument classifiers and the overall multiclassifier for FrameNet semantic roles Semantic Role learning curve Examples of pseudo features Sources of the training data Number of candidates for each target language Examples of the top3 candidates in the transliteration of English Chinese Examples of the top3 candidates in the transliteration of EnglishKorean Number of evaluated English Name MRRs of the phonetic transliteration Size of the test data MRRs for the phonetic transliteration 2 MRRs of the phonetic transliteration Gibbs sampling algorithm for IBM Model 1 im plemented in the accompanying software Sizes of bilingual dictionaries induced by differ ent alignment methods Distribution of inferred alignment fertilities The four blocks of rows from top to bottom correspond to in order the total number of source tokens source tokens with fertilities in the range 47 source tokens with fertil ities higher than 7 and the maximum observed fertility The first language listed is the source in alignment Sec tion 2 BLEU scores in translation experiments E En glish T Turkish C Czech A Arabic Dependency tree for the sentence PROT1 contains a sequence motif binds to PROT2 Comparison with other PPI extraction systems in the AIMed corpus Comparison of contributions of different features to relation detection across multiple domains Comparison of performance across the five PPI corpora Number of Sentences for bilingual training de velopment and test and monolingual forum data sets Few examples of the untranslatable tokens in forum posts Evaluation results for all combinations of mixture adapted language and translation models Baselinebl scores are italicized best scores are in bold Bayesian network and are vectors of hy perparameters and i for i 1 nc and are distributions u is a vector of underlying forms generated from and si for i nu is a set of observed surface forms generated from the hidden variable ui according to i Sample dataset constructed by hand Finnish verbs with inflection for person and number Posterior likelihood at each of the first 100 iter ations from 4 runs with different random seeds on 10 of the Morphochallenge dataset i6j 0001 ij 100 01 indicating convergence within the first 15 iterations Accuracy of underlying segment hypotheses Resampling probabilities for alternations after 1000 iterations Parse likelihood Nave FSA with duplicated paths Overgenerating FSA 4tape representation for the Hebrew word htpqdut FSRA2 for Arabic nominative definite and indefinite nouns Interdigitation FSRA general Reduplication for n 4 Time comparison between FSAs and FSRAs Space comparison between FSAs and FSRAs NIL expression forms based on POS attribute NIL expression forms based on word formation Workflow for NIL knowledge engineering component NILE refers to NIL expression which is identified and annotated by human annotator Architecture of NILER system Smoothed precision curves over the five corpora Experimental results for the two methods on the five corpora PRE denotes precision REC denotes recall and F1 denotes F1Measure Smoothed recall curves over the five corpora Smoothed quality curves for SVM method over the five corpora Smoothed quality curves for PM method over the five corpora Smoothed F1Measure curves over the five corpora MEDLDA Mixed Membership MEDLDA Relation types for ACE 05 corpus Overall performance of the 3 systems Multiclass Classification Results with PlusCOMP for SVM LLDA and MEDLDA for the six ACE 05 categories and NOREL LLDA Fmeausres for 3 feature conditions SVM Fmeausres for 3 feature conditions MEDLDA Fmeausres for 3 feature conditions Fmeasures for every kernel in Khayyamian et al 2009 and MEDLDA Optimized TERp Edit Costs Optimization Test Set Pearson Correlation Results Average Metric Rank in NIST Metrics MATR 2008 Official Results MT06 Dev Optimization Test Set Spearman Correlation Results Optimized Edit Costs Results on the NIST MATR 2008 test set for several variations of paraphrase usage Features used by paraphrase classifier Illustration of features f812 Bidirectional checking of entailment relation of p1 p2 and p2 p1 p1 is reduces bone mass in s1 and p2 is decreases the quantity of bone in s2 p1 and p2 are exchanged between s1 and s2 to generate corresponding paraphrased sentences s01 and s02 p1 p2 p2 p1 is verified if s1 s01 s2 s02 holds In this case both of them hold English is used for ease of explanation Number of extracted paraphrases Examples of correct and incorrect paraphrases extracted by our supervised method with their rank Precision curves of paraphrase extraction Top7 Chinese longform candidates for the En glish acronym TAA according to the LH score The performances of the transliteration models and their comparison on EMatch The BLEU score of selftrained h4 translitera tion models under four selection strategies nt n15 stands for the nth iteration The BLEU score of selftrained cascaded trans lation model under five initial training sets Maximally Accurate Assignment Numeric Assignment Experimental Results Word prediction from a partial parse Dependency structure of a sentence A conceptual gure of the lexicalization Corpus Relation between cross entropy and pars Cross entorpy and accuracy of each model Number of feedback items per speaker Distribution of isolated vs initial posi tion for the most frequent lexical items Duration in seconds of each lexical type Distribution of the lexical items Dendrogram of the participants cluster based on their feedback profile The Ensemble Semantics framework for information extraction Feature space describing each candidate instance S indicates the set of seeds for a given class Average precision AP and coverage Cov results for our proposed system ESall and the baselines indicates AP statistical signifi cance at the 095 level wrt all baselines Number of extracted instances and the sample sizes P and N indicate positive and neg ative annotations Overall AP results of the different feature configurations compared to two baselines in dicates statistical significance at the 095 level wrt B3 indicates statistical significance at 095 level wrt both B3 and B4 Precision at rank for the different sys tems on the Athletes class Ablation study of the web w query log q and table t features bold letters indicate whole feature families Listing of all seeds used for KEdis and KEpat as well as the top10 entities discovered by ESall on one of our test folds 25 noun lexicographer files in W ORD N ET Example nouns and their supersenses 2 billion word corpus statistics Grammatical relations from S EXTANT Handcoded rules for supersense guessing Breakdown of results by supersense Summary of supersense tagging accuracies The best two performing systems of each type according to finegrained recall in Senseval2 and 3 Polysemous word types in the Senseval2 and 3 English allwords tasks test documents with no data in SemCor 0 columns or with very little data 1 and 5 occurrences Note that there are no annotations for adverbs in the Senseval3 documents Words excluding multiwords in WordNet 171 and the BNC without any data in SemCor Most frequent sense analysis for Senseval2 and 3 polysemous lemmas occurring more than once in a document adverb data is only from Senseval2 Most frequent sense analysis for all polysemous lemmas in the Senseval2 and 3 test data broken down by their frequencies of occurrence in SemCor adverb data is only from Senseval2 The prevalence ranking process for the noun star Example dss and sss scores for star and its neighbors Grammatical contexts used for acquiring the BNC thesaurus Thesaurus coverage of polysemous words excluding multiwords in WordNet 16 Evaluation on SemCor polysemous words only Simplified prevalence score evaluation on SemCor polysemous words only Results of the error analysis for the sample of 80 words SemCor results for Nouns using jcn Evaluating predominant sense information for polysemous nouns on the Senseval2 allwords task data Senseval2 results polysemous nouns only broken down by their frequencies of occurrence in SemCor TYPE precision on finding the predominant sense for the Senseval2 English allwords test data for nouns having a frequency less than or equal to various thresholds WSD precision on the Senseval2 English allwords test data for nouns having a frequency less than or equal to various thresholds Most frequent SFC labels for all senses of polysemous words in WordNet by part of speech Distribution of domain labels of predominant senses for 38 polysemous nouns ranked using the SPORTS and FINANCE corpora Distribution of domain labels of predominant senses for 20 polysemous verbs ranked using the SPORTS and FINANCE corpora WSD using predominant senses training and testing on all domain combinations handclassified corpora WSD using predominant senses training and testing on all domain combinations automatically classified corpora Example consensus network with votes on word arcs Three confusion networks with prior prob abilities Mixedcase TER and BLEU and lower case METEOR scores on Arabic NIST MT05 Mixedcase TER and BLEU and lowercase METEOR scores on Arabic NIST MT03MT04 Mixedcase TER and BLEU and lower case METEOR scores on Chinese NIST MT05 Mixedcase TER and BLEU and lowercase METEOR scores on Chinese NIST MT03MT04 Example of a word with internal structure Example of telescopic compound a and sepa rable word b Structure of the outofvocabulary word English People Two words that differ only in one character but have different internal structures The character people is part of a personal name in tree a but is a suffix in b An example word which has very complex structures The actual output of our parser trained with a fully annotated treebank Proposed output for the new Chinese word seg mentation paradigm Difference between our output a of parsing the word olive oil and the output b of Luo 2003 In c we have a true flat word namely the loca tion name Los Angeles Example word structure annotation We add an f to the POS tags of words with no further structures Example of parser error Tree a is correct and b is the wrong result by our parser Labeled precision and recall for the three types of labels The line labeled Flat is for unlabeled met rics of flat words which is effectively the ordinary word segmentation accuracy BLEU scores achieved with different sets of parallel corpora All systems are base line ncode with POS factor models The follow ing shorthands are used to denote corpora N stands for NewsCommentary E for Europarl C for CommonCrawl U for UN and nf for non filtered corpora BLEU scores for different configuration of factored translation models The big prefix de notes experiments with the larger context for n gram translation models BLEU scores for preordering experi ments with a ncode system and the approach pro posed by Neubig et al 2012 Histogram of token movement size ver sus its occurrences performed by the model Neu big on the source english data BLEU scores for the FrenchtoEnglish translation task measured on nt10 with systems tuned on development sets selected according to their original language adapted tuning Perplexity measured on nt08 with the baseline LM std with the LM estimated on the sampled texts generated texts and with the inter polation of both Impact of the use of sampled texts Results for development and test set for the two languages by ME1 Best results For English name lists are used For German partofspeech tags are used List of keywords used in WordNet search for generating WN CLASS features Distribution of SCs in the ACE corpus SC classification accuracies of different methods for the ACE training set and test set Results for feature ablation experiments Accuracies of singlefeature classifiers Resolution accuracies for the ACE test set Coreference results obtained via the MUC scoring program for the ACE test set Metaevaluation results at document level Metaevaluation results at system level Metaevaluation results at document and system level for submitted metrics 1 Statistics of relation types and subtypes in the training data of the ACE RDC 2003 corpus Note According to frequency all the subtypes are divided into three bins large middle small with 400 as the lower threshold for the large bin and 200 as the upper threshold for the small bin Learning curve of the hierarchical strategy and its comparison with the flat strategy for some major relation subtypes Note FS for the flat strategy and HS for the hierarchical strategy Comparison of the hierarchical and flat learning strategies on the relation subtypes of differ ent training data sizes Notes the figures in the parentheses indicate the cosine similarities between the weight vectors of the linear discriminative functions learned using the two strategies Comparison of our system with other bestreported systems The density of the F1 scores with the three approaches The prior used is a symmetric Dirichlet with 01 BLEU scores when testing on the com bined test set newstest2012 PDTB 23 on PDTB section 23 only 2416 sentences 923 con nectives and when randomizing the sense tags PDTB 23 random for the BASELINE system and the two systems using PDTB connective labels SYSTEM 1 complex labels SYSTEM 2 simplified labels When testing on randomized sense labels PDTB 23 random the BLEU scores are statisti cally significantly lower than the ones on the cor rectly labeled test set PDTB 23 which is indi cated by starred values Performance of SYSTEM 2 simplified PDTB tags when manually counting for improved equal and degraded translations compared to the BASELINE in samples from the PDTB section 23 test set Translation outputs for the EN con nective as which was translated more correctly by SYSTEM 2 thanks to the disambiguating sense tags compared to the BASELINE that often just produces the prepositional as jako The erro neous translations are marked in bold The PDTB sense tags indicate the meaning of the CZ trans lations and are encoded as follows Synchrony Sy Asynchrony Asy Contingency Co Cause Ca The combined sequence and parse tree representation of the relation instance leader of a minority government The nonessential nodes for a and for minority are removed based on the algorithm from Qian et al 2008 Examples of similar syntactic structures across different relation types The head words of the first and the second arguments are shown in italic and bold respectively Examples of unigram and bigram features extracted from Figure 1 Comparison of different methods on ACE 2004 data set P R and F stand for precision recall and F1 respectively The average performance of TLcomb with different T k 104 and 1 Performance of TLcomb and TLauto as H changes Performance of TLNE BL and BLA as the number of seed instances S of the target type increases H 500 T was set to 104 and 102 Average F1 using different hypothesized typespecific features Terminologies An example for grouped entity tuples Entity tuples in big frame are those suitable for the template X direct Y whereas entity tuples in small frame are those held the same relation A realworld situation An example of the mutual reinforcement between P rparati tj and P rcoordek eg Input sentences Performance of our method for paraphrase acquisition Another example of some discovered paraphrases An example of some discovered paraphrases Size of cooccurrence databases WSI and WSD Pipeline Results for the submitted runs General architecture of LINGUA Success rate of anaphora resolution Complexity of the evaluation data Summary of LINGUA performance Structure of a term in the original documents Total documents Modified query Results Evaluation A BCN x Baseline WCN x Baseline Topic 141 Results Evaluation B Topics with MWEs Ranking for Topic 141 CN Ranking for Topic 141 Baseline Parallel Corpus Baseline Results Bayesian Alignment Results Development Sets Results Compound Splitting Results Tuning Results Language Model Results GermantoEnglish Final System Results GermanEnglish Official Test Submis sion English to German Final System Re sults Average of Weights Results Table 1 Data sets used for our alignment quality experiments The total number of sentences in the respective corpora are given along with the number of sentences and goldstandard Sure and Possible alignment links in the corresponding test set Results of our alignment quality experiments All timing and accuracy figures use means from five independently initial ized runs Note that lower is better for AER higher is better for F05 All experiments are run on a system with two Intel Xeon E5645 CPUs running at 24 GHz in total 12 physical 24 virtual cores shows the result of varying the number of samplers and iterations for all Data used for training SMT models all counts in millions Parallel data sets refer to the bitexts aligned to English and their token counts include both languages Table 5 Timings from the word alignments for our SMT evaluation The values are averaged over both alignment directions For these experiments we used systems with 8core Intel E52670 processors running at 26 GHz Results from our SMT evaluation The BLEU scores are the maximum over the Moses parameters explored for the given word alignment conguration Incremental Improvement from Selftraining English Bootstrapping for Name Tagging SelfTraining for Name Tagging Data Description English Name Tagger Chinese Name Tagger Impact of Data Size Chinese Impact of Confidence Measures Impact of Data Selection Chinese Impact of Data Size English Upper triangle of the sentencesimilarity matrix An example of a hierarchical cluster tree The porposed TSHAC algorithm An example of words and their bit string representations obtained in this paper Words in bold are head words that appeared in Table 1 Lexical features for relation extraction Cluster features ordered by importance Performance comparison on the ACE 2004 data over the 7 relation types Performance 12 of the baseline and using different cluster features with PC4 over the 7 types Performance of each individual relation type based on 5fold crossvalidation Performance over the 7 relation types with different sizes of training data Prefix10 uses the single prefix length 10 to generate word clusters as used by Chan and Roth 2010 The F1Measure value is shown for every kernel on each ACE2005 main relation type For every relation type the best result is shown in bold font An example confusion network construc tion An example packed forest representing hy potheses in Figure 1a The deductive system for Earleys genera tion algorithm WMT10 system combination tuningtesting data Oracle lowercase BLEU Translation results in lowercase BLEU CN for confusion network and CF for confusion forest with different vertical v and horizontal h Markovization order Average minmax hypothesis length pro ducible by each method h 1 for CF Hypegraph size measured by the average number of hyperedges h 1 for CF lattice is the average number of edges in the original CN Example of DIRT algorithm output Most confident paraphrases of X put emphasis on Y Example of inference rules needed in RTE Lexical variations creating new rules based on DIRT rule X face threat of Y X at risk of Y Dependency structure of text Tree skeleton in bold Precision on full RTE data Coverageprecision with various rule collections Precision on the covered RTE data Error analysis Aligned parsed sentence GermanEnglish results for hierarchical and syntactic models in BLEU Training tuning and test conditions Example input and best output found Reachability of 1000 training sentences can they be translated with the model Source span lengths Chunked sentence Derivation with Hierarchical model EnglishGerman results in BLEU Effect on BLEU of varying number of nonterminals Derivation with soft syntax model Chunk Length and count of glue rules used decoding test set Translated chunked sentence Frequency of Relation SubTypes in the ACE training and devtest corpus The Performance of SVM and LP algorithm with different sizes of labeled data for relation detection on relation subtypes The LP algorithm is run with two similarity measures cosine similarity and JS divergence The performance of SVM and LP algorithm with different sizes of labeled data for relation detection and classification on relation subtypes The LP algorithm is run with two similarity measures cosine similarity and JS divergence Comparison of the performance of previous methods on ACE RDC task Comparison of the performance of the bootstrapped SVM method from Zhang 2004 and LP method with 100 seed labeled examples for relation type classification task Growing Algorithm for Language Model Pruning Language Model Pruning Algorithm Calculation of Importance of Bigrams Stepbystep Growing Algorithm Comparison of Number of Bigrams at FMeasure 9633 Performance Comparison of Different Pruning Methods Performance Comparison of Combined Model and KLD Model Correlation between Perplexity Perplexity Comparison of Different Pruning Methods Translation results for EnglishFrench Translation results for FrenchEnglish Translation results for GermanEnglish Translation results for EnglishGerman Gibbs Sampling Statistical Information of Corpora Experimental Procedure Features Used for Initial Distribution Ordered List of IncreasedDecreased Number of Correctly Tagged Words Results of Multiple Trials and Compari son to Simulated Annealing Results of POS Guessing of Unknown Words Overview of the method Evaluation results within sets Evaluation results for links Extracted NE pair instances and context Figure 3 High TFITF words in ComCom Numbers are TFITF score frequency in the collec tion TF frequency in the corpus TF and word Examples and number of them in Semcor for sense approach and for class approach BLC for WN16 using all or hyponym relations Average polysemy on SE2 and SE3 Results for nouns Learning curve of BLC20 on SE3 Learning curve of BLC20 on SE2 Results for verbs Learning curve of SuperSense on SE3 Learning curve of SuperSense on SE2 Representation of Bigram Counts Experimental Results Decision Tree and Stump Characteristics Architecture of Nameaware Machine Translation System Translation Performance Statistics and Name Distribution of Test Data Sets A u to m a tic M e tr ic s H u m a n E v a lu a tio n Figure 2 Scores based on Automatic Metrics and Human Evaluation Impact of Joint Bilingual Name Tagging on Word Alignment name tokensall tokens Figure 3 Word alignment gains according to the percentage of name words in each sentence Proportion of OOV words in some corpora used for real world applications Numbers in parentheses exclude words whose first letters are capitalized because they are likely to refer to named entities Examples of positive and negative words Accuracy for SOPMI with different data set sizes the spin model the label propagation model and the random walks model for 10fold crossvalidation and 14 seeds Accuracy for adjectives only for the spin model the bootstrap method and the random walk model The effect of varying the number of samples k on accuracy The effect of varying the maximum number of steps m on accuracy k 1000 Accuracy for words with high confidence measure The effect of varying the number of seeds on accuracy Accuracy for three classes on a general purpose list of 2000 words Accuracy of foreign word polarity identification Accuracy of different methods in predicting OOV words polarity The effect of varying the number of extracted related words on accuracy feature templates accuracy using nonaveraged and averaged perceptron learning curves of the averaged and non averaged perceptron algorithms the influence of agenda size the accuracies over the second SIGHAN bakeoff data the accuracies over the first SIGHAN bake off data the influence of features F Fmeasure Feature numbers are from Table 1 Different representations of a relation instance in the example sentence provide bene five different tree kernel setups on the ACE 2003 five major types using the parse tree structure information only regardless of any entityrelated information Performance comparison on the ACE 2004 data over both 7 major types the numbers outside parentheses and 23 subtypes the num bers in parentheses Performance comparison on the ACE 20032003 data over both 5 major types the numbers outside parentheses and 24 subtypes the numbers in parentheses Error distribution of major types on both the 2003 and 2004 data for the compos ite kernel by polynomial expansion Nouns and verbs supersense labels and short description from the Wordnet documentation The noun box in Wordnet each line lists one synset the set of synonyms a definition an optional example sentence and the supersense label Statistics of the datasets The row Super senses lists the number of instances of supersense labels partitioned in the following two rows between verb and noun supersense labels The lowest four rows summarize average polysemy figures at the synset and supersense level for both nouns and verbs Summary of results for random and first sense baselines and supersense tagger is the standard error computed on the five trials results Summary of results of baseline and tagger on selected subsets of labels NER categories evaluated on Semcor upper section and 5 most frequent verb middle and noun bottom categories evaluated on Senseval Overview of experiments applying WSMs to determine semantic compositionality of word expressions BNC British National Corpus GR grammatical relations GNC German newspaper corpus TREC TREC corpus SY substitutabilitybased methods CT componentbased methods CTn componentbased methods comparing WSM neighbors of expressions and their components CY compositionalitybased methods NVAP c noun verb adjective adverb combinations NN nounnoun VP verbparticles AN adjectivenoun VO verbobject SV subjectverb PV phrasalverb PNV prepositionnounverb dicts dictionaries of idioms WN Wordnet MA use of manually annotated data S Spearman correlation PC Pearson correlation CR Spearman and Kendall correlations APD average point difference CL classification PR PrecisionRecall PRc PrecisionRecall curves Fm F measure R2 goodness A sample of manually annotated expressions from DiscoEnGold with their numerical scores Ns and coarse scores Cs The values of AP Spearman and Kendall correlations between the LSAbased and PMIbased model respectively and the Gold data with regards to the expression type Every zero value in the table corresponds to the theoretically achieved mean value of correlation calculated from the infinite number of correlation values between the ranking of scores assigned by the annotators and the rankings of scores being obtained by a random number genarator ReddyWSM stands for the best performing WSM in the DISCO task Reddy et al 2011b StatMix stands for the best performing system based upon association measures Chakraborty et al 2011 Only All and All are available for the models explored by Reddy et al 2011b and Chakraborty et al 2011 Smoothed graphs depicting the dependency of Precision upon Recall using the LSA and PMIbased models ordering the expressions in TrainValD left and TestD right according to their noncompositionality Smoothed graphs depicting the dependency of Precision left and Recall right upon the nBest selected noncompositional candidates from the ordered list of expressions in TestD created by the LSA and PMIbased models Context Clustering with Spectralbased Clustering technique Frequency of Major Relation SubTypes in the ACE training and devtest corpus Performance of our proposed method Spectral based clustering compared with other unsupervised methods Hasegawa et al 2004s clustering method and Kmeans clustering Different Context Window Size Setting Example of a MUC4 template The topranking feature for each group of features and the classifier of a slot Accuracy of string slots with and without full parsing Systems whose Fmeasures are not signif icantly different from AliceME at the 010 signifi cance level with 099 confidence Accuracy of all slots on the TST3 and TST4 test set Accuracy of string slots on the TST3 and TST4 test set Layers used in our model A standard logical form derivation using CCG The NP notation means that the subject is typeraised and taking the verbphrase as an argumentso is an ab breviation of SSNP This is necessary in part to sup port a correct semantics for quantifiers Example initial lexical entries Using the type model for disambiguation in the derivation of file a suit Type distributions are shown after the variable declarations Both suit and the object of file are lexically ambiguous between different types but after the reduction only one interpretation is likely If the verb were wear a different interpretation would be preferred Most probable terms in some clusters induced by the Type Model Results on widecoverage Question Answer ing task CCGDistributional ranks questionanswer pairs by confidence250 means we evaluate the top 250 of these It is not possible to give a recall figure as the total number of correct answers in the corpus is unknown Accuracy on Section 1 of the FraCaS suite Problems are divided into those with one premise sen tence 44 and those with multiple premises 30 Example problem from the FraCaS suite Example questions correctly answered by CCGDistributional Comparison with other approaches F1measures with in 0 3 F1measure with in 01 Example sentence and extracted features from the SENSEVAL 2 word church Empiricallyderived classifier similarity Individual Classifier Properties crossvalidation on SENSEVAL training data Accuracy for different EMweighted probability interpolation models for SENSEVAL 2 Training set characteristics Individual feature type contribution to perfor mance Fields marked with indicate that the difference in performance was not statistically significant at a level paired McNemar test Classifier combination accuracy over 5 base classifiers NB BR TBL DL MMVC Best perform ing methods are shown in bold Individual basic classifiers contribution to the final classifier combination performance Final Performance Frozen Systems on SENSEVAL Lexical Sample WSD Test Data Example of semantic trees Two STs composing a STN ROUGE2 measures in kmeans learning ROUGEW in empirical approach ROUGEW measures in kmeans learning ROUGEW measures in EM learning ROUGESU in kmeans learning Fmeasures for different systems ROUGESU in empirical approach ROUGE2 in empirical approach ROUGE2 measures in EM learning ROUGESU measures in EM learning System architecture overview Procedure to mine key lexicons for each semantic type Some key lexicons and verbs for two semantic types Salience grading for candidate antecedents Procedure to find semantic types for antecedent candidates Statistics of anaphor and antecedent pairs Feature impact experiments FScore of Medstract and 100Medlines Comparisons among different strategies on Medstract Impacts of the mined semantic lexicons and the use of PubMed CATiB Annotation example 23 1 4 0 t ml HfydAt AlkA AlkyAt fy AlmdArs AlHkwmy The writers smart granddaughters work for public schools The words in the tree are presented in the Arabic reading direction from right to left Penn Arabic Treebank part 3 v31 data split Parsing performance with each POS tag set on gold and predicted input L AS labeled attachment accuracy dependency relation U AS unlabeled attachment accuracy dependency only L S relation label prediction accuracy L AS diff difference between labeled attachment accuracy on gold and predicted input POS acc POS tag prediction accuracy Prediction accuracy value set sizes descriptions and value examples of features used in this work Accuracy was measured over the development set The set includes a NA values CORE 12 POS tag set with morphological inflectional features Left half Using gold POS tag and feature values In it Top part All Adding all nine inflectional features to CORE 12 Second part Sep Adding each feature separately to CORE 12 Third part Greedy Greedily adding next best feature from Sep and keeping it if improving score Right half Same as left half but with predicted POS tag and feature values Statistical significance tested only on predicted nongold input against the CORE 12 baseline Models with lexical morphosemantic features Top Adding all lexical features together on top of the CORE 12 baseline Center Adding each feature separately Bottom Greedily adding best features from previous part on predicted input Statistical significance tested only on predicted nongold input against the CORE 12 baseline Models with inflectional and lexical morphological features together predicted valueguided heuristic Statistical significance tested only on predicted input against the CORE 12 baseline Models with reengineered DET and PERSON inflectional features Statistical significance tested only on predicted input against the CORE 12 baseline Models with functional features GENDER NUMBER rationality RAT F N functional features based on Alkuhlani and Habash 2011 GN GENDER NUMBER GNR GENDER NUMBER RAT Statistical significance tested only for CORE 12 models on predicted input against the CORE 12 baseline Select models trained using the EasyFirst Parser Statistical significance tested only for CORE 12 models on predicted input significance of the EasyFirst Parser CORE 12 baseline model against its MaltParser counterpart and significance of all other CORE 12 models against the EasyFirst Parser CORE 12 baseline model Alternatives to training on goldonly feature values Top Select MaltParser CORE 12 models retrained on predicted or gold predicted feature values Bottom Similar models to the top half with the EasyFirst Parser Statistical significance tested only for CORE 12 models on predicted input significance of the MaltParser models from the MaltParser CORE 12 baseline model and significance of the EasyFirst Parser models from the EasyFirst Parser CORE 12 baseline 3 7 19 6 7 Figure 2 Error analysis example 82 mrt yAm l AxtfA Alzmyl Almhnd Several days have passed since the disappearance of the colleague the engineer as parsed by the baseline system using only CORE 12 left and as using the best performing model right Bad predictions are marked with The words in the tree are presented in the Arabic reading direction from right to left Training the MaltParser on gold tags accuracy by gold attachment type selected subject object modification of a verb or a noun by a noun modification of a verb or a noun by a preposition idafa and overall results repeated Training the EasyFirst Parser on gold and predicted tags accuracy by gold attachment type selected subject object modification of a verb or a noun by a noun modification of a verb or a noun by a preposition idafa and overall results repeated A corpus of two trees A derivation for Mary likes Susan Another derivation yielding same tree All binary trees for NNS VBD JJ NNS Investors suffered heavy losses Some subtrees from trees in figure 4 Fscores of UMLDOP compared to previous models on the same data Fscores of UDOP UMLDOP and a supervised treebank PCFG MLPCFG for a random 9010 split of WSJ10 and WSJ40 Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt STTS accuracies of the TnT tagger trained on the STTS tagset the TnT tagger trained on the Tiger tagset and our tagger trained on the Tiger tagset Tagging accuracy on development data depending on context size Tagging accuracies on test data Context Clustering with Spectralbased Clustering technique Frequency of Major Relation SubTypes in the ACE training and devtest corpus Different Context Window Size Setting Comparison of the existing efforts on ACE RDC task The ANNIS user interface displaying data from the PCC Sources of conflict in crosslingual subjectivity transfer Definitions and synonyms of the fourth sense of the noun argument the fourth sense of verb decide and the first sense of adjective free as provided by the English and Romanian WordNets for Romanian we also provide the manual translation into English Crosslingual bootstrapping Multilingual bootstrapping Macroaccuracy for crosslingual bootstrapping Fmeasure for the objective and subjective classes for crosslingual bootstrapping Fmeasure for the objective and subjective classes for multilingual bootstrapping versus crosslingual framework Macroaccuracy for multilingual bootstrapping versus crosslingual framework Sample of subjective features appearing in the top 100 discriminant attributes selected with Information Gain on the 3rd fold training data at iteration 10 The words in italics in the multilingual features represent equivalent translations in English and Romanian Training set statistics OutOfVocabulary OOV rate is regarding the development sets Example training run of a pruned 1st order model on German showing the fraction of pruned gold se quences sentences during training for training train and development sets dev POS tagging experiments with pruned and unpruned CRFs with different orders n For every language the training time in minutes TT and the POS accuracy ACC are given indicates models significantly better than CRF first line Accuracies for models with and without oracle pruning indicates models significantly worse than the oracle model Test results for POSMORPH tagging Best baseline results are underlined and the overall best results bold indicates a significant difference between the best baseline and a PCRF model Development results for POSMORPH tagging Given are training times in minutes TT and accuracies ACC Best baseline results are underlined and the overall best results bold indicates a significant difference between the best baseline and a PCRF model Test results for POS tagging Best baseline results are underlined and the overall best results bold indicates a significant difference between the best baseline and a PCRF model Development results for POS tagging Given are training times in minutes TT and accuracies ACC Best baseline results are underlined and the overall best results bold indicates a significant difference positive or negative between the best baseline and a PCRF model Statistics on the Italian EVALITA 2009 and English CoNLL 2003 corpora Three kinds of tree kernels Semantic structure of the first sequence Global features in the entity kernel for reranking These features are anchored for each entity instance and adapted to entity categories For example the entity string first feature of the entity United Nations with entity type ORG is ORG United Nations Reranking results of the three tagging kernels on the Italian and English testset Regular expression notation in foma Illustration of a worsening filter for morpheme boundaries OT grammar for devoicing compiled into an FST Violation permutation transducer Devoicing transducer compiled through a rule Example outputs of matching implementation of Finnish OT An nonregular OT approximation Illustrative tableau for a simple constraint sys tem not capturable as a regular relation Baseline results for human word lists Data 700 positive and 700 negative reviews Results for baseline using introspection and simple statistics of the data including test data Average threefold crossvalidation accuracies in percent Boldface best performance for a given setting row Recall that our baseline results ranged from 50 to 69 Outline of word segmentation process Three different vocabulary sizes used in subword based tagging s1 contains all the characters s2 and s3 contains some common words Corpus statistics in Sighan Bakeoff 2005 Segmentation results of dictionarybased segmentation in closed test of Bakeoff 2005 A separates the results of unigram bigram and trigram Segmentation results by the pure subwordbased IOB tagging The separator divides the results by three lexicon sizes as illustrated in Table 3 The first is characterbased s1 while the other two are subwordbased with different lexicons s2s3 Riv and Roov varing as the confidence threshold t Effects of combination using the confidence measure Here we used 08 and confidence threshold t 07 The separator divides the results of s1 s2 and s3 Effects of using CRF The separator divides the results of s1 and s3 List of results in Sighan Bakeoff 2005 NPs in a sample from the Catalan training data left and the English translation right Evolution of A means relative to the length of the nbest sequence The MCPG algorithm Comparison of paraphrase generators Top the MOSES baseline middle and bold the truescore MCPG down the translator MCPG The use of truescore improves the MCPG per formances MCPG reaches MOSES performance level Verb classes see Section 31 their Levin class numbers and the number of experimental verbs in each see Section 32 Experimental Results C50 is supervised accuracy Base is on random clusters set Ling is manually selected subset Seed is seedverbselected set See text for further description Feature counts for Ling and Seed feature sets Event descriptions spread across two sentences Counts of matches between MUC and Soderland data Matches between MUC and Soderland data at field level Relation extraction results on the JDPA Corpus test set broken down by document source Selected document statistics for three JDPA Corpus document sources Size of Seed Lexicons Performance on Bilingual Lexicon Extraction Seeds with the Highest Weight Translation Candidates for manic depression BLEU scores on the NewsCommentary development test data BLEU scores on the Europarl development test data Examples of templates suggested by DIRT and TEASE as having an entailment relation in some direction with the input template X change Y The entailment direction arrows were judged manually and added for readability Rule evaluation examples and their judgment Average Precision P and Yield Y at the rule and template levels Examples for disagreement between the two judges Characteristics of the parallel corpus used for experiments The role of the standard Basque Batua ana lyzer in filtering out unwanted output candidates created by the induced rule set produced by method 1 Values obtained for Precision Recall and F scores with method 1 by changing the minimum fre quency of the correspondences to construct rules for foma The rest of the options are the same in all three experiments only one rule is applied within a word Values obtained for Precision Recall and F score with method 1 by changing the threshold frequency of the correspondences and applying a postfilter Experiments with the ILP method using a thresh old of 14 times a wordpair is seen to trigger rule learn ing The figures in parentheses are the same results with the added postprocessing unigram filter that given sev eral output candidates of the standard dialect chooses the most frequent one Method 1 Exp1 frequency 2 2 rules applied in parallel without contextual conditioning Exp2 fre quency 1 1 rule applied with contextual conditioning Exp3 frequency 2 2 rules applied in parallel with con textual conditioning Tradeoffs of precision and recall values in the experiments with method 1 using various different pa rameters When the unigram filter is applied the precision is much better but the recall drops The best results per F1 score of the two meth ods The parameters of method 1 included using only those string transformations that occur at least 2 times in the training data and limiting rule application to a maxi mum of 2 times within a word and including a unigram postfilter Rules were contextually conditioned For method 2 all the examples threshold 1 in the training data were used as positive and negative evidence with out a unigram filter MT system combination Each 1best outputs are aligned to create as many Confusion Networks which are connected together to form a lattice This lattice is then decoded with a tokenpass decoder using a Language Model to produce 1best andor nbest hypotheses Incremental alignment with TERp resulting in a confusion network Results of system combination on Dev7 development corpus and Test09 the ocial test corpus of IWSLT09 evaluation campaign Entity type constraints BasicRE gives the performance of our basic RE system on predicting finegrained relations obtained by performing 5fold cross validation on only the news wire corpus of ACE2004 Each sub sequent row Hier HierrelEntC Coref Wiki and Cluster gives the individual contribution from using each knowledge The bottom row ALL gives the performance improvements from adding HierrelEntCCorefWikiCluster indicates no change in score Corpus Excerpt with Dialogue Act Annotation Figure 2 Automatically detected posture points H headDepth M midTorsoDepth L lowerTorsoDepth reports the average classification accuracies from the fivefold cross validation The majority baseline accuracy for our data is 347 when the classifier always chooses the most frequent dialog act A The first group of rows in Table 3 report the accuracies of individual feature classes All of the individual features performed better than the baseline The improvement from the baseline was significant except for D with CRF The most powerful feature class was dialogue context class when the full set was used The second group in Table 3 shows the effects of incrementally combining the feature classes Adding dialogue act features to the lexical features L D brought significant improvement in the classification accuracy for ME and CRF Adding posture features L D T P also improved the accuracy of ME by a statistically significant margin The last group shows similar results for ME when the previous tutor dialogue act was excluded from the dialogue context except that the improvement achieved by adding the posture features L D T P was not significant each line of the 6best translations and BLEU scores with 1best translation selected by the current param eter This shows the shapes of BLEU and 1slack SVM objective function for one parameter These lines were calculated by 800 development sentences randomly selected from dev06 for development data when the hyperparameter Q is fixed 10000 BLEU scores on the test08 and news08 test data obtained by models trained by MERT and SVM Tuninig test for hyperparameter Q of structural SVM fixed 10 by increasing it The average improvements of BLEU scores on the test08 and news08 outofdomain when we trained the paramenters using only 400 development sentences with MERT and SVMbased algorithms four times BLEU scores of two open test sets obtained when training by MERT SslackSVM and 1slackSVM using four development sets containing 400 sentences randomly se lecting from WMT08 dev2006 BLEU scores as a function of development data size The polarity classification positive and negative based on product aspect framework Result for microblog classification Cell phone experiment result 17 aspects Comparison of the news and reports corpora Words with the highest association scores in decreasing order for the word cigarette as extracted automatically Integration of confidence measures recallprecision curves figures in the legend correspond to resp 1 and 2 Integration of paradigmatic relations recallprecision curves Integration of semantic relations news corpus best F1measures Integration of confidence measures and interpolation recallprecision curves Interpolation recallprecision curves Best F1measure values for all possible combination Schematic of our proposed method English MWEs and their components with their translation in Persian Direct matches between the trans lation of a MWE and its components are shown in bold partial matches are underlined The 10 best languages for R EDDY using LCS The 10 best languages for the verb component of BANNARD using LCS The 10 best languages for the particle compo nent of BANNARD using LCS Results for the classification task S TRING S IM MEAN is our method using Mean for f1 Correlation after combining Reddy et als method and our method with Mean for f1 S TRING S IM MEAN The correlation using Reddy et als method is 0714 Tuple extraction from a sentence pair Translation results in terms of BLEU score and translation edit rate TER estimated on newstest2010 with the NIST scoring script EnglishFrench translation results in terms of BLEU score and TER estimated on newstest2010 with the NIST script All means that the translation model is trained on newscommentary Europarl and the whole GigaWord The rows upper quartile and median corre spond to the use of a filtered version of the GigaWord Architecture of the Structured Output Layer Neural Network language model Translation results from English to French and English to German measured on newstest2010 using a 100best rescoring with SOUL LMs of different orders CCG derivation and unresolved semantics for the sentence I saw nothing suspicious DRS for the sentence I saw nothing suspi cious Results of the second run with postprocessing Results of the first run without postprocessing Results of negated eventproperty detection on gold standard cue and scope annotation The System architecture1 Learning curves using different sam pling strategies Performance comparsion of the rule based robust semantic parser the reversed two stage classification system and our SLU systems TER Topic Error Rate SER Slot Error Rate Performance comparison of two SLU systems through weakly supervised and super vised training on the three test sets TER Topic Error Rate SER Slot Error Rate Learning curves of bootstrapping meth ods for semantic classification on TS1 Most frequent phrase dependencies with at least 2 words in one of the phrases dependencies in which one phrase is entirely punctuation are not shown indicates the root of the tree Key notation Most probable child phrases for the parent phrase made up for each direction sorted by the con ditional probability of the child phrase given the parent phrase and direction Stringtotree configurations each is associated with a feature that counts its occurrences in a derivation UrduEnglish Results BLEU ChineseEnglish Results BLEU a Moses translation output along with and a An English gloss is shown above the Chinese sentence and above the gloss is shown the dependency parse from the Stanford parser b QPDG system output with additional structure c reference translations Average feature values across best translations of sentences in the MT03 tuning set both before MERT column 2 and after column 3 Same versions of tree totree configuration features are shown the rarer swap features showed a similar trend Results when using unsupervised dependency parsers Cells contain averaged BLEU on the three test sets and BLEU on tuning data MT03 in parentheses A general architecture for paraphrasing approaches leveraging the distributional similarity hypothesis Two different dependency tree paths a and b that are considered paraphrastic because the same words John and problem are used to ll the corresponding slots shown coindexed in both the paths The implied meaning of each dependency path is also shown Using Chinese translations as the distributional elements to extract a set of English paraphrastic patterns from a large English corpus A bootstrapping algorithm to extract phrasal paraphrase pairs from monolingual parallel corpora The merging algorithm a How the merging algorithm works for two simple parse trees to produce a shared forest Note that for clarity not all constituents are expanded fully Leaf nodes with two entries represent paraphrases b The word lattice generated by linearizing the forest in a A paraphrase generation lattice for the sentence He ate lunch at a cafe near Paris Alternate paths between various nodes represent phrasal replacements The probability values associated with each edge are not shown for the sake of clarity An example showing the generalization of the word lattice a into a slotted lattice b The word lattice is produced by aligning seven sentences Nodes having indegrees 1 occur in more than one sentence Nodes with thick incoming edges occur in all sentences Extracting consistent bilingual phrasal correspondences from the shown sentence pairs i1 j1 i2 j2 denotes the correspondence fi1 fj1 ei2 ej2 Not all extracted correspondences are shown An example of syntactically motivated paraphrastic patterns that can be extracted from the paraphrase corpus constructed by Cohn CallisonBurch and Lapata 2008 Test verbs and their monosemouspolysemic gold standard senses Connected components nearest neighbour NN clustering D is the KullbackLeibler distance Information Bottleneck IB iterative clustering D is the KullbackLeibler distance Clustering performance on the predominant senses with and without prepositions The last entry presents the per formance of random clustering with K 25 which yielded the best results among the three values K25 35 and 42 The fraction of verb pairs clustered together as a function of the number of shared senses results of the NN algo rithm Evaluation against the monosemous Pred and pol ysemous Multiple gold standards The figures in parentheses are results of evaluation on randomly polysemous data sig nificance of the actual figure Results were obtained with fine grained SCFs including prepositions The fraction of verb pairs clustered together as a function of the number of different senses between pair mem bers results of the NN algorithm Addition method Evaluation of the manual annotation improvement summarization ratio 30 Evaluation of the manual annotation improvement summarization ratio 15 Evaluation of the GUITAR improvement summarization ratio 30 Evaluation of the GUITAR improvement summarization ratio 15 Performance of our system versus a baseline The greedy binding problem a The correct binding b the greedy binding c the result The Lattice of the 8 Patterns Segmentation accuracy of different seg menters Results on three query categories MAP of different IR systems with differ ent segmenters The feature set for coreference resolution Nonrelational features describe a mention and in most cases take on a value of YES or NO Relational features describe the relationship between the two mentions and indicate whether they are COMPATIBLE INCOMPATIBLE or NOT APPLICABLE Statistics for the ACE 2005 corpus MUC CEAF and B3 coreference results using system mentions MUC CEAF and B3 coreference results using true mentions Examples of SMT errors due to MWEs Framework for MWE acquisition from corpora MWE acquisition applied to lexicography Evaluation of translation of phrasal verbs in test set Two characteristic topics for the Y slot of acquire along with their topicbiased Lin sim ilarities scores Lint compared with the original Lin similarity for two rules The relevance of each topic to different arguments of acquire is illus trated by showing the top 5 words in the argument y vector vacquire for which the illustrated topic is the most likely one Contextsensitive similarity scores in bold for the Y slots of four rule applications The components of the score calculation are shown for the topics of Table 1 For each rule application the table shows a couple of the topicbiased scores Lint of the rule as in Table 1 along with the topic relevance for the given context ptdv w which weighs the topicbiased scores in the LinW T cal culation The contextinsensitive Lin score is shown for comparison MAP values on corresponding test set ob tained by each method Figures in parentheses in dicate optimal number of LDA topics Sizes of rule application test set for each learned ruleset MAP results for the two split Lin test sets Network after pairwise TER alignment Network after incremental TER alignment Results on the Arabic GALE Phase 2 evaluation set with one reference translation Results on the Arabic GALE Phase 2 system combination tuning set with four reference translations NIST BLEU scores on the GermanEnglish de en and FrenchEnglish fren Europarl test2008 set Proposed discourse structures for Ex 4 a In terms of informational relations b in terms of inten tional relations Stages of the proposed method Graph of words for the target word paper Numbers inside vertices correspond to their degree Running example of graph creation Two dendrograms for the graph in Figure 3 Sensetagged corpus for the example in Figure 3 A current configuration for internal node Dk and its associated subtrees B first alternative configuration C second alternative configuration Note that swapping st1 st2 in A results in an equivalent tree Hence this configuration is excluded Parameter values used in the evaluation Performance analysis of HRGs CWU CWW HAC for different parameter combinations Table 2 A All combinations of p1 p2 and p3 005 B All combinations of p1 p2 and p3 009 Performance of HRGs and HAC for different parameter combinations Table 2 All combinations of p1 p2 and p3 013 HRGs against recent methods baselines The Buckwalter Arabic Morphological Analyzers lookup process exemplified for the word lilkitAbi Skeleton of basic lexicon transducer in LEXC generated from BAMA lexicons How the IBM models model the translation process This is a hypothetical example and not taken from any actual training or decoding logs Runtimes for sentences of length 1080 The graph shows the average runtimes of 10 different sample sentences of the respective length with swap op erations restricted to a maximum swap segment size of 5 and a maximum swap distance of 2 A decoding trace using improvement caching and tiling ICT The search in the second and later iterations is limited to areas where a change has been applied marked in bold print note that the number of alignment checked goes down over time The higher number of alignments checked in the second iteration is due to the insertion of an additional word which increases the number of possible swap and insertion operations Decoding without ICT results in the same translation but requires 11 iterations and checks a total of 17701 alignments as opposed to 5 iterations with a total of 4464 alignments with caching Number of search iterations left and total number of alignments considered right during search in depen dence of input length The data is taken from the translation of the Chinese testset from the TIDES MT evaluation in June 2002 Translations were performed with a maximum swap distance of 2 and a maximum swap segment size of 5 BLEUscores for the Chinese test set de coding in dependence of maximum swap distance and maximum swap segment size Decoder performance on the June 2002 TIDES MT evluation test set with multiple searches from randomized starting points MSD2 MSSS5 sentence length Figure 6 Time consumption of the various change types in Parsing tree FDG Analysers output example Evaluation results from DSOWSJ Features used by the polyglot ranking system 0 Figure 1 The computation of DKL Pve i kPe0i using a toy corpus for e looking forward to Note that the sec Particles and prepositions allowed in phrasal verbs gathered from Wiktionary Our boosted ranker combining monolingual and bilingual features bottom compared to three base lines top gives comparable performance to the human curated upper bound The solid line shows recallat1220 when com bining the k bestperforming bilingual statistics and three monolingual statistics The dotted line shows the indi vidual performance of the kth bestperforming bilingual statistic when applied in isolation to rank candidates An ablation of monolingual statistics shows that they are useful in addition to the 50 bilingual statistics combined and no single statistic provides maximal per formance The highest ranked phrasal verb candidates from our full system that do not appear in either Wiktionary set Candidates are presented in decreasing rank pat on is the second highest ranked candidate Overlaid bilingual embeddings English words are plotted in yellow boxes and Chinese words in green reference translations to English are provided in boxes with green borders directly below the original word Vector Matching Alignment AER lower is bet ter Results on Chinese Semantic Similarity Results on Named Entity Recognition NIST08 ChineseEnglish translation BLEU PATTree Instantiation for Figure 1 In the extraction process the PATtree is Results for feature combination Results for value setting Results for initial ranking manner Figure 5 Results for webpage snippet number 73 Experiment on Multiple Feature Fusion To verify the effectiveness for multiple feature fusion the test on the feature combination for OOV term translation is implemented As shown in Table 1 the highest accuracy the percentage of the correct translations in all the extracted translations of 831367 can be ac OOV term translation examples Results for EnglishChinese CLIR com bining our OOV term translation model Results for 4fold sitewise crossvalidation us ing the DP corpus DP corpus comparison for OPUS features based on frequent vs domainrelevant verbs Results on the Bitter Lemons corpus Architecture of the translation approach based on Bayes decision rule Wordtoword alignment Example of a word alignment and of ex tracted alignment templates Bilingual training corpus recognition lex icon and translation lexicon PM punctuation mark Illustration of search in statistical trans lation Illustration of bottomtotop search Comparison of three statistical translation approaches test on text input 251 sentences 2197 words 430 punctuation marks Sentence error rates of endtoend evalua tion speech recognizer with WER25 corpus of 5069 and 4136 dialogue turns for translation Ger man to English and English to German respec tively Disambiguation examples using morphosyntactic analysis Composition Gold Standards Sample of Gold Standard entries Results tested against gsso Diff results tested against gsswaco Results tested against gsswacosubjective LL results tested against gsswaco Detailed DIFF results Glue Semantics proof for 80 Swedish Directed Motion Construction Glue Semantics proof for 83 English Way Construction means interpretation Glue Semantics proof for 83 English Way Construction manner interpretation Glue Semantics proof for 86 English Way Construction means interpretation Bilingual training size vs BLEU score mid dle line left axis and phrase table composition top line right axis on Arabic Development Set The baseline BLEU score bottom line is included for comparison Accuracy of our system in each period M 10 Precision and recall for different values of Rank of correct translation for period Dec 01 Dec 15 and Dec 16 Dec 31 Cont rank is the context rank Trans Rank is the transliteration rank NA means the word cannot be transliterated insuff means the correct translation appears less than 10 times in the English part of the comparable corpus comm means the correct translation is a word ap pearing in the dictionary we used or is a stop word phrase means the correct translation contains multi ple English words Dataset Statistics Pronouns as Opinion Targets Results of AR for Opinion Targets Op Target Op Word Pair Extraction Graphical representation of our model Hyper parameters the stickiness factor and the frame and event initial and transition distributions are not shown for clar ity A partial frame learned by P RO F INDER from the MUC4 data set with the most probable emissions for each event and slot Labels are assigned by the authors for readability Results on MUC4 entity extraction CJ 2011 granularity refers to their experiment in which they mapped one of their templates to five learned clusters rather than one Results on TAC 2010 entity extraction with N best mapping for N 1 and N 5 Intermediate values of N produce intermediate results and are not shown for brevity Syntagmatic vs paradigmatic axes for words in a simple sentence Chandler 2007 Summary of results in terms of the MTO and VM scores Standard errors are given in parentheses when available Starred entries have been reported in the review paper Christodoulopoulos et al 2010 Distributional models use only the identity of the target word and its context The models on the right incorporate orthographic and morphological features MTO is not sensitive to the number of partitions used to discretize the substitute vector space within our experimental range MTO falls sharply for less than 10 SCODE dimensions but more than 25 do not help MTO is fairly stable as long as the Z constant 54 Morphological and orthographic features is within an order of magnitude of the real Z value MTO is not sensitive to the number of random substitutes sampled per word token Hinton diagram comparing most frequent tags and clusters AER results IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM We choose t 1 5 and 30 for the fertility HMM AER comparison encn Training time comparison The training time for each model is calculated from scratch For example the training time of IBM Model 4 includes the training time of IBM Model 1 the HMM and IBM Model 3 AER comparison cn en BLEU results An example sequence representation The subgraph on the left represents a bigram feature The subgraph on the right represents a unigram feature that states the entity type of arg 2 An example dependency parse tree rep resentation The subgraph represents a dependency relation feature between arg 1 Palestinians and of Comparison among the three feature sub spaces and the effect of including larger features Segmentation algorithm Known topic changes found in 90 generated texts using a block size of six Position of news story boundaries in a CNN news summary in relation to troughs found by the algorithm CNN news summaries segmented using different block sizes Example candidate space of dimensionality 2 Note I 1 2 J1 J2 1 2 3 We also show a local scoring function hw i j where w 2 1 and a local gold scoring function gi j Result of synthetic data learning experiment for MERT and PRO with and without added noise As the dimensionality increases MERT is unable to learn the original weights but PRO still performs adequately Machine translation performance for the experiments listed in this paper Scores are casesensitive IBM B LEU For every choice of system language pair and feature set PRO performs comparably with the other methods Data sizes for the experiments reported in this paper English words shown Summary of features used in experiments in this paper Comparison of MERT PRO and MIRA on tuning UrduEnglish SBMT systems and test results at every iteration PRO performs comparably to MERT and MIRA Tune and test curves of five repetitions of the same UrduEnglish PBMT baseline feature experiment PRO is more stable than MERT State classification by minimum input consumed for the Finnish dictionary The sizes of error models as automata The sizes of dictionaries as automata Effect of language and error models to quality recall proportion of suggestion sets containing a cor rectly suggested word Effect of language and error models to speed time in seconds per 10000 word forms Effect of text type on error models to speed in seconds per 10000 wordforms CTB 10fold CV word segmentation F measure for our word segmenter Comparison of word segmentation F measure for SIGHAN bakeoff3 tasks POS tagging accuracy using oneata time wordbased POS tagger POS tagging accuracy using oneata time characterbased POS tagger Summary table on the various methods investigated for POS tagging CTB 10fold CV word segmentation F measure using an allatonce approach CTB 10fold CV POS tagging accuracy using an allatonce approach Some of the words extracted from the small corpus Experiments on the thresholdprecision relationship of the small corpus Experiments on the word lengthprecision relationship of the small corpus Experiments on the thresholdpartial recall relationship of the small corpus Some words extracted from the large corpus Experiments on the thresholdprecision relationship of the large corpus Experiments on the thresholdpartial recall relationship of the large corpus Experiments on the word lengthprecision relationship of the large corpus with threshold nine Experiments on the word lengthprecision relationship of the large corpus with threshold three Numerictype compounds extracted Precision and partial recall of word lengths two to seven of the second experiment on IT and AV Precision and partial recall of word lengths two to four of the first experiment on IT and AV Semantic expansion example Note that the expanded queries that were generated in the first two retrieved texts listed under matched query do not contain the original query Examples for correct templates that were learned by TEASE for input templates Analysis of the number of relevant documents out of the top 10 and the total number of retrieved documents up to 100 for a sample of queries Translation example Experimental results using different smoothing methods Experimental results using different phrase ta bles OutBp the outofdomain phrase table AdapBp the adapted phrase table Effect of indomain monolingual corpus size on translation quality CORE 12 with inflectional features predicted input Top Adding all nine features to CORE 12 Second part Adding each feature separately comparing difference from CORE 12 Third part Greedily adding best features from second part Feature prediction accuracy and set sizes The set includes a NA value Lexical features Top part Adding each feature separately difference from CORE 12 predicted Bottom part Greedily adding best features from previous part Functional features gender number rationality Inflectionallexical features together Extended inflectional features Results on unseen test set for models which performed best on dev set predicted input Comparison of SWSD systems SO classifier with and without SWSD NP classifier with and without SWSD SO classifier with learned SWSD integration NP classifier with learned SWSD integration Polarity classifier with and without SWSD Results obtained by applying different types of features in isolation to the Baseline system Results obtained by adding different types of features incrementally to the Baseline system Examples errors introduced by YAGO and FrameNet Contingency table for the children of canine in the subject position of run Contingency table for the children of liquid in the object position of drink Example levels of generalization for different values of Extent of generalization for different values of and sample sizes Results for the pseudodisambiguation task Results for the pseudodisambiguation task with onefifth training data Disambiguation results for G2 and X2 Features used in baseline system Accuracy of 5fold crossvalidation with sta tisticsbased semantic features Accuracy of 5fold crossvalidation with self extracted semantic features based on different levels of syntacticsemantic relations Accuracy of 5fold crossvalidation with self extracted semantic features Architecture of the statistical translation approach based on Bayes decision rule Regular alignment example for the translation direction German to English For each German source word there is exactly one English target word on the alignment path Illustration of the transitions in the regular and in the inverted alignment model The regular alignment model left figure is used to generate the sentence from left to right the inverted alignment model right figure is used to generate the sentence from bottom to top DPbased algorithm for solving travelingsalesman problems due to Held and Karp The outermost loop is over the cardinality of subsets of already visited cities Illustration of the algorithm by Held and Karp for a traveling salesman problem with J 5 cities Not all permutations of cities have to be evaluated explicitly For a given subset of cities the order in which the cities have been visited can be ignored DPbased algorithm for statistical MT that consecutively processes subsets C of source sentence positions of increasing cardinality Word reordering for the translation direction German to English The reordering is restricted to the German verb group Order in which the German source positions are covered for the GermantoEnglish reordering example given in Figure 5 Word reordering for the translation direction English to German The reordering is restricted to the English verb group Order in which the English source positions are covered for the EnglishtoGerman reordering example given in Figure 7 Procedural description to compute the set Succ of successor hypotheses by which to extend a partial hypothesis S C j Illustration of the IBMstyle reordering constraint Number of processed arcs for the pseudotranslation task as a function of the input sentence length J yaxis is given in log scale The complexity for the four different reordering constraints MON GE EG and S3 is given The complexity of the S3 constraint is close to J4 Twolist implementation of a DPbased search algorithm for statistical MT Training and test conditions for the GermantoEnglish Verbmobil corpus number of words without punctuation Example translations for the translation direction German to English using three different reordering constraints MON GE and S3 Translation results for the translation direction English to German on the TEST331 test set The results are given in terms of computing time WER and PER for three different reordering constraints MON EG and S3 Demonstration of the combination of the two pruning thresholds tC 50 and tc 125 to speed up the search process for the two reordering constraints GE and S3 no 50 The translation performance is shown in terms of mWER on the TEST331 test set Training and test conditions for the Hansards task number of words without punctuation Example translations for the translation direction English to German using three different reordering constraints MON EG and S3 Example translations for the translation direction French to English using the S3 reordering constraint Outline of the segmentation process Scores for UPUC corpus Scores for MSRA corpus Scores for CityU corpus Example of a long jump alignment grid All possible deletion insertion identity and substitution op erations are depicted Only long jump edges from the best path are drawn Corpus statistics of the MATR MT06 corpus that was used for experimental evaluation of the proposed measures Example of worddependent substitution costs Pearsons r and Kendalls absolute between adequacy and automatic evaluation measures on different levels of the MATR MT06 data Examples of TERp alignment output In each example R H and H denote the reference the original hypothesis and the hypothesis after shifting respectively Shifted words are bolded and other edits are in brackets Number of edits shown TERp TER Metric correlations with adequacy on the MetricsMATR 2008 development set Correlations are significantly different if the center point of one correlation does not lie within the confidence interval of the other correlation TERp edit costs optimized for adequacy Pearson correlation of TERp with selective features Numbers of expressions of all the differ ent types from the DISCO and Reddy datasets All the parameters of Measures for de termining semantic compositionality described in Section 3 used in our experiments All the parameters of WSMs described in Section 2 used in all our experiments Semicolon denotes OR All the examined combinations of parameters are implied from reading the diagram from left to right Parameters of WSMs Section 2 which combined with particular Measures achieved the highest average correlation in TrValD Parameters of Measures Section 3 which combined with particular WSMs achieved the highest average correlation in TrValD An example graph modeling relations between mentions Number of clustering decisions made ac cording to mention type rows anaphor columns antecedent and percentage of wrong decisions Results of different systems on the CoNLL12 English data sets Number of recall errors according to mention type rows anaphor columns antecedent Precision statistics for pronouns Rows are pronoun surfaces columns number of cluster ing decisions and percentage of wrong decisions for all and only anaphoric pronouns respectively Removal and reduction of constituents using dependencies Different setups for entityrelated se mantic tree EST Contribution of constituent dependen cies in respective mode inside parentheses and accumulative mode outside parentheses Improvements of different tree setups Translation of PCC sample commentary Screenshot of Annis Linguistic Database Selection and weighting of words from the collocation network Automaton for topic shift detection Pk for C99 corpus Pk for Le Monde corpus Precisionrecall for Le Monde corpus Error rates for Le Monde corpus POS Tagging of Unknown Word using Contextual and Lexical features in a Sequential Model The input for capitalized classifier has 2 values and therefore 2 ways to create confusion k sets There are at most F different in puts for the suffix classifier 26 character 10 digits 5 other symbols therefore suffix may k emit up to R R confusion sets POS tagging of unknown words using contextual and lexical Features accuracy in per cent is based only on contextual features T is based on contextual and lexical features SM 2 denotes that follows in the sequential model 2 POS tagging of unknown words using contextual features accuracy in percent is a classifier that uses only contextual features baseline is the same classifier with the addition of the baseline feature NNP or NN Processing time for POS tagging of known words using contextual features In CPU seconds Train training time over sentences Brills learner was interrupted after 12 days of train ing default threshold was used Test average number of seconds to evaluate a single sentence All runs were done on the same machine POS Tagging of known words using con textual features accuracy in percent onevsall denotes training where example serves as positive example to the true tag and as negative example to all the other tags SM R denotes training where 2 example serves as positive example to the true tag Combinatorial Search Problems in Decoding Average decoding time P r f ae for IBM Models Log score NIST scores 5fold crossvalidation results on training data Results obtained on the official test set of the 2011 DDI Extraction challenge LII filtering refers to the techniques proposed in Chowdhury and Lavelli 2012b for reducing skewness in RE data distribution stat sig in dicates that the improvement of Fscore due to usage of Stage 1 classifier is statistically significant verified using Approximate Randomization Procedure Noreen 1989 number of iterations 1000 confidence level 001 Examples of phrase meaningfulness Note that the comments are not presented to Turkers Examples given in the description of Task 2 Plate diagram depicting the joint model Hyper parameters have been omitted for clarity The Lshaped plate contains the tokens while the square plates contain the morphological analyses The t are latent tags zi is an assignment to a morphological analysis lk sk fk and wi is the observed word T is the number of distinct tags and Kt the number of tables used by tag type t Plate diagram depicting the morphology model adapted from Goldwater et al 2006 Hyperparameters have been omitted for clarity The lefthand plate depicts the base distribution P0 note that the morphological anal yses lk are generated deterministically as tk sk fk The observed words wi are also deterministic given zi k and lk since wi sk fk The posterior distribution of our joint model Because the sequence of words w is deterministic given analyses l and assignments to analyses tables z the joint posterior over all variables Pw t l zt a b s f is equal to Pt l zt a b s f when lzi wi for all i and 0 otherwise We give equations for the nonzero case ns refer to token counts ms to table counts We add two dummy tokens at the start end and between sentences to pad the context history Example sentences in the synthetic languages Words in Category 1 are made of characters ad Cate gory 2 eh Category 3 mp Category 4 ru Suffixes in Language B are separated with periods for illustrative purposes only Log probability of the sampler state over 1000 iterations on Languages A and B Spanish Ornat corpus results Standard devia tions are in parentheses denotes a significant difference from the M ORTAG model English Eve corpus results Standard deviations are in parentheses denotes a significant difference from the M ORTAG model Left An entailment graph For clarity edges that can be inferred by transitivity are omitted Right A hierarchical summary of propositions involving nausea as an argument such as headache is related to nausea acupuncture helps with nausea and Lorazepam treats nausea Results for all experiments Subgraph of Local1 output forheadache Subgraph of tunedLP output for headache Comparing disagreements between the best local and global algorithms against the gold standard Parse Feature Example for the sentence GM says the addition of OnStar which includes a system that automatically notifies an OnStar operator if the vehicle is involved in a collision complements the Vues top fivestar safety rating for the driver and front passenger in both front and sideimpact crash tests Training Data Sizes for Common ESL Confused Words Spelling correction accuracy impact of combining word cooccurrence CLASSIFIER Logistic Regression trained on 1G words of news text tested on 9months NYT data COMBINED SYSTEM CLASSIFER plus system based on firstorder word cooccurrence Relative increase or decrease in error rate compared to CLASSIFIER As in Bergsma et al 2009 2010 no morphological variants of the words are used in evaluation Spelling correction precision impact of adding parse features SVM trained on 1G words of news text tested on 9months of NYT data Improvement of NGLEXPAR vs NGLEX is statistically significant Improvement of NGLEXPAR vs NG is statistically significant Relative increase or decrease of error rate compared to NGLEX As in Bergsma et al 2009 2010 no morphological variants of the words are used in evaluation The semantic representations of a word W its inverse W inv and its negation W The domain part of the representation remains un changed while the value part will partially be in verted inverse or inverted and scaled negation with 0 1 The separate functional repre sentation also remains unchanged A partially scaled and inverted identity matrix J Such a matrix can be used to trans form a vector storing a domain and value repre sentation into one containing the same domain but a partially inverted value such as W and W de scribed in Figure 1 The parse tree for This car is not blue highlighting the limited scope of the negation Relation between number of classes and alternations Estimation of model parameters Estimation of Fc f v and Fv c Ten most frequent classes using equal distribution of verb frequencies Estimation of Fv c for the verb feed Ten most frequent classes using unequal distribution of verb frequencies Smoothed estimates Model accuracy using equal distribution of verb frequencies for the estimation of Pc Model accuracy using unequal distribution of verb frequencies for the estimation of Pc Model accuracy using unequal distribution of verb frequencies for the estimation of Pc Model accuracy using equal distribution of verb frequencies for the estimation of Pc Semantic preferences for verbs with the doubleobject frame Features for collocations Word sense disambiguation accuracy for NP1 V NP2 to NP3 frame Word sense disambiguation accuracy for NP1 V NP2 frame Word sense disambiguation accuracy for NP1 V NP2 NP3 frame Word sense disambiguation accuracy for NP1 V NP2 for NP3 frame Word sense disambiguation accuracy for NP1 V NP2 NP3 frame Word sense disambiguation accuracy for NP1 V NP2 frame Word sense disambiguation accuracy for NP1 V to NP2 NP3 frame Word sense disambiguation accuracy for NP1 V for NP2 NP3 frame Examples of context free and contextsensitive sub trees related with Figure 1b Note the bold node is the root for a subtree Different tree span categories with SPT dotted circle and an ex ample of the dynamic contextsensitive tree span solid circle Evaluation of contextsensitive convolution tree kernels using SPT on the major relation types of the ACE RDC 2003 inside the parentheses and 2004 outside the parentheses corpora Comparison of dynamic contextsensitive tree span with SPT using our contextsensitive convolution tree kernel on the major relation types of the ACE RDC 2003 inside the parentheses and 2004 outside the parentheses corpora 18 of positive instances in the ACE RDC 2003 test data belong to the predicatelinked category Comparison of difference systems on the ACE RDC 2004 corpus over both 7 types outside the parentheses and 23 subtypes inside the parentheses Comparison of difference systems on the performs the stateoftheart Collins and Duffys con ACE RDC 2003 corpus over both 5 types outside the volution tree kernel It also shows that featurebased parentheses and 24 subtypes inside the parentheses Possible Relations between ARG1 and ARG2 Evaluation on Structure Features Evaluation of Correction and Inference Mechanisms Evaluation of Feature and Their Combinations Evaluation of Two Detection and Classification Modes Imbalance Training Class Problem IBM Model 3 Linguistic levels as feature sets Semantic features Accuracy results for binary decisions Accuracy results for combined decisions Comparison of accuracy scores across linguistic levels Levels all and morph against the Gold Standard Results for ensemble classifier Example of BLC selection Most frequent monosemic words in BG Most frequent BLC20 semantic classes on WordNet 30 Number of training examples Results of task17 a RM and large margin solution comparison and b the spread of the projections given by each RM and large margin solutions are shown with a darker dotted line and a darker solid line respectively RM update with margin and bounding con straints The diagonal dotted line depicts costmargin equi librium The vertical gray dotted line depicts the bound B White arrows indicate updates triggered by constraint viola tions Squares are data points in the kbest list not selected for update in this round Corpus statistics Active sparse feature templates Performance on ArEn with basic left and sparse right feature sets on MT05 and MT08 RM gain over other optimizers averaged over all test sets Performance on ZhEn with basic left and sparse right feature sets on MT03 and MT05 The B I E S Tag Set Example of Lattice Used in the Markov ModelBased Method Example of the Character Tagging Method Word boundaries are indicated by vertical lines Example of the Hybrid Method Character Types Calculated Values of i Statistical Information of Corpora Performance of Japanese Word Segmentation Performance of Chinese Word Segmentation Case restoration performance using an MDtrie English Improvement in fscore through restoring case Accuracy on seen and unseen tokens Recognition performance Final results for English and German develop ment and test sets GETARUNS AR algorithm Expletive it compared results GETARUNS pronouns collapsed at structural level Overall results CoverageAccuracy The regular expressions available in Foma from highest to lower precedence Horizontal lines separate precedence classes A relative comparison of running a se lection of regular expressions and scripts against other finitestate toolkits The first and second en tries are short regular expressions that exhibit ex ponential behavior The second results in a FSM with 221 states and 222 arcs The others are scripts that can be run on both XeroxPARC and Foma The file lexiconlex is a LEXC format English dic tionary with 38418 entries North Sami is a large lexicon lexc file for the North Sami language available from httpdivvunno Coreference Definition Differences for MUC and ACE GPE refers to geopolitical entities Dataset characteristics including the number of documents annotated CEs coreference chains annotated CEs per chain average and number of documents in the traintest split We use st to indicate a standard traintest split Impact of Three Subtasks on Coreference Resolution Performance A score marked with a indicates that a 05 threshold was used because threshold selection from the training data resulted in an extreme version of the system ie one that places all CEs into a single coreference chain Correlations of resolution class scores with respect to the average Frequencies and scores for each resolution class Predicted P vs Observed O scores Notation used in this article Examples of word alignment patterns in GermanEnglish that require the increased expressive power of synchronous tree adjoining grammar Examples of dependency trees with word alignment Arrows are drawn from children to parents A child word is a modifier of its parent Each word has exactly one parent and is a special wall symbol that serves as the parent of all root words in the tree ie those with no other parent Example of a sentence pair containing a frequentlyobserved sibling relationship in GermanEnglish data in the theus dependency the aligned German words are siblings in the source dependency tree This occurs due to differences in treebank and head rule conventions between the two data sets The German parser produces flat PPs with little internal structure so when the dependency tree is generated each word in the PP attaches to the P the head of the phrase Key definitions for our model Example output of our model for ChineseEnglish translation The wordsegmented Chinese sentence and dependency tree are inputs Our models outputs include the English translation phrase segmentations for each sentence a box surrounds each phrase a onetoone alignment between the English and Chinese phrases and a projective dependency tree on the English phrases Note that the Chinese dependency tree is on words whereas the English dependency tree is on phrases Most frequent phrase dependencies in DEEN data shown with their counts and attachment directions Child phrases point to their parents To focus on interesting phrase dependencies we only show those in which one phrase has at least two tokens and neither phrase is entirely punctuation The words forming the longest lexical dependency in each extracted phrase dependency are shown in bold these are used for backoff features Top 60 most frequent root phrases in DEEN data with at least two words shown with their counts Shown in bold are the actual root words in the lexical dependency trees from which these phrases were extracted these are extracted along with the phrases and used for backoff features Most frequent Brown cluster phrase dependencies extracted from DEEN data shown with their counts As in Table 4 we only show those in which one phrase has at least two tokens and neither phrase is entirely punctuation Each cluster is shown as a set of words large enough to cover 95 of the token counts in the cluster up to a maximum of four words It is characteristic of Brown clustering that very frequent tokens eg function words often receive their own clusters Examples of illustrative sentence pairs and frequently extracted rules that model verb movement between German and English An ellipsis indicates that there must be material between the two phrases for the rule to apply a Example of movement of the finite verb to the end of a dependent clause b Example of movement of an infinitive to the end of an independent clause following a modal verb mochte would like Discussion of the features used to score these stringtotree rules is given in Section 52 Stringtotree configurations each is associated with a feature that counts its occurrences in a derivation Quasisynchronous treetotree configurations from Smith and Eisner 2006 There are additional configurations involving NULL alignments and an other category for those that do not fit into any of the named categories Lattice dependency parsing using an arcfactored dependency model Lone indices like p and i denote nodes in the lattice and an ordered pair like i j denotes the lattice edge from node i to node j S TART is the single start node in the lattice and F INAL is a set of final nodes We use edgeScorei j to denote the model score of crossing lattice edge i j which only includes the phrasebased features h 0 We use arcScorei j l m to denote the score of building the dependency arc from lattice edge i j to its parent l m arcScore only includes the QPD features h 00 BLEU on tune and test sets for ZHEN translation showing the contribution of feature sets in our QPD model Both QPD models are significantly better than the best Moses numbers on test sets 1 and 2 but not on test set 3 The full QPD model is significantly better than the version with only T GT T REE features on test set 1 but statistically indistinguishable on the other two test sets Hiero is significantly better than the full QPD model on test set 2 but not on the other two BLEU on tune and test sets for DEEN translation comparing the baselines to our QPD model with target syntactic features T GT T REE and then also with source syntax T REE T O T REE Here merely using the additional round of tuning with the SSVM reranker improves the BLEU score to 199 which is statistically indistinguishable from the two QPD feature sets Differences between Hiero and the three 199 numbers are at the border of statistical significance the first two are statistically indistinguishable from Hiero but the third is different at p 004 BLEU on tune and test sets for UREN translation using our unsupervised Urdu parser to incorporate source syntactic features The two QPD rows are statistically indistinguishable on both test sets Both are significantly better than all Moses results but Hiero is significantly better than all others BLEU on tune and test sets for ENMG translation using a supervised English parser and an unsupervised Malagasy parser The 156 BLEU reached by the full QPD model is statistically significantly better than all other results on the test set All other test set numbers are statistically indistinguishable BLEU on tune and test sets when comparing parsers for ZHEN translation QPD uses all features including T GT T REE and T REE T O T REE The table first pairs supervised English parsing with supervised unsupervised and random Chinese parsing then pairs unsupervised English parsing with supervised and unsupervised Chinese parsing significantly better than supsup significantly worse than supsup Feature ablation experiments for UREN translation with stringtotree features showing the drop in BLEU when separately removing word W ORD cluster C LUST and configuration C FG feature sets significantly worse than T GT T REE Removing word features causes no significant difference Removing cluster features results in a significant difference on both test sets and removing configuration features results in a significant difference on test 2 only Results of human evaluation performed via Amazon Mechanical Turk The percentages represent the portion of sentences for which one system had more preference judgments than the other system If a sentence had an equal number of judgments for the two systems it was counted in the final row neither preferred BLEU on tune and test sets for UREN translation comparing several settings for maximum dependency lengths in the decoder x is for the source side and y is for the target side The upper table shows Moses BLEU scores for comparison The lower table compares two max dependency length settings during tuning and several for decoding on the test sets showing both BLEU scores and average decoding times per sentence See text for discussion Four ambiguous words their senses and frequency Mutual information between feature subset and class label with f req based feature ranking Mutual information between feature subset and class label with 2 based feature ranking Average accuracy over three procedures in Figure 1 as a function of context window size horizontal axis for 4 datasets Results for three procedures over 4 datases The horizontal axis corresponds to the context window size Solid line represents the result of F SGM M binary dashed line denotes the result of CGDSV D idf and dotted line is the result of CGDterm idf Square marker denotes 2 based feature ranking while cross marker denotes f req based feature ranking Average accuracy of three procedures with various settings over 4 datasets Automatically determined mixture component num Discourse tree for two sentences in RSTDT Each of the sentences contains three EDUs The second sentence has a wellformed discourse tree but the first sentence does not have one Discourse parsing framework Distributions of six most frequent relations in intrasentential and multisentential parsing scenarios A chainstructured DCRF as our intra sentential parsing model Our parsing model applied to the sequences at different levels of a sentencelevel DT a Only possible se quence at the first level b Three possible sequences at the second level c Three possible sequences at the third level A CRF as a multisentential parsing model Features used in our parsing models Extracting subtrees for S2 Two possible DTs for three sentences Confusion matrix for relation labels on the RSTDT test set Yaxis represents true and Xaxis repre sents predicted relations The relations are TopicChange TC TopicComment TCM Textual Organization T O MannerMeans MM Comparison CMP Evaluation EV Summary SU Condition CND Enablement EN Cause CA Temporal TE Explanation EX Background BA Contrast CO Joint JO SameUnit SU Attribu tion AT and Elaboration EL Parsing results of different models using manual gold segmentation Performances significantly superior to HILDA with p71e05 are denoted by Significant differences between TSP 11 and TSP SW with p001 are denoted by RST Spanish Treebank statistics Rhetorical relations in RST Spanish Treebank Example of the nonrelation SameUnit Interannotator agreement Topic transfer in bilingual LSA model DirichletTree prior of depth two Parallel topics extracted by the bLSA model Top words on the Chinese side are translated into English for illustration purpose English word perplexity PPL on the RT04 test set using a unigram LM Comparison of training log likelihood of English LSA models bootstrapped from a Chinese LSA and from a flat monolingual English LSA Word perplexity with different using manual reference or ASR hypotheses on CCTV BLEU score for those 25 utterances which resulted in different translations after bLSA adaptation manual transcriptions Translation performance of baseline and bLSAAdapted ChineseEnglish SMT systems on manual transcriptions and 1best ASR hypotheses Examples of new alternations New Verb Classes Average results for 35 verbs Relation weights Method 2 Graph of Word Senses Results of SVM and Mincuts with different settings of feature Accuracy with different sizes of unlabeled data random selection Accuracy for Different PartOfSpeech Learning curve with different sizes of labeled data Accuracy with different sizes of labeled data Accuracy with different sizes of unlabeled data from WordNet relation The clausal and topological field structure of a German sentence Notice that the subordinate clause receives its own topology a An example of a document from TuBaDZ b an abbreviated entity grid representation of it and c the feature vector representation of the abbreviated entity grid for transitions of length two Mentions of the entity Frauen are underlined nom nominative acc accusative oth dative oblique and other arguments Accuracy of the permutation de tection experiment with various entity represen tations using manual and automatic annotations of topological fields and grammatical roles The baseline without any additional annotation is un derlined Twotailed sign tests were calculated for each result against the best performing model in each column 1 p 0101 2 p 0053 statis tically significant p 005 very statistically significant p 001 Accuracy of permutation detection experiment with various entity representations us ing manual and automatic annotations of topolog ical fields and grammatical roles on subset of cor pus used by Filippova and Strube 2007a Results of adding coherence features into a natural language generation system VF Acc is the accuracy of selecting the first constituent in main clauses Acc is the percentage of per fectly ordered clauses tau is Kendalls on the constituent ordering The test set contains 2246 clauses of which 1662 are main clauses Association frequencies for target verb Association overlap for target verbs Coverage of verb association features by grammarwindow resources Accuracy for induced verb classes Sample pairs of similar caseframes by relation type and the similarity score assigned to them by our distributional model A sentence decomposed into its depen dency edges and the caseframes derived from those edges that we consider in black Average sentence cover size the average number of sentences needed to generate the case frames in a summary sentence Study 1 Model summaries are shown in darker bars Peer system numbers that we focus on are in bold The average number of source text sen tences needed to cover a summary sentence The model average is statistically significantly differ ent from all the other conditions p 107 Study 1 Signature caseframe densities for differ ent sets of summarizers for the initial and update guided summarization tasks Study 2 p 0005 Density of signature caseframes Study 2 Examples of signature caseframes found in Study 2 Density of signature caseframes after merging to various threshold for the initial Init and update Up summarization tasks Study 2 The effect on caseframe coverage of adding indomain and outofdomain documents The difference between adding indomain and out ofdomain text is significant p 103 Study 3 Coverage of caseframes in summaries with respect to the source text The model aver age is statistically significantly different from all the other conditions p 108 Study 3 Coverage of summary text caseframes in source text Study 3 Latent Dependency coupling for the RE task The DC ONNECT factor expresses ternary connection re lations because the shared head word of the proposed re lation is unknown As is convention variables are repre sented by circles factors by rectangles Relation Extraction Results Models using hidden constituency syntax provide significant gains over the syntacticallyuniformed baseline model in both languages but the advantages of the latent syntax were mitigated on the smaller Chinese data set A tiered graphic representing the three different SRL model configurations The baseline system is described in the bottom c d the separate panels highlighting the independent predictions of this model sense labels are assigned in an entirely separate process from argument prediction Pruning in the model takes place primarily in this tier since we observe true predicates we only instantiate over these indices The middle tier b illustrates the syntactic representation layer and the connective factors between syntax and SRL In the observed syntax model the Link variables are clamped to their correct values with no need for a factor to coordinate them to form a valid tree Finally the hidden model comprises all layers including a combinatorial syntactic constraint a over syntactic variables In this scenario all labels in b are hidden at both training and test time Examining the learned hidden representation for SRL In this example the syntactic dependency arcs derived from gold standard syntactic annotations left are entirely disjoint from the correct predicatearguments pairs shown in the heatmaps by the squares outlined in black and the observed syntax model fails to recover any of the correct predictions In contrast the hidden model structure right learns a representation that closely parallels the desired end task predictions helping it recover three of the four correct SRL predictions shaded arcs red corresponds to a correct prediction with true labels GA KARA etc and providing some evidence towards the fourth The dependency tree corresponding to the hidden structure is derived by edgefactored decoding dependency variables whose beliefs 05 are classified as true though some arcs not relevant to the SRL predictions are omitted for clarity SRL Results The hidden model excels on the unlabeled prediction results often besting the scores obtained using the parses distributed with the CoNLL data sets These gains did not always translate to the labeled task where poor sense prediction hindered absolute performance a An example in which an English sentence is parsed into a tree structure with 12 PCFG rules b an instance in which a Chinese sentence both Chinese characters and Chinese Pinyin are provided and note that we will use Chinese Pinyin throughout the paper is converted into an English tree using 6 STSG rules The symbol to the upper right of a node indicates that this node is constructed using rule Lexicalized training example POS of target headword is not explicitly given Examples of rules used during decoding Two real translation examples Example MERT values along one coordi nate first unregularized When regularized with 2 the piecewise constant function becomes piecewise quadratic When using 0 the function remains piecewise constant with a point discontinuity at 0 Datasets for the two experimental conditions Comparison of rate of convergence between coordinate ascent and our expected BLEU direction finder D 500 Noisy refers to the noisy experimental setting BLEU scores for GBM features Model parameters were optimized on the Tune set For PRO and regularized MERT we optimized with different hyperparameters regularization weight etc and retained for each experimental condition the model that worked best on Dev The table shows the performance of these retained models BLEU scores for SparseHRM features Notes in Table 2 also apply here A tree showing head information Perplexity results for two previous grammarbased language models A nounphrase with substructure Perplexity results for the immediate bihead model Perplexity results for the immediate trihead model Precisionrecall for sentences in which trigramgrammar models performed best An example of the label consistency problem Here we would like our model to encourage entities Albert Einstein and Einstein to get the same label so as to improve the chance that both are labeled PERSON Table showing the number of token sequence token subsequence pairs where the token sequence is assigned a certain entity label and the token subsequence is assigned a certain entity label We show these counts both within documents as well as over the whole corpus Rows correspond to sequences and columns to subsequences These statistics are from the CoNLL 2003 English training set Table showing the number of pairs of different occurrences of the same token sequence where one occurrence is given a certain label and the other occurrence is given a certain label We show these counts both within documents as well as over the whole corpus As we would expect most pairs of the same entity sequence are labeled the sameie the diagonal has most of the density at both the document and corpus levels These statistics are from the CoNLL 2003 English training set Table showing improvements obtained with our additional features over the baseline CRF We also compare our performance against Bunescu and Mooney 2004 and Finkel et al 2005 and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF Equivalent Left LM State Computation BLEU scores after discriminative hypergraph reranking Only the language model LM or the transla tion model TM or both LMTM may be discrimina tively trained to prefer the oraclebest hypotheses Baseline and oraclebest 4gram BLEU scores with 4 references for NIST ChineseEnglish MT datasets Speed of oracle extraction from hypergraphs The basic dynamic program Sec 21 improves signifi cantly by collapsing equivalent oracle states Sec 22 Accuracy in Lexical Sample Tasks System Pairwise Agreement Illustration on temporality Symmetry of window size Optimality of window size The translation examples where shaded cells indicate the correctly translated pairs MRR Precision Recall and F1score Performance using FBIS training corpus top and NIST corpus bottom Improvements are significant at the p 005 level except where indicated ns Corpus statistics Filters to improve the dictionary precision Un less otherwise noted the filter was applied if either men tion in the relation satisfied the condition Rules of the baseline system Coreference relations in our dictionary Dataset statistics development dev and test Performance on the test set Scores are on gold mentions Stars indicate a statistically significant difference with respect to the baseline Incremental results for the four sieves using our dictionary on the development set Baseline is the Stanford system without the WordNet sieves Scores are on gold mentions Supersense evaluation results Values are the percentage of correctly assigned supersenses k indicates the number of nearest neighbours considered Pseudodisambiguation Percentage of correct choices made Lbound denotes the Web1T lower bound on the a1 n bigram size the number of decisions made Results on the unseen plausibility dataset a A standard PTB parse of Example 1a b The MWE part of speech functions syntactically like the ordinary nominal category as shown by this paraphrase c We incorporate the presence of the MWE into the syntactic analysis by flattening the tree dominating part of speech and introducing a new nonterminal label multiword noun MWN for the resulting span The new representation classifies an MWE according to a global syntactic type and assigns a POS to each of the internal tokens It makes no commitment to the internal syntactic structure of the MWE however Semifixed MWEs in French and English The French adverb terme in the end can be modified by a small set of adjectives and in turn some of these adjectives can be modified by an adverb such as trs very Similar restrictions appear in English French grammar development Incremental effects on grammar size and labeled F1 for each of the manual grammar features development set sentences 40 words The baseline is a parentannotated grammar The features tradeoff between maximizing two objectives overall parsing F1 and MWE F1 Unknown word model features for Arabic and French DPTSG notation For consistency we largely follow the notation of Liang Jordan and Klein 2010 Example of two conflicting sites of the same type in a training tree Define the type of a def site tz s ns0 ns1 Sites 1 and 2 have the same type because tz s1 tz s2 The two sites conflict however because the probabilities of setting bs1 and bs2 both depend on counts for the tree fragment rooted at NP Consequently sites 1 and 2 are not exchangeable The probabilities of their assignments depend on the order in which they are sampled Gross corpus statistics for the preprocessed corpora used to train and evaluate our models We compare to the WSJ section of the PTB train Sections 0221 dev Section 22 test Section 23 Due to its flat annotation style the FTB sentences have fewer constituents per sentence In the ATB morphological variation accounts for the high proportion of word types to sentences Frequency distribution of the MWE types in the ATB and FTB training sets French standard parsing experiments test set sentences 40 words FactLex uses basic POS tags predicted by the parser and morphological analyses from Morfette FactLex uses gold morphological analyses Arabic standard parsing experiments test set sentences 40 words SplitPCFG is the same grammar used in the Stanford parser but without the dependency model FactLex uses basic POS tags predicted by the parser and morphological analyses from MADA FactLex uses gold morphological analyses Berkeley and DPTSG results are the average of three independent runs French MWE identification per category and overall results test set sentences 40 words MWI and MWCL do not occur in the test set Arabic MWE identification per category and overall results test set sentences 40 words MWE identification F1 of the parsing models vs the mwetoolkit baseline test set sentences 40 words FactLex uses gold morphological analyses at test time Sample of humaninterpretable French TSG rules Sample of humaninterpretable Arabic TSG rules Recursive rules like MWAA MWA result from memoryless binarization of nary rules This preprocessing step not only increases parsing accuracy but also allows the generation of previously unseen MWEs of a given type An underspecified discourse structure and its five configurations A wRTG modelling the interdependency constraint for Fig 1 A filter RTG corresponding to Ex 2 C2 Figure 3 An underspecified d A RTG integrating the attachment constraint for Contrast from Ex 2 into Fig 3 Texts used for the evaluation Normalization accuracy after training on n tokens and evaluating on 1000 tokens average of 10 random training and evaluation sets compared to the baseline score of the full text without any normalization Tagging accuracy on the goldstandard normalizations OrigP original punctuation ModP modern punctuation NoP no punctu ation Tagging accuracy on the combined TIGERTuba corpus using 10fold CV evaluated with and without capitalization punctuation and sentence boundaries SB POS tagging accuracy on texts without punctuation and capitalization for tagging on the original data the goldstandard normalization and automatic normalizations using the first n tokens as training data Growth of the Wiktionary over the last three years showing total number of entries for all languages and for the 9 languages we consider left axis We also show the corresponding increase in average accuracy right axis achieved by our model across the 9 languages see details below Typelevel top and tokenlevel bottom cov erage for the nine languages in three versions of the Wik tionary Examples of constructing Universal POS tag sets from the Wiktionary Word type coverage by normalized frequency words are grouped by word count highest word count ratio low 0 001 medium 001 01 high 01 1 PTB vs Wiktionary type coverage across sec tions of the Brown corpus The Wiktionary vs tree bank tag sets Around 90 of the Wiktionary tag sets are identical or subsume tree bank tag sets See text for details Tag errors broken down by the word type clas sified into the six classes oov identical superset subset overlap disjoint see text for detail The largest source of error across languages are outofvocabulary oov word types followed by tag set mismatch types subset over lap disjoint Model accuracy across the Brown cor pus sections ST Stanford tagger Wik Wiktionary tagsettrained SHMMME PTBD PTBtagsettrained SHMMME PTB Supervised SHMMME Wik outper forms PTB and PTBD overall Accuracy for Unsupervised Bilingual Wiktionary and Supervised models Avg is the average of all lan guages except English Unsupervised models are trained without dictionary and use an oracle to map tags to clusters Bilingual systems are trained using a dictionary transferred from English into the target language using word align ments The Projection model uses a dictionary build directly from the partofspeech projection The DP model extends the Projection model dictionary by using Label Propagation Supervised models are trained using tree bank information with SHMMME Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary and All TBD uses tree bank tag sets for all words 50 100 and All Sent models are trained in a supervised manner using increasing numbers of training sentences Experimental results over the 120 evalu ation sentences Alignment error rates in both di rections are provided here Results on the Arabic Treebank ATB data set We compare our models against Poon et al 2009 PCT09 and the Morfessor system MorfessorCAT For our full model T OKEN S EG and its simplifica tions BASIC POS T OKEN POS we perform five random restarts and show the mean scores The sample standard deviations are shown in brackets The last col umn shows results of a paired ttest against the preceding model significant at 1 significant at 5 not significant test not applicable Segmentation performance on words that have the same final suffix as their preceding words The F1 scores are computed based on all boundaries within the words but the accuracies are obtained using only the final suffixes Segmentation performance on words that begin with prefix Al determiner and end with suffix At plural noun suffix The mean F1 scores are computed using all boundaries of words in this set For each word we also determine if both affixes are recovered while ig noring any other boundaries between them The other two columns report this accuracy at both the typelevel and the tokenlevel Results Chinese character usage in 3 corpora The numbers in brackets indicate the percentage of characters that are shared by at least 2 corpora Number of entries in 3 corpora Number of unique entries in training and test sets categorized by semantic attributes Language detection accuracies using a 4gram language model for the letter sequence of the source name in Latin script The effect of language and gender in MRR performance of phonetic translit eration for 3 corpora using unigram and bigram language models Gender detection accuracies using a 4gram language model for the letter sequence of the source name in Latin script Overall transliteration performance The effect of language detection The effect of gender detection schemes Number of synset relations TS of partyn1 first 10 out of 12890 total words First ten words with weigths and number of senses in WN of the Topic Signature for airportn1 obtained from BNC using InfoMap Minimum distances from airportn1 Sense disambiguated TS for airportn1 obtained from BNC using InfoMap and SSIDijkstra Size and percentage of overlapping relations between KnowNet versions and WNXWN Percentage of overlapping relations between KnowNet versions P R and F1 finegrained results for the resources evaluated at Senseval3 English Lexical Sample Task Results of the French to English system WMT2012 The marked system corresponds to the system submitted for manual evaluation cs casesensitive ci caseinsensitive Rules for morphological simplification Text normalization for FREN Processing steps for the input sentence dire warnings from pentagon over potential defence cuts Results for French inflection prediction on the WMT2012 test set The marked system corresponds to the system submitted for manual evaluation Russian to English machine translation system evaluated on WMT2012 and WMT2013 Human evaluation in WMT13 is performed on the system trained using the original corpus with TA GIZA for alignment marked with Rules for simplifying the morphological complexity for RU Results on WMT2013 blindtest An example of alignment for Japanese and English sentences Translation Model IBM Model 4 string insertion operator for lefttoright decoding method A string e0 was appended after the partial output string e and the last word in e 0 was aligned from f j string insertion operation for righttoleft decoding method A string e0 was prepended before the partial output string e and the first word in e 0 was aligned from f j Merging lefttoright and righttoleft hypotheses ef and eb in bidirectional decoding method Figure 5a merge two open hypotheses while Figure 5b merge them with inserted zero fer tility words Comparison of the three decoders by the ratio each decoder produced search errors Statistics on a travel conversation corpus Russian morphological disambiguation Evaluation of the Russian ngram model The trigram version of our language model rep resented as a graphical model G1w is the unigram model of 22 Evaluation of the Turkish ngram model Evaluation of Turkish predictive text input A complex TurkishEnglish word alignment alignment points in gray EMPYUV black PY US Word alignment experiments on EnglishTurkish entr and EnglishCzech encs data Our alignment model represented as a graphi cal model Key notation Feature factorings are elaborated in Tab 2 Factoring of global feature collections g into f xji denotes hxi xj i in sequence x hx1 i Decoding as lattice parsing with the highestscoring translation denoted by black lattice arcs others are grayed out and thicker blue arcs forming a dependency tree over them Eq 9 Loglikelihood Eq 10 Pseudolikelihood In both cases we maximize wrt Eqs 1113 Recursive DP equations for summing over t and a Comparison of size of kbest list for cube decoding with various feature sets Feature set comparison BLEU QG configuration comparison The name of each configuration following Smith and Eisner 2006 refers to the relationship between at j and aj in s Consistently formatted sentence trans lation pairs Consistently formatted term translation pairs The framework of our approach Example segmentations indicates the separator between adjacent snippets Character classes Performance of different settings Contribution of every feature Derivation with DRSs including conversion for A record date Com binatory rules are indicated by solid lines semantic rules by dotted lines CCG derivation as generated by the CC tools Boxer output for Shared Task Text 2 Total corpus sizes in sentences and number of Sure and Possible alignment links in their respective evaluation sets Results from the empirical evaluation including the Bayesian model without PoS tags Base line the alternating alignmentannotation algorithm AAA the corresponding method but with super vised PoS taggers for both languages Supervised and comparable previous results on the same data The number of alignment links A of which A S are considered Sure and A P Possible are reported For convenience precision P recall R F1 score F and Alignment Error Rate AER are also given Kendalls correlation over WMT 2013 all en for the full dataset and also the subset of the data containing a noun compound in both the reference and the MT output Pearsons r correlation results over the WMT allen dataset and the subset of the dataset that contains noun compounds Two examples from the allen dataset Each example shows a reference translation and the outputs of two machine translation systems In each case the output of MT system 1 is annotated as the better translation Definitions for the top four senses of law according to WordNet Example confounders for festival and laws and their similarities Pseudoword discrimination performance The error frequency distributions for confusing the correct sense with another sense of the given similarity when using a 5word cooccurrence window as context Dashed lines indicate the null models Unsupervised and Supervised scores on the SemEval2010 WSI Task for each feature and clustering models with reference scores for the top performing systems for each evaluation shown below A typed narrative chain The four top arguments are given The ordering O is not shown Graphical view of an unordered schema automatically built starting from the verb arrest A value that encouraged splitting was used Merging typed chains into a single unordered Narrative Schema Graphical view of an unordered schema automatically built from the verb convict Each node shape is a chain in the schema Six of the top 20 scored Narrative Schemas Events and arguments in italics were marked misaligned by FrameNet definitions indicates verbs not in FrameNet indicates verb senses not in FameNet Results with varying sizes of training data Missing argument examples of biological interactions A protein domainreferring phrase example Statistics of anaphoric expressions A subjective pronoun resolution example Nonanaphoric DNP examples Possessive pronoun resolution examples Example patterns for parallelism An example antecedent of a nominal in teraction keyword Example patterns of nominal interaction keywords Example patterns of proteins and their do mains An annotation example for the necessity of species information Term variation examples Protein name grounding examples Experimental results of test corpus An example result of BioAR Incorrect resolution example of pronoun resolution module The templates for generating potentially deter ministic constraints of English POS tagging Morph features of frequent words and rare words as computed from the WSJ Corpus of Penn Treebank Comparison of raw input and constrained input Comparison of raw input and constrained input Character and wordbased features of a possi ble word wi over the input character sequence c Suppose that wi ci0 ci1 ci2 and its preceding and following char acters are cl and cr respectively Deterministic constraints for POS tagging POS tagging with deterministic constraints The maximum in each column is bold Character tagging with deterministic constraints ILP problem size and segmentation speed Results of the filtering experiments Comparison of Moses and KIT phrase extraction systems Analysis of context length Translation results for GermanEnglish Translation results for EnglishGerman Translation results for FrenchEnglish Translation results for EnglishFrench Substitutioninsertiondeletion patterns for phonemes based on English secondlanguage learners data reported in Swan and Smith 2002 Each row shows an input phoneme class possi ble output phonemes including null and the positions where the substitution or deletion is likely to occur Examples of features and associated costs Pseudofeatures are shown in boldface Exceptional denotes a situation such as the semivowel j substituting for the affricate dZ Substitutions between these two sounds actually occur frequently in secondlanguage error data Substitutiondeletioninsertion costs for g Examples of the three top candidates in the transliteration of EnglishArabic EnglishHindi and EnglishChinese The second column is the rank Languagepair datasets Number of evaluated English NEs CoreMRR scores with different values using score combination A higher puts more weight on the phonetic model MRRs and CorrRate for the pronunciation method top and time correlation method middle The bottom table shows the scores for the combination CoreMRR Partofspeech tags of the Penn Chinese Treebank that are referenced in this paper Please see Xia 2000 for the full list Categories of multicharacter words that are considered strings without internal structures see Section 41 Each category is illustrated with one example from our corpus Categories of multicharacter words that are considered strings with internal structures see Section 42 Each category is illustrated with an example from our corpus Both the individual characters and the compound they form receive a POS tag POS annotations of a couplet ie a pair of two verses in a classical Chinese poem See Table 1 for the meaning of the POS tags POS annotations of an example sentence with a string wan lai evening that has internal structure See Section 42 for two possible translations and Table 1 for the meaning of the POS tags Partofspeech annotations of the three character strings xi liu ying Little Willow military camp and xin feng shi Xinfeng city Both are strings with internal structures with nested structures that perfectly match at all three levels They are the noun phrases that end both verses in the couplet Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag Across all languages high performance can be attained by selecting a single tag per word type Graphical depiction of our model and summary of latent variables and parameters The typelevel tag assignments T generate features associated with word types W The tag assignments constrain the HMM emission parameters The tokens w are generated by tokenlevel tags t from an HMM parameterized by the lexicon structure The hyperparameters and represent the concentration parameters of the token and typelevel components of the model respectively They are set to fixed constants Graph of the onetoone accuracy of our full model FEATS under the best hyperparameter setting by iteration see Section 5 Performance typically stabi lizes across languages after only a few number of itera tions Multilingual Results We report tokenlevel onetoone and manytoone accuracy on a variety of languages under several experimental settings Section 5 For each language and setting we report onetoone 11 and many toone m1 accuracies For each cell the first row corresponds to the result using the best hyperparameter choice where best is defined by the 11 metric The second row represents the performance of the median hyperparameter setting Model components cascade so the row corresponding to FEATS also includes the PRIOR component see Section 3 Statistics for various corpora utilized in exper iments See Section 5 The English data comes from the WSJ portion of the Penn Treebank and the other lan guages from the training set of the CoNLLX multilin gual dependency parsing shared task Typelevel English POS Tag Ranking We list the top 5 and bottom 5 POS tags in the lexicon and the predictions of our models under the best hyperparameter setting Typelevel Results Each cell report the type level accuracy computed against the most frequent tag of each word type The statetotag mapping is obtained from the best hyperparameter setting for 11 mapping shown in Table 3 Comparison of our method FEATS to stateoftheart methods Featurebased HMM Model Berg Kirkpatrick et al 2010 The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm Posterior regulariation model Graca et al 2009 The G10 model uses the posterior regular ization approach to ensure tag sparsity constraint Results for morphological processing GermanEnglish Results for morphological processing EnglishGerman Resu EnglishGerman Resu GermanEnglish Number of affected words by OOV preprocessing Results for OOVprocessing and MBR GermanEnglish Results for OOVprocessing and MBR EnglishGerman Example of the effects of OOV processing for GermanEnglish 1 The semantic roles of cases beside C3 verb cluster The semantic roles of cases beside C1 verb cluster Rule expansion with minimal context Example 3 Sample of extracted entailment rules Accuracy of the extracted sets of rules Resulting sets of entailment rules Question tracking interface to a summa rization system LexRank example sentence similarity graph with a cosine threshold of 015 Corpus of complex news stories Development testing evaluation Average scores by cluster baseline versus LR020095 Training phase effect of similarity thresh old a on Ave MRR and TRDR Training phase effect of question bias d on Ave MRR and TRDR Training phase systems outperforming the baseline in terms of TRDR score Top ranked sentences using baseline system on the question What caused the Kursk to sink Testing phase baseline vs LR020095 Top ranked sentences using the LR020095 system on the question What caused the Kursk to sink Visualisation examples Top named en tity recognition middle dependency syntax bot tom verb frames Screenshot of the main BRAT userinterface showing a connection being made between the annotations for moving and Citibank Incomplete T RANSFER event indicated to the annotator The BRAT search dialog Total annotation time portion spent se lecting annotation type and absolute improve ment for rapid mode Example annotation from the BioNLP Shared Task 2011 Epigenetics and Posttranslational Modifications event extraction task Segmentation recall relative to gold word frequency Baseline performance Segmentation precisionrecall relative to gold word length in training data Word length statistics on test sets Fscore of two segmenters with and without word tokentype features Upper bound for combination The error reduction ER rate is a comparison between the Fscore produced by the oracle combination sys tem and the characterbased system see Tab 1 Segmentation performance presented in previous work and of our combination model PrecisionRecallFscore of different models Accuracy of maximum entropy system using different subsets of features for S ENSEVAL 2 verbs Overall accuracy of maximum entropy sys tem using different subsets of features for Penn Chi nese Treebank words manually segmented partof speechtagged parsed Overall accuracy of maximum entropy sys tem using different subsets of features for Peoples Daily News words manually segmented partof speechtagged Overall accuracy of maximum entropy sys tem using different subsets of features for Peoples Daily News words automatically segmented part ofspeechtagged parsed The total costs for the three MTurk subtasks in volved with the creation of our Dialectal ArabicEnglish parallel corpus One possible breakdown of spoken Arabic into dialect groups Maghrebi Egyptian Levantine Gulf and Iraqi Habash 2010 gives a breakdown along mostly the same lines We used this map as an illustration for annotators in our dialect classification task Section 31 with Arabic names for the dialects instead of English Statistics about the trainingtuningtest datasets used in our experiments The token counts are calculated before MADA segmentation Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal Arabic web text The morphological segmentation uniformly improves translation quality but the improvements are more dramatic for MSA than for Dialectal Arabic when comparing similarlysized training corpora A comparison of translation quality of Egyptian Levantine and MSA web text using various training corpora The highest BLEU scores are achieved using the full set of dialectal data which combines Levantine and Egyptian since the Egyptian alone is sparse For Levantine adding Egyptian has no effect In both cases adding MSA to the dialectal data results in marginally worse translations Examples of ambiguous words that are trans lated incorrectly by the MSAEnglish system but cor rectly by the Dialectal ArabicEnglish system Examples of improvement in MT output when training on our Dialectal ArabicEnglish parallel corpus instead of an MSAEnglish parallel corpus The most frequent OOVs with counts 10 of the dialectal test sets against the MSA training data Learning curves showing the effects of increas ing the size of dialectal training data when combined with the 150Mword MSA parallel corpus and when used alone Adding the MSA training data is only use ful when the dialectal data is scarce 200k words Results on a truly independent test set consisting of data harvested from Egyptian Facebook pages that are entirely distinct from the our dialectal training set The improvements over the MSA baseline are still considerable 29 BLEU points when no Facebook data is available for tuning and 27 with a Facebook tuning set A comparison of the effectiveness of performing LevantinetoMSA mapping before translating into English versus translating directly from Levantine into English The mapping from Levantine to MSA was done manually so it is an optimistic estimate of what might be done automatically Although initially helpful to the MSA baseline system the usefulness of pivoting through MSA drops as more dialectal data is added eventually hurting performance Overview of the system The pool of features for all languages Experiment results as F1 scores where IM is identification of mentions and S Setting Final system results as F1 scores where IM is identification of mentions and S Setting For more details cf Recasens et al 2010 Three representations of NP modifications a the original treebank representation b Selective leftcorner representation and c a flat structure that is unambiguously equivalent to b Conditioning features for the probabilistic CFG used in the reported empirical trials Corpus sizes Parser performance on BrownE baselines Note that the Gildea results are for sentences 40 words in length Parser performance on WSJ23 baselines Note that the Gildea results are for sentences 40 words in length All others include all sentences Parser performance on WSJ23 baselines Parser performance on BrownE supervised adaptation Parser performance on WSJ23 unsupervised adaptation For all trials the base training is BrownT the held out is BrownH plus the parser output for WSJ24 and the mixing parameter A is 020e cA Parser performance on WSJ23 supervised adaptation All models use BrownTH as the outofdomain treebank Baseline models are built from the fractions of WSJ221 with no outofdomain treebank The MEDLINE semantic categories Filtered 5gram dataset statistics Performance relationship between WMEB and BASILISK on Sgold UNION Variation in precision with random gold seed sets Bagging with 50 gold seed sets Bagging with 50 unsupervised seed sets Semantic drift in CELL n20 m20 Final accuracy with drift detection Semantic drift detection results Interannotator agreement of ACE 2005 relation annotation Numbers are the distinct relation mentions whose both arguments are in the list of adjudicated entity mentions Percentage of examples of major syntactic classes cumulative distribution of frequency CDF of the relative ranking of modelpredicted probability of being positive for false negatives in a pool mixed of false negatives and true negatives and the CDF of the relative ranking of modelpredicted probability of being negative for false positives in a pool mixed of false positives and true positives Categories of spurious relation mentions in fp1 on a sample of 10 of relation mentions ranked by the percentage of the examples in each category In the sample text red text also marked with dotted underlines shows head words of the first arguments and the underlined text shows head words of the second arguments Performance of RDC trained on fp1fp2adj and tested on adj TSVM optimization function for nonseparable case Joachims 1999 Performance with SVM trained on a fraction of adj It shows 5 fold cross validation results 5fold crossvalidation results All are trained on fp1 except the last row showing the unchanged algorithm trained on adj for comparison and tested on adj McNemars test show that the improvement from purify to tSVM and from tSVM to ADJ are statistically significant with p005 ESA and inputoutput data A character sequence and its subsequence pairs The path of Selection The binary tree of Selection The initial frequencies of character sequences The adjusted frequencies of character sequences The hierarchical form of a result The scales of corpora The results of setting 1 Punctuation and other encoding information are not used the maximum length is 30 The results of setting 2 Punctuation and other encoding information are not used the maximum length is 10 The results of setting 3 Punctuation is used the maximum length is 30 The results of setting 4 Punctuation and other encoding information are used the maximum length is 30 The difference between the results of four settings The results brought by different maximum lengths The empirical formulae for the prediction linear model The correlation between the scales and the proper exponents The four types of changes Convergence of results The time complexity in practice 4 samples 10 30 50 and 100 The comparison between NPYLM and ESA The comparison between DLG AV BE and ESA The comparison between IWSLRRI SSS TONGOO THT and ESA Probabilistic Approaches Outputs from each algorithm at different sorted ranks WordNetbased scores Performance on Internet data Equation 1 settings Precisionrecall curve for rescoring Syntactic frames for VerbNet classes VerbNet classes An excerpt from SemLink Training instances obtained from Verb Net upper and VerbNetSemLink lower Example of features for sway 14 classes used in Joanis et al 2008 and their corresponding Levin class numbers Corpus size vs accuracy Accuracy and KLdivergence for the all class task the VerbNetSemLink setting Accuracy and KLdivergence for the all class task the VerbNet only setting Accuracy for the 14class task Corpus size vs KLdivergence Contribution of features PoCoS Core Scheme Extended Scheme and languagespecific instantiations A summary of the parsing and evaluation sce narios X depicts gold information depicts unknown information to be predicted by the system Overview of participating languages and treebank properties Sents number of sentences Tokens number of raw surface forms Lex size and Avg Length are computed in terms of tagged terminals NT non terminals in constituency treebanks Dep Labels dependency labels on the arcs of dependency treebanks A more comprehensive table is available at httpwwwspmrlorgspmrl2013sharedtaskhtmlProp File formats Trees a and b are aligned constituency and dependency trees for a mockup English example Boxed labels are shared across the treebanks Figure c shows an ambiguous lattice The red part represents the yield of the gold tree For brevity we use empty feature columns but of course lattice arcs may carry any morphological features in the FEATS CoNLL format Dependency parsing LAS scores for full and 5k training sets and for gold and predicted input Results in bold show the best results per language and setting Dependency Parsing MWE results Constituent Parsing ParsEval Fscores for full and 5k training sets and for gold and predicted input Results in bold show the best results per language and setting Constituent Parsing LeafAncestor scores for full and 5k training sets and for gold and predicted input Realistic Scenario Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario Top upper part refers to constituency results the lower part refers to dependency results Realistic Scenario Tedeval Labeled Accuracy and Exact Match for the Raw scenario The upper part refers to constituency results the lower part refers to dependency results The correlation between treebank size label set size and LAS scores x treebank size labels y LAS treebank Correlation between treebank size Non sizenumber terminal labelsof sentences sent and mean F1 The correlation between the non terminals per sentence ratio and Leaf Accuracy macro scores x non terminal sentence y Acc Cross Framework Evaluation Unlabeled TedEval on generalized gold trees in gold scenario trained on 5k sentences and tested on 5k terminals CrossLanguage Evaluation Unlabeled TedEval Results in gold input scenario On a 5ksentences set set and a 5kterminals test set The upper part refers to constituency parsing and the lower part refers to dependency parsing For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics Labeled and Unlabeled TedEval Results for raw Scenarios Trained on 5k sentences and tested on 5k terminals The upper part refers to constituency parsing and the lower part refers to dependency parsing The parts of taxonomic names The Classification Process Abbreviations Comparison to Related Approaches Test results An excerpt from the text with core ferring noun phrases annotated English trans lation in italics Mention Detection Results Features and functions used in clustering algorithm Coreference Resolution Performance Contribution of individual features to overall performance Examples of the ACE Relation Types The RCM structure System Pipeline Test Procedure Chinese system performance with system mentions and system relations 4 Performance of English system with system mentions and system relations 2 Performance of English system with perfect mentions and perfect relations 3 Performance of Chinese system with perfect mentions and perfect relations Results of paraphrases evaluation for 100 sentences in French using English as the pivot lan guage Comparison between the baseline system MOSES and our algorithm MCPG A brief description of the tested parsers Note that the Tune data is not the data used to train the individual parsers Higher numbers in the right column reflect just the fact that the Test part is slightly easier to parse Comparison of various groups of parsers All percentages refer to the share of the total words in test data attached correctly The single parser part shows shares of the data where a single parser is the only one to know how to parse them The sizes of the shares should correlate with the uniqueness of the individual parsers strategies and with their contributions to the overall success The at least rows give clues about what can be got by majority voting if the number represents over 50 of parsers compared or by hypothetical oracle selection if the number represents 50 of the parsers or less an oracle would generally be needed to point to the parsers that know the correct attachment Results of voting experiments Voting under handinvented schemes Contexts where ec is better than mcdz J are coordination conjunctions is the root V are verbs Nn are nouns in case n R are preposi tions Z are punctuation marks An are adjectives The decision tree for ecmcz learned by C5 Besides pairwise agreement be tween the parsers only morphological case and negativeness matter Contextsensitive voting Contexts trained on the Tune data set accuracy figures apply to the Test data set Contextfree results are given for the sake of comparison Unbalanced vs balanced combining All runs ignored the context Evaluated on the Test data set Arabic Verbal Inflection Arabic Nominal Inflection Broken Plural Adjective Full Inflection Verb Stem Alternation Noun Stem Alternation Derivation by Means of Adding a Suffix Arabic Clitics Example 1 Syncretism Example 2 Syncretism Example 1 Arabic Pronoun Dropping Compounding in Russian Arabic Clitics Example 2 Challenge in Elliptical Constructions Arabic Equational Sentences ZeroCopula in Russian Arabic OrderFree Structure Six Accepted Word Orders in Russian Arabic idafa Construct Phonetic Stress in Russian Phonetic Stress in Russian Fake Homograph Arabic Vocalization Problem Arabic Diglossia A snippet from Russian and Czech tag comparison WordNet representation Splitting Compounds in Russian Arabic Tokenization Schemes Arabic Tokenization Schemes Performance comparison between original and universal tagsets Arabic POS Studies with Different Tagsets Different Arabic Transliterations of Los Angeles Shallow parsing chunking Extracted from httpkontextfraunhoferde An example CCG parse obtained from 60 Relation phrase compliance with semanticlexical constraints 32 Dependency parsing and clause constituents Relation extraction rules used by Gamallo etal36 A comparison between QA semantic parsing approaches12 a Parsing of input sentence78 Transliteration Scheme Combined systems Basque in cross validation best recall in bold Only vectorf was used for combination Single systems Basque in cross validation sorted by recall Single systems English in cross validation sorted by recall Combined systems English in cross validation best recall in bold Ocial results for the English and Basque lexical tasks recall Table 2 Chunk feature templates i f jFk is the chunk label plus the tag of its right most child if the j2r t tree is a chunk Otherwise i f is the con stituent label of the j r t tree Extend feature templates G fA k l is the root constituent label of th Check feature templates G fj Learning curves wordsegmentation F measure and parsing label Fmeasure vs percentage of training data Languagedependent lexical features A word list can be collected to encode different WS wordsegmentation Baseline languageindependent features LexFeat plus lex ical features Numbers are averaged over the 10 ex periments in Figure 2 Parsing and word segmentation F measures vs the experiment numbers Lines with triangles segmentation Lines with circles label Dottedlines languageindependent features only Solid lines plus lexical features Usefulness of syntactic information black dashdotted line word boundaries only red dashed line POS info and blue solid line full parse trees IV and OOV recall in Zhang et al 2006a Corpora statistics of Bakeoff 2005 Feature templates used for CRF in our experiments Error analysis of confidence measure with and without EIV tag Results of CT when MP is less than 0875 Results of different approach used in our experiments White background lines are the results we repeat Zhangs methods and they have some trivial difference with Table 1 A text segment from MUC6 data set Feature set for the baseline pronoun resolution system Backward features used to capture the coreferential information of a candidate Results of different systems for pronoun resolution on MUC6 and MUC7 Here we only list backward feature assigner for pronominal candidates In RealResolve1 to RealResolve4 the backward features for nonpronominal candidates are all found by DTnonpron The pronoun resolution algorithm by incorporating coreferential information of can didates The classifier refining algorithm New training and testing procedures Annotated RST Tree for example 4 Properties of the training and test sets used in the shared task The training data is the Europarl cor pus from which also the indomain test set is taken There is twice as much language modelling data since training data for the machine translation system is filtered against sentences of length larger than 40 words Outofdomain test data is from the Project Syndicate web site a compendium of political commentary Participants in the shared task Not all groups participated in all translation directions Annotation tool for manual judgement of adequacy and fluency of the system output Translations from 5 randomly selected systems for a randomly selected sentence is presented No additional information beyond the instructions on this page are given to the judges The tool tracks and reports annotation speed Evaluation scores for indomain and out ofdomain test sets averaged over all systems Number and ratio of statistically signifi cant distinction between system performance Au tomatic scores are computed on a larger tested than manual scores 3064 sentences vs 300400 sen tences Average scores for different language pairs Manual scoring is done by different judges resulting in a not very meaningful comparison Evaluation of translation to English on indomain test data Evaluation of translation from English on indomain test data Evaluation of translation to English on outofdomain test data Evaluation of translation from English on outofdomain test data Correlation between manual and automatic scores for FrenchEnglish Correlation between manual and automatic scores for SpanishEnglish Correlation between manual and automatic scores for GermanEnglish Correlation between manual and automatic scores for EnglishFrench Correlation between manual and automatic scores for EnglishSpanish Correlation between manual and automatic scores for EnglishGerman Our probabilistic model a question x is mapped to a latent logical form z which is then evaluated with respect to a world w database of facts producing an answer y We represent logical forms z as labeled trees induced automatically from x y pairs Possible relations appearing on the edges of a DCS tree Here j j 0 1 2 and i 1 2 a An example of a DCS tree written in both the mathematical and graphical notation Each node is labeled with a predicate and each edge is labeled with a relation b A DCS tree z with only join relations en codes a constraint satisfaction problem c The denota tion of z is the set of consistent values for the root node Examples of DCS trees that use the aggregate relation to a compute the cardinality of a set and b take the average over a set Example DCS trees for utterances in which syntactic and semantic scope diverge These trees reflect the syntactic structure which facilitates parsing but importantly these trees also precisely encode the correct semantic scope The main mechanism is using a mark relation E Q or C low in the tree paired with an execute relation Xi higher up at the desired semantic point Example of the denotation for a DCS tree with a compare relation C This denotation has two columns one for each active nodethe root node state and the marked node size Accuracy recall of systems on the two bench marks The systems are divided into three groups Group 1 uses 10fold crossvalidation groups 2 and 3 use the in dependent test set Groups 1 and 2 measure accuracy of logical form group 3 measures accuracy of the answer but there is very small difference between the two as seen from the Kwiatkowski et al 2010 numbers Our best system improves substantially over past work despite us ing no logical forms as training data Results on G EO with 250 training and 250 test examples Our results are averaged over 10 random 250250 splits taken from our 600 training examples Of the three systems that do not use logical forms our two systems yield significant improvements Our better sys tem even outperforms the system that uses logical forms Graphical model for PLTM EuroParl topics T400 Smoothed histograms of the JensenShannon Smoothed histograms of the probability of the Topics sorted by number of words assigned Topics are meaningful within languages but di Are the single most probable words for a given Percent of query language documents for which Squares represent the proportion of tokens in each language assigned to a topic The left topic world ski km won Wikipedia topics T400 Structure of Cascaded Linear Model R denotes the scale of the feature space of the core perceptron Feature templates and instances Suppose we are considering the third character U in e U Feature space growing curve The horizontal scope Xij denotes the introduction of different tem plates X05 Cn n 22 X59 Cn Cn1 n 21 X910 C1 C1 X1015 C0 Cn n 22 X1519 C0 Cn Cn1 n 21 X1920 C0 C1 C1 X2021 W0 X2122 W1 W0 W0 de notes the current considering word while W1 denotes the word in front of W0 All the data are collected from the training procedure on MSR corpus of SIGHAN bake off 2 Averaged perceptron learning curves with Non lexicaltarget and Lexicaltarget feature templates Fmeasure on SIGHAN bakeoff 2 SIGHAN best best scores SIGHAN reported on the four corpus cited from Zhang and Clark 2007 Contribution of each feture ALL all features PER perceptron model WLM word language model PLM POS language model GPR generating model LPR labelling model LEN word count penalty Fmeasure on segmentation and Joint ST of perceptrons POS perceptron trained without POS POS perceptron trained with POS Corpora Various features used for computing edge weights between foreign trigram types An excerpt from the graph for Italian Three of the Italian vertices are connected to an automatically la beled English vertex Label propagation is used to propa gate these tags inwards and results in tag distributions for the middle word of each Italian trigram Partofspeech tagging accuracies for various baselines and oracles as well as our approach Avg denotes macroaverage across the eight languages Tags produced by the different models along with the reference set of tags for a part of a sentence from the Italian test set Italicized tags denote incorrect labels Size of the vocabularies for the No LP and With LP models for which we can impose constraints A hypothetical semantic space for horse and run Distribution of elicited ratings for High and Low similarity items Example Stimuli with High and Low similarity landmarks Model means for High and Low similarity items and correlation coefficients with human judgments p 005 p 001 Distribution of predicted similarities for the vector multiplication model on High and Low similarity items Lookup of is one of in a reverse trie Children of each node are sorted by vocabulary identifier so order is consistent but not alphabetical is always appears be fore are Nodes are stored in columnmajor order For example nodes corresponding to these ngrams appear in this order are one s Australia is one of are one of s Australia is and Australia is one Speed in lookups per microsecond by data structure and number of 64bit entries Performance dips as each data structure outgrows the processors 12 MB L2 cache Among hash tables indicated by shapes probing is initially slower but converges to 43 faster than un ordered or hash set Interpolation search has a more ex pensive pivot function but does less reads and iterations so it is initially slower than binary search and set but be comes faster above 4096 entries Singlethreaded speed and memory use on the perplexity task The P ROBING model is fastest by a sub stantial margin but generally uses more memory T RIE is faster than competing packages and uses less memory than nonlossy competitors The timing basis for Queriesms in cludes kernel and user time but excludes loading time we also subtracted time to run a program that just reads the query file Peak virtual memory is reported final resident memory is similar except for BerkeleyLM We tried both aggressive reading and lazy memory mapping where appli cable but results were much the same Multithreaded time and memory consumption of Moses translating 3003 sentences on eight cores Our code supports lazy memory mapping L and prefault ing P with MAP POPULATE the default IRST is not threadsafe Time for Moses itself to load including load ing the language model and phrase table is included Along with locking and background kernel operations such as prefaulting this explains why wall time is not oneeighth that of the singlethreaded case Singlethreaded time and memory consumption of Moses translating 3003 sentences Where applicable models were loaded with lazy memory mapping L prefaulting P and normal reading R results differ by at most than 06 minute CPU time memory usage and uncased BLEU Papineni et al 2002 score for singlethreaded Moses translating the same test set We ran each lossy model twice once with speciallytuned weights and once with weights tuned using an exact model The difference in BLEU was minor and we report the better result The first sense heuristic compared with the SENSEVAL 2 English allwords task results SemCor results Evaluating predominant sense information on SENSEVAL 2 allwords data Domain specific results Distribution of domain labels of predom inant senses for 38 polysemous words ranked using the SPORTS and FINANCE corpus The Lattice for the Hebrew Phrase bclm hneim Segmentation Parsing and Tagging Results us ing the Setup of Cohen and Smith 2007 sentence length 40 The Models are Ordered by Performance Segmentation tagging and parsing results on the Standard devtrain Split for all Sentences Dependency graph for Czech sentence from the Prague Dependency Treebank1 Projectivized dependency graph for Czech sentence Encoding schemes d dependent h syntactic head p path n number of dependency types Features used in predicting the next parser action Nonprojective sentences and arcs in PDT and DDT NonP nonprojective Percentage of nonprojective arcs recovered correctly number of labels in parentheses Precision recall and Fmeasure for nonprojective arcs Parsing accuracy AS attachment score EM exact match U unlabeled L labeled Dependency accuracy on 13 languages Unlabeled UA and Labeled Accuracy LA Error analysis of parser components av eraged over Arabic Bulgarian Danish Dutch Japanese Portuguese Slovene Spanish Swedish and Turkish NP Allow nonprojectiveForce pro jective SA Sequential labelingAtomic labeling MB Include morphology featuresNo morphology features Morph Examples and Motivations Distributions of Morph Examples Overview of Morph Resolution Crosssource Comparable Data Example each morph and target pair is shown in the same color Network Schema of MorphRelated Het erogeneous Information Network Example of MorphRelated Heteroge neous Information Network The System Performance Based on Com binations of Surface and Semantic Features The System Performance Based on Each Single Feature Set Description of feature sets Glob only uses the same set of similarity measures when combined with other semantic features The Effects of Temporal Constraint Accuracy of Target Candidate Detection The Effects of Social Features The System Performance of Integrating Cross Source and Cross Genre Information Effects of Popularity of Morphs Performance of Two Categories Confusion Sets An Initial Learning Curve for Confusable Disambiguation Learning Curves for Confusable Disambiguation Distribution of Error Types Turning distributional similarity into a weighted inference rule Results on the RTE1 Test Set Results on the STS video dataset Architecture of the translation approach based on a loglinear modeling approach Example of a symmetrized word alignment Verbmobil task Algorithm phraseextract for extracting phrases from a wordaligned sentence pair Here quasiconsecutiveTP is a predicate that tests whether the set of words TP is consecutive with the possible exception of words that are not aligned Examples of alignment templates obtained in training Dependencies in the alignment template model Example of segmentation of German sentence and its English translation into alignment templates Algorithm for breadthfirst search with pruning Algorithm minjumps to compute the minimum number of needed jumps DcJ1 j to complete the translation Statistics for Verbmobil task training corpus Train conventional dictionary Lex development corpus Dev test corpus Test Words words without punctuation marks Effect of alignment template length on translation quality Effect of pruning parameter tp and heuristic function on search efficiency for directtranslation model Np 50000 Effect of pruning parameter tp and heuristic function on error rate for directtranslation model Np 50000 Effect of pruning parameter Np and heuristic function on search efficiency for directtranslation model tp 1012 Effect of pruning parameter Np and heuristic function on error rate for directtranslation model tp 1012 Corpus statistics for Hansards task Words words without punctuation marks Corpus statistics for ChineseEnglish corporalarge data track Words words without punctuation marks Translation results on the Hansards task Example translations for ChineseEnglish MT 1 Feature set for coreference resolution Feature 22 23 and features involving Cj are not used in the singlecandidate model Results for the nonpronoun resolution Results for the pronoun resolution Results for the coreference resolution Distribution of dialogue acts in our dataset The 7 speakers from ICSIMRDA dataset used in our experiments The table lists the Speaker ID orig inal speaker tag the type of meeting selected for this speaker the number of meetings this speaker participated and the total number of dialogue acts by this speaker Effect of samespeaker data on dialogue act recognition We compare two approaches 1 when a recognizer is trained on the same person and tested on new utterances from the same person and 2 when the recognizer was trained on another speaker same test set We vary the amount of training data to be 200 500 1000 1500 and 2000 dialogue acts In all cases using speakerspecific recognizer outperforms recognizer from other speakers The average results among all 7 speakers when train with different combinations of speaker specific data and other speakers data are displayed In both Constant adaptation and Reweighted adaptation models the num ber of speaker specific data are varied from 200 500 1000 1500 to 2000 In Generic model only all other speakers data are used for training data Average results of Reweighting among all 7 speakers when the amount of speaker specific data is 0 500 2000 Beam search algorithm for joint tagging and de pendency parsing of input sentence x with weight vector w and beam parameters b1 and b2 The symbols hc hs and hf denote respectively the configuration score and feature representation of a hypothesis h hcA denotes the arc set of hc Transitions for joint tagging and dependency parsing extending the system of Nivre 2009 The stack is represented as a list with its head to the right and tail and the buffer B as a list with its head to the left and tail The notation f a b is used to denote the function that is exactly like f except that it maps a to b Specialized feature templates for tagging We use i and Bi to denote the ith token in the stack and buffer B respectively with indexing starting at 0 and we use the following functors to extract properties of a token i ith best tag si score of ith best tag finally predicted tag w word form pi word prefix of i characters si word suffix of i characters Score differences are binned in discrete steps of 005 Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and the score threshold Beam parameters fixed at b1 40 b2 4 Accuracy scores for the CoNLL 2009 shared task test sets Rows 12 Top performing systems in the shared CoNLL Shared Task 2009 Gesmundo et al 2009 was placed first in the shared task for Bohnet 2010 we include the updated scores later reported due to some improvements of the parser Rows 34 Baseline k 1 and best settings for k and on development set Rows 56 Wider beam b1 80 and added graph features G and cluster features C Second beam parameter b2 fixed at 4 in all cases Accuracy scores for WSJPTB converted with head rules of Yamada and Matsumoto 2003 and labeling rules of Nivre 2006 Best dev setting k 3 04 Results marked with use additional information sources and are not directly comparable to the others Selected entries from the confusion matrix for parts of speech in German with Fscores for the lefthand side category ADJ ADJD or ADJA adjective ADV adverb ART determiner APPR preposition NE proper noun NN common noun PRELS relative pronoun VVFIN finite verb VVINF nonfinite verb VAFIN finite auxiliary verb VAINF nonfinite auxil iary verb VVPP participle XY not a word We use to denote the set of categories with as a prefix Accuracy scores for Penn Chinese Treebank converted with the head rules of Zhang and Clark 2008 Best dev setting k 3 01 MSTParser results from Li et al 2011 UAS scores from Li et al 2011 and Ha tori et al 2011 recalculated from the separate accuracy scores for root words and nonroot words reported in the original papers Selected entries from the confusion matrix for parts of speech in English with Fscores for the lefthand side category DT determiner IN preposition or sub ordinating conjunction JJ adjective JJR compara tive adjective NN singular or mass noun NNS plural noun POS possessive clitic RB adverb RBR com parative adverb RP particle UH interjection VB base form verb VBD past tense verb VBG gerund or present participle VBN past participle VBP present tense verb not 3rd person singular VBZ present tense verb 3rd person singular We use to denote the set of categories with as a prefix Example of the transfer of a verbal chunk Examples of translations An example of the label consistency problem excerpted from a document in the CoNLL 2003 English dataset Features used by the CRF for the two tasks named entity recognition NER and template filling TF Counts of the number of times multiple occurrences of a token sequence is labeled as different entity types in the same document Taken from the CoNLL training set F1 scores of the local CRF and nonlocal models on the CMU Seminar Announcements dataset We also provide the results from Sutton and McCallum 2004 for comparison F1 scores of the local CRF and nonlocal models on the CoNLL 2003 named entity recognition dataset We also provide the results from Bunescu and Mooney 2004 for comparison Summary of the previous work on coreference resolution that employs the learning algorithms the clustering algorithms the feature sets and the training instance creation methods discussed in Section 31 Statistics for the ACE corpus Results for the three ACE data sets obtained via the MUC scoring program Results for the three ACE data sets obtained via the BCUBED scoring program The coreference systems that achieved the highest Fmeasure scores for each test set and scorer combination The average rank of the candidate partitions produced by each system for the corresponding test set is also shown List of semantic labels Examples of aggregated instances Results of semantic classification Confusion matrix of acquired nouns Results of ablation experiments Overview of the tasks investigated in this paper n size of ngram POS parts of speech Ling linguistic knowledge Type type of task Meaning of diacritics indicating statistical sig nificance 2 tests Performance comparison with the literature for candidate selection for MT Performance of Altavista counts and BNC counts for candidate selection for MT data from Prescher et al 2000 Performance comparison with the literature for context sensitive spelling correction Performance of Altavista counts and BNC counts for context sensitive spelling correction data from Cucerzan and Yarowsky 2002 Performance comparison with the literature for compound bracketing Performance of Altavista counts and BNC counts for compound bracketing data from Lauer 1995 Performance of Altavista counts and BNC counts for adjective ordering data from Malouf 2000 Performance comparison with the literature for compound interpretation Performance of Altavista counts and BNC counts for compound interpretation data from Lauer 1995 Performance comparison with the literature for noun countability detection Performance of Altavista counts and BNC counts for noun countability detection data from Bald win and Bond 2003 Syntactic Seeding Heuristics Caseframe Network Examples Lexical Caseframe Expectations Semantic Caseframe Expectations General Knowledge Sources Individual Performance of KSs for Disasters Individual Performance of KSs for Terrorism General Contextual Role Knowledge Sources General Knowledge Sources Linking FrameNet frames and VerbNet classes Mapping algorithm refining step Results of the mapping algorithm F1s of some individual FN role classifiers and the overall multiclassifier accuracy 454 roles F1s of some individual ILC classifiers and the overall multiclassifier accuracy 180 classes on PB and 133 on FN Semantic role learning curve Graph representing transliteration pairs and cooccurence relations Ten highestscoring matches for the Xin hua corpus for 81301 The final column is the log P estimate for the transliteration Starred entries are incorrect MRRs of the frequency correlation meth ods MRRs on the augmented candidate list Effectiveness of combining the two scor ing methods Propagation Core items Effectiveness of score propagation Propagation All items An example of MOD feature extraction An oval in the dependency tree denotes a bunsetsu of features Precision for 200 candidates EvRec Precision for each phrase type EvLing Anaphora resolution preferences Description of the unrestricted corpora used in the evaluation Results obtained in the detection of zeropronouns Classification of third person zero pronouns a A related work section extracted from Wu and Oard 2008 b An associated topic hierar chy tree of a c An associated topic tree annotated with key wordsphrases The demographics of RWSData No RW RA SbL WbL TS and TD are labeled as Number of Related Works Referenced Articles Sentencebased Length of Word based Length of Tree Size and Tree Depth respectively A context modeling example Evaluation results for ReWoS variants and baselines Syntactic tree kernel STK Integrating Brown cluster information Overview of the ACE 2005 data Distribution of relations in ACE 2005 For each domain the percentage of target domain words types that are unseen in the source together with the most frequent OOV words Comparison to previous work on the 7 re lations of ACE 2004 K kernelbased F feature based yesno models argument order explicitly Brown clusters in tree kernels cf Fig 2 F1 per coarse relation type ACE 2005 SYS is the final model ie last row PETPET WCPET LSA of Table 5 Indomain first column and outofdomain performance columns two to four on ACE 2005 PET and BOW are abbreviated by P and B respectively If not specified BOW is marked Bell tree representation for three mentions numbers in denote a partial entity Infocus entities are marked on the solid arrows and active mentions are marked by Solid arrows signify that a mention is linked with an infocus partial entity while dashed arrows indicate starting of a new entity Basic features used in the maximum entropy model Statistics of three test sets Table 4 Impact of feature categories Numbers after are the standard deviations indicates that the result is significantly pairwise ttest different from the line above at Performance vs log start penalty Results on the MUC6 formal test set Entityconstrained Mention Fmeasure MP uses Table 3 Coreference results on true mentions MP mentionpair model EM entityme features wh features None of the ECMF differences between MP and EM is statistically significant at A verse written in the BAD web application The response of the rhyme search engine The BAD application before entering a verse showing two possible rhyme patterns The generic beamsearch algorithm Feature templates for the word segmentor Feature templates of a typical characterbased word segmentor Speedaccuracy tradeoff of the segmentor Training development and test data for word segmentation on CTB5 The accuracies of various word segmentors over the first SIGHAN bakeoff data The accuracies of various word segmentors over the second SIGHAN bakeoff data The extended generic beamsearch algorithm with multiple beams Comparison between three different decoders for word segmentation POS feature templates for the joint segmentor and POS tagger The influence of beamsizes and the convergence of the perceptron for the joint segmentor and POS tagger The speeds of joint word segmentation and POStagging by 10fold cross validation The accuracies of joint segmentation and POStagging by 10fold cross validation The comparison of overall accuracies of various joint segmentor and POStaggers by 10fold cross validation using CTB Training development and test data from CTB5 for joint word segmentation and POStagging Accuracy comparisons between various joint segmentors and POStaggers on CTB5 The accuracyspeed tradeoff graph for the joint segmentor and POStaggers and the twostage baseline An example Chinese dependency tree Transitionbased feature templates for the dependency parser Transitionbased feature context for the dependency parser Graphbased feature templates for the dependency parser The training development and test data for English dependency parsing The accuracyspeed tradeoff graph for the transitionbased and combined dependency parsers Accuracy comparisons between various dependency parsers on English data Training development and test data for Chinese dependency parsing Test accuracies of various dependency parsers on CTB5 data The combined segmentation POStagging and dependency parsing Fscores using different pipelined systems An example Chinese lexicalized phrasestructure parse tree Feature templates for the phrasestructure parser The standard split of CTB2 data for phrasestructure parsing Accuracies of various phrasestructure parsers on CTB2 with goldstandard POStags Accuracies of various phrasestructure parsers on CTB2 with automatically assigned tags Accuracies of our phrasestructure parser on CTB5 using goldstandard and automatically assigned POStags The accuracyspeed tradeoff graph for the phrasestructure parser Comparison of dependency accuracies between phrasestructure parsing and dependency parsing using CTB5 data Composition of two FSTs maintaining separate transitions Finitestate cascades for five natural language problems Changing a decision in the derivation lattice All paths generate the observed data The bold path rep resents the current sample and the dotted path represents a sidetrack in which one decision is changed Multiple EM restarts for POS tagging Each point represents one random restart the yaxis is tag ging accuracy and the xaxis is EMs objective function log Pdata Multiple Bayesian learning runs using anneal ing with temperature decreasing from 2 to 008 for POS tagging Each point represents one run the yaxis is tag ging accuracy and the xaxis is the log Pderivation of the final sample Multiple Bayesian learning runs using averag ing for POS tagging Each point represents one run the yaxis is tagging accuracy and the xaxis is the average log Pderivation over all samples after burnin Gibbs sampling for Bayesian inference outperforms both EM and Variational Bayesian EM The output of EM alignment was used as the gold standard Extract of a FrenchEnglish sentence pair segmented into bilingual units The original org French sentence appears at the top of the figure just above the reordered source s and target t The pair s t decomposes into a sequence of L bilingual units tuples u1 uL Each tuple ui contains a source and a target phrase si and ti Experimental results in terms of BLEU scores measured on the newstest2011 and newstest2012 For newstest2012 the scores are provided by the organizers Percentage of major punctuation marks in the Chinese corpus 4 Syntax of Chinese Dash Rhetorical pattern of CQuestion Rhetorical pattern of CExclamation Rhetorical pattern of CEllipses Rhetorical pattern of CSemicolon Rhetorical pattern of CDash Rhetorical pattern of CColon Rhetorical Function of Exclamation Mark in Chinese and German corpora POSmorphological feature accuracies on the development sets PARSEVAL scores on the development sets Architecture of the dependency ranking system Baseline performance and nbest oracle scores UASLAS on the development sets mate uses the prepro cessing provided by the organizers the other parsers use the preprocessing described in Section 2 Unlabeled TedEval scores accuracyexact match for the test sets in the predicted segmentation set ting Only sentences of length 70 are evaluated Feature sets for the dependency ranker for each language default denotes the default ranker feature set Performance UASLAS of the reranker on the development sets Baseline denotes our baseline Rankeddflt and Ranked denote the default and optimized ranker feature sets respectively Oracle denotes the oracle scores Final PARSEVAL F1 scores for constituents on the test set for the predicted setting ST Baseline denotes the best baseline out of 2 provided by the Shared Task organizers Our submission is underlined Final UASLAS scores for dependencies on the test sets for the predicted setting Other denotes the highest scoring other participant in the Shared Task ST Baseline denotes the MaltParser baseline provided by the Shared Task organizers Comparison of Min Simple Fulland DynamicExpansions More Examples DynamicExpansion Tree Span Scheme Comparison of different contextsensitive Comparison of tree span schemes with antecedents in different sentences apart Feature templates used in Rphase Ex ample used is 32 ddd Tags used in LMR Tagging scheme Example of LMR Tagging Additional feature templates used in C phase Example used is 32 ddd with tagging results after Rphase as SSLMR Official BakeOff2005 results Keys F Regular Tagging only all training data are used P1 Regular Tagging only 90 of training data are used P2 Regular Tagging only 70 of training data are used S Regular and Correctional Tagging Separated Mode I Regular and Correctional Tagging Integrated Mode Experimental results of CityU corpus measured in Fmeasure Clustering an 11nodes graph with CW in two iterations The middle node gets the grey or the black class Small numbers denote edge weights Percentage of obtaining two clusters when applying CW on nbipartite cliques oscillating states in matrix CW for an unweighted graph The 10bipartite clique Rate of obtaining two clusters for mix tures of SWgraphs dependent on merge rate r normalized Mutual Information values for three graphs and different iterations in Bipartite neighboring cooccurrence graph a and secondorder graph on neighboring cooccurrences b clustered with CW Disambiguation results in dependent on frequency Disambiguation results in dependent on word class nouns verbs adjectives the largest clusters from partitioning the second order graph with CW Graphical model for the Bayesian QueryFocused Summarization Model Empirical results for the baseline models as well as BAYE S UM when all query fields are used Performance with noisy relevance judg ments The Xaxis is the Rprecision of the IR engine and the Yaxis is the summarization per formance in MAP Solid lines are BAYE S UM dot ted lines are KLRel Bluestars indicate title only redcircles indicated titledescriptionsummary and blackpluses indicate all fields Empirical results for the positionbased model the KLbased models and BAYE S UM with different inputs Dependency representation of example 2 from Talbanken05 Animacy classification scheme Zaenen et al 2004 The animacy data set from Talbanken05 number of noun lemmas Types and tokens in each class Accuracy for MBL and SVM classifiers on Talbanken05 nouns in accumulated frequency bins by Parole frequency Precision recall and Fscores for the two classes in MBLexperiments with a general feature space Confusion matrix for the MBLclassifier with a general feature space on the 10 data set on Talbanken05 nouns Overall results in experiments with au tomatic features compared to gold standard fea tures expressed as unlabeled and labeled attach ment scores System overview Table Construction Usefulness evaluation result Correctness evaluation result Example of a prediction for English to French translation s is the source sentence h is the part of its translation that has already been typed x is what the translator wants to type and x is the prediction Probability that a prediction will be ac cepted versus its gain Time to read and accept or reject proposals versus their length Approximate times in seconds to generate predictions of maximum word sequence length M on a 12GHz processor for the MEMD model Results for different user simulations Numbers give reductions in keystrokes Results for different predictor configura tions Numbers give reductions in keystrokes Snapshot of the supersenseannotated data The 7 article titles translated in each domain with total counts of sentences tokens and supersense mentions Overall there are 2219 sentences with 65452 tokens and 23239 mentions 13 tokensmention on average Counts exclude sentences marked as problematic and mentions marked Illustration of the alignment of steps Statistics of datasets F1 scores in of SegTagDep on CTB 5c1 wrt the training epoch xaxis and parsing feature weights in legend Performance of baseline and joint models wrt the average processing time in sec per sen tence Each point corresponds to the beam size of 4 8 16 32 64 The beam size of 16 is used for SegTag in SegTagDep and SegTagTagDep Final results on CTB5j F1 scores and speed in sentences per sec of SegTagDep on CTB5c1 wrt the beam size Final results on CTB6 and CTB7 Segmentation POS tagging and unlabeled attachment dependency F1 scores averaged over five trials on CTB5c Figures in parentheses show the differences over SegTagDep p 001 Examples of the semantic role features Decoding algorithm for the standard TreetoString transducer lef twrightw denote the leftright boundary word of s c1 c2 denote the descendants of v ordered based on RHS of t An example showing the combination of the se mantic role sequences of the states Abovemiddle is the state information beforeafter applying the TTS template and bot tom is the used TTS template and the triggered SRFs during the combination An example showing how to compute the target side position of a semantic role by using the median of its aligning points Decoding algorithm using semantic role features Semac1 role c2 role t denotes the triggered semantic role features when combining two children states and ex amples can be found in Figure 3 Computing the partition function of the conditional probability P rST Semas1 s2 t denotes all the seman tic role features generated by combining s1 and s2 using t Examples of the MT outputs with and without SRFs The first and second example shows that SRFs improve the completeness and the ordering of the MT outputs respectively the third example shows that SRFs improve both properties The subscripts of each Chinese phrase show their aligned words in English Distribution of the sentences where the semantic role features give nopositivenegative impact to the sentence fluency in terms of the completeness and ordering of the semantic roles BLEU4 scores of different systems The results of different systems for coreference resolution Top patterns chosen under different scoring schemes The decision tree Nwire for the system using the single semantic relatedness feature Abridged grammatical representation for the example sentence 9 Sequence of POStagged units used to estimate the bilingual ngram LM Statistics for the training tune and test data sets Size in words of reorderings ob served in training bitexts Size of translation unit ngrams seen in test for different ngram models Reordering accuracy BLEU results Translation accuracy BLEU results Example context for the spelling confusion set piecepeace and extracted features Example context for WSD S ENSEVAL2 target word bar inventory of 21 senses and extracted features The increase in performance for successive variants of Bayes and Mixture Model as evaluated by 5fold cross vali dation on S ENSEVAL 2 English data A WSD example that shows the influence of syntactic collocational and longdistance context features the probability estimates used by Nave Bayes and MM and their associated weights and the posterior probabilities of the true sense as computed by the two models 2 Figure 4 WSD example showing the utility of the MVC method A sense with a high variational coefficient is preferred to the mode of the MM distribution the fields corresponding to the true sense are highlighted MM and MMVC performance by performing 5 fold cross validation on S ENSEVAL 2 data for 4 languages Results using 5fold cross validation on S ENSEVAL 2 English lexicalsample training data The contribution of MMVC in a rankbased classi fier combination on S ENSEVAL 1 and S ENSEVAL 2 English as computed by 5fold cross validation over training data Results on the standard 14 CSSC data sets Results using 5fold cross validation on S ENSEVAL 1 training data English Learning Curve for MM and MMVC on S ENSEVAL 2 English crossvalidated on heldout data ings of the 13th International Conference pages 182190 Table 6 Accuracy on S ENSEVAL1 and S ENSEVAL2 En A R Golding and D Roth 1999 A winnowbased appro glish test data only the supervised systems with a coverage of to contextsensitive spelling correction Machine Learni at least 97 were used to compute the mean and variance 3413107130 a An undirected graph G representing the similarity matrix b The bipartite graph showing three clusters on G c The induced clusters U d The new graph G1 over clusters U e The new bipartite graph over G1 Performance on T3 using a predefined tree structure Performance on T2 using a predefined tree structure Comparison against Stevenson and Joanis 2003s result on T1 using similar features NMI of unconstrained HGFC when trees for T2 and T3 are inferred automatically Effect of Factors Sentence Recency Performance of Algorithms Notation and signatures for our framework Additional notation and signatures for CAM CoreLexs basic types with their corresponding WordNet anchors CAM adopts these as meta senses Sample of experimental items for the meta alternation anmfod Abbreviations are listed in Table 2 Sample targets for meta alternations with high AP and midcoherence values Average Precision and Coherence for each meta alternation Correlation r 0743 p 0001 Meta alternations and their average precision values for the task The random baseline performs at 0313 while the frequency baseline ranges from 0255 to 0369 with a mean of 0291 Alternations for which the model outperforms the frequency baseline are in boldface mean AP 0399 standard deviation 0119 Illustration of dictionary based segmenta tion finite state transducer Performance of the mention detection sys tem using lexical features only Performance of the mention detection sys tem using lexical syntactic gazetteer features as well as features obtained by running other namedentity classifiers Performance of the mention detection sys tem including all ACE04 subtasks Effect of Arabic stemming features on coref erence resolution The row marked with Truth represents the results with true mentions while the row marked with System represents that mentions are detected by the system Numbers under ECM F are EntityConstrainedMention Fmeasure and numbers under ACEVal are ACEvalues Organisation of the hierarchical graph of concepts a An undirected graph G representing the similarity matrix b The bipartite graph showing three clusters on G c The induced clusters U d The new graph G1 over clusters U e The new bipartite graph over G1 Salient features for fire and the violence cluster Discovered metaphorical associations Identified metaphorical expressions for the mappings FEELING IS FIRE and CRIME IS A DISEASE Metaphors tagged by the system in bold A portion of the local cooccurrence graph for mouse from the SemEval2010 Task 14 corpus VMeasure and paired FScore results for different partitionings of the dendrogram The dashed vertical line indicates SP D Performance results on the SemEval2010 WSI Task with rank shown in parentheses Refer ence scores of the best submitted systems are shown in the bottom Manual analysis of suggested corrections on CLC data Precision and recall for articles Precision and recall for prepositions Using different amounts of annotated training data for the article metaclassifier Using different amounts of annotated training The Stanford parser Klein and Manning 2002 is unable to recover the verbal reading of the unvocalized surface form an Table 1 Diacritized particles and pseudoverbs that after orthographic normalization have the equivalent surface form an The distinctions in the ATB are linguistically justified but complicate parsing Table 8a shows that the best model recovers SBAR at only 710 F1 Frequency distribution for sentence lengths in the WSJ sections 223 and the ATB p13 English parsing evaluations usually report results on sentences up to length 40 Arabic sentences of up to length 63 would need to be evaluated to account for the same fraction of the data We propose a limit of 70 words for Arabic parsing evaluations Gross statistics for several different treebanks Test set OOV rate is computed using the following splits ATB Chiang et al 2006 CTB6 Huang and Harper 2009 Ne gra Dubey and Keller 2003 English sections 221 train and section 23 test An ATB sample from the human evaluation The ATB annotation guidelines specify that proper nouns should be specified with a flat NP a But the city name Sharm Al Sheikh is also iDafa hence the possibility for the incorrect annotation in b Evaluation of 100 randomly sampled variation nu clei types The samples from each corpus were indepen dently evaluated The ATB has a much higher fraction of nuclei per tree and a higher typelevel error rate Dev set learning curves for sentence lengths 70 All three curves remain steep at the maximum training set size of 18818 trees Test set results Maamouri et al 2009b evaluated the Bikel parser using the same ATB split but only reported dev set results with gold POS tags for sentences of length 40 The Bikel GoldPOS configuration only supplies the gold POS tags it does not force the parser to use them We are unaware of prior results for the Stanford parser The constituent Restoring of its constructive and effective role parsed by the three different models gold segmen tation The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals Like verbs maSdar takes arguments and assigns case to its objects whereas it also demonstrates nominal characteristics by eg taking determiners and heading iDafa Fassi Fehri 1993 In the ATB 2 3 astaadah is tagged 48 times as a noun and 9 times as verbal noun Consequently all three parsers prefer the nominal reading Table 8b shows that verbal nouns are the hardest preterminal categories to identify None of the models attach the attributive adjectives correctly Dev set results for sentences of length 70 Cov erage indicates the fraction of hypotheses in which the char acter yield exactly matched the reference Each model was able to produce hypotheses for all input sentences In these experiments the input lacks segmentation markers hence the slightly different dev set baseline than in Table 6 Per category performance of the Berkeley parser on sentence lengths 70 dev set gold segmentation a Of the high frequency phrasal categories ADJP and SBAR are the hardest to parse We showed in 2 that lexical ambiguity explains the underperformance of these categories b POS tagging accuracy is lowest for maSdar verbal nouns VBGVN and adjectives eg JJ Richer tag sets have been suggested for modeling morphologically complex distinctions Diab 2007 but we find that linguistically rich tag sets do not help parsing c Coordination ambiguity is shown in dependency scores by eg S S S R and NP NP NP R NP NP PP R and NP NP ADJP R are both iDafa attachment BiTAM models for Bilingual document and sentencepairs A node in the graph represents a random variable and a hexagon denotes a parameter Unshaded nodes are hidden variables All the plates represent replicates The outmost plate M plate represents M bilingual documentpairs while the inner N plate represents the N repeated choice of topics for each sentencepairs in the document the inner Jplate represents J wordpairs within each sentencepair a BiTAM1 samples one topic denoted by z per sentencepair b BiTAM2 utilizes the sentencelevel topics for both the translation model ie pf e z and the monolingual word distribution ie pez c BiTAM3 samples one topic per wordpair Training and Test Data Statistics Topicspecific translation lexicons are learned by a 3topic BiTAM1 The third lexicon Topic3 prefers to translate the word Korean into ChaoXian mNorth Korean The cooccurrence Cooc IBM14 and HMM only prefer to translate into HanGuo ISouth Korean The two candidate translations may both fade out in the learned translation lexicons Three most distinctive topics are displayed The English words for each topic are ranked according to pez estimated from the topicspecific English sentences weighted by dnk 33 functional words were removed to highlight the main content of each topic Topic A is about UsChina economic relationships Topic B relates to Chinese companies merging Topic C shows the sports of handicapped people performances over eight Variational EM itera tions of BiTAM1 using both the Null word and the laplace smoothing IBM1 is shown over eight EM iterations for comparison Word Alignment Accuracy Fmeasure and Machine Translation Quality for BiTAM Models comparing with IBM Models and HMMs with a training scheme of 18 h7 43 on the Treebank data listed in Table 1 For each column the highlighted alignment the best one under that model setting is picked up to further evaluate the translation quality Evaluating Word Alignment Accuracies and Machine Translation Qualities for BiTAM Models IBM Models HMMs and boosted BiTAMs using all the training data listed in Table 1 Other experimental conditions are similar to Table 4 Outline of word segmentation process Segmentation results by a pure subwordbased IOB tagging The upper numbers are of the character based and the lower ones the subwordbased Our segmentation results by the dictionary based approach for the closed test of Bakeoff 2005 very low Roov rates due to no OOV recognition applied Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of Fscore Effects of combination using the confidence measure The upper numbers and the lower numbers are of the characterbased and the subwordbased respec tively An underspecified discourse structure and its five configurations A wRTG modelling Fig 1 Runtime Comparison Feature set for the baseline pronoun res olution system structuredfeatures for the instance ihim the man Results of the syntactic structured fea tures Results using different parsers Comparison of the structured feature and the flat features extracted from parse trees Learning curves of systems with different features Feature templates used for CRF in our system performance each step of our system achieves Experimental data sets Precision at top 200 Results of 1000 sentences Precision at top 50 Results of 3000 sentences Precision at top 100 Results of 2000 sentences A framework for jointly identifying and aligning bilingual NEs Initial typesensitive ChineseEnglish NER performance NEA typeinsensitive typesensitive performance on the test set NEA typeinsensitive typesensitive performance with the same Chinese NE recognizer Wus system and different English NE recognizers NER typeinsensitive typesensitive performance of different English NE recognizers NER typeinsensitive typesensitive performance of different Chinese NE recognizers NEA typeinsensitive typesensitive performance with the same English NE recognizer Mallet system and different Chinese NE recognizers NEA typeinsensitive typesensitive performance with a different English NE recognizer and another Chinese NE recognizer Initial NE recognition typeinsensitive typesensitive performance across various domains The superiority of our joint model on three different domains indicated by typeinsensitive typesensitive performance those signicant entries are marked in comparison with baseline Comparison between a ME framework and the derived model on the same test set Distribution of various error categories typeinsensitive Distribution of Category VI error classes typeinsensitive Top four worstcase statistics of features for NE boundary errors Distribution of the NE type errors MERTW Effect of adjacent contextual nonNE bigrams on the test set Typesensitive improvement for ChineseEnglish NER Typeinsensitive improvement for ChineseEnglish NER English NE recognition on test data after semisupervised learning NE alignment on test data after semisupervised learning Syntactical Variations of activate Extraction of Raw Pattern Division of Raw Pattern into Combina tion Pattern Components EntityMainEntity Example Demonstrating Advantages of Full Parsing Features for SVM Learning of Prediction Model Results of IE Experiment Effect of Training Corpus Size 2 Effect of Training Corpus Size 1 Causes of Error for FPs Plate diagram representation of the trigram HMM The indexes i and j range over the set of tags and k ranges over the set of characters Hyperparameters have been omitted from the figure for clarity The conditioning structure of the hierarchical PYP with an embedded character language models Simulation comparing the expected table count solid lines versus the approximation under Eq 3 dashed lines for various values of a This data was gen erated from a single PYP with b 1 P0 i 14 and n 100 customers which all share the same tag WSJ performance comparing previous work to our own model The columns display the manyto1 accuracy and the V measure both averaged over 5 inde pendent runs Our model was run with the local sampler HMM the typelevel sampler 1HMM and also with the character LM 1HMMLM Also shown are results using Dirichlet Process DP priors by fixing a 0 The system abbreviations are CGS10 Christodoulopoulos et al 2010 BBDK10 BergKirkpatrick et al 2010 and GG07 Goldwater and Griffiths 2007 Starred entries denote results reported in CGS10 Sorted frequency of tags for WSJ The gold standard distribution follows a steep exponential curve while the induced model distributions are more uniform Cooccurence between frequent gold yaxis and predicted xaxis tags comparing mkcls top and PYP1HMMLM bottom Both axes are sorted in terms of frequency Darker shades indicate more frequent cooc curence and columns represent the induced tags M1 accuracy vs number of samples Manyto1 accuracy across a range of languages comparing our model with mkcls and the best published result BergKirkpatrick et al 2010 and Lee et al 2010 This data was taken from the CoNLLX shared task training sets resulting in listed corpus sizes Fine PoS tags were used for evaluation except for items marked with c which used the coarse tags For each language the systems were trained to produce the same number of tags as the gold standard Pipeline architecture for dialogue act recognition and reranking component Here the input is a list of dialogue acts with confidence scores and the output is the same list of dialogue acts but with recomputed confidence scores A dialogue act is represented as DialogueActTypeattributevalue pairs Bayesian network for probabilistic rea soning of locations variable from desc which incorporates ASR Nbest information in the vari ablefrom desc nbest and dialogue history in formation in the remaining random variables Bayesian dialogue act recognisers show ing the impact of ASR Nbest information Comparison for Head and Tail datasets Illustration of entityrelationship graphs Summary for graphs and test datasets obtained from each seed pair How to get the ordered set B t i j MRR of baseline and reinforced matrices Distribution over number of hits Matched translations over Ve and Vc Parameter setup for and Precision Recall and F1score of Engkoo Google and Ours with head and tail datasets Precision Recall and F1score of Baseline Engkoo Google and Ours over test sets Ti The first set of features in our model All of them are binary The final feature set includes two sets the set here and a set obtained by its conjunction with the verbs lemma An example parse tree for the second head word feature Accuracy and error reduction ER results in percents for our model and the MF baseline Error reduction is computed as M ODELM 100M F F Results are given for the WSJ and GENIA corpora test sets The top table is for a model receiving gold standard parses of the test data The bottom is for a model using Charniak and Johnson 2005 stateoftheart parses of the test data In the main scenario left instances were always mapped to VN classes while in the OIP one right it was possible during both training and test to map instances as not belonging to any existing class For the latter no results are displayed for polysemous verbs since each verb can be mapped both to other and to at least one class Selected morphosyntactic categories in the OLiA Reference Model Attributive demonstrative pronouns PDAT in the STTS Annotation Model Individuals for accusative and sin gular in the TIGER Annotation Model Selected morphological features in the OLiA Reference Model The STTS tags PDAT and ART their rep resentation in the Annotation Model and linking with the Reference Model Evaluation setup Confidence scores for diese in ex 1 Recall for morphological hasXY descriptions Examples of DTs and their ICDcodes The dataset shuffled and divided into 3 sets The dataset of DT ICDcode pairs A simplified version in Foma source code of the regular expressions and transducers used to bypass several sources of distortion within the DTs in order to parse variations of unseen input DTs Performance of different FS machines in terms of the percentage of unclassified entries All the classified entries were correctly classified yielding as a result a precision of 100 Taxonomy of Chinese words used in developing MSRSeg a A Chinese sentence Slashes indicate word boundaries b An output of our word segmentation system Square brackets indicate word boundaries indicates a morpheme boundary Taxonomy of morphologically derived words MDWs in MSRSeg Words in the MSR gold test set Domainstyle distribution in the MSR test corpus Standards and corpora Evaluation measures for Chinese word segmenter Context model word classes class models and feature functions The perceptron training algorithm for Chinese word segmentation Overall architecture of MSRSeg Generative patterns of ONA where sij denotes the jth character of the ith word of ON Sun Zhou and Gao 2003 FT detection results on the MSR gold test set The All column shows the results of detecting all 10 types of factoids as described in Table 1 which amount to 6630 factoids as shown in Table 3 NWI results on HK and AS corpora NWI as postprocessor versus unified approach NW 11 identification results on PK test set NW 21 identification results on PK test set NWI results on PK and CTB corpora NWI as postprocessor versus unified approach Word internal structure and classtype transformation templates Comparison scores for PK open and CTB open Size of training data set and the adaptation results on AS open Comparison scores for HK open and AS open Methods of resolving OAs in word segmentation on the MSR test set Figure 7 a A Chinese OAS b Two sentences in the training set which contain t whose OASs have been replaced with the single tokens OAS Li et al 2003 Results of 70 highfrequency twocharacter CASs Voting indicates the accuracy of the baseline method that always chooses the more frequent case of a given CAS ME indicates the accuracy of the maximumentropy classifier VSM indicates the accuracy of the method of using VSM for disambiguation Comparison of performance of MSRSeg The versions that are trained using semisupervised iterative training with different initial training sets Rows 1 to 8 versus the version that is trained on annotated corpus of 20 million words Row 9 Precision of organization name recognition on the MSR test set using Viterbi iterative training initialized by four seed sets with different sizes Precision of location name recognition on the MSR test set using Viterbi iterative training initialized by four seed sets with different sizes Precision of person name recognition on the MSR test set using Viterbi iterative training initialized by four seed sets with different sizes MSRSeg system results for the MSR test set Comparisons against other segmenters In Column 1 SXX indicates participating sites in the 1st SIGHAN International Chinese Word Segmentation Bakeoff and CRFs indicates the word segmenter reported in Peng et al 2004 In Columns 2 to 5 entries contain the Fmeasure of each segmenter on different open runs with the best performance in bold Column SiteAvg is the average Fmeasure over the data sets on which a segmenter reported results of open runs where a bolded entry indicates the segmenter outperforms MSRSeg Column OurAvg is the average Fmeasure of MSRSeg over the same data sets where a bolded entry indicates that MSRSeg outperforms the other segmenter Crosssystem comparison results The screenshot of our webbased system shows a simple quantitative analysis of the frequency of two terms in news articles over time While in the 90s the term Friedensmission peace operation was predominant a reverse tendency can be observed since 2001 with Auslandseinsatz foreign intervention being now frequently used Overview of the complete processing chain Dependency parse of a sentence that contains indirect speech see Sentence 2 10 most used verbs lemma in indirect speech Results of a 10fold crossvalidation for various machine learning algorithms Learning Curves for Confusion Set Disambiguation Representation Size vs Training Corpus Size Complementarity Voting Among Classifiers Active Learning with Large Corpora CommitteeBased Unsupervised Learning Committee Agreement vs Accuracy Comparison of Unsupervised Learning Methods Results for the acquisition of subcategori sation frames Clustering evaluation for the experiment with Named Entities Clustering evaluation for the experiment without Named Entities ELUS Segmenter Open test in percentages Closed test in percentages Graphical model of HMBiTAM Graphical model of synonym pair gen erative process The number of vocabularies in the 10k 50k and 100k data sets Comparison of word alignment accuracy The best results are indicated in bold type The additional data set sizes are a 10k b 50k c 100k Entity detection and tracking system flow The procedure of TBL entity track ingcoreference model EDT and mention detection results Statistics of the ACE corpus Examples of transformation rules of Templates for feedback Three narrative events and the six most likely events to include in the same chain One of the 69 test documents containing 10 narrative events The protagonist is President Bush Results with varying sizes of training data Year 2003 is not explicitly shown because it has an unusually small number of documents compared to other years A narrative chain and its reverse order Results for choosing the correct ordered chain 10 means there were at least 10 pairs of ordered events in the chain An Employment Chain Dotted lines indicate incorrect before relations An automatically learned Prosecution Chain Arrows indicate the before relation Examples of zero anaphora Examples of zero anaphora Annotation statistics Results for baseline BAS system standard multiclass SVM Results for basic DAC system perclass feature optimization followed by maximum confidence based choice ER refers to error reduction in percent over standard multiclass SVM Table 2 Results for cascading minoritypreference DAC system DACCMP consult classifiers in reverse order of frequency of class ER refers to error reduction in percent over standard multiclass SVM Table 2 Results for ODP system using various sources of DA tags Posthoc analysis on the models built by the DAC system some of the top features with corresponding feature weights in parentheses for each individual tagger POS tags are capitalized BOS stands for Beginning Of Sentence a A Chinese sentence Slashes indicate word boundaries b An output of our word segmentation system Square brackets indicate word boundaries indicates a morpheme boundary Class models system results Comparison results Probability of of boundaries f10 m 3 Results of segmentation of entry titles Fscore precisionrecall Acceptance rates for a noun phrase in the course of iteration All models were with backoff mix ing BM Diffs in the course of iteration All models were with backoff mixing BM Effect of matching skip Fscore precisionrecall Features based on the token string Sources of Dictionaries The whole process of retraining the upper case NER Q signifies that the text is converted to upper case before processing Fmeasure on MUC6 and MUC7 test data Improvements in Fmeasure on MUC6 plotted against amount of selected unlabeled data used Improvements in Fmeasure on MUC7 plotted against amount of selected unlabeled data used Subtypes of the ArgM modifier tag Split constituents In this case a single semantic role label points to multiple nodes in the original treebank tree Interannotator agreement Confusion matrix among subtypes of ArgM defined in Table 1 Entries are fraction of all ArgM labels Entries are a fraction of all ArgM labels true zeros are omitted while other entries are rounded to zero Confusion matrix for argument labels with ArgM labels collapsed into one category Entries are a fraction of total annotations true zeros are omitted while other entries are rounded to zero Comparison of frames Most frequent semantic roles for each syntactic position Most frequent syntactic positions for each semantic role Semantic roles of verbs subjects for the verb classes of Merlo and Stevenson 2001 cont Semantic roles for different frame sets of kick In this example the path from the predicate ate to the argument NP He can be represented as VBjVPjSNP with j indicating upward movement in the parse tree and downward movement Backoff lattice with more specific distributions towards the top Accuracy of semanticrole prediction in percentages for known boundaries the system is given the constituents to classify Accuracy of semanticrole prediction in percentages for unknown boundaries the system must identify the correct constituents as arguments and give them the correct roles Common values in percentages for parse tree path in PropBank data using goldstandard parses Accuracy of semanticrole prediction for unknown boundaries the system must identify the correct constituents as arguments and give them the correct roles Summary of results for unknownboundary condition The directional matching relationships between a hypothesis h an entailment rule r and a text t in the Contextual Preferences framework Recall R Precision P and Mean Average Pre cision MAP when also using rules for matching Recall R Precision P and Mean Average Pre cision MAP when only matching template hypotheses directly RecallPrecision curves for ranking using a only the prior baseline b allCP c allCPpr MAP under the 50 rules All setup when adding component match scores to Precision P or prior only MAP baselines and when ranking with allCP or allCPpr methods but ignoring that component scores Trivial and singlefeature baselines using SVM acc unless noted otherwise Combination results using SVMacc Results by relation Unigram bigram and trigram counts of the ligature corpus Unigram bigram and trigram counts of the word corpus Results changing beam width k of the tree Lattice representation of the sentence bclm hneim Doublecircles denote token boundaries Lattice arcs correspond to different segments of the token each lattice path encodes a possible reading of the sentence Notice how the token bclm have analyses which include segments which are not directly present in the unsegmented form such as the definite article h 13 and the pronominal suffix which is expanded to the sequence fl hm of them 24 45 Parsing scores of the various systems Sample analysis of an English sentence Input Do we have to reserve rooms Resolution of ambiguity on the Verbmobil corpus Candidates for equivalence classes Training and test with hierarchical lexicon Inverse restructuring analyze and annotation all require morphosyntactic analysis of the transformed sentences Disambiguation of conventional dictionaries Learn phrases analyze and annotation require morphosyntactic analysis of the transformed sentences Training with scarce resources Restructuring learn phrases and annotation all require morphosyntactic analysis of the transformed sentences Statistics of corpora for training Verbmobil and Nespole Singletons are types occurring only once in training The official vocabularies in Verbmobil Statistics for the test sets for German to English translation Verbmobil Eval2000 Test and Develop and Nespole Conventional dictionary used to complement the training corpus Impact of corpus size measured in number of running words in the corpus on vocabulary size measured in number of different fullform words found in the corpus for the German part of the Verbmobil corpus Results for hierarchical lexicon model Nespole Restructuring entails treatment of question inversion and separated verb prefixes as well as merging of phrases in both languages The same conventional dictionary was used as in the experiments the Verbmobil The language model was trained on a combination of the English parts of the Nespole corpus and the Verbmobil corpus Examples of the effect of the hierarchical lexicon The performance on the set of unknown Experimental result of total unknown The performance on the set of unknown Speech Act Categories and Kappa values An example discussion thread Statistics for each Speech Act Category Some of the top selected features by Infor mation Gain SA classification results Thread Classification Results Example patterns in student discussion threads Results from Direct Thread Classification Two fragments of a hierarchy over word class distributions Error reduction as a function of vocabulary size Evaluation of coarsegrained POS tagging on test data Evaluation of coarsegrained POS tagging on test data Example of a socalled semiformal text where one can see that here more time points are available and that those can be complemen tary to the time points to be extracted from formal texts So already at this level a unification or merging of extracted time points is necessary Example of a German noun phrase First and last word agree in number gender and case value Syntactic features h and ld mark features from the head and the leftmost daughter dir is a binary fea ture marking the direction of the head with respect to the current token The effect of syntactic features when predicting morphological information mark statistically signifi cantly better models compared to our baseline sentence based ttest with 005 The effect of syntactic features when predicting morphology using lexicons mark statistically signifi cantly better models compared to our baseline sentence based ttest with 005 Syntactic features for featurama Czech mark statistically significantly better models compared to feat urama sentencebased ttest with 005 Agreement counts in morphological annotation compared between the baseline system and the oracle system using gold syntax Simple parser vs full parser syntactic quality Trained on first 5000 sentences of the training set Dependency between amount of training data for syntactic parser and quality of morphological prediction Impact of the improved morphology on the qual ity of the dependency parser for Czech and German Simple parser vs full parser morphological quality The parsing models were trained on the first 5000 sentences of the training data the morphological tagger was trained on the full training set Baseline Outofthebox BerkeleyParser performance on the devset Number of learned splits per POS category after five splitmerge cycles Number of learned splits per NTcategory after five splitmerge cycles A layered POS tag representation A latent layered POS tag representation The lattice for the Hebrew sequence see footnote 19 Devset results when using lattice parsing on top of an external lexiconanalyzer Devset results when incorporating an external lexicon Devset results of using the agreementfilter on top of the lexiconenhanced lattice parser parser does both segmentation and parsing Devset results of using the agreementfilter on top of the lexiconenhanced parser starting from gold segmentation Numbers of parsetree nodes in the 1best parses of the development set that triggered gender or number agreement checks and the results of these checks Testset results of the bestperforming models NP agreement violations that were caught by the agreement filter system a Nouncompound case that was correctly handled b Case involving conjunction that was correctly handled c A case where fixing the agreement violation introduces a PPattachment mistake The NLMWSD test set and some of its sub sets Note that the test set used by Joshi et al 2005 comprises the set union of the terms used by Liu et al 2004 and Leroy and Rindflesch 2005 while the com mon subset is formed from their intersection Results from WSD system applied to various sections of the NLMWSD data set using a variety of fea tures and machine learning algorithms Results from baseline and previously published approaches are included for comparison Perword performance of best reported systems The value of the penalized loss based on the number of iterations DPLVMs vs CRFs on the MSR data Details of the corpora WT represents word types CT represents character types SC represents simplied Chinese TC represents traditional Chinese Error analysis on the latent variable seg menter The errors are grouped into four types over generalization errors on named entities errors on idioms and errors from datainconsistency Example of semantic trees Two STs composing a STN ROUGE Fscores for different systems LDA Phrase pairs extracted from a document pair with an economic topic Topic words extracted from targetside doc uments Corpus statistics 4 Contribution of various caches in our cache based documentlevel SMT system Note that signific ance tests are done against Moses Contribution of combining the three caches Contribution of combining the dynamic Contribution of combining the dynamic and Contribution of employing the dynamic cache on different test documents Impact of the topic cache size Contribution of the static cache on the first sentence of each test document ie with empty dynamic cache Positive and negative examples Descriptions of the 10 corpora Average precisions over the 10 corpora of different window size 3 seeds Precision top N with 3 seeds and window size w 3 Selfbootstrapping algorithm Numbers of relations on the ACE RDC 2004 break down by relation types and subtypes Stratefied Sampling for initial seeds The initial performance of applying various sampling strategies to selecting the initial The highest performance of applying various sampling strategies in selecting the initial seed set on the ACE RDC 2004 corpus Bootstrapping time for different p values Performance for different p values Comparison of two augmentation strategies over different sampling strategies in selecting the initial seed set Comparison of semisupervised relation classification systems on the ACE RDC 2003 corpus A pruned phrase tokenization lattice Edges are tokenizations of phrases eg e5 represents tokenizing question into a word and e7 represents tokenizing doubt him into a partial word doubt followed by a word him The pseudo code of Algorithm 1 The Riv over the bakeoff2 data The Roov over the bakeoff2 data The Fscore over the bakeoff2 data Denition of NE in IREX Example of morphological analyses Case frame of haken dispatch Experimental results Fmeasure Comparison with previous work The set of types and subtypes of relations used in the 2004 ACE evaluation The Division of LDC annotated data into training and development test sets Comparing Fmeasure precision and recall of different voting schemes for English relation extraction Comparing Fmeasure precision and recall of different voting schemes for Chinese relation extraction Comparing Fmeasure precision and recall of different voting schemes for Arabic relation ex traction Comparing the best Fmeasure obtained by AtLeastN Voting with Majority Voting Summing and the single best classifier A focused entailment graph For clarity edges that can be inferred by transitivity are omitted The single strongly connected component is surrounded by a dashed line Positive and negative examples for entailment in the training set The direction of entailment is from the left template to the right template The similarity score features used to represent pairs of templates The columns specify the corpus over which the similarity score was computed the template representation the similarity measure employed and the feature representation as described in Section 41 Scenarios in which we added hard constraints to the ILP Results when tuning for performance over the development set Results when the development set is not used to estimate and K Results with prior estimated on the development set that is 01 which is equivalent to 23 Recallprecision curve comparing ILPGlobal with GreedyGlobal and ILPLocal Results per concept for the ILPGlobal Results of all distributional similarity measures when tuning K over the development set We encode the description of the measures presented in Table 2 in the following manner h healthcare corpus R RCV1 corpus b binary templates u unary templates L Lin similarity measure B BInc similarity measure pCt pair of CUI tuples representation pC pair of CUIs representation Ct CUI tuple representation C CUI representation Lin Pantel similarity lists learned by Lin and Pantel Comparing disagreements between ILPGlobal and ILPLocal against the goldstandard graphs A comparison between ILPGlobal and ILPLocal for two fragments of the testset concept seizure A comparison between ILPGlobal and ILPlocal for two fragments of the testset concept diarrhea Comparing disagreements between ILPGlobal and GreedyGlobal against the goldstandard graphs A comparison between ILPGlobal and GreedyGlobal Parts A1A3 depict the incremental progress of Greedy Global for a fragment of the headache graph Part B depicts the corresponding fragment in ILPGlobal Nodes surrounded by a bold oval shape are strongly connected components Distribution of probabilities given by the classier over all node pairs of the testset graphs Error analysis for false positives and false negatives A scenario where ILPGlobal makes a mistake but ILPLocal is correct The set of new features The last two columns denote the number and percentage of examples for which the value of the feature is nonzero in examples generated from the 23 goldstandard graphs Macroaverage recall precision and F1 on the development set and test set using the parameters that maximize F1 of the learned edges over the development set Results of feature analysis The second column denotes the proportion of manually annotated examples for which the feature value is nonzero A detailed explanation of the other columns is provided in the body of the article A hierarchical summary of propositions involving nausea as an argument such as headache is related to nausea acupuncture helps with nausea and Lorazepam treats nausea Example distributions of German verbs Data similarity measures kmeans experiment baseline and upper bound Comparing distributions on D1 and D2 Comparing similarity measures on D1 and D2 Comparing clustering initializations on D2 Comparing clustering initializations on D1 Comparing feature descriptions Comparing selectional preference frame definitions Comparing selectional preference slot definitions Varying the number of clusters evaluation Randadj Largescale clustering on D1 Largescale clustering on D3 with nnandnadnsdass Largescale clustering on D2 SyntSem tagged corpus extract Optimal context size S and criteria space distribution by partofspeech of Precision P and usage proportion optimal context size using both precision with and without feature precision decrease when omitting non space distribution of most reliable target word frequency F average LMR Tagging Learning curves on the development dataset of the Beijing Univ corpus Learning curves on the development dataset of the HK City Univ corpus Official Bakeoff Outcome Fscore on development data Evaluation measures tp true positives fp false positives tn true negatives fn false negatives pr precision re recall Classification results with XLE starredness parser exceptions and zero parses Method 1 Classification results with 5gram and fre quency threshold 4 Method 2 Classification results with decision tree on XLE output Method 3 Classification results with decision tree on vectors of frequency of rarest ngrams Method 4 Classification results with decision tree on joined feature set Method 5 Accuracy by sentence length for Method 5 measured on separate grammatical and ungrammat ical data Gr Grammatical AG Agreement RW RealWord EW Extra Word MW Missing Word Sample Embedded Answer Summariser and VPA Architecture Positional sentence weight for varying Precision by Named Entity Class Recall by Named Entity Class Average Precision and Recall Utility Score Comparison Summaries Recall and Precision Training and test set sources genres sizes in terms of numbers of tokens and unigram and bi gram coverage of the training set on the test sets Learning curves of word prediction accu racies of IGT REE trained on TRAIN REUTERS and tested on REUTERS ALICE and BROWN Word prediction speed in terms of the number of classified test examples per second mea sured on the three test sets with increasing training examples Both axes have a logarithmic scale Learning curves in terms of word predic tion accuracy on deciding between the confusable pair there their and theyre by IGT REE trained on TRAIN REUTERS and tested on REUTERS AL ICE and BROWN The top graphs are accuracies at tained by the confusable expert the bottom graphs are attained by the allwords predictor trained on TRAIN REUTERS until 130 million examples and on TRAIN NYT beyond marked by the vertical bar Disambiguation scores on nine confusable set attained by confusable experts trained on ex amples extracted from 1 billion words of text from TRAIN REUTERS plus TRAIN NYT on the three test sets Disambiguation scores on nine confusable set attained by the allwords prediction classifier trained on 30 million examples of TRAIN REUTERS and by confusable experts on the same training set The second column displays the number of exam ples of each confusable set in the 30million word training set the list is ordered on this column Relation Feature Spaces of the Example Sentence to stop the merger of an estimated Performance of seven relation feature spaces over the 5 ACE major types using parse tree information only Performance comparison the numbers in parentheses report the performance over the 24 ACE subtypes while the numbers outside paren theses is for the 5 ACE major types Error Distribution Kleene RegularExpression Assignment Examples Four synchronous rules with topic distributions Each subgraph shows a rule with its topic distribution where the Xaxis means topic index and the Yaxis means the topic probability Notably the rule b and rule c shares the same source Chinese string but they have different topic distributions due to the different English translations Example of topictotopic correspondence The last line shows the correspondence probability Each col umn means a topic represented by its top10 topical word s The first column is a targetside topic while the rest three columns are sourceside topics Result of our topic similarity model in terms of BLEU and speed words per second comparing with the traditional hierarchical system Baseline and the topicspecific lexicon translation method TopicLex SimSrc and SimTgt denote similarity by sourceside and targetside ruledistribution respectively while SimSen acti vates the two similarity and two sensitivity features Avg is the average B LEU score on the two test sets Scores marked in bold mean significantly Koehn 2004 better than Baseline p 001 Effects of onetoone and onetomany topic pro jection Distribution of antecedent NP types in the otheranaphora data set Overview of the results for all baselines for otheranaphora Descriptive statistics for WordNet hypsyn relations for otheranaphora Patterns and instantiations for otheranaphora Descriptive statistics for Web scores and BNC scores for otheranaphora Properties of the variations for the corpusbased algorithms for otheranaphora Web results for otheranaphora BNC results for otheranaphora Overview of the results for the best algorithms for otheranaphora Occurrences of error types for the best otheranaphora algorithm algoWebv4 Distribution of antecedent NP types for definite NP anaphora Overview of the results for all baselines for coreference Descriptive statistics for WordNet hypsyn relations on the coreference data set Overview of the results for all WordNet algorithms for coreference Overview of the results for all Web algorithms for coreference Overview of the results for all BNC algorithms for coreference Occurrences of error types for the best coreference algorithm algoWebv4n Word segmentation on NIST data sets Example of 1ton word alignments be tween English words and Chinese characters Word segmentation on IWSLT data sets Example of a word lattice BS on IWSLT 2007 task BS on IWSLT 2006 task Corpus statistics for Chinese Zh character segmentation and English En BS on NIST task Scalability of BS on NIST task Vocabulary size of NIST task 40K Scaleup to 160K on IWSLT data sets Vocabulary size of IWSLT task 40K The search graph on development set of IWSLT task BS on IWSLT data sets using MTTK Illustration of similarities in POS tag statistics across languages a The unigram frequency statistics on five tags for two close languages English and German b Sample sentences in English and German Verbs are shown in blue prepositions in red and noun phrases in green It can be seen that noun phrases follow prepositions An iterative algorithm for minimizing our ob jective in Eq 7 For simplicity we assume that all the weights i and are equal to one It can be shown that the objective monotonically decreases in every iteration The set of typological features that we use for source language selection The first column gives the ID of the feature as listed in WALS The second column describes the feature and the last column enumerates the allowable values for each feature besides these values each feature can also have a value of No dominant order Directed dependency accuracy of our model and the baselines using gold POS tags for the target language The first section of the table is for the direct transfer of the MST parser McDonald et al 2011 The second section is for the weighted mixture parsing model Cohen et al 2011 The first two columns Random and Greedy of each section present the parsing performance with a random or a greedy mapping The third column Petrov shows the results when the mapping of Petrov et al 2011 is used The fourth column Model shows the results when our mapping is used and the fifth column in the first section Best Pair shows the performance of our model when the best source language is selected for every target language The last column Tag Diff presents the difference between our mapping and the mapping of Petrov et al 2011 by showing the percentage of target language tokens for which the two mappings select a different universal tag Objective values for the different mappings used in our experiments for four languages Note that the goal of the optimization procedure is to minimize the objective value Word alignment based translation model PJ AE IBM Model 4 Example of word alignment Example of chunkbased alignment Chunkbased translation model The words in bold are head words Basic Travel Expression Corpus Experimental results for JapaneseEnglish Examples of viterbi chunking and chunk alignment for EnglishtoJapanese translation model Chunks are bracketed and the words with to the left are head words Opinion PageRank Opinion HITS model Opinion Question Answering System Sentiment lexicon description Opinion PageRank Performance with varying parameter 02 Sentiment Lexicon Performance Opinion PageRank Performance with varying parameter 05 Questionspecific popular topic words and opinion words generated by Opinion HITS Opinion HITS Performance with vary ing parameter Comparison results with TAC 2008 Three Top Ranked Systems system 13 demonstrate top 3 systems in TAC Screenshot of ConAno Overview of annotation environment An example of English Chinese and French terms consisting of the same morphemes Example of first and second order features using a predefined ngram size of 2 Example of a term construction rule as a branch in a decision tree FScore of the RF and SVM GIZA and Levenshtein distancebased classifier on the first order dataset Best observed performance of RF SVM and GIZA and Levenshtein Distance FScore of the RF and SVM GIZA and Levenshtein distancebased classifier on the second order dataset 1 OBI vs BI where the lost of F 1 such as SCB is caused by incorrect English segments that will be discussed in the section 4 Baseline vs Submitted Results An example of annotation projection for relation detection of a bitext in English and Korean Numbers of projected instances Experimental Results Possible interpretations for the text wAlY Habash and Rambow 2005 Syntactic dependency scheme used in this work Labels that arent selfexplanatory or similar to the labels used by Tratz and Hovy 2011 for English or CATiB for Arabic Habash and Roth 2009 are in bold for completely new relations or italics for similarly named but semantically different relations Counts of the number of files sentences Sent original spacedelimited tokens Tok ATB tree tokens Tree Toks and affixes in the experimental data Counts for the POS tags mentioned in Table 5 Top 10 POS mistakes made more often by either the CTFTM with parsing or the CTFTM without on the ATB part 1 2 and 3 development set Results for the various experiments Exp for both the development and test portions of the data including per token clitic separation tokenization accuracy partofspeech tagging F1 affix boundary detection F1 affix labeling F1 and both unlabeled and labeled attachment scores Chinese parse tree with empty elements marked The meaning of the sentence is Implementation of the law is temporarily suspended English parse tree with empty elements marked a As annotated in the Penn Treebank b With empty elements recongured and slash categories added Recall on different types of empty categories YX Yang and Xue 2010 Ours split 6 Results on Penn Chinese Treebank Results on Penn English Treebank Wall Street Journal sentences with 100 words or fewer Length distribution of entities in the train ing set of the shared task in 2004 JNLPBA Modification of O other labels to transfer information on a preceding named entity The framework of our system We first enumerate all possible candidate states and then filter out low probability states by using a lightweight classifier and represent them by using feature forest Features used in the naive Bayes Classi fier for the entity candidate ws ws1 we spi is the result of shallow parsing at wi Example of feature forest representation of linear chain CRFs Feature functions are as signed to and nodes Example of packed representation of semiCRFs The states that have the same end po sition and preventity label are packed Filtering results using the naive Bayes classifier The number of entity candidates for the training set was 4179662 and that of the develop ment set was 418628 Feature templates used for the chunk s ws ws1 we where ws and we represent the words at the beginning and ending of the target chunk respectively pi is the part of speech tag of wi and sci is the shallow parse result of wi Comparison with other systems Performance with filtering on the development data 10 1012 means the threshold probability of the filtering is 10 1012 Performance of our system on the evalu ation set Overall performance on the evaluation set L is the upper bound of the length of possible chunks in semiCRFs Basic Statistics of DUC2007 Update Data Set System Comparison Experiment Results Rules and patterns for the four syntacticosemantic structures Regular expression notations matches the preceding element zero or more times matches the preceding element one or more times indicates that the preceding element is optional indicates or Abbreviations Ec m coarsegrained entity type of mention m Ld labels in dependency path between the headword of two mentions We use square brackets and to denote mention boundaries The in the Formulaic row denotes the occurrence of a lexical in text Additional RE features Microaveraged across the 5 folds RE results using predicted mentions Microaveraged across the 5 folds RE results using gold mentions Recall and precision of the patterns Improvement in predicted mention RE Improvement in gold mention RE Bahktins characterization of dialogue Bahktin 1986 describes a discourse along the three major properties style situation and topic Current information retrieval systems focus on the topical as pect which might be crucial in written documents Furthermore since throughout text analysis is still a hard problem information retrieval has mostly used keywords to characterize topic Many features that could be extracted are therefore ignored in a tradi tional keyword based approach Information access hierarchy Oral com munications take place in very different formats and the first step in the search is to determine the database or subdatabase of the rejoinder The next step is to find the specific rejoinder Since re joinders can be very long the rejoinder has to seg mented and a segment has to be selected Distribution of activity types Both databases contain a lot of discussing informing and storytelling activities however the meeting data contains a lot more planning and advising Intercoder agreement for activities The meeting dialogues and Santa Barbara corpus have been annotated by a seminaive coder and the first author of the paper The coefficient is determined as in Carletta et al 1997 and mutual information measures how much one label informs the other see Sec 3 For CallHome Spanish 3 dialogues were coded for activities by two coders and the result seems to indicate that the task was easier Activity detection Activities are detected on the Santa Barbara Corpus SBC and the meet ing database meet either without clustering the activities all or clustering them according to their interactivity interactive see Sec 2 for details TV show types The distribution of show types in a large database of TV shows 1067 shows that has been recorded over the period of a couple of months until April 2000 in Pittsburgh PA Detection accuracy summary The detec tion of highlevel genre as exemplified by the differ entiation of corpora can be done with high accuracy using simple features Ries 1999 Similar it was fairly easy to discriminate between male and female speakers on Switchboard Ries 1999 Discrimi nating between subgenre such as TVshow types Sec 4 can be done with reasonable accuracy How ever it is a lot harder to discriminate between ac tivities within one conversation for personal phone calls CallHome Ries et al 2000 or for general rejoinders Santa and meetings Sec 2 Show type detection Using the neural net work described in Sec 2 the show type was detected If there is a number in the word column the word feature is being used The number indicates how many wordpart of speech pairs are in the vocabu lary additionally to the parts of speech Crossdomain B3 Bagga and Baldwin 1998 results for Reconcile with its general feature set The Paired Permutation test Pesarin 2001 was used for statistical significance testing and gray cells represent results that are not significantly different from the best result B3 results for baselines and lexicalized feature sets across four domains B3 results for baselines and lexicalized feature sets on the broadcoverage ACE 2004 data set Crossdomain B3 and MUC results for Reconcile and Sieve with lexical features Gray cells represent results that are not significantly different from the best results in the column at the 005 plevel System Architecture Automatic Evaluations Biography Text Evaluations Weather Text Evaluations Weather Sentence Evaluations Biography Sentence Evaluations Comparison of our system with the bestreported systems on MUC6 and MUC7 Counts of different misspellings of Albert Einsteins name in a web query log Modified Viterbi search stopword treatment Example of trellis of the modified Viterbi search Accuracy and recall as functions of the number of monthly query logs used to train the language model Accuracy of the proposed system on a set which contains misspelled queries that the users had reformulated Accuracy of various instantiations of the system A graphical representation of the HMM ap proach for speaker role labeling This is a simple first order HMM Automatic role labeling results using the HMM and Maxent classifiers Impact of role sequence information on the HMM and Maxent classifiers The combination results of the HMM and Maxent are also provided Plate diagram of the basic model with a single feature per token the observed variable f M Z and nj are the number of word types syntactic classes z and features tokens per word type respectively Plate diagram of the extended model with T kinds of tokenlevel features f t variables and a single kind of typelevel feature morphology m Vmeasure VM and manytoone M1 results on the languages in the MULTEXTEast corpus using the gold standard number of classes shown in Table 4 BASE results use 1word context features alone or with morphology ALIGNMENTS adds alignment features reporting the average score across all possible choices of paired language and the scores under the best performing paired language in parens alone or with morphology features Final results on 25 corpora in 20 languages with the number of induced classes equal to the number of gold standard tags in all cases kmeans and SVD2 models could not produce a clustering in the Czech CoNLL corpus due its size Best published results are from Christodoulopoulos et al 2010 BergKirkpatrick et al 2010 and Lee et al 2010 The latter two papers do not report VM scores No best published results are shown for the MULTEXT languages Christodoulopoulos et al 2010 report results based on 45 tags suggesting that clark performs best on these corpora Examples of sentences x domainindependent underspecified logical forms l0 fully specified logical forms y and answers a drawn from the Freebase domain A sample CCG parse Definition of the operations used to transform the structure of the underspecified logical form l0 to match the ontology O The function typec calculates a constant cs type The function freevlf returns the set of variables that are free in lf not bound by a lambda term or quantifier The function subexpslf generates the set of all subexpressions of the lambda calculus expression lf Example derivation for the query how many people visit the public library of new york annu ally Underspecified constants are labelled with the words from the query that they are associated with for readability Constants from O written in typeset are introduced in step c Parameter estimation from QA pairs GeoQuery Results Ablation Results Results on the FQ dataset Example error cases with associated frequencies illustrating system output and gold standard references 5 of the cases were miscellaneous or otherwise difficult to categorize A second example of disagreement in segmentation guidelines Examples of disagreement in segmentation guidelines Analysis of results of segmentation on LDC training and test data for all CWS schemes BLEU scores for CWS schemes Correlation between Fscore and BLEU Feature blending of translation models Feature interpolation of translation models AICTCLAS Bdicthybrid CdictPKULDC DdictCITYU ECRFAS Common grammatical relations of Minipar involving nouns The top 20 most similar words for country and their ranks in the similarity list of LIN followed by the next four words in the similarity list that were judged as entailing at least in one direction The top 10 ranked features for country produced by MI the weighting function employed in the LIN method Lexical entailment precision values for topn similar words by the Bootstrapped LIN and the original LIN method Percentage of correct entailments within the top 40 candidate pairs of each of the methods LIN and Bootstrapped LIN denoted as LINB in the gure when using varying numbers of topranked features in the feature vector The value of All corresponds to the full size of vectors and is typically in the range of 300400 features Comparative precision values for the top 20 similarity lists of the three selected similarity measures with MI and Bootstrapped feature weighting for each Top 30 features of town by bootstrapped weighting based on LIN WJ and COS as initial similarities The three sets of words are almost identical with relatively minor ranking differences Top 10 features of country by the Bootstrapped feature weighting LIN MI weighting The top 10 common features for countrystate and countryparty along with their corresponding ranks in each of the two feature vectors The features are sorted by the sum of their feature weights with both words Top 20 most similar words for country and their ranks in the similarity list by the Bootstrapped LIN measure Bootstrapped weighting top 10 common features for countrystate and countryparty along with their corresponding ranks in the two sorted feature vectors Comparison between the acfrratio for MI and Bootstrapped LIN methods when using varying numbers of common topranked features in the words feature vectors The comparative error rates of the pseudodisambiguation task for the three examined similarity measures with and without applying the bootstrapped weighting for each of them Tradeoff between Margin Threshold and name recognition performance Accuracy of obscure name recognition Coreference factors for name recognition System Flow Results for Mutiple Document System with additional retrieved texts Contributions of features Results for Mutiple Document System Results for Single Document System Results with Coref Rules Alone Baseline Name Tagger Comparison with voted cache Statistical results Evaluation results Wrong assignment due to missing sense from the Hound of the Baskervilles Ch 14 Sample Minipar parse and extracted gram matical function features Experiment 1 Results for label unknown sense WSD confidence level approach confi dence threshold std dev Outlier detection by comparing distances between nearest neighbors Experiment 2 Results for label unknown sense NNbased outlier detection 10 stan dard deviation Experiment 2 Results by training set size 10 Experiment 3 Results for label unknown sense NNbased outlier detection 10 stan dard deviation Extending training sets an example Acceptance radius of an outlier within the training set left and a more normal training set object right Experiment 3 Results by training set size 10 Experiments 2 and 3 Results by the num ber of senses of a lemma condition All 10 Example entries for the Transfer of a Message levels 1 and 2 classes Clusters for transitive unaccusative and ditransitive Outcome of clustering procedure Logical form graphs aligned with sur face forms in two languages Logical form graph Encoding local word order Graph for a neoDavidsonian structure DRS and corresponding DRG in tuples and in graph format for A customer did not pay From DRS to DRG labelling Wordaligned DRG for A customer did not pay All alignment information including surface tuples is highlighted Surface composition of embedded structures Wordaligned DRG for the sentence Michelle thinks that Obama smokes Analysis of NP coordination in a distributive left and a collective interpretation right Average ratings and Pearson correlation for rules from the personal stories corpus Lower ratings are better see Fig 2 Instructions for judging of unsharpened factoids Average Precision Recall and F1 at dif ferent top K rule cutoff points Distribution of reasons for false negatives missed argument mentions by BInc at K20 Distribution of reasons for false positives incorrect argument extractions by BInc at K20 Rule type distribution of a sample of 200 rules that extracted incorrect mentions The corre sponding numbers of incorrect mentions extracted by the sampled rules is shown in parentheses The Maytag interface Translation extraction from comparable corpora using crosslingual WSI and WSD Entries from the EnglishSlovene sense cluster inventory Disambiguation results Comparison of different configurations Results of the experiment GC examples Devtest Set Statistics by Language Fmeasure Breakdown by Mention Type NAMe NOMinal PREmodifier and PROnoun Chinese data does not have the PRE type Impact of Syntactic Features on English Sys tem After Taking out Distance Features Numbers are Fmeasures An example where syntactic features help to link the PRO mention hm with its antecedent the NAM Distribution of Pronoun Mentions and Fre quency of ccommand Features Summary Results on the 2004 ACE Evaluation Data A Portion of the Syntactic Tree Ungrammatical Arabic output of Google Trans late for the English input The car goes quickly The subject should agree with the verb in both gender and number but the verb has masculine inflection For clarity the Arabic tokens are arranged lefttoright Segmentation and tagging of the Arabic token AEJ JJ and they will write it This token has four seg ments with conflicting grammatical features For example the number feature is singular for the pronominal object and plural for the verb Our model segments the raw to ken tags each segment with a morphosyntactic class eg PronFemSg and then scores the class sequences Notation used in this paper The convention eIi indicates a subsequence of a length I sequence Breadthfirst beam search algorithm of Och and Ney 2004 Typically a hypothesis stack H is maintained for each unique source coverage set Procedure for scoring agreement for each hy pothesis generated during the search algorithm of Fig 4 In the extended hypothesis eI1 the index n 1 indicates the start of the new attachment Intrinsic evaluation accuracy development set for Arabic segmentation and tagging Translation quality results BLEU4 for newswire nw sets Avg is the weighted averaged by number of sentences of the individual test set gains All improvements are statistically significant at p 001 An example of NE and nonNE Possibility combination of neighboring tokens within the corpus for PER A NE detection window 2 All possible NEs identified in a test article c Unification When the tokens of two NEs are NEs after agentsbased modification Results of MET2 under different configurations Features based on the token string Sources of Dictionaries Comparison of results for MUC6 Training Data Fmeasure after successive addition of each global feature group Priority Order for Second Person ADs Priority Order for Third Person ADs Priority Order for First Person ADs Basic Features for CRFbased Segmenter Performance of our system in the compe tition Effectiveness of postprocessing rules Overview of the WEBRE algorithm Illustrated with examples sampled from experiment results The tables and rec tangles with a database sign show knowledge sources shaded rectangles show the 2 phases and the dotted shapes show the sys tem output a set of Type A relations and a set of Type B relations The orange arrows denote resources used in phase 1 and the green arrows show the resources used in phase 2 Pairwise precisionrecallF1 of WEBRE and SNE Graphical model representing M L SLDA Shaded nodes represent observations plates denote repli cation and lines show probabilistic dependencies Two methods for constructing multilingual distributions over words On the left paths to the German word wunsch in GermaNet are shown On the right paths to the English word room are shown Both English and German words are shown some internal nodes in GermaNet have been omitted for space represented by dashed lines Note that different senses are denoted by different internal paths and that internal paths are distinct from the perlanguage expression Topics along with associated regression coefficient from a learned 25topic model on GermanEnglish left and GermanChinese right documents Notice that themerelated topics have regression parameter near zero topics discussing the number of pages have negative regression parameters topics with good great hao good and uberzeugt convinced have positive regression parameters For the GermanChinese corpus note the presence of gut good in one of the negative sentiment topics showing the difficulty of learning collocations Overall results by Vieira and Poesio Discoursenew prediction results by Bean and Riloff Evaluation of the three anaphoric resolvers discussed by Ng and Cardie Results of Uryupinas discourse new clas sifier Results of Uryupinas uniqueness classifier Using an oracle Evaluation of the GUITAR system without DN detection off raw text Evaluation of the GUITAR system without DN detection over a handannotated treebank Relation types and subtypes in the ACE training data Contribution of different features over 43 relation subtypes in the test data Distribution of errors Comparison of our system with other bestreported systems on the ACE corpus Performance of different relation types and major subtypes in the test data Distribution of relations over words and other mentions in between in the training data Feature templates for POS tagging wi is the ith word in the sentence ti is its POS tag For a word w cj w is its j th character cj w is the last j th character and lw is its length Parse tree binarization Unary rule normalization Nonterminalyield unary chains are collapsed to single unary rules Identity unary rules are added to spans that have no unary rule Feature templates for parsing where X can be word first and last character of word first and last character bigram of word POS tag Xla Xra denotes the firstlast ath X in the span while Xla Xra denotes the ath X leftright to span Xm is the first X of right child and Xm1 is the last X of the left child len lenl lenr denote the length of the span left child and right child respectively wl is the length of word ROOTLEAF means the template can only generate the features for the rootinitial span Complexity Analysis of Algorithm 1 Boundary information is added to states to cal culate the bracket scores in the face of word segmentation errors Left the original parse tree Right the converted parse tree The numbers in the brackets are the indices of the character boundaries based on word segmentation Training development and test data of CTB 5 Results for the joint word segmentation and POS tagging task Word segmentation results Parameters used in our system Results for the joint segmentation tagging and parsing task using pipeline and joint models POS tagging error patterns means the error number of the corresponding pattern made by the pipeline tagging model and mean the error number reduced or increased by the joint model Summary of the results obtained by our algorithm in comparison to Word 2007 Results of the fillintheblank exercise Results of the inflection exercise Solution of the multiple choice exercise Replication of the experiment with a corpus of nonnative speakers CEDEL2 Lozano 2009 The number of OAS types CAS types LUW types and EIW types for our CWS The differences of Fmeasure and ROOV between nearby steps of our CWS The scored results of our CWS in the MSRC track OOV is 0034 for 3rd bakeoff The Fmeasure improvement between the BMMbased CWS and it with WSM in the MSRC track OOV is 0034 using a b and c system dictionary Results of the baseline model best guess Word distribution in the extended Cilin Results of the baseline model best 5 guesses Results of combining the charactercategory association and rulebased models best guess Results of the charactercategory association model best guess Results of the charactercategory association model best 5 guesses Results of the rulebased model best guess Results of the corpusbased model on words with different frequency Results of the corpusbased model Parameter settings of the corpusbased model Results of the combined model for classify ing unknown words into major and medium catego ries best guess Performance of the statistical approach using a trigram model based on Google Web1T Influence of the ngram model on the perfor mance of the statistical approach Performance of the knowledgebased ap proach using the JiangConrath semantic relatedness measure Performance of knowledgebased approach using different relatedness measures Results obtained by a combination of the best statistical and knowledgebased configuration Best Single is the best precision or recall obtained by a sin gle measure Union merges the detections of both approaches Intersection only detects an error if both methods agree on a detection Reordering for the German verbgroup DP algorithm for statistical machine translation Coverage set hypothesis extensions for the IBM reordering Multireference word error rate mWER Example Translations for the Verbmobil task Statistics about the results of our word sense discovery algorithm Average precision of discovered senses for English in relation with WordNet Senses found by our algorithm from first order cooccurrences LM1 and LAT1 Examples of learned pronoun probabilities EM input for our example sentence jvalues follow each lexical candidate Comparison to SVM Weights set by maximum entropy Accuracy for all cases all excluding sen tences with quotes and only sentences with quotes Examples of nonphonetic translations Dissimilarity of temporal distributions of WTO in English and Chinese corpora Framework overview Evidence cardinality in the corpora Network of relations Edges indicate that the relations have a nonempty support inter section and edge labels show the size of the inter section Relation clusters and a few individual relations Edge labels show the size of the inter section MRR with decreasing comparability Example translations from the different methods Boldface indicates correct translations Evaluation results of the methods Example of similar document pairs Automatically generated training set examples Impact of scaling techinques ILP ILPscale microaverage F1 and AUC for the algorithms Precisionrecall curve for the algorithms A pair of comparable nonparallel documents A pair of comparable sentences A Parallel Fragment Extraction System Translated fragments according to the lexicon Our approach for detecting parallel fragments The lower part of the figure shows the source and target sentence together with their alignment Above are displayed the initial signal and the filtered signal The circles indicate which fragments of the target sentence are selected by the procedure Sizes of the extracted datasets Sizes of our comparable corpora SMT performance results Language families in our data set The Other category includes 9 language isolates and 21 language family singletons Average accuracy for EM baseline and model variants across 503 languages First panel results on all languages Second panel results for 30 isolate and singleton languages Third panel results for 27 nonLatin alphabet languages Cyril lic and Greek Standard Deviations across lan guages are about 2 Plurality language families across 20 clusters The columns indicate portion of lan guages in the plurality family number of lan guages and entropy over families Inferred Dirichlet transition hyperparameters for bigram CLUST on threeway classification task with four latent clusters Row gives starting state column gives target state Size of red blobs are proportional to magnitude of corresponding hyperparameters Incremental evaluations by incrementally adding new features word features and high dimensional edge features new word detection and ADF training replacing SGD training with ADF training Number of passes is decided by empirical convergence of the training methods Fscore curves on the MSR CU and PKU datasets ADF learning vs SGD and LBFGS training methods Comparing our method with the stateoftheart CWS systems Test corpora details Evaluation closed results on all data sets Evaluation open results on all test sets Comparison our closed results with the top three in all test sets Architecture of the translation approach based on Bayes decision rule Some training events for the English word which The symbol is the placeholder of the English word which in the English context In the German part the placeholder corresponds to the word aligned to which in the first example the German word die the word das in the second and the word was in the third The considered English and German contexts are separated by the double bar p The last number in the rightmost position is the number of occurrences of the event in the whole corpus Meaning of different feature categories where s represents a specific target word and t repre sents a specific source word The 10 most important features and their respective category and values for the English word which f Number of features used according to different cutoff threshold In the second column of the table are shown the number of features used when only the English context is considered The third column correspond to English German and WordClasses contexts Corpus characteristics for translation task Training and Test perplexities us ing different contextual information and different thresholds The reference perplexities obtained with the basic translation model 5 are TrainPP 1038 and TestPP 1322 Corpus characteristics for perplexity quality experiments Preliminary translation results for the Verbmobil Test147 for different contextual infor mation and different thresholds using the top10 translations The baseline translation results for model 4 are WER5480 and PER4307 Four examples showing the translation obtained with the Model 4 and the ME model for a given German source sentence Monolingual and Crosslingual Baseline Slot Filling Pipelines Baseline Pipeline Results Distribution of Spurious Errors Validation Features for Crosslingual Slot Filling Using Basic Features to Filter Answers Fact vs Statistical CrossDoc Features The count of the types of anaphora per corpus Bibliome system scores at Bacteria Biotope Task in BioNLP shared tasks 2011 Etrees and Derivation Trees for 2abc Data examined by the two systems for the ATB A correct tree tree1 and an incorrect tree tree2 for BCLM HNEIM indexed by terminal boundaries Erroneous nodes in the parse hypothesis are marked in italics Missing nodes from the hypothesis are marked in bold The morphological segmentation possibilities of BCLM HNEIM Doublecircles are word boundaries Architecture of the translation approach based on Bayes decision rule Candidates for equivalence classes Corpus statistics Verbmobil training Singletons are types occurring only once in train ing Statistics of the Verbmobil test corpus for GermantoEnglish translation Unknowns are word forms not contained in the training corpus Effect of the introduction of equivalence classes For the baseline we used the original in flected word forms Examples for the effect of the combined lexica Effect of twolevel lexicon combination For the baseline we used the conventional onelevel full form lexicon Examples for the effect of equivalence classes resulting from dropping morphosyntactic tags not relevant for translation First the translation using the original representation then the new representation its reduced form and the resulting translation The incompleteness of Freebase are must have attributes for a person Plate diagram of our model False negative matches on the Riedel Riedel et al 2010 and KBP dataset Surdeanu et al 2012 All numbers are on bag pairs of entities level BD are the numbers before downsampling the negative set to 10 and 5 in Riedel and KBP dataset respectively Performance on the KBP dataset The figures on the left middle and right show MIML Hoffmann and Mintz compared to the same MIMLSemi curve respectively MIMLSemi is shown in red curves lighter curves in black and white while other algorithms are shown in black curves darker curves in black and white Coarse overview From multilingual in put to typed relations and instances Some examples for MEDLINE tagset Number of lex entries per tag and sample words Quasimorphological operations Translation results for EnglishGerman Translation results for GermanEnglish Translation results for FrenchEnglish Translation results for EnglishFrench BLEU scores of English to Russian ma chine translation system evaluated on tst2012 us ing baseline GIZA alignment and translitera tion augmentedGIZA OOVTI presents the score of the system trained using TAGIZA af ter transliterating OOVs BLEU scores of English to Russian ma chine translation system evaluated on tst2012 and tst2013 using baseline GIZA alignment and transliteration augmentedGIZA alignment and postprocessed the output by transliterating OOVs Human evaluation in WMT13 is performed on TAGIZA tested on tst2013 marked with Russian to English machine translation system evaluated on tst2012 and tst2013 Human evaluation in WMT13 is performed on the system trained using the original corpus with TAGIZA for alignment marked with TreeTagger and RFTagger outputs Starred word forms are modified during preprocessing Sequence of units built from surface word forms top and POStags bottom Results for FrenchEnglish Results for GermanEnglish Example dependency tree Example coreferent paths Italicized entities generally corefer Example noncoreferent paths Italicized entities do not generally corefer Gender classification performance Example gendernumber probability ANC pronoun resolution accuracy for varying SVMthresholds Resolution accuracy Dependency parse tree for the sentence in the ACE corpus Toujan Faisal 54 said she was informed of the refusal by an Interior Min istry committee overseeing election preparations Undersampled system for the task of rela tion detection The proportion of positive examples in the training and test corpus is 500 and 206 respectively System for the task of relation classifica tion The two classes are INR and COG and we evaluate using accuracy Acc The proportion of INR relations in training and test set is 497 and 4963 respectively A Dictionary based Word Graph Synthetic Data Set from Xinhua News Quantitative Evaluation of Common Topic Finding crosscollection loglikelihood Effectiveness of Extracting Common Topics Qualitative Evaluation Effectiveness of Latent Topic Extraction from MultiLanguage Corpus Selfbootstrapping algorithm Clusteringbased stratified seed sampling Performance of various clusteringbased seed sampling strategies on the heldout test data with the optimal cluster number for each clustering algorithm Performance in F1score over different cluster numbers with intrastratum sampling on the develop ment data Misspellings of receive Classification of corpus token by type Contextsensitive spelling correction denotes also using 60 WSJ 5 corrupted Memorybased learner results Synonyms for chain Synonyms for home Average I NV R for 300 headwords InvR scores ranked by difference Giga word to Web Corpus Step 4 Step 2 Step 5 Step 3 50document corpora averages LO Dice configuration scores Best LO and LL configurations scores Best MAP in Experiment 1 LO sentence configuration scores LO cosine sentence configuration scores Average rank of correct translation according to average source term frequency LO cosine sentence configuration scores Feature set for our pronoun resolution systemed feature is only for the singlecandidate model while ed feature is only for the twincandidate mode The performance of different resolution systems Results of different feature groups under the TC model for Npron resolution Relationship types and their argument type con straints Count of relationships in 77 gold standard documents The relationship extraction system Feature sets used for learning relationships The size of a set is the number of features in that set Variation in performance by feature set Features sets are abbreviated as in Table 3 For the first seven columns features were added cumulatively to each other The next two columns allgen and notok are as de scribed in Table 3 The final two columns give inter annotator agreement and corrected inter annotator agreement for comparison Variation in performance by number of sentence boundaries n and by training corpus size Mechanical evaluation of translation JapanesetoEnglish Display of NICT ATR SpeechtoSpeech Translation System Evaluation of speech recognition Human Evaluation of translation A fragment of an entailment graph a its SCC graph b and its reduced graph c Nodes are predicates with typed variables see Section 5 which are omitted in b and c for compactness Three types of transitivity constraint violations Runtime in seconds for various values Computation of probabilities using the language model During training a classified instance in this case for the confusible pair then than are generated from a sentence During testing a similar instance is generated The classifier decides what the corresponding class and hence which word should be the focus word This table shows the performance achieved by the different systems shown in accuracy The Number of cases denotes the number of instances in the testset The chunking results for the six systems associated with the project shared task CoNLL 2000 The baseline results have been obtained by selecting the most frequent chunk tag associ ated with each partofspeech tag The best results at CoNLL2000 were obtained by Support Vector Machines A majority vote of the six LCG sys tems does not perform much worse than this best result A majority vote of the five best systems outperforms the best result slightly error re duction The NP chunking results for six sys tems associated with the project The baseline results have been obtained by selecting the most frequent chunk tag associated with each partof speech tag The best results for this task have been obtained with a combination of seven learn ers five of which were operated by project mem bers The combination of these five performances is not far off these best results The results for three systems associ ated with the project for the NP bracketing task the shared task at CoNLL99 The baseline re sults have been obtained by finding NP chunks in the text with an algorithm which selects the most frequent chunk tag associated with each partof speech tag The best results at CoNLL99 was obtained with a bottomup memorybased learner An improved version of that system MBL deliv ered the best project result The MDL results have been obtained on a different data set and therefore combination of the three systems was not feasible Predictive power of admissible and almost admissible heuristic functions Training corpus statistics without punctuation marks Test corpora statistics Average search time s per sentence Search Success Rate 1 million hypothe ses Search errors Effect of observation pruning on the translation quality average over all test sets A E Success Rate for 12 and 14word sentences Translation quality Proposed method data flow Feature set used in the Stage 2 classifier and their number for the causal relation experiments Precision of acquired relations causality L and S denote lenient and strict evaluation Precision of acquired relations prevention L and S denote lenient and strict evaluation Precision of acquired relations material L and S denote lenient and strict evaluation Frequencies of patterns in the evaluation data causation Contribution of feature sets material Contribution of feature sets prevention Contribution of feature sets causality Stemmed results on 3138utterance test set Asterisked results are significantly better than the baseline p 005 using 1000 iterations of paired bootstrap resampling Koehn 2004 Rank trajectories of 4 LDA inferred topics with incremental topic inference The xaxis indicates the utterance number The yaxis indicates a topics rank at each utterance Example of the context of in Eat fruits and the context of in Play basketball An example for the BMES representa tion The sentence is I love Bei jing Tiananmen square which consists of 4 Chi nese words I love Beijing and Tiananmen square Algorithm description Comparison of fscores when changing the size of labeled data 110 14 12 and all labeled data The size of unlabeled data is xed as 5 million characters Details of the PKU data Details of the unlabeled data Comparison of our approach with the stateofart systems Comparison of our approach with using only the Gigaword corpus Derivational entropy of Gq and cross entropies for three different corpora A Sample Network MUC7 Level Distribution of the Facts Combined MUC7 Level Distribution of Each of the Facts MUC7 Level Distribution of Each of the Facts MUC7 Level Distribution of Each of the Facts MUC7 Level Distribution of Each of the Facts MUC4 Level Distribution of the Five Facts Combined MUC4 Level Distribution of Each of the Five Facts MUC5 Level Distribution of the Five Facts Combined MUC5 Level Distribution of Each of the Five Facts MUC6 Level Distribution of Each of the Six Facts Domain Numbers of MUC4 MUC5 MUC6 and MUC7 MUC6 Level Distribution of the Six Facts Combined GermanEnglish translation results Results are cumulative EnglishGerman translation results results are cumulative except for the three alternative PAL configurations Examples of parallel phrases used in word alignment Summary of devtest results and shared task test results for submitted systems and LIU baseline with hier archical reordering A Motivating Example Processed Data Statistics Sample Is Values Human Assessment of Errors Slot Value Translation Assessment from Ran dom Sample of 1000 Performance of Unsupervised Name Mining Example of Learned Name Pairs with Gloss Translations in Parentheses Name Pairs Mined Using Previous Methods Types of message speech acts in corpus Categories of Message Speech Act Thread length distribution Frequency of speech acts Gold standard length distribution Sample poster scores System Performance Comparison SA strength scores Example queries for abbreviation BSA Properties of abbreviations corpus retrieved from Medline Performance of WSD system using various combinations of learning algorithms and features Performance of WSD system over individual ab breviations in three reduced corpora Average word accuracy for transduced sentences Fraction of the sentences that were transduced Sizes of the automata Time consumption of transduction confirms that names participating in re Baseline Word Clustering by Relation Reranking by Coreference Reranking by Relation Baseline Word Clustering by Relation Reranking by Coreference System Flow Plate diagram representation of the model ti s wi s and si s denote the tags words and segmentations respectively Gs are various DPs in the model Ej s and j s are the tagspecific emission distributions and their respective Dirichlet prior parameters H is Gamma base distribution S is the base distribution over segments Coupled DP concetrations parameters have been omitted for clarity Loglikelihood of samples plotted against iter ations Dark lines show the average over five runs grey lines in the back show the real samples Tagging part of loglikelihood plotted against Vmeasure Segmentation results on different languages Results are calculated based on word types For each language we report precision recall and F1 measure number of word types in the corpus and number of word types with gold standard segmentation available For each language we report the segmentation result without and with emission likelihood scaling without LLS and with LLS respectively Tagging results for different languages For each language we report median onetoone 11 manytoone m1 and Vmeasure Vm together with standard deviation from five runs where median is taken over Vmeasure Types is the number of word types in each corpus True is the number of gold tags and Induced reports the median number of tags induced by the model together with standard deviation Best Pub lists the best published results so far also 11 m1 and Vm in Christodoulopoulos et al 2011 Blunsom and Cohn 2011 and Lee et al 2010 Tagging and segmentation results on Estonian MultextEast corpus Learned seg and Learned tag com pared to the semisupervised setting where segmentations are fixed to gold standard Fixed seg and tags are fixed to gold standard Fixed tag Finally the segmentatation results from Morfessor system for comparison are pre sented Ten relation instances extracted by our system that did not appear in Freebase The 23 largest Freebase relations we use with their size and an instance of each relation Dependency parse with dependency path from Edwin Hubble to Marshfield highlighted in boldface Features for Astronomer Edwin Hubble was born in Marshfield Missouri Examples of highweight features for several relations Key SYN syntactic feature LEX lexical feature x reversed NE named entity tag of entity Automatic evaluation with 50 of Freebase relation data held out and 50 used in training on the 102 largest relations we use Precision for three different feature sets lexical features syntactic features and both is reported at recall levels from 10 to 100000 At the 100000 recall level we classify most of the instances into three relations 60 as locationcontains 13 as personplaceofbirth and 10 as personnationality Estimated precision on humanevaluation experiments of the highestranked 100 and 1000 results per relation using stratified samples Average gives the mean precision of the 10 relations Key Syn syntactic features only Lex lexical features only We use stratified samples because of the overabundance of locationcontains instances among our highconfidence results Linking FrameNet frames and VerbNet classes Results of the mapping algorithm Mapping algorithm refining step F1 and accuracy of the argument classifiers and the overall multiclassifier for FrameNet semantic roles Semantic Role learning curve Examples of pseudo features Sources of the training data Number of candidates for each target language Examples of the top3 candidates in the transliteration of English Chinese Examples of the top3 candidates in the transliteration of EnglishKorean Number of evaluated English Name MRRs of the phonetic transliteration Size of the test data MRRs for the phonetic transliteration 2 MRRs of the phonetic transliteration Gibbs sampling algorithm for IBM Model 1 im plemented in the accompanying software Sizes of bilingual dictionaries induced by differ ent alignment methods Distribution of inferred alignment fertilities The four blocks of rows from top to bottom correspond to in order the total number of source tokens source tokens with fertilities in the range 47 source tokens with fertil ities higher than 7 and the maximum observed fertility The first language listed is the source in alignment Sec tion 2 BLEU scores in translation experiments E En glish T Turkish C Czech A Arabic Dependency tree for the sentence PROT1 contains a sequence motif binds to PROT2 Comparison with other PPI extraction systems in the AIMed corpus Comparison of contributions of different features to relation detection across multiple domains Comparison of performance across the five PPI corpora NIL expression forms based on POS attribute NIL expression forms based on word formation Workflow for NIL knowledge engineering component NILE refers to NIL expression which is identified and annotated by human annotator Architecture of NILER system Smoothed precision curves over the five corpora Experimental results for the two methods on the five corpora PRE denotes precision REC denotes recall and F1 denotes F1Measure Smoothed recall curves over the five corpora Smoothed quality curves for SVM method over the five corpora Smoothed quality curves for PM method over the five corpora Smoothed F1Measure curves over the five corpora MEDLDA Mixed Membership MEDLDA Relation types for ACE 05 corpus Overall performance of the 3 systems Multiclass Classification Results with PlusCOMP for SVM LLDA and MEDLDA for the six ACE 05 categories and NOREL LLDA Fmeausres for 3 feature conditions SVM Fmeausres for 3 feature conditions MEDLDA Fmeausres for 3 feature conditions Fmeasures for every kernel in Khayyamian et al 2009 and MEDLDA Features used by paraphrase classifier Illustration of features f812 Bidirectional checking of entailment relation of p1 p2 and p2 p1 p1 is reduces bone mass in s1 and p2 is decreases the quantity of bone in s2 p1 and p2 are exchanged between s1 and s2 to generate corresponding paraphrased sentences s01 and s02 p1 p2 p2 p1 is verified if s1 s01 s2 s02 holds In this case both of them hold English is used for ease of explanation Number of extracted paraphrases Examples of correct and incorrect paraphrases extracted by our supervised method with their rank Precision curves of paraphrase extraction Top7 Chinese longform candidates for the En glish acronym TAA according to the LH score The performances of the transliteration models and their comparison on EMatch The BLEU score of selftrained h4 translitera tion models under four selection strategies nt n15 stands for the nth iteration The BLEU score of selftrained cascaded trans lation model under five initial training sets Maximally Accurate Assignment Numeric Assignment Experimental Results Word prediction from a partial parse Dependency structure of a sentence A conceptual gure of the lexicalization Corpus Relation between cross entropy and pars Cross entorpy and accuracy of each model Number of feedback items per speaker Distribution of isolated vs initial posi tion for the most frequent lexical items Duration in seconds of each lexical type Distribution of the lexical items Dendrogram of the participants cluster based on their feedback profile 25 noun lexicographer files in W ORD N ET Example nouns and their supersenses 2 billion word corpus statistics Grammatical relations from S EXTANT Handcoded rules for supersense guessing Breakdown of results by supersense Summary of supersense tagging accuracies The best two performing systems of each type according to finegrained recall in Senseval2 and 3 Polysemous word types in the Senseval2 and 3 English allwords tasks test documents with no data in SemCor 0 columns or with very little data 1 and 5 occurrences Note that there are no annotations for adverbs in the Senseval3 documents Words excluding multiwords in WordNet 171 and the BNC without any data in SemCor Most frequent sense analysis for Senseval2 and 3 polysemous lemmas occurring more than once in a document adverb data is only from Senseval2 Most frequent sense analysis for all polysemous lemmas in the Senseval2 and 3 test data broken down by their frequencies of occurrence in SemCor adverb data is only from Senseval2 The prevalence ranking process for the noun star Example dss and sss scores for star and its neighbors Grammatical contexts used for acquiring the BNC thesaurus Thesaurus coverage of polysemous words excluding multiwords in WordNet 16 Evaluation on SemCor polysemous words only Simplified prevalence score evaluation on SemCor polysemous words only Results of the error analysis for the sample of 80 words SemCor results for Nouns using jcn Evaluating predominant sense information for polysemous nouns on the Senseval2 allwords task data Senseval2 results polysemous nouns only broken down by their frequencies of occurrence in SemCor TYPE precision on finding the predominant sense for the Senseval2 English allwords test data for nouns having a frequency less than or equal to various thresholds WSD precision on the Senseval2 English allwords test data for nouns having a frequency less than or equal to various thresholds Most frequent SFC labels for all senses of polysemous words in WordNet by part of speech Distribution of domain labels of predominant senses for 38 polysemous nouns ranked using the SPORTS and FINANCE corpora Distribution of domain labels of predominant senses for 20 polysemous verbs ranked using the SPORTS and FINANCE corpora WSD using predominant senses training and testing on all domain combinations handclassified corpora WSD using predominant senses training and testing on all domain combinations automatically classified corpora Example of a word with internal structure Example of telescopic compound a and sepa rable word b Structure of the outofvocabulary word English People Two words that differ only in one character but have different internal structures The character people is part of a personal name in tree a but is a suffix in b An example word which has very complex structures The actual output of our parser trained with a fully annotated treebank Proposed output for the new Chinese word seg mentation paradigm Difference between our output a of parsing the word olive oil and the output b of Luo 2003 In c we have a true flat word namely the loca tion name Los Angeles Example word structure annotation We add an f to the POS tags of words with no further structures Example of parser error Tree a is correct and b is the wrong result by our parser Labeled precision and recall for the three types of labels The line labeled Flat is for unlabeled met rics of flat words which is effectively the ordinary word segmentation accuracy BLEU scores achieved with different sets of parallel corpora All systems are base line ncode with POS factor models The follow ing shorthands are used to denote corpora N stands for NewsCommentary E for Europarl C for CommonCrawl U for UN and nf for non filtered corpora BLEU scores for different configuration of factored translation models The big prefix de notes experiments with the larger context for n gram translation models BLEU scores for preordering experi ments with a ncode system and the approach pro posed by Neubig et al 2012 Histogram of token movement size ver sus its occurrences performed by the model Neu big on the source english data BLEU scores for the FrenchtoEnglish translation task measured on nt10 with systems tuned on development sets selected according to their original language adapted tuning Perplexity measured on nt08 with the baseline LM std with the LM estimated on the sampled texts generated texts and with the inter polation of both Impact of the use of sampled texts Results for development and test set for the two languages by ME1 Best results For English name lists are used For German partofspeech tags are used List of keywords used in WordNet search for generating WN CLASS features Distribution of SCs in the ACE corpus SC classification accuracies of different methods for the ACE training set and test set Results for feature ablation experiments Accuracies of singlefeature classifiers Resolution accuracies for the ACE test set Coreference results obtained via the MUC scoring program for the ACE test set Metaevaluation results at document level Metaevaluation results at system level Metaevaluation results at document and system level for submitted metrics 1 Statistics of relation types and subtypes in the training data of the ACE RDC 2003 corpus Note According to frequency all the subtypes are divided into three bins large middle small with 400 as the lower threshold for the large bin and 200 as the upper threshold for the small bin Learning curve of the hierarchical strategy and its comparison with the flat strategy for some major relation subtypes Note FS for the flat strategy and HS for the hierarchical strategy Comparison of the hierarchical and flat learning strategies on the relation subtypes of differ ent training data sizes Notes the figures in the parentheses indicate the cosine similarities between the weight vectors of the linear discriminative functions learned using the two strategies Comparison of our system with other bestreported systems The density of the F1 scores with the three approaches The prior used is a symmetric Dirichlet with 01 BLEU scores when testing on the com bined test set newstest2012 PDTB 23 on PDTB section 23 only 2416 sentences 923 con nectives and when randomizing the sense tags PDTB 23 random for the BASELINE system and the two systems using PDTB connective labels SYSTEM 1 complex labels SYSTEM 2 simplified labels When testing on randomized sense labels PDTB 23 random the BLEU scores are statisti cally significantly lower than the ones on the cor rectly labeled test set PDTB 23 which is indi cated by starred values Performance of SYSTEM 2 simplified PDTB tags when manually counting for improved equal and degraded translations compared to the BASELINE in samples from the PDTB section 23 test set Translation outputs for the EN con nective as which was translated more correctly by SYSTEM 2 thanks to the disambiguating sense tags compared to the BASELINE that often just produces the prepositional as jako The erro neous translations are marked in bold The PDTB sense tags indicate the meaning of the CZ trans lations and are encoded as follows Synchrony Sy Asynchrony Asy Contingency Co Cause Ca The combined sequence and parse tree representation of the relation instance leader of a minority government The nonessential nodes for a and for minority are removed based on the algorithm from Qian et al 2008 Examples of similar syntactic structures across different relation types The head words of the first and the second arguments are shown in italic and bold respectively Examples of unigram and bigram features extracted from Figure 1 Comparison of different methods on ACE 2004 data set P R and F stand for precision recall and F1 respectively The average performance of TLcomb with different T k 104 and 1 Performance of TLcomb and TLauto as H changes Performance of TLNE BL and BLA as the number of seed instances S of the target type increases H 500 T was set to 104 and 102 Average F1 using different hypothesized typespecific features Size of cooccurrence databases WSI and WSD Pipeline Results for the submitted runs General architecture of LINGUA Success rate of anaphora resolution Complexity of the evaluation data Summary of LINGUA performance Parallel Corpus Baseline Results Bayesian Alignment Results Development Sets Results Compound Splitting Results Tuning Results Language Model Results GermantoEnglish Final System Results GermanEnglish Official Test Submis sion English to German Final System Re sults Average of Weights Results Incremental Improvement from Selftraining English Bootstrapping for Name Tagging SelfTraining for Name Tagging Data Description English Name Tagger Chinese Name Tagger Impact of Data Size Chinese Impact of Confidence Measures Impact of Data Selection Chinese Impact of Data Size English An example of words and their bit string representations obtained in this paper Words in bold are head words that appeared in Table 1 Lexical features for relation extraction Cluster features ordered by importance Performance comparison on the ACE 2004 data over the 7 relation types Performance 12 of the baseline and using different cluster features with PC4 over the 7 types Performance of each individual relation type based on 5fold crossvalidation Performance over the 7 relation types with different sizes of training data Prefix10 uses the single prefix length 10 to generate word clusters as used by Chan and Roth 2010 The F1Measure value is shown for every kernel on each ACE2005 main relation type For every relation type the best result is shown in bold font Example of DIRT algorithm output Most confident paraphrases of X put emphasis on Y Example of inference rules needed in RTE Lexical variations creating new rules based on DIRT rule X face threat of Y X at risk of Y Dependency structure of text Tree skeleton in bold Precision on full RTE data Coverageprecision with various rule collections Precision on the covered RTE data Error analysis Frequency of Relation SubTypes in the ACE training and devtest corpus The Performance of SVM and LP algorithm with different sizes of labeled data for relation detection on relation subtypes The LP algorithm is run with two similarity measures cosine similarity and JS divergence The performance of SVM and LP algorithm with different sizes of labeled data for relation detection and classification on relation subtypes The LP algorithm is run with two similarity measures cosine similarity and JS divergence Comparison of the performance of previous methods on ACE RDC task Comparison of the performance of the bootstrapped SVM method from Zhang 2004 and LP method with 100 seed labeled examples for relation type classification task Growing Algorithm for Language Model Pruning Language Model Pruning Algorithm Calculation of Importance of Bigrams Stepbystep Growing Algorithm Comparison of Number of Bigrams at FMeasure 9633 Performance Comparison of Different Pruning Methods Performance Comparison of Combined Model and KLD Model Correlation between Perplexity Perplexity Comparison of Different Pruning Methods Translation results for EnglishFrench Translation results for FrenchEnglish Translation results for GermanEnglish Translation results for EnglishGerman Gibbs Sampling Statistical Information of Corpora Experimental Procedure Features Used for Initial Distribution Ordered List of IncreasedDecreased Number of Correctly Tagged Words Results of Multiple Trials and Compari son to Simulated Annealing Results of POS Guessing of Unknown Words Overview of the method Evaluation results within sets Evaluation results for links Extracted NE pair instances and context Figure 3 High TFITF words in ComCom Numbers are TFITF score frequency in the collec tion TF frequency in the corpus TF and word Examples and number of them in Semcor for sense approach and for class approach BLC for WN16 using all or hyponym relations Average polysemy on SE2 and SE3 Results for nouns Learning curve of BLC20 on SE3 Learning curve of BLC20 on SE2 Results for verbs Learning curve of SuperSense on SE3 Learning curve of SuperSense on SE2 Representation of Bigram Counts Experimental Results Decision Tree and Stump Characteristics Architecture of Nameaware Machine Translation System Translation Performance Statistics and Name Distribution of Test Data Sets A u to m a tic M e tr ic s H u m a n E v a lu a tio n Figure 2 Scores based on Automatic Metrics and Human Evaluation Impact of Joint Bilingual Name Tagging on Word Alignment name tokensall tokens Figure 3 Word alignment gains according to the percentage of name words in each sentence feature templates accuracy using nonaveraged and averaged perceptron learning curves of the averaged and non averaged perceptron algorithms the influence of agenda size the accuracies over the second SIGHAN bakeoff data the accuracies over the first SIGHAN bake off data the influence of features F Fmeasure Feature numbers are from Table 1 Different representations of a relation instance in the example sentence provide bene five different tree kernel setups on the ACE 2003 five major types using the parse tree structure information only regardless of any entityrelated information Performance comparison on the ACE 2004 data over both 7 major types the numbers outside parentheses and 23 subtypes the num bers in parentheses Performance comparison on the ACE 20032003 data over both 5 major types the numbers outside parentheses and 24 subtypes the numbers in parentheses Error distribution of major types on both the 2003 and 2004 data for the compos ite kernel by polynomial expansion Nouns and verbs supersense labels and short description from the Wordnet documentation The noun box in Wordnet each line lists one synset the set of synonyms a definition an optional example sentence and the supersense label Statistics of the datasets The row Super senses lists the number of instances of supersense labels partitioned in the following two rows between verb and noun supersense labels The lowest four rows summarize average polysemy figures at the synset and supersense level for both nouns and verbs Summary of results for random and first sense baselines and supersense tagger is the standard error computed on the five trials results Summary of results of baseline and tagger on selected subsets of labels NER categories evaluated on Semcor upper section and 5 most frequent verb middle and noun bottom categories evaluated on Senseval Context Clustering with Spectralbased Clustering technique Frequency of Major Relation SubTypes in the ACE training and devtest corpus Performance of our proposed method Spectral based clustering compared with other unsupervised methods Hasegawa et al 2004s clustering method and Kmeans clustering Different Context Window Size Setting Example of a MUC4 template The topranking feature for each group of features and the classifier of a slot Accuracy of string slots with and without full parsing Systems whose Fmeasures are not signif icantly different from AliceME at the 010 signifi cance level with 099 confidence Accuracy of all slots on the TST3 and TST4 test set Accuracy of string slots on the TST3 and TST4 test set Layers used in our model A standard logical form derivation using CCG The NP notation means that the subject is typeraised and taking the verbphrase as an argumentso is an ab breviation of SSNP This is necessary in part to sup port a correct semantics for quantifiers Example initial lexical entries Using the type model for disambiguation in the derivation of file a suit Type distributions are shown after the variable declarations Both suit and the object of file are lexically ambiguous between different types but after the reduction only one interpretation is likely If the verb were wear a different interpretation would be preferred Most probable terms in some clusters induced by the Type Model Results on widecoverage Question Answer ing task CCGDistributional ranks questionanswer pairs by confidence250 means we evaluate the top 250 of these It is not possible to give a recall figure as the total number of correct answers in the corpus is unknown Accuracy on Section 1 of the FraCaS suite Problems are divided into those with one premise sen tence 44 and those with multiple premises 30 Example problem from the FraCaS suite Example questions correctly answered by CCGDistributional Comparison with other approaches F1measures with in 0 3 F1measure with in 01 Example sentence and extracted features from the SENSEVAL 2 word church Empiricallyderived classifier similarity Individual Classifier Properties crossvalidation on SENSEVAL training data Accuracy for different EMweighted probability interpolation models for SENSEVAL 2 Training set characteristics Individual feature type contribution to perfor mance Fields marked with indicate that the difference in performance was not statistically significant at a level paired McNemar test Classifier combination accuracy over 5 base classifiers NB BR TBL DL MMVC Best perform ing methods are shown in bold Individual basic classifiers contribution to the final classifier combination performance Final Performance Frozen Systems on SENSEVAL Lexical Sample WSD Test Data Example of semantic trees Two STs composing a STN ROUGE2 measures in kmeans learning ROUGEW in empirical approach ROUGEW measures in kmeans learning ROUGEW measures in EM learning ROUGESU in kmeans learning Fmeasures for different systems ROUGESU in empirical approach ROUGE2 in empirical approach ROUGE2 measures in EM learning ROUGESU measures in EM learning System architecture overview Procedure to mine key lexicons for each semantic type Some key lexicons and verbs for two semantic types Salience grading for candidate antecedents Procedure to find semantic types for antecedent candidates Statistics of anaphor and antecedent pairs Feature impact experiments FScore of Medstract and 100Medlines Comparisons among different strategies on Medstract Impacts of the mined semantic lexicons and the use of PubMed CATiB Annotation example 23 1 4 0 t ml HfydAt AlkA AlkyAt fy AlmdArs AlHkwmy The writers smart granddaughters work for public schools The words in the tree are presented in the Arabic reading direction from right to left Penn Arabic Treebank part 3 v31 data split Parsing performance with each POS tag set on gold and predicted input L AS labeled attachment accuracy dependency relation U AS unlabeled attachment accuracy dependency only L S relation label prediction accuracy L AS diff difference between labeled attachment accuracy on gold and predicted input POS acc POS tag prediction accuracy Prediction accuracy value set sizes descriptions and value examples of features used in this work Accuracy was measured over the development set The set includes a NA values CORE 12 POS tag set with morphological inflectional features Left half Using gold POS tag and feature values In it Top part All Adding all nine inflectional features to CORE 12 Second part Sep Adding each feature separately to CORE 12 Third part Greedy Greedily adding next best feature from Sep and keeping it if improving score Right half Same as left half but with predicted POS tag and feature values Statistical significance tested only on predicted nongold input against the CORE 12 baseline Models with lexical morphosemantic features Top Adding all lexical features together on top of the CORE 12 baseline Center Adding each feature separately Bottom Greedily adding best features from previous part on predicted input Statistical significance tested only on predicted nongold input against the CORE 12 baseline Models with inflectional and lexical morphological features together predicted valueguided heuristic Statistical significance tested only on predicted input against the CORE 12 baseline Models with reengineered DET and PERSON inflectional features Statistical significance tested only on predicted input against the CORE 12 baseline Models with functional features GENDER NUMBER rationality RAT F N functional features based on Alkuhlani and Habash 2011 GN GENDER NUMBER GNR GENDER NUMBER RAT Statistical significance tested only for CORE 12 models on predicted input against the CORE 12 baseline Select models trained using the EasyFirst Parser Statistical significance tested only for CORE 12 models on predicted input significance of the EasyFirst Parser CORE 12 baseline model against its MaltParser counterpart and significance of all other CORE 12 models against the EasyFirst Parser CORE 12 baseline model Alternatives to training on goldonly feature values Top Select MaltParser CORE 12 models retrained on predicted or gold predicted feature values Bottom Similar models to the top half with the EasyFirst Parser Statistical significance tested only for CORE 12 models on predicted input significance of the MaltParser models from the MaltParser CORE 12 baseline model and significance of the EasyFirst Parser models from the EasyFirst Parser CORE 12 baseline 3 7 19 6 7 Figure 2 Error analysis example 82 mrt yAm l AxtfA Alzmyl Almhnd Several days have passed since the disappearance of the colleague the engineer as parsed by the baseline system using only CORE 12 left and as using the best performing model right Bad predictions are marked with The words in the tree are presented in the Arabic reading direction from right to left Training the MaltParser on gold tags accuracy by gold attachment type selected subject object modification of a verb or a noun by a noun modification of a verb or a noun by a preposition idafa and overall results repeated Training the EasyFirst Parser on gold and predicted tags accuracy by gold attachment type selected subject object modification of a verb or a noun by a noun modification of a verb or a noun by a preposition idafa and overall results repeated A corpus of two trees A derivation for Mary likes Susan Another derivation yielding same tree All binary trees for NNS VBD JJ NNS Investors suffered heavy losses Some subtrees from trees in figure 4 Fscores of UMLDOP compared to previous models on the same data Fscores of UDOP UMLDOP and a supervised treebank PCFG MLPCFG for a random 9010 split of WSJ10 and WSJ40 Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt STTS accuracies of the TnT tagger trained on the STTS tagset the TnT tagger trained on the Tiger tagset and our tagger trained on the Tiger tagset Tagging accuracy on development data depending on context size Tagging accuracies on test data Context Clustering with Spectralbased Clustering technique Frequency of Major Relation SubTypes in the ACE training and devtest corpus Different Context Window Size Setting Comparison of the existing efforts on ACE RDC task The ANNIS user interface displaying data from the PCC Training set statistics OutOfVocabulary OOV rate is regarding the development sets Example training run of a pruned 1st order model on German showing the fraction of pruned gold se quences sentences during training for training train and development sets dev POS tagging experiments with pruned and unpruned CRFs with different orders n For every language the training time in minutes TT and the POS accuracy ACC are given indicates models significantly better than CRF first line Accuracies for models with and without oracle pruning indicates models significantly worse than the oracle model Test results for POSMORPH tagging Best baseline results are underlined and the overall best results bold indicates a significant difference between the best baseline and a PCRF model Development results for POSMORPH tagging Given are training times in minutes TT and accuracies ACC Best baseline results are underlined and the overall best results bold indicates a significant difference between the best baseline and a PCRF model Test results for POS tagging Best baseline results are underlined and the overall best results bold indicates a significant difference between the best baseline and a PCRF model Development results for POS tagging Given are training times in minutes TT and accuracies ACC Best baseline results are underlined and the overall best results bold indicates a significant difference positive or negative between the best baseline and a PCRF model Statistics on the Italian EVALITA 2009 and English CoNLL 2003 corpora Three kinds of tree kernels Semantic structure of the first sequence Global features in the entity kernel for reranking These features are anchored for each entity instance and adapted to entity categories For example the entity string first feature of the entity United Nations with entity type ORG is ORG United Nations Reranking results of the three tagging kernels on the Italian and English testset Regular expression notation in foma Illustration of a worsening filter for morpheme boundaries OT grammar for devoicing compiled into an FST Violation permutation transducer Devoicing transducer compiled through a rule Example outputs of matching implementation of Finnish OT An nonregular OT approximation Illustrative tableau for a simple constraint sys tem not capturable as a regular relation Baseline results for human word lists Data 700 positive and 700 negative reviews Results for baseline using introspection and simple statistics of the data including test data Average threefold crossvalidation accuracies in percent Boldface best performance for a given setting row Recall that our baseline results ranged from 50 to 69 Outline of word segmentation process Three different vocabulary sizes used in subword based tagging s1 contains all the characters s2 and s3 contains some common words Corpus statistics in Sighan Bakeoff 2005 Segmentation results of dictionarybased segmentation in closed test of Bakeoff 2005 A separates the results of unigram bigram and trigram Segmentation results by the pure subwordbased IOB tagging The separator divides the results by three lexicon sizes as illustrated in Table 3 The first is characterbased s1 while the other two are subwordbased with different lexicons s2s3 Riv and Roov varing as the confidence threshold t Effects of combination using the confidence measure Here we used 08 and confidence threshold t 07 The separator divides the results of s1 s2 and s3 Effects of using CRF The separator divides the results of s1 and s3 List of results in Sighan Bakeoff 2005 NPs in a sample from the Catalan training data left and the English translation right Evolution of A means relative to the length of the nbest sequence The MCPG algorithm Comparison of paraphrase generators Top the MOSES baseline middle and bold the truescore MCPG down the translator MCPG The use of truescore improves the MCPG per formances MCPG reaches MOSES performance level Verb classes see Section 31 their Levin class numbers and the number of experimental verbs in each see Section 32 Experimental Results C50 is supervised accuracy Base is on random clusters set Ling is manually selected subset Seed is seedverbselected set See text for further description Feature counts for Ling and Seed feature sets Event descriptions spread across two sentences Counts of matches between MUC and Soderland data Matches between MUC and Soderland data at field level Relation extraction results on the JDPA Corpus test set broken down by document source Selected document statistics for three JDPA Corpus document sources Size of Seed Lexicons Performance on Bilingual Lexicon Extraction Seeds with the Highest Weight Translation Candidates for manic depression BLEU scores on the NewsCommentary development test data BLEU scores on the Europarl development test data Examples of templates suggested by DIRT and TEASE as having an entailment relation in some direction with the input template X change Y The entailment direction arrows were judged manually and added for readability Rule evaluation examples and their judgment Average Precision P and Yield Y at the rule and template levels Examples for disagreement between the two judges Characteristics of the parallel corpus used for experiments The role of the standard Basque Batua ana lyzer in filtering out unwanted output candidates created by the induced rule set produced by method 1 Values obtained for Precision Recall and F scores with method 1 by changing the minimum fre quency of the correspondences to construct rules for foma The rest of the options are the same in all three experiments only one rule is applied within a word Values obtained for Precision Recall and F score with method 1 by changing the threshold frequency of the correspondences and applying a postfilter Experiments with the ILP method using a thresh old of 14 times a wordpair is seen to trigger rule learn ing The figures in parentheses are the same results with the added postprocessing unigram filter that given sev eral output candidates of the standard dialect chooses the most frequent one Method 1 Exp1 frequency 2 2 rules applied in parallel without contextual conditioning Exp2 fre quency 1 1 rule applied with contextual conditioning Exp3 frequency 2 2 rules applied in parallel with con textual conditioning Tradeoffs of precision and recall values in the experiments with method 1 using various different pa rameters When the unigram filter is applied the precision is much better but the recall drops The best results per F1 score of the two meth ods The parameters of method 1 included using only those string transformations that occur at least 2 times in the training data and limiting rule application to a maxi mum of 2 times within a word and including a unigram postfilter Rules were contextually conditioned For method 2 all the examples threshold 1 in the training data were used as positive and negative evidence with out a unigram filter Entity type constraints BasicRE gives the performance of our basic RE system on predicting finegrained relations obtained by performing 5fold cross validation on only the news wire corpus of ACE2004 Each sub sequent row Hier HierrelEntC Coref Wiki and Cluster gives the individual contribution from using each knowledge The bottom row ALL gives the performance improvements from adding HierrelEntCCorefWikiCluster indicates no change in score Corpus Excerpt with Dialogue Act Annotation Figure 2 Automatically detected posture points H headDepth M midTorsoDepth L lowerTorsoDepth reports the average classification accuracies from the fivefold cross validation The majority baseline accuracy for our data is 347 when the classifier always chooses the most frequent dialog act A The first group of rows in Table 3 report the accuracies of individual feature classes All of the individual features performed better than the baseline The improvement from the baseline was significant except for D with CRF The most powerful feature class was dialogue context class when the full set was used The second group in Table 3 shows the effects of incrementally combining the feature classes Adding dialogue act features to the lexical features L D brought significant improvement in the classification accuracy for ME and CRF Adding posture features L D T P also improved the accuracy of ME by a statistically significant margin The last group shows similar results for ME when the previous tutor dialogue act was excluded from the dialogue context except that the improvement achieved by adding the posture features L D T P was not significant Tuple extraction from a sentence pair Translation results in terms of BLEU score and translation edit rate TER estimated on newstest2010 with the NIST scoring script EnglishFrench translation results in terms of BLEU score and TER estimated on newstest2010 with the NIST script All means that the translation model is trained on newscommentary Europarl and the whole GigaWord The rows upper quartile and median corre spond to the use of a filtered version of the GigaWord Architecture of the Structured Output Layer Neural Network language model Translation results from English to French and English to German measured on newstest2010 using a 100best rescoring with SOUL LMs of different orders CCG derivation and unresolved semantics for the sentence I saw nothing suspicious DRS for the sentence I saw nothing suspi cious Results of the second run with postprocessing Results of the first run without postprocessing Results of negated eventproperty detection on gold standard cue and scope annotation The System architecture1 Learning curves using different sam pling strategies Performance comparsion of the rule based robust semantic parser the reversed two stage classification system and our SLU systems TER Topic Error Rate SER Slot Error Rate Performance comparison of two SLU systems through weakly supervised and super vised training on the three test sets TER Topic Error Rate SER Slot Error Rate Learning curves of bootstrapping meth ods for semantic classification on TS1 A general architecture for paraphrasing approaches leveraging the distributional similarity hypothesis Two different dependency tree paths a and b that are considered paraphrastic because the same words John and problem are used to ll the corresponding slots shown coindexed in both the paths The implied meaning of each dependency path is also shown Using Chinese translations as the distributional elements to extract a set of English paraphrastic patterns from a large English corpus A bootstrapping algorithm to extract phrasal paraphrase pairs from monolingual parallel corpora The merging algorithm a How the merging algorithm works for two simple parse trees to produce a shared forest Note that for clarity not all constituents are expanded fully Leaf nodes with two entries represent paraphrases b The word lattice generated by linearizing the forest in a A paraphrase generation lattice for the sentence He ate lunch at a cafe near Paris Alternate paths between various nodes represent phrasal replacements The probability values associated with each edge are not shown for the sake of clarity An example showing the generalization of the word lattice a into a slotted lattice b The word lattice is produced by aligning seven sentences Nodes having indegrees 1 occur in more than one sentence Nodes with thick incoming edges occur in all sentences Extracting consistent bilingual phrasal correspondences from the shown sentence pairs i1 j1 i2 j2 denotes the correspondence fi1 fj1 ei2 ej2 Not all extracted correspondences are shown An example of syntactically motivated paraphrastic patterns that can be extracted from the paraphrase corpus constructed by Cohn CallisonBurch and Lapata 2008 Test verbs and their monosemouspolysemic gold standard senses Connected components nearest neighbour NN clustering D is the KullbackLeibler distance Information Bottleneck IB iterative clustering D is the KullbackLeibler distance Clustering performance on the predominant senses with and without prepositions The last entry presents the per formance of random clustering with K 25 which yielded the best results among the three values K25 35 and 42 The fraction of verb pairs clustered together as a function of the number of shared senses results of the NN algo rithm Evaluation against the monosemous Pred and pol ysemous Multiple gold standards The figures in parentheses are results of evaluation on randomly polysemous data sig nificance of the actual figure Results were obtained with fine grained SCFs including prepositions The fraction of verb pairs clustered together as a function of the number of different senses between pair mem bers results of the NN algorithm Addition method Evaluation of the manual annotation improvement summarization ratio 30 Evaluation of the manual annotation improvement summarization ratio 15 Evaluation of the GUITAR improvement summarization ratio 30 Evaluation of the GUITAR improvement summarization ratio 15 Performance of our system versus a baseline The greedy binding problem a The correct binding b the greedy binding c the result The Lattice of the 8 Patterns Segmentation accuracy of different seg menters Results on three query categories MAP of different IR systems with differ ent segmenters The feature set for coreference resolution Nonrelational features describe a mention and in most cases take on a value of YES or NO Relational features describe the relationship between the two mentions and indicate whether they are COMPATIBLE INCOMPATIBLE or NOT APPLICABLE Statistics for the ACE 2005 corpus MUC CEAF and B3 coreference results using system mentions MUC CEAF and B3 coreference results using true mentions Two characteristic topics for the Y slot of acquire along with their topicbiased Lin sim ilarities scores Lint compared with the original Lin similarity for two rules The relevance of each topic to different arguments of acquire is illus trated by showing the top 5 words in the argument y vector vacquire for which the illustrated topic is the most likely one Contextsensitive similarity scores in bold for the Y slots of four rule applications The components of the score calculation are shown for the topics of Table 1 For each rule application the table shows a couple of the topicbiased scores Lint of the rule as in Table 1 along with the topic relevance for the given context ptdv w which weighs the topicbiased scores in the LinW T cal culation The contextinsensitive Lin score is shown for comparison MAP values on corresponding test set ob tained by each method Figures in parentheses in dicate optimal number of LDA topics Sizes of rule application test set for each learned ruleset MAP results for the two split Lin test sets Proposed discourse structures for Ex 4 a In terms of informational relations b in terms of inten tional relations Stages of the proposed method Graph of words for the target word paper Numbers inside vertices correspond to their degree Running example of graph creation Two dendrograms for the graph in Figure 3 Sensetagged corpus for the example in Figure 3 A current configuration for internal node Dk and its associated subtrees B first alternative configuration C second alternative configuration Note that swapping st1 st2 in A results in an equivalent tree Hence this configuration is excluded Parameter values used in the evaluation Performance analysis of HRGs CWU CWW HAC for different parameter combinations Table 2 A All combinations of p1 p2 and p3 005 B All combinations of p1 p2 and p3 009 Performance of HRGs and HAC for different parameter combinations Table 2 All combinations of p1 p2 and p3 013 HRGs against recent methods baselines The Buckwalter Arabic Morphological Analyzers lookup process exemplified for the word lilkitAbi Skeleton of basic lexicon transducer in LEXC generated from BAMA lexicons How the IBM models model the translation process This is a hypothetical example and not taken from any actual training or decoding logs Runtimes for sentences of length 1080 The graph shows the average runtimes of 10 different sample sentences of the respective length with swap op erations restricted to a maximum swap segment size of 5 and a maximum swap distance of 2 A decoding trace using improvement caching and tiling ICT The search in the second and later iterations is limited to areas where a change has been applied marked in bold print note that the number of alignment checked goes down over time The higher number of alignments checked in the second iteration is due to the insertion of an additional word which increases the number of possible swap and insertion operations Decoding without ICT results in the same translation but requires 11 iterations and checks a total of 17701 alignments as opposed to 5 iterations with a total of 4464 alignments with caching Number of search iterations left and total number of alignments considered right during search in depen dence of input length The data is taken from the translation of the Chinese testset from the TIDES MT evaluation in June 2002 Translations were performed with a maximum swap distance of 2 and a maximum swap segment size of 5 BLEUscores for the Chinese test set de coding in dependence of maximum swap distance and maximum swap segment size Decoder performance on the June 2002 TIDES MT evluation test set with multiple searches from randomized starting points MSD2 MSSS5 sentence length Figure 6 Time consumption of the various change types in Parsing tree FDG Analysers output example Evaluation results from DSOWSJ Overlaid bilingual embeddings English words are plotted in yellow boxes and Chinese words in green reference translations to English are provided in boxes with green borders directly below the original word Vector Matching Alignment AER lower is bet ter Results on Chinese Semantic Similarity Results on Named Entity Recognition NIST08 ChineseEnglish translation BLEU PATTree Instantiation for Figure 1 In the extraction process the PATtree is Results for feature combination Results for value setting Results for initial ranking manner Figure 5 Results for webpage snippet number 73 Experiment on Multiple Feature Fusion To verify the effectiveness for multiple feature fusion the test on the feature combination for OOV term translation is implemented As shown in Table 1 the highest accuracy the percentage of the correct translations in all the extracted translations of 831367 can be ac OOV term translation examples Results for EnglishChinese CLIR com bining our OOV term translation model Results for 4fold sitewise crossvalidation us ing the DP corpus DP corpus comparison for OPUS features based on frequent vs domainrelevant verbs Results on the Bitter Lemons corpus Architecture of the translation approach based on Bayes decision rule Wordtoword alignment Example of a word alignment and of ex tracted alignment templates Bilingual training corpus recognition lex icon and translation lexicon PM punctuation mark Illustration of search in statistical trans lation Illustration of bottomtotop search Comparison of three statistical translation approaches test on text input 251 sentences 2197 words 430 punctuation marks Sentence error rates of endtoend evalua tion speech recognizer with WER25 corpus of 5069 and 4136 dialogue turns for translation Ger man to English and English to German respec tively Disambiguation examples using morphosyntactic analysis Accuracy of our system in each period M 10 Precision and recall for different values of Rank of correct translation for period Dec 01 Dec 15 and Dec 16 Dec 31 Cont rank is the context rank Trans Rank is the transliteration rank NA means the word cannot be transliterated insuff means the correct translation appears less than 10 times in the English part of the comparable corpus comm means the correct translation is a word ap pearing in the dictionary we used or is a stop word phrase means the correct translation contains multi ple English words Dataset Statistics Pronouns as Opinion Targets Results of AR for Opinion Targets Op Target Op Word Pair Extraction Graphical representation of our model Hyper parameters the stickiness factor and the frame and event initial and transition distributions are not shown for clar ity A partial frame learned by P RO F INDER from the MUC4 data set with the most probable emissions for each event and slot Labels are assigned by the authors for readability Results on MUC4 entity extraction CJ 2011 granularity refers to their experiment in which they mapped one of their templates to five learned clusters rather than one Results on TAC 2010 entity extraction with N best mapping for N 1 and N 5 Intermediate values of N produce intermediate results and are not shown for brevity Syntagmatic vs paradigmatic axes for words in a simple sentence Chandler 2007 Summary of results in terms of the MTO and VM scores Standard errors are given in parentheses when available Starred entries have been reported in the review paper Christodoulopoulos et al 2010 Distributional models use only the identity of the target word and its context The models on the right incorporate orthographic and morphological features MTO is not sensitive to the number of partitions used to discretize the substitute vector space within our experimental range MTO falls sharply for less than 10 SCODE dimensions but more than 25 do not help MTO is fairly stable as long as the Z constant 54 Morphological and orthographic features is within an order of magnitude of the real Z value MTO is not sensitive to the number of random substitutes sampled per word token Hinton diagram comparing most frequent tags and clusters An example sequence representation The subgraph on the left represents a bigram feature The subgraph on the right represents a unigram feature that states the entity type of arg 2 An example dependency parse tree rep resentation The subgraph represents a dependency relation feature between arg 1 Palestinians and of Comparison among the three feature sub spaces and the effect of including larger features State classification by minimum input consumed for the Finnish dictionary The sizes of error models as automata The sizes of dictionaries as automata Effect of language and error models to quality recall proportion of suggestion sets containing a cor rectly suggested word Effect of language and error models to speed time in seconds per 10000 word forms Effect of text type on error models to speed in seconds per 10000 wordforms CTB 10fold CV word segmentation F measure for our word segmenter Comparison of word segmentation F measure for SIGHAN bakeoff3 tasks POS tagging accuracy using oneata time wordbased POS tagger POS tagging accuracy using oneata time characterbased POS tagger Summary table on the various methods investigated for POS tagging CTB 10fold CV word segmentation F measure using an allatonce approach CTB 10fold CV POS tagging accuracy using an allatonce approach Some of the words extracted from the small corpus Experiments on the thresholdprecision relationship of the small corpus Experiments on the word lengthprecision relationship of the small corpus Experiments on the thresholdpartial recall relationship of the small corpus Some words extracted from the large corpus Experiments on the thresholdprecision relationship of the large corpus Experiments on the thresholdpartial recall relationship of the large corpus Experiments on the word lengthprecision relationship of the large corpus with threshold nine Experiments on the word lengthprecision relationship of the large corpus with threshold three Numerictype compounds extracted Precision and partial recall of word lengths two to seven of the second experiment on IT and AV Precision and partial recall of word lengths two to four of the first experiment on IT and AV Semantic expansion example Note that the expanded queries that were generated in the first two retrieved texts listed under matched query do not contain the original query Examples for correct templates that were learned by TEASE for input templates Analysis of the number of relevant documents out of the top 10 and the total number of retrieved documents up to 100 for a sample of queries Translation example Experimental results using different smoothing methods Experimental results using different phrase ta bles OutBp the outofdomain phrase table AdapBp the adapted phrase table Effect of indomain monolingual corpus size on translation quality CORE 12 with inflectional features predicted input Top Adding all nine features to CORE 12 Second part Adding each feature separately comparing difference from CORE 12 Third part Greedily adding best features from second part Feature prediction accuracy and set sizes The set includes a NA value Lexical features Top part Adding each feature separately difference from CORE 12 predicted Bottom part Greedily adding best features from previous part Functional features gender number rationality Inflectionallexical features together Extended inflectional features Results on unseen test set for models which performed best on dev set predicted input Results obtained by applying different types of features in isolation to the Baseline system Results obtained by adding different types of features incrementally to the Baseline system Examples errors introduced by YAGO and FrameNet Contingency table for the children of canine in the subject position of run Contingency table for the children of liquid in the object position of drink Example levels of generalization for different values of Extent of generalization for different values of and sample sizes Results for the pseudodisambiguation task Results for the pseudodisambiguation task with onefifth training data Disambiguation results for G2 and X2 Features used in baseline system Accuracy of 5fold crossvalidation with sta tisticsbased semantic features Accuracy of 5fold crossvalidation with self extracted semantic features based on different levels of syntacticsemantic relations Accuracy of 5fold crossvalidation with self extracted semantic features Architecture of the statistical translation approach based on Bayes decision rule Regular alignment example for the translation direction German to English For each German source word there is exactly one English target word on the alignment path Illustration of the transitions in the regular and in the inverted alignment model The regular alignment model left figure is used to generate the sentence from left to right the inverted alignment model right figure is used to generate the sentence from bottom to top DPbased algorithm for solving travelingsalesman problems due to Held and Karp The outermost loop is over the cardinality of subsets of already visited cities Illustration of the algorithm by Held and Karp for a traveling salesman problem with J 5 cities Not all permutations of cities have to be evaluated explicitly For a given subset of cities the order in which the cities have been visited can be ignored DPbased algorithm for statistical MT that consecutively processes subsets C of source sentence positions of increasing cardinality Word reordering for the translation direction German to English The reordering is restricted to the German verb group Order in which the German source positions are covered for the GermantoEnglish reordering example given in Figure 5 Word reordering for the translation direction English to German The reordering is restricted to the English verb group Order in which the English source positions are covered for the EnglishtoGerman reordering example given in Figure 7 Procedural description to compute the set Succ of successor hypotheses by which to extend a partial hypothesis S C j Illustration of the IBMstyle reordering constraint Number of processed arcs for the pseudotranslation task as a function of the input sentence length J yaxis is given in log scale The complexity for the four different reordering constraints MON GE EG and S3 is given The complexity of the S3 constraint is close to J4 Twolist implementation of a DPbased search algorithm for statistical MT Training and test conditions for the GermantoEnglish Verbmobil corpus number of words without punctuation Example translations for the translation direction German to English using three different reordering constraints MON GE and S3 Translation results for the translation direction English to German on the TEST331 test set The results are given in terms of computing time WER and PER for three different reordering constraints MON EG and S3 Demonstration of the combination of the two pruning thresholds tC 50 and tc 125 to speed up the search process for the two reordering constraints GE and S3 no 50 The translation performance is shown in terms of mWER on the TEST331 test set Training and test conditions for the Hansards task number of words without punctuation Example translations for the translation direction English to German using three different reordering constraints MON EG and S3 Example translations for the translation direction French to English using the S3 reordering constraint Outline of the segmentation process Scores for UPUC corpus Scores for MSRA corpus Scores for CityU corpus An example graph modeling relations between mentions Number of clustering decisions made ac cording to mention type rows anaphor columns antecedent and percentage of wrong decisions Results of different systems on the CoNLL12 English data sets Number of recall errors according to mention type rows anaphor columns antecedent Precision statistics for pronouns Rows are pronoun surfaces columns number of cluster ing decisions and percentage of wrong decisions for all and only anaphoric pronouns respectively Removal and reduction of constituents using dependencies Different setups for entityrelated se mantic tree EST Contribution of constituent dependen cies in respective mode inside parentheses and accumulative mode outside parentheses Improvements of different tree setups Translation of PCC sample commentary Screenshot of Annis Linguistic Database POS Tagging of Unknown Word using Contextual and Lexical features in a Sequential Model The input for capitalized classifier has 2 values and therefore 2 ways to create confusion k sets There are at most F different in puts for the suffix classifier 26 character 10 digits 5 other symbols therefore suffix may k emit up to R R confusion sets POS tagging of unknown words using contextual and lexical Features accuracy in per cent is based only on contextual features T is based on contextual and lexical features SM 2 denotes that follows in the sequential model 2 POS tagging of unknown words using contextual features accuracy in percent is a classifier that uses only contextual features baseline is the same classifier with the addition of the baseline feature NNP or NN Processing time for POS tagging of known words using contextual features In CPU seconds Train training time over sentences Brills learner was interrupted after 12 days of train ing default threshold was used Test average number of seconds to evaluate a single sentence All runs were done on the same machine POS Tagging of known words using con textual features accuracy in percent onevsall denotes training where example serves as positive example to the true tag and as negative example to all the other tags SM R denotes training where 2 example serves as positive example to the true tag Combinatorial Search Problems in Decoding Average decoding time P r f ae for IBM Models Log score NIST scores 5fold crossvalidation results on training data Results obtained on the official test set of the 2011 DDI Extraction challenge LII filtering refers to the techniques proposed in Chowdhury and Lavelli 2012b for reducing skewness in RE data distribution stat sig in dicates that the improvement of Fscore due to usage of Stage 1 classifier is statistically significant verified using Approximate Randomization Procedure Noreen 1989 number of iterations 1000 confidence level 001 Examples of phrase meaningfulness Note that the comments are not presented to Turkers Examples given in the description of Task 2 Plate diagram depicting the joint model Hyper parameters have been omitted for clarity The Lshaped plate contains the tokens while the square plates contain the morphological analyses The t are latent tags zi is an assignment to a morphological analysis lk sk fk and wi is the observed word T is the number of distinct tags and Kt the number of tables used by tag type t Plate diagram depicting the morphology model adapted from Goldwater et al 2006 Hyperparameters have been omitted for clarity The lefthand plate depicts the base distribution P0 note that the morphological anal yses lk are generated deterministically as tk sk fk The observed words wi are also deterministic given zi k and lk since wi sk fk The posterior distribution of our joint model Because the sequence of words w is deterministic given analyses l and assignments to analyses tables z the joint posterior over all variables Pw t l zt a b s f is equal to Pt l zt a b s f when lzi wi for all i and 0 otherwise We give equations for the nonzero case ns refer to token counts ms to table counts We add two dummy tokens at the start end and between sentences to pad the context history Example sentences in the synthetic languages Words in Category 1 are made of characters ad Cate gory 2 eh Category 3 mp Category 4 ru Suffixes in Language B are separated with periods for illustrative purposes only Log probability of the sampler state over 1000 iterations on Languages A and B Spanish Ornat corpus results Standard devia tions are in parentheses denotes a significant difference from the M ORTAG model English Eve corpus results Standard deviations are in parentheses denotes a significant difference from the M ORTAG model Left An entailment graph For clarity edges that can be inferred by transitivity are omitted Right A hierarchical summary of propositions involving nausea as an argument such as headache is related to nausea acupuncture helps with nausea and Lorazepam treats nausea Results for all experiments Subgraph of Local1 output forheadache Subgraph of tunedLP output for headache Comparing disagreements between the best local and global algorithms against the gold standard Parse Feature Example for the sentence GM says the addition of OnStar which includes a system that automatically notifies an OnStar operator if the vehicle is involved in a collision complements the Vues top fivestar safety rating for the driver and front passenger in both front and sideimpact crash tests Training Data Sizes for Common ESL Confused Words Spelling correction accuracy impact of combining word cooccurrence CLASSIFIER Logistic Regression trained on 1G words of news text tested on 9months NYT data COMBINED SYSTEM CLASSIFER plus system based on firstorder word cooccurrence Relative increase or decrease in error rate compared to CLASSIFIER As in Bergsma et al 2009 2010 no morphological variants of the words are used in evaluation Spelling correction precision impact of adding parse features SVM trained on 1G words of news text tested on 9months of NYT data Improvement of NGLEXPAR vs NGLEX is statistically significant Improvement of NGLEXPAR vs NG is statistically significant Relative increase or decrease of error rate compared to NGLEX As in Bergsma et al 2009 2010 no morphological variants of the words are used in evaluation The semantic representations of a word W its inverse W inv and its negation W The domain part of the representation remains un changed while the value part will partially be in verted inverse or inverted and scaled negation with 0 1 The separate functional repre sentation also remains unchanged A partially scaled and inverted identity matrix J Such a matrix can be used to trans form a vector storing a domain and value repre sentation into one containing the same domain but a partially inverted value such as W and W de scribed in Figure 1 The parse tree for This car is not blue highlighting the limited scope of the negation Relation between number of classes and alternations Estimation of model parameters Estimation of Fc f v and Fv c Ten most frequent classes using equal distribution of verb frequencies Estimation of Fv c for the verb feed Ten most frequent classes using unequal distribution of verb frequencies Smoothed estimates Model accuracy using equal distribution of verb frequencies for the estimation of Pc Model accuracy using unequal distribution of verb frequencies for the estimation of Pc Model accuracy using unequal distribution of verb frequencies for the estimation of Pc Model accuracy using equal distribution of verb frequencies for the estimation of Pc Semantic preferences for verbs with the doubleobject frame Features for collocations Word sense disambiguation accuracy for NP1 V NP2 to NP3 frame Word sense disambiguation accuracy for NP1 V NP2 frame Word sense disambiguation accuracy for NP1 V NP2 NP3 frame Word sense disambiguation accuracy for NP1 V NP2 for NP3 frame Word sense disambiguation accuracy for NP1 V NP2 NP3 frame Word sense disambiguation accuracy for NP1 V NP2 frame Word sense disambiguation accuracy for NP1 V to NP2 NP3 frame Word sense disambiguation accuracy for NP1 V for NP2 NP3 frame Examples of context free and contextsensitive sub trees related with Figure 1b Note the bold node is the root for a subtree Different tree span categories with SPT dotted circle and an ex ample of the dynamic contextsensitive tree span solid circle Evaluation of contextsensitive convolution tree kernels using SPT on the major relation types of the ACE RDC 2003 inside the parentheses and 2004 outside the parentheses corpora Comparison of dynamic contextsensitive tree span with SPT using our contextsensitive convolution tree kernel on the major relation types of the ACE RDC 2003 inside the parentheses and 2004 outside the parentheses corpora 18 of positive instances in the ACE RDC 2003 test data belong to the predicatelinked category Comparison of difference systems on the ACE RDC 2004 corpus over both 7 types outside the parentheses and 23 subtypes inside the parentheses Comparison of difference systems on the performs the stateoftheart Collins and Duffys con ACE RDC 2003 corpus over both 5 types outside the volution tree kernel It also shows that featurebased parentheses and 24 subtypes inside the parentheses Possible Relations between ARG1 and ARG2 Evaluation on Structure Features Evaluation of Correction and Inference Mechanisms Evaluation of Feature and Their Combinations Evaluation of Two Detection and Classification Modes Imbalance Training Class Problem IBM Model 3 Linguistic levels as feature sets Semantic features Accuracy results for binary decisions Accuracy results for combined decisions Comparison of accuracy scores across linguistic levels Levels all and morph against the Gold Standard Results for ensemble classifier Example of BLC selection Most frequent monosemic words in BG Most frequent BLC20 semantic classes on WordNet 30 Number of training examples Results of task17 The B I E S Tag Set Example of Lattice Used in the Markov ModelBased Method Example of the Character Tagging Method Word boundaries are indicated by vertical lines Example of the Hybrid Method Character Types Calculated Values of i Statistical Information of Corpora Performance of Japanese Word Segmentation Performance of Chinese Word Segmentation Case restoration performance using an MDtrie English Improvement in fscore through restoring case Accuracy on seen and unseen tokens Recognition performance Final results for English and German develop ment and test sets GETARUNS AR algorithm Expletive it compared results GETARUNS pronouns collapsed at structural level Overall results CoverageAccuracy The regular expressions available in Foma from highest to lower precedence Horizontal lines separate precedence classes A relative comparison of running a se lection of regular expressions and scripts against other finitestate toolkits The first and second en tries are short regular expressions that exhibit ex ponential behavior The second results in a FSM with 221 states and 222 arcs The others are scripts that can be run on both XeroxPARC and Foma The file lexiconlex is a LEXC format English dic tionary with 38418 entries North Sami is a large lexicon lexc file for the North Sami language available from httpdivvunno Coreference Definition Differences for MUC and ACE GPE refers to geopolitical entities Dataset characteristics including the number of documents annotated CEs coreference chains annotated CEs per chain average and number of documents in the traintest split We use st to indicate a standard traintest split Impact of Three Subtasks on Coreference Resolution Performance A score marked with a indicates that a 05 threshold was used because threshold selection from the training data resulted in an extreme version of the system ie one that places all CEs into a single coreference chain Correlations of resolution class scores with respect to the average Frequencies and scores for each resolution class Predicted P vs Observed O scores Four ambiguous words their senses and frequency Mutual information between feature subset and class label with f req based feature ranking Mutual information between feature subset and class label with 2 based feature ranking Average accuracy over three procedures in Figure 1 as a function of context window size horizontal axis for 4 datasets Results for three procedures over 4 datases The horizontal axis corresponds to the context window size Solid line represents the result of F SGM M binary dashed line denotes the result of CGDSV D idf and dotted line is the result of CGDterm idf Square marker denotes 2 based feature ranking while cross marker denotes f req based feature ranking Average accuracy of three procedures with various settings over 4 datasets Automatically determined mixture component num Discourse tree for two sentences in RSTDT Each of the sentences contains three EDUs The second sentence has a wellformed discourse tree but the first sentence does not have one Discourse parsing framework Distributions of six most frequent relations in intrasentential and multisentential parsing scenarios A chainstructured DCRF as our intra sentential parsing model Our parsing model applied to the sequences at different levels of a sentencelevel DT a Only possible se quence at the first level b Three possible sequences at the second level c Three possible sequences at the third level A CRF as a multisentential parsing model Features used in our parsing models Extracting subtrees for S2 Two possible DTs for three sentences Confusion matrix for relation labels on the RSTDT test set Yaxis represents true and Xaxis repre sents predicted relations The relations are TopicChange TC TopicComment TCM Textual Organization T O MannerMeans MM Comparison CMP Evaluation EV Summary SU Condition CND Enablement EN Cause CA Temporal TE Explanation EX Background BA Contrast CO Joint JO SameUnit SU Attribu tion AT and Elaboration EL Parsing results of different models using manual gold segmentation Performances significantly superior to HILDA with p71e05 are denoted by Significant differences between TSP 11 and TSP SW with p001 are denoted by RST Spanish Treebank statistics Rhetorical relations in RST Spanish Treebank Example of the nonrelation SameUnit Interannotator agreement Topic transfer in bilingual LSA model DirichletTree prior of depth two Parallel topics extracted by the bLSA model Top words on the Chinese side are translated into English for illustration purpose English word perplexity PPL on the RT04 test set using a unigram LM Comparison of training log likelihood of English LSA models bootstrapped from a Chinese LSA and from a flat monolingual English LSA Word perplexity with different using manual reference or ASR hypotheses on CCTV BLEU score for those 25 utterances which resulted in different translations after bLSA adaptation manual transcriptions Translation performance of baseline and bLSAAdapted ChineseEnglish SMT systems on manual transcriptions and 1best ASR hypotheses Examples of new alternations New Verb Classes Average results for 35 verbs The clausal and topological field structure of a German sentence Notice that the subordinate clause receives its own topology a An example of a document from TuBaDZ b an abbreviated entity grid representation of it and c the feature vector representation of the abbreviated entity grid for transitions of length two Mentions of the entity Frauen are underlined nom nominative acc accusative oth dative oblique and other arguments Accuracy of the permutation de tection experiment with various entity represen tations using manual and automatic annotations of topological fields and grammatical roles The baseline without any additional annotation is un derlined Twotailed sign tests were calculated for each result against the best performing model in each column 1 p 0101 2 p 0053 statis tically significant p 005 very statistically significant p 001 Accuracy of permutation detection experiment with various entity representations us ing manual and automatic annotations of topolog ical fields and grammatical roles on subset of cor pus used by Filippova and Strube 2007a Results of adding coherence features into a natural language generation system VF Acc is the accuracy of selecting the first constituent in main clauses Acc is the percentage of per fectly ordered clauses tau is Kendalls on the constituent ordering The test set contains 2246 clauses of which 1662 are main clauses Association frequencies for target verb Association overlap for target verbs Coverage of verb association features by grammarwindow resources Accuracy for induced verb classes Sample pairs of similar caseframes by relation type and the similarity score assigned to them by our distributional model A sentence decomposed into its depen dency edges and the caseframes derived from those edges that we consider in black Average sentence cover size the average number of sentences needed to generate the case frames in a summary sentence Study 1 Model summaries are shown in darker bars Peer system numbers that we focus on are in bold The average number of source text sen tences needed to cover a summary sentence The model average is statistically significantly differ ent from all the other conditions p 107 Study 1 Signature caseframe densities for differ ent sets of summarizers for the initial and update guided summarization tasks Study 2 p 0005 Density of signature caseframes Study 2 Examples of signature caseframes found in Study 2 Density of signature caseframes after merging to various threshold for the initial Init and update Up summarization tasks Study 2 The effect on caseframe coverage of adding indomain and outofdomain documents The difference between adding indomain and out ofdomain text is significant p 103 Study 3 Coverage of caseframes in summaries with respect to the source text The model aver age is statistically significantly different from all the other conditions p 108 Study 3 Coverage of summary text caseframes in source text Study 3 Latent Dependency coupling for the RE task The DC ONNECT factor expresses ternary connection re lations because the shared head word of the proposed re lation is unknown As is convention variables are repre sented by circles factors by rectangles Relation Extraction Results Models using hidden constituency syntax provide significant gains over the syntacticallyuniformed baseline model in both languages but the advantages of the latent syntax were mitigated on the smaller Chinese data set A tiered graphic representing the three different SRL model configurations The baseline system is described in the bottom c d the separate panels highlighting the independent predictions of this model sense labels are assigned in an entirely separate process from argument prediction Pruning in the model takes place primarily in this tier since we observe true predicates we only instantiate over these indices The middle tier b illustrates the syntactic representation layer and the connective factors between syntax and SRL In the observed syntax model the Link variables are clamped to their correct values with no need for a factor to coordinate them to form a valid tree Finally the hidden model comprises all layers including a combinatorial syntactic constraint a over syntactic variables In this scenario all labels in b are hidden at both training and test time Examining the learned hidden representation for SRL In this example the syntactic dependency arcs derived from gold standard syntactic annotations left are entirely disjoint from the correct predicatearguments pairs shown in the heatmaps by the squares outlined in black and the observed syntax model fails to recover any of the correct predictions In contrast the hidden model structure right learns a representation that closely parallels the desired end task predictions helping it recover three of the four correct SRL predictions shaded arcs red corresponds to a correct prediction with true labels GA KARA etc and providing some evidence towards the fourth The dependency tree corresponding to the hidden structure is derived by edgefactored decoding dependency variables whose beliefs 05 are classified as true though some arcs not relevant to the SRL predictions are omitted for clarity SRL Results The hidden model excels on the unlabeled prediction results often besting the scores obtained using the parses distributed with the CoNLL data sets These gains did not always translate to the labeled task where poor sense prediction hindered absolute performance A tree showing head information Perplexity results for two previous grammarbased language models A nounphrase with substructure Perplexity results for the immediate bihead model Perplexity results for the immediate trihead model Precisionrecall for sentences in which trigramgrammar models performed best An example of the label consistency problem Here we would like our model to encourage entities Albert Einstein and Einstein to get the same label so as to improve the chance that both are labeled PERSON Table showing the number of token sequence token subsequence pairs where the token sequence is assigned a certain entity label and the token subsequence is assigned a certain entity label We show these counts both within documents as well as over the whole corpus Rows correspond to sequences and columns to subsequences These statistics are from the CoNLL 2003 English training set Table showing the number of pairs of different occurrences of the same token sequence where one occurrence is given a certain label and the other occurrence is given a certain label We show these counts both within documents as well as over the whole corpus As we would expect most pairs of the same entity sequence are labeled the sameie the diagonal has most of the density at both the document and corpus levels These statistics are from the CoNLL 2003 English training set Table showing improvements obtained with our additional features over the baseline CRF We also compare our performance against Bunescu and Mooney 2004 and Finkel et al 2005 and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF Accuracy in Lexical Sample Tasks System Pairwise Agreement Illustration on temporality Symmetry of window size Optimality of window size The translation examples where shaded cells indicate the correctly translated pairs MRR Precision Recall and F1score Performance using FBIS training corpus top and NIST corpus bottom Improvements are significant at the p 005 level except where indicated ns Corpus statistics Filters to improve the dictionary precision Un less otherwise noted the filter was applied if either men tion in the relation satisfied the condition Rules of the baseline system Coreference relations in our dictionary Dataset statistics development dev and test Performance on the test set Scores are on gold mentions Stars indicate a statistically significant difference with respect to the baseline Incremental results for the four sieves using our dictionary on the development set Baseline is the Stanford system without the WordNet sieves Scores are on gold mentions Supersense evaluation results Values are the percentage of correctly assigned supersenses k indicates the number of nearest neighbours considered Pseudodisambiguation Percentage of correct choices made Lbound denotes the Web1T lower bound on the a1 n bigram size the number of decisions made Results on the unseen plausibility dataset a A standard PTB parse of Example 1a b The MWE part of speech functions syntactically like the ordinary nominal category as shown by this paraphrase c We incorporate the presence of the MWE into the syntactic analysis by flattening the tree dominating part of speech and introducing a new nonterminal label multiword noun MWN for the resulting span The new representation classifies an MWE according to a global syntactic type and assigns a POS to each of the internal tokens It makes no commitment to the internal syntactic structure of the MWE however Semifixed MWEs in French and English The French adverb terme in the end can be modified by a small set of adjectives and in turn some of these adjectives can be modified by an adverb such as trs very Similar restrictions appear in English French grammar development Incremental effects on grammar size and labeled F1 for each of the manual grammar features development set sentences 40 words The baseline is a parentannotated grammar The features tradeoff between maximizing two objectives overall parsing F1 and MWE F1 Unknown word model features for Arabic and French DPTSG notation For consistency we largely follow the notation of Liang Jordan and Klein 2010 Example of two conflicting sites of the same type in a training tree Define the type of a def site tz s ns0 ns1 Sites 1 and 2 have the same type because tz s1 tz s2 The two sites conflict however because the probabilities of setting bs1 and bs2 both depend on counts for the tree fragment rooted at NP Consequently sites 1 and 2 are not exchangeable The probabilities of their assignments depend on the order in which they are sampled Gross corpus statistics for the preprocessed corpora used to train and evaluate our models We compare to the WSJ section of the PTB train Sections 0221 dev Section 22 test Section 23 Due to its flat annotation style the FTB sentences have fewer constituents per sentence In the ATB morphological variation accounts for the high proportion of word types to sentences Frequency distribution of the MWE types in the ATB and FTB training sets French standard parsing experiments test set sentences 40 words FactLex uses basic POS tags predicted by the parser and morphological analyses from Morfette FactLex uses gold morphological analyses Arabic standard parsing experiments test set sentences 40 words SplitPCFG is the same grammar used in the Stanford parser but without the dependency model FactLex uses basic POS tags predicted by the parser and morphological analyses from MADA FactLex uses gold morphological analyses Berkeley and DPTSG results are the average of three independent runs French MWE identification per category and overall results test set sentences 40 words MWI and MWCL do not occur in the test set Arabic MWE identification per category and overall results test set sentences 40 words MWE identification F1 of the parsing models vs the mwetoolkit baseline test set sentences 40 words FactLex uses gold morphological analyses at test time Sample of humaninterpretable French TSG rules Sample of humaninterpretable Arabic TSG rules Recursive rules like MWAA MWA result from memoryless binarization of nary rules This preprocessing step not only increases parsing accuracy but also allows the generation of previously unseen MWEs of a given type An underspecified discourse structure and its five configurations A wRTG modelling the interdependency constraint for Fig 1 A filter RTG corresponding to Ex 2 C2 Figure 3 An underspecified d A RTG integrating the attachment constraint for Contrast from Ex 2 into Fig 3 Texts used for the evaluation Normalization accuracy after training on n tokens and evaluating on 1000 tokens average of 10 random training and evaluation sets compared to the baseline score of the full text without any normalization Tagging accuracy on the goldstandard normalizations OrigP original punctuation ModP modern punctuation NoP no punctu ation Tagging accuracy on the combined TIGERTuba corpus using 10fold CV evaluated with and without capitalization punctuation and sentence boundaries SB POS tagging accuracy on texts without punctuation and capitalization for tagging on the original data the goldstandard normalization and automatic normalizations using the first n tokens as training data Growth of the Wiktionary over the last three years showing total number of entries for all languages and for the 9 languages we consider left axis We also show the corresponding increase in average accuracy right axis achieved by our model across the 9 languages see details below Typelevel top and tokenlevel bottom cov erage for the nine languages in three versions of the Wik tionary Examples of constructing Universal POS tag sets from the Wiktionary Word type coverage by normalized frequency words are grouped by word count highest word count ratio low 0 001 medium 001 01 high 01 1 PTB vs Wiktionary type coverage across sec tions of the Brown corpus The Wiktionary vs tree bank tag sets Around 90 of the Wiktionary tag sets are identical or subsume tree bank tag sets See text for details Tag errors broken down by the word type clas sified into the six classes oov identical superset subset overlap disjoint see text for detail The largest source of error across languages are outofvocabulary oov word types followed by tag set mismatch types subset over lap disjoint Model accuracy across the Brown cor pus sections ST Stanford tagger Wik Wiktionary tagsettrained SHMMME PTBD PTBtagsettrained SHMMME PTB Supervised SHMMME Wik outper forms PTB and PTBD overall Accuracy for Unsupervised Bilingual Wiktionary and Supervised models Avg is the average of all lan guages except English Unsupervised models are trained without dictionary and use an oracle to map tags to clusters Bilingual systems are trained using a dictionary transferred from English into the target language using word align ments The Projection model uses a dictionary build directly from the partofspeech projection The DP model extends the Projection model dictionary by using Label Propagation Supervised models are trained using tree bank information with SHMMME Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary and All TBD uses tree bank tag sets for all words 50 100 and All Sent models are trained in a supervised manner using increasing numbers of training sentences Results on the Arabic Treebank ATB data set We compare our models against Poon et al 2009 PCT09 and the Morfessor system MorfessorCAT For our full model T OKEN S EG and its simplifica tions BASIC POS T OKEN POS we perform five random restarts and show the mean scores The sample standard deviations are shown in brackets The last col umn shows results of a paired ttest against the preceding model significant at 1 significant at 5 not significant test not applicable Segmentation performance on words that have the same final suffix as their preceding words The F1 scores are computed based on all boundaries within the words but the accuracies are obtained using only the final suffixes Segmentation performance on words that begin with prefix Al determiner and end with suffix At plural noun suffix The mean F1 scores are computed using all boundaries of words in this set For each word we also determine if both affixes are recovered while ig noring any other boundaries between them The other two columns report this accuracy at both the typelevel and the tokenlevel Results Chinese character usage in 3 corpora The numbers in brackets indicate the percentage of characters that are shared by at least 2 corpora Number of entries in 3 corpora Number of unique entries in training and test sets categorized by semantic attributes Language detection accuracies using a 4gram language model for the letter sequence of the source name in Latin script The effect of language and gender in MRR performance of phonetic translit eration for 3 corpora using unigram and bigram language models Gender detection accuracies using a 4gram language model for the letter sequence of the source name in Latin script Overall transliteration performance The effect of language detection The effect of gender detection schemes Number of synset relations TS of partyn1 first 10 out of 12890 total words First ten words with weigths and number of senses in WN of the Topic Signature for airportn1 obtained from BNC using InfoMap Minimum distances from airportn1 Sense disambiguated TS for airportn1 obtained from BNC using InfoMap and SSIDijkstra Size and percentage of overlapping relations between KnowNet versions and WNXWN Percentage of overlapping relations between KnowNet versions P R and F1 finegrained results for the resources evaluated at Senseval3 English Lexical Sample Task Results of the French to English system WMT2012 The marked system corresponds to the system submitted for manual evaluation cs casesensitive ci caseinsensitive Rules for morphological simplification Text normalization for FREN Processing steps for the input sentence dire warnings from pentagon over potential defence cuts Results for French inflection prediction on the WMT2012 test set The marked system corresponds to the system submitted for manual evaluation Russian to English machine translation system evaluated on WMT2012 and WMT2013 Human evaluation in WMT13 is performed on the system trained using the original corpus with TA GIZA for alignment marked with Rules for simplifying the morphological complexity for RU Results on WMT2013 blindtest An example of alignment for Japanese and English sentences Translation Model IBM Model 4 string insertion operator for lefttoright decoding method A string e0 was appended after the partial output string e and the last word in e 0 was aligned from f j string insertion operation for righttoleft decoding method A string e0 was prepended before the partial output string e and the first word in e 0 was aligned from f j Merging lefttoright and righttoleft hypotheses ef and eb in bidirectional decoding method Figure 5a merge two open hypotheses while Figure 5b merge them with inserted zero fer tility words Comparison of the three decoders by the ratio each decoder produced search errors Statistics on a travel conversation corpus Russian morphological disambiguation Evaluation of the Russian ngram model The trigram version of our language model rep resented as a graphical model G1w is the unigram model of 22 Evaluation of the Turkish ngram model Evaluation of Turkish predictive text input A complex TurkishEnglish word alignment alignment points in gray EMPYUV black PY US Word alignment experiments on EnglishTurkish entr and EnglishCzech encs data Our alignment model represented as a graphi cal model Consistently formatted sentence trans lation pairs Consistently formatted term translation pairs The framework of our approach Example segmentations indicates the separator between adjacent snippets Character classes Performance of different settings Contribution of every feature Derivation with DRSs including conversion for A record date Com binatory rules are indicated by solid lines semantic rules by dotted lines CCG derivation as generated by the CC tools Boxer output for Shared Task Text 2 Definitions for the top four senses of law according to WordNet Example confounders for festival and laws and their similarities Pseudoword discrimination performance The error frequency distributions for confusing the correct sense with another sense of the given similarity when using a 5word cooccurrence window as context Dashed lines indicate the null models Unsupervised and Supervised scores on the SemEval2010 WSI Task for each feature and clustering models with reference scores for the top performing systems for each evaluation shown below A typed narrative chain The four top arguments are given The ordering O is not shown Graphical view of an unordered schema automatically built starting from the verb arrest A value that encouraged splitting was used Merging typed chains into a single unordered Narrative Schema Graphical view of an unordered schema automatically built from the verb convict Each node shape is a chain in the schema Six of the top 20 scored Narrative Schemas Events and arguments in italics were marked misaligned by FrameNet definitions indicates verbs not in FrameNet indicates verb senses not in FameNet Results with varying sizes of training data Missing argument examples of biological interactions A protein domainreferring phrase example Statistics of anaphoric expressions A subjective pronoun resolution example Nonanaphoric DNP examples Possessive pronoun resolution examples Example patterns for parallelism An example antecedent of a nominal in teraction keyword Example patterns of nominal interaction keywords Example patterns of proteins and their do mains An annotation example for the necessity of species information Term variation examples Protein name grounding examples Experimental results of test corpus An example result of BioAR Incorrect resolution example of pronoun resolution module The templates for generating potentially deter ministic constraints of English POS tagging Morph features of frequent words and rare words as computed from the WSJ Corpus of Penn Treebank Comparison of raw input and constrained input Comparison of raw input and constrained input Character and wordbased features of a possi ble word wi over the input character sequence c Suppose that wi ci0 ci1 ci2 and its preceding and following char acters are cl and cr respectively Deterministic constraints for POS tagging POS tagging with deterministic constraints The maximum in each column is bold Character tagging with deterministic constraints ILP problem size and segmentation speed Results of the filtering experiments Comparison of Moses and KIT phrase extraction systems Analysis of context length Translation results for GermanEnglish Translation results for EnglishGerman Translation results for FrenchEnglish Translation results for EnglishFrench Substitutioninsertiondeletion patterns for phonemes based on English secondlanguage learners data reported in Swan and Smith 2002 Each row shows an input phoneme class possi ble output phonemes including null and the positions where the substitution or deletion is likely to occur Examples of features and associated costs Pseudofeatures are shown in boldface Exceptional denotes a situation such as the semivowel j substituting for the affricate dZ Substitutions between these two sounds actually occur frequently in secondlanguage error data Substitutiondeletioninsertion costs for g Examples of the three top candidates in the transliteration of EnglishArabic EnglishHindi and EnglishChinese The second column is the rank Languagepair datasets Number of evaluated English NEs CoreMRR scores with different values using score combination A higher puts more weight on the phonetic model MRRs and CorrRate for the pronunciation method top and time correlation method middle The bottom table shows the scores for the combination CoreMRR Partofspeech tags of the Penn Chinese Treebank that are referenced in this paper Please see Xia 2000 for the full list Categories of multicharacter words that are considered strings without internal structures see Section 41 Each category is illustrated with one example from our corpus Categories of multicharacter words that are considered strings with internal structures see Section 42 Each category is illustrated with an example from our corpus Both the individual characters and the compound they form receive a POS tag POS annotations of a couplet ie a pair of two verses in a classical Chinese poem See Table 1 for the meaning of the POS tags POS annotations of an example sentence with a string wan lai evening that has internal structure See Section 42 for two possible translations and Table 1 for the meaning of the POS tags Partofspeech annotations of the three character strings xi liu ying Little Willow military camp and xin feng shi Xinfeng city Both are strings with internal structures with nested structures that perfectly match at all three levels They are the noun phrases that end both verses in the couplet Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag Across all languages high performance can be attained by selecting a single tag per word type Graphical depiction of our model and summary of latent variables and parameters The typelevel tag assignments T generate features associated with word types W The tag assignments constrain the HMM emission parameters The tokens w are generated by tokenlevel tags t from an HMM parameterized by the lexicon structure The hyperparameters and represent the concentration parameters of the token and typelevel components of the model respectively They are set to fixed constants Graph of the onetoone accuracy of our full model FEATS under the best hyperparameter setting by iteration see Section 5 Performance typically stabi lizes across languages after only a few number of itera tions Multilingual Results We report tokenlevel onetoone and manytoone accuracy on a variety of languages under several experimental settings Section 5 For each language and setting we report onetoone 11 and many toone m1 accuracies For each cell the first row corresponds to the result using the best hyperparameter choice where best is defined by the 11 metric The second row represents the performance of the median hyperparameter setting Model components cascade so the row corresponding to FEATS also includes the PRIOR component see Section 3 Statistics for various corpora utilized in exper iments See Section 5 The English data comes from the WSJ portion of the Penn Treebank and the other lan guages from the training set of the CoNLLX multilin gual dependency parsing shared task Typelevel English POS Tag Ranking We list the top 5 and bottom 5 POS tags in the lexicon and the predictions of our models under the best hyperparameter setting Typelevel Results Each cell report the type level accuracy computed against the most frequent tag of each word type The statetotag mapping is obtained from the best hyperparameter setting for 11 mapping shown in Table 3 Comparison of our method FEATS to stateoftheart methods Featurebased HMM Model Berg Kirkpatrick et al 2010 The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm Posterior regulariation model Graca et al 2009 The G10 model uses the posterior regular ization approach to ensure tag sparsity constraint Results for morphological processing GermanEnglish Results for morphological processing EnglishGerman Resu EnglishGerman Resu GermanEnglish Number of affected words by OOV preprocessing Results for OOVprocessing and MBR GermanEnglish Results for OOVprocessing and MBR EnglishGerman Example of the effects of OOV processing for GermanEnglish 1 The semantic roles of cases beside C3 verb cluster The semantic roles of cases beside C1 verb cluster Rule expansion with minimal context Example 3 Sample of extracted entailment rules Accuracy of the extracted sets of rules Resulting sets of entailment rules Question tracking interface to a summa rization system LexRank example sentence similarity graph with a cosine threshold of 015 Corpus of complex news stories Development testing evaluation Average scores by cluster baseline versus LR020095 Training phase effect of similarity thresh old a on Ave MRR and TRDR Training phase effect of question bias d on Ave MRR and TRDR Training phase systems outperforming the baseline in terms of TRDR score Top ranked sentences using baseline system on the question What caused the Kursk to sink Testing phase baseline vs LR020095 Top ranked sentences using the LR020095 system on the question What caused the Kursk to sink Visualisation examples Top named en tity recognition middle dependency syntax bot tom verb frames Screenshot of the main BRAT userinterface showing a connection being made between the annotations for moving and Citibank Incomplete T RANSFER event indicated to the annotator The BRAT search dialog Total annotation time portion spent se lecting annotation type and absolute improve ment for rapid mode Example annotation from the BioNLP Shared Task 2011 Epigenetics and Posttranslational Modifications event extraction task Segmentation recall relative to gold word frequency Baseline performance Segmentation precisionrecall relative to gold word length in training data Word length statistics on test sets Fscore of two segmenters with and without word tokentype features Upper bound for combination The error reduction ER rate is a comparison between the Fscore produced by the oracle combination sys tem and the characterbased system see Tab 1 Segmentation performance presented in previous work and of our combination model PrecisionRecallFscore of different models Accuracy of maximum entropy system using different subsets of features for S ENSEVAL 2 verbs Overall accuracy of maximum entropy sys tem using different subsets of features for Penn Chi nese Treebank words manually segmented partof speechtagged parsed Overall accuracy of maximum entropy sys tem using different subsets of features for Peoples Daily News words manually segmented partof speechtagged Overall accuracy of maximum entropy sys tem using different subsets of features for Peoples Daily News words automatically segmented part ofspeechtagged parsed Overview of the system The pool of features for all languages Experiment results as F1 scores where IM is identification of mentions and S Setting Final system results as F1 scores where IM is identification of mentions and S Setting For more details cf Recasens et al 2010 Three representations of NP modifications a the original treebank representation b Selective leftcorner representation and c a flat structure that is unambiguously equivalent to b Conditioning features for the probabilistic CFG used in the reported empirical trials Corpus sizes Parser performance on BrownE baselines Note that the Gildea results are for sentences 40 words in length Parser performance on WSJ23 baselines Note that the Gildea results are for sentences 40 words in length All others include all sentences Parser performance on WSJ23 baselines Parser performance on BrownE supervised adaptation Parser performance on WSJ23 unsupervised adaptation For all trials the base training is BrownT the held out is BrownH plus the parser output for WSJ24 and the mixing parameter A is 020e cA Parser performance on WSJ23 supervised adaptation All models use BrownTH as the outofdomain treebank Baseline models are built from the fractions of WSJ221 with no outofdomain treebank Interannotator agreement of ACE 2005 relation annotation Numbers are the distinct relation mentions whose both arguments are in the list of adjudicated entity mentions Percentage of examples of major syntactic classes cumulative distribution of frequency CDF of the relative ranking of modelpredicted probability of being positive for false negatives in a pool mixed of false negatives and true negatives and the CDF of the relative ranking of modelpredicted probability of being negative for false positives in a pool mixed of false positives and true positives Categories of spurious relation mentions in fp1 on a sample of 10 of relation mentions ranked by the percentage of the examples in each category In the sample text red text also marked with dotted underlines shows head words of the first arguments and the underlined text shows head words of the second arguments Performance of RDC trained on fp1fp2adj and tested on adj TSVM optimization function for nonseparable case Joachims 1999 Performance with SVM trained on a fraction of adj It shows 5 fold cross validation results 5fold crossvalidation results All are trained on fp1 except the last row showing the unchanged algorithm trained on adj for comparison and tested on adj McNemars test show that the improvement from purify to tSVM and from tSVM to ADJ are statistically significant with p005 ESA and inputoutput data A character sequence and its subsequence pairs The path of Selection The binary tree of Selection The initial frequencies of character sequences The adjusted frequencies of character sequences The hierarchical form of a result The scales of corpora The results of setting 1 Punctuation and other encoding information are not used the maximum length is 30 The results of setting 2 Punctuation and other encoding information are not used the maximum length is 10 The results of setting 3 Punctuation is used the maximum length is 30 The results of setting 4 Punctuation and other encoding information are used the maximum length is 30 The difference between the results of four settings The results brought by different maximum lengths The empirical formulae for the prediction linear model The correlation between the scales and the proper exponents The four types of changes Convergence of results The time complexity in practice 4 samples 10 30 50 and 100 The comparison between NPYLM and ESA The comparison between DLG AV BE and ESA The comparison between IWSLRRI SSS TONGOO THT and ESA Probabilistic Approaches Outputs from each algorithm at different sorted ranks WordNetbased scores Performance on Internet data Equation 1 settings Precisionrecall curve for rescoring Syntactic frames for VerbNet classes VerbNet classes An excerpt from SemLink Training instances obtained from Verb Net upper and VerbNetSemLink lower Example of features for sway 14 classes used in Joanis et al 2008 and their corresponding Levin class numbers Corpus size vs accuracy Accuracy and KLdivergence for the all class task the VerbNetSemLink setting Accuracy and KLdivergence for the all class task the VerbNet only setting Accuracy for the 14class task Corpus size vs KLdivergence Contribution of features PoCoS Core Scheme Extended Scheme and languagespecific instantiations A summary of the parsing and evaluation sce narios X depicts gold information depicts unknown information to be predicted by the system Overview of participating languages and treebank properties Sents number of sentences Tokens number of raw surface forms Lex size and Avg Length are computed in terms of tagged terminals NT non terminals in constituency treebanks Dep Labels dependency labels on the arcs of dependency treebanks A more comprehensive table is available at httpwwwspmrlorgspmrl2013sharedtaskhtmlProp File formats Trees a and b are aligned constituency and dependency trees for a mockup English example Boxed labels are shared across the treebanks Figure c shows an ambiguous lattice The red part represents the yield of the gold tree For brevity we use empty feature columns but of course lattice arcs may carry any morphological features in the FEATS CoNLL format Dependency parsing LAS scores for full and 5k training sets and for gold and predicted input Results in bold show the best results per language and setting Dependency Parsing MWE results Constituent Parsing ParsEval Fscores for full and 5k training sets and for gold and predicted input Results in bold show the best results per language and setting Constituent Parsing LeafAncestor scores for full and 5k training sets and for gold and predicted input Realistic Scenario Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario Top upper part refers to constituency results the lower part refers to dependency results Realistic Scenario Tedeval Labeled Accuracy and Exact Match for the Raw scenario The upper part refers to constituency results the lower part refers to dependency results The correlation between treebank size label set size and LAS scores x treebank size labels y LAS treebank Correlation between treebank size Non sizenumber terminal labelsof sentences sent and mean F1 The correlation between the non terminals per sentence ratio and Leaf Accuracy macro scores x non terminal sentence y Acc Cross Framework Evaluation Unlabeled TedEval on generalized gold trees in gold scenario trained on 5k sentences and tested on 5k terminals CrossLanguage Evaluation Unlabeled TedEval Results in gold input scenario On a 5ksentences set set and a 5kterminals test set The upper part refers to constituency parsing and the lower part refers to dependency parsing For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics Labeled and Unlabeled TedEval Results for raw Scenarios Trained on 5k sentences and tested on 5k terminals The upper part refers to constituency parsing and the lower part refers to dependency parsing The parts of taxonomic names The Classification Process Abbreviations Comparison to Related Approaches Test results An excerpt from the text with core ferring noun phrases annotated English trans lation in italics Mention Detection Results Features and functions used in clustering algorithm Coreference Resolution Performance Contribution of individual features to overall performance Examples of the ACE Relation Types The RCM structure System Pipeline Test Procedure Chinese system performance with system mentions and system relations 4 Performance of English system with system mentions and system relations 2 Performance of English system with perfect mentions and perfect relations 3 Performance of Chinese system with perfect mentions and perfect relations Results of paraphrases evaluation for 100 sentences in French using English as the pivot lan guage Comparison between the baseline system MOSES and our algorithm MCPG A brief description of the tested parsers Note that the Tune data is not the data used to train the individual parsers Higher numbers in the right column reflect just the fact that the Test part is slightly easier to parse Comparison of various groups of parsers All percentages refer to the share of the total words in test data attached correctly The single parser part shows shares of the data where a single parser is the only one to know how to parse them The sizes of the shares should correlate with the uniqueness of the individual parsers strategies and with their contributions to the overall success The at least rows give clues about what can be got by majority voting if the number represents over 50 of parsers compared or by hypothetical oracle selection if the number represents 50 of the parsers or less an oracle would generally be needed to point to the parsers that know the correct attachment Results of voting experiments Voting under handinvented schemes Contexts where ec is better than mcdz J are coordination conjunctions is the root V are verbs Nn are nouns in case n R are preposi tions Z are punctuation marks An are adjectives The decision tree for ecmcz learned by C5 Besides pairwise agreement be tween the parsers only morphological case and negativeness matter Contextsensitive voting Contexts trained on the Tune data set accuracy figures apply to the Test data set Contextfree results are given for the sake of comparison Unbalanced vs balanced combining All runs ignored the context Evaluated on the Test data set Combined systems Basque in cross validation best recall in bold Only vectorf was used for combination Single systems Basque in cross validation sorted by recall Single systems English in cross validation sorted by recall Combined systems English in cross validation best recall in bold Ocial results for the English and Basque lexical tasks recall Table 2 Chunk feature templates i f jFk is the chunk label plus the tag of its right most child if the j2r t tree is a chunk Otherwise i f is the con stituent label of the j r t tree Extend feature templates G fA k l is the root constituent label of th Check feature templates G fj Learning curves wordsegmentation F measure and parsing label Fmeasure vs percentage of training data Languagedependent lexical features A word list can be collected to encode different WS wordsegmentation Baseline languageindependent features LexFeat plus lex ical features Numbers are averaged over the 10 ex periments in Figure 2 Parsing and word segmentation F measures vs the experiment numbers Lines with triangles segmentation Lines with circles label Dottedlines languageindependent features only Solid lines plus lexical features Usefulness of syntactic information black dashdotted line word boundaries only red dashed line POS info and blue solid line full parse trees IV and OOV recall in Zhang et al 2006a Corpora statistics of Bakeoff 2005 Feature templates used for CRF in our experiments Error analysis of confidence measure with and without EIV tag Results of CT when MP is less than 0875 Results of different approach used in our experiments White background lines are the results we repeat Zhangs methods and they have some trivial difference with Table 1 A text segment from MUC6 data set Feature set for the baseline pronoun resolution system Backward features used to capture the coreferential information of a candidate Results of different systems for pronoun resolution on MUC6 and MUC7 Here we only list backward feature assigner for pronominal candidates In RealResolve1 to RealResolve4 the backward features for nonpronominal candidates are all found by DTnonpron The pronoun resolution algorithm by incorporating coreferential information of can didates The classifier refining algorithm New training and testing procedures Annotated RST Tree for example 4 Confusion Sets An Initial Learning Curve for Confusable Disambiguation Learning Curves for Confusable Disambiguation Distribution of Error Types Turning distributional similarity into a weighted inference rule Results on the RTE1 Test Set Results on the STS video dataset Beam search algorithm for joint tagging and de pendency parsing of input sentence x with weight vector w and beam parameters b1 and b2 The symbols hc hs and hf denote respectively the configuration score and feature representation of a hypothesis h hcA denotes the arc set of hc Transitions for joint tagging and dependency parsing extending the system of Nivre 2009 The stack is represented as a list with its head to the right and tail and the buffer B as a list with its head to the left and tail The notation f a b is used to denote the function that is exactly like f except that it maps a to b Specialized feature templates for tagging We use i and Bi to denote the ith token in the stack and buffer B respectively with indexing starting at 0 and we use the following functors to extract properties of a token i ith best tag si score of ith best tag finally predicted tag w word form pi word prefix of i characters si word suffix of i characters Score differences are binned in discrete steps of 005 Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and the score threshold Beam parameters fixed at b1 40 b2 4 Accuracy scores for the CoNLL 2009 shared task test sets Rows 12 Top performing systems in the shared CoNLL Shared Task 2009 Gesmundo et al 2009 was placed first in the shared task for Bohnet 2010 we include the updated scores later reported due to some improvements of the parser Rows 34 Baseline k 1 and best settings for k and on development set Rows 56 Wider beam b1 80 and added graph features G and cluster features C Second beam parameter b2 fixed at 4 in all cases Accuracy scores for WSJPTB converted with head rules of Yamada and Matsumoto 2003 and labeling rules of Nivre 2006 Best dev setting k 3 04 Results marked with use additional information sources and are not directly comparable to the others Selected entries from the confusion matrix for parts of speech in German with Fscores for the lefthand side category ADJ ADJD or ADJA adjective ADV adverb ART determiner APPR preposition NE proper noun NN common noun PRELS relative pronoun VVFIN finite verb VVINF nonfinite verb VAFIN finite auxiliary verb VAINF nonfinite auxil iary verb VVPP participle XY not a word We use to denote the set of categories with as a prefix Accuracy scores for Penn Chinese Treebank converted with the head rules of Zhang and Clark 2008 Best dev setting k 3 01 MSTParser results from Li et al 2011 UAS scores from Li et al 2011 and Ha tori et al 2011 recalculated from the separate accuracy scores for root words and nonroot words reported in the original papers Selected entries from the confusion matrix for parts of speech in English with Fscores for the lefthand side category DT determiner IN preposition or sub ordinating conjunction JJ adjective JJR compara tive adjective NN singular or mass noun NNS plural noun POS possessive clitic RB adverb RBR com parative adverb RP particle UH interjection VB base form verb VBD past tense verb VBG gerund or present participle VBN past participle VBP present tense verb not 3rd person singular VBZ present tense verb 3rd person singular We use to denote the set of categories with as a prefix Example of the transfer of a verbal chunk Examples of translations An example of the label consistency problem excerpted from a document in the CoNLL 2003 English dataset Features used by the CRF for the two tasks named entity recognition NER and template filling TF Counts of the number of times multiple occurrences of a token sequence is labeled as different entity types in the same document Taken from the CoNLL training set F1 scores of the local CRF and nonlocal models on the CMU Seminar Announcements dataset We also provide the results from Sutton and McCallum 2004 for comparison F1 scores of the local CRF and nonlocal models on the CoNLL 2003 named entity recognition dataset We also provide the results from Bunescu and Mooney 2004 for comparison Summary of the previous work on coreference resolution that employs the learning algorithms the clustering algorithms the feature sets and the training instance creation methods discussed in Section 31 Statistics for the ACE corpus Results for the three ACE data sets obtained via the MUC scoring program Results for the three ACE data sets obtained via the BCUBED scoring program The coreference systems that achieved the highest Fmeasure scores for each test set and scorer combination The average rank of the candidate partitions produced by each system for the corresponding test set is also shown Overview of the tasks investigated in this paper n size of ngram POS parts of speech Ling linguistic knowledge Type type of task Meaning of diacritics indicating statistical sig nificance 2 tests Performance comparison with the literature for candidate selection for MT Performance of Altavista counts and BNC counts for candidate selection for MT data from Prescher et al 2000 Performance comparison with the literature for context sensitive spelling correction Performance of Altavista counts and BNC counts for context sensitive spelling correction data from Cucerzan and Yarowsky 2002 Performance comparison with the literature for compound bracketing Performance of Altavista counts and BNC counts for compound bracketing data from Lauer 1995 Performance of Altavista counts and BNC counts for adjective ordering data from Malouf 2000 Performance comparison with the literature for compound interpretation Performance of Altavista counts and BNC counts for compound interpretation data from Lauer 1995 Performance comparison with the literature for noun countability detection Performance of Altavista counts and BNC counts for noun countability detection data from Bald win and Bond 2003 Syntactic Seeding Heuristics Caseframe Network Examples Lexical Caseframe Expectations Semantic Caseframe Expectations General Knowledge Sources Individual Performance of KSs for Disasters Individual Performance of KSs for Terrorism General Contextual Role Knowledge Sources General Knowledge Sources A verse written in the BAD web application The response of the rhyme search engine The BAD application before entering a verse showing two possible rhyme patterns Extract of a FrenchEnglish sentence pair segmented into bilingual units The original org French sentence appears at the top of the figure just above the reordered source s and target t The pair s t decomposes into a sequence of L bilingual units tuples u1 uL Each tuple ui contains a source and a target phrase si and ti Experimental results in terms of BLEU scores measured on the newstest2011 and newstest2012 For newstest2012 the scores are provided by the organizers Percentage of major punctuation marks in the Chinese corpus 4 Syntax of Chinese Dash Rhetorical pattern of CQuestion Rhetorical pattern of CExclamation Rhetorical pattern of CEllipses Rhetorical pattern of CSemicolon Rhetorical pattern of CDash Rhetorical pattern of CColon Rhetorical Function of Exclamation Mark in Chinese and German corpora POSmorphological feature accuracies on the development sets PARSEVAL scores on the development sets Architecture of the dependency ranking system Baseline performance and nbest oracle scores UASLAS on the development sets mate uses the prepro cessing provided by the organizers the other parsers use the preprocessing described in Section 2 Unlabeled TedEval scores accuracyexact match for the test sets in the predicted segmentation set ting Only sentences of length 70 are evaluated Feature sets for the dependency ranker for each language default denotes the default ranker feature set Performance UASLAS of the reranker on the development sets Baseline denotes our baseline Rankeddflt and Ranked denote the default and optimized ranker feature sets respectively Oracle denotes the oracle scores Final PARSEVAL F1 scores for constituents on the test set for the predicted setting ST Baseline denotes the best baseline out of 2 provided by the Shared Task organizers Our submission is underlined Final UASLAS scores for dependencies on the test sets for the predicted setting Other denotes the highest scoring other participant in the Shared Task ST Baseline denotes the MaltParser baseline provided by the Shared Task organizers The results of different systems for coreference resolution Top patterns chosen under different scoring schemes The decision tree Nwire for the system using the single semantic relatedness feature Sequence of POStagged units used to estimate the bilingual ngram LM Statistics for the training tune and test data sets Size in words of reorderings ob served in training bitexts Size of translation unit ngrams seen in test for different ngram models Reordering accuracy BLEU results Translation accuracy BLEU results Example context for the spelling confusion set piecepeace and extracted features Example context for WSD S ENSEVAL2 target word bar inventory of 21 senses and extracted features The increase in performance for successive variants of Bayes and Mixture Model as evaluated by 5fold cross vali dation on S ENSEVAL 2 English data A WSD example that shows the influence of syntactic collocational and longdistance context features the probability estimates used by Nave Bayes and MM and their associated weights and the posterior probabilities of the true sense as computed by the two models 2 Figure 4 WSD example showing the utility of the MVC method A sense with a high variational coefficient is preferred to the mode of the MM distribution the fields corresponding to the true sense are highlighted MM and MMVC performance by performing 5 fold cross validation on S ENSEVAL 2 data for 4 languages Results using 5fold cross validation on S ENSEVAL 2 English lexicalsample training data The contribution of MMVC in a rankbased classi fier combination on S ENSEVAL 1 and S ENSEVAL 2 English as computed by 5fold cross validation over training data Results on the standard 14 CSSC data sets Results using 5fold cross validation on S ENSEVAL 1 training data English Learning Curve for MM and MMVC on S ENSEVAL 2 English crossvalidated on heldout data ings of the 13th International Conference pages 182190 Table 6 Accuracy on S ENSEVAL1 and S ENSEVAL2 En A R Golding and D Roth 1999 A winnowbased appro glish test data only the supervised systems with a coverage of to contextsensitive spelling correction Machine Learni at least 97 were used to compute the mean and variance 3413107130 Manual analysis of suggested corrections on CLC data Precision and recall for articles Precision and recall for prepositions Using different amounts of annotated training data for the article metaclassifier Using different amounts of annotated training The Stanford parser Klein and Manning 2002 is unable to recover the verbal reading of the unvocalized surface form an Table 1 Diacritized particles and pseudoverbs that after orthographic normalization have the equivalent surface form an The distinctions in the ATB are linguistically justified but complicate parsing Table 8a shows that the best model recovers SBAR at only 710 F1 Frequency distribution for sentence lengths in the WSJ sections 223 and the ATB p13 English parsing evaluations usually report results on sentences up to length 40 Arabic sentences of up to length 63 would need to be evaluated to account for the same fraction of the data We propose a limit of 70 words for Arabic parsing evaluations Gross statistics for several different treebanks Test set OOV rate is computed using the following splits ATB Chiang et al 2006 CTB6 Huang and Harper 2009 Ne gra Dubey and Keller 2003 English sections 221 train and section 23 test An ATB sample from the human evaluation The ATB annotation guidelines specify that proper nouns should be specified with a flat NP a But the city name Sharm Al Sheikh is also iDafa hence the possibility for the incorrect annotation in b Evaluation of 100 randomly sampled variation nu clei types The samples from each corpus were indepen dently evaluated The ATB has a much higher fraction of nuclei per tree and a higher typelevel error rate Dev set learning curves for sentence lengths 70 All three curves remain steep at the maximum training set size of 18818 trees Test set results Maamouri et al 2009b evaluated the Bikel parser using the same ATB split but only reported dev set results with gold POS tags for sentences of length 40 The Bikel GoldPOS configuration only supplies the gold POS tags it does not force the parser to use them We are unaware of prior results for the Stanford parser The constituent Restoring of its constructive and effective role parsed by the three different models gold segmen tation The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals Like verbs maSdar takes arguments and assigns case to its objects whereas it also demonstrates nominal characteristics by eg taking determiners and heading iDafa Fassi Fehri 1993 In the ATB 2 3 astaadah is tagged 48 times as a noun and 9 times as verbal noun Consequently all three parsers prefer the nominal reading Table 8b shows that verbal nouns are the hardest preterminal categories to identify None of the models attach the attributive adjectives correctly Dev set results for sentences of length 70 Cov erage indicates the fraction of hypotheses in which the char acter yield exactly matched the reference Each model was able to produce hypotheses for all input sentences In these experiments the input lacks segmentation markers hence the slightly different dev set baseline than in Table 6 Per category performance of the Berkeley parser on sentence lengths 70 dev set gold segmentation a Of the high frequency phrasal categories ADJP and SBAR are the hardest to parse We showed in 2 that lexical ambiguity explains the underperformance of these categories b POS tagging accuracy is lowest for maSdar verbal nouns VBGVN and adjectives eg JJ Richer tag sets have been suggested for modeling morphologically complex distinctions Diab 2007 but we find that linguistically rich tag sets do not help parsing c Coordination ambiguity is shown in dependency scores by eg S S S R and NP NP NP R NP NP PP R and NP NP ADJP R are both iDafa attachment BiTAM models for Bilingual document and sentencepairs A node in the graph represents a random variable and a hexagon denotes a parameter Unshaded nodes are hidden variables All the plates represent replicates The outmost plate M plate represents M bilingual documentpairs while the inner N plate represents the N repeated choice of topics for each sentencepairs in the document the inner Jplate represents J wordpairs within each sentencepair a BiTAM1 samples one topic denoted by z per sentencepair b BiTAM2 utilizes the sentencelevel topics for both the translation model ie pf e z and the monolingual word distribution ie pez c BiTAM3 samples one topic per wordpair Training and Test Data Statistics Topicspecific translation lexicons are learned by a 3topic BiTAM1 The third lexicon Topic3 prefers to translate the word Korean into ChaoXian mNorth Korean The cooccurrence Cooc IBM14 and HMM only prefer to translate into HanGuo ISouth Korean The two candidate translations may both fade out in the learned translation lexicons Three most distinctive topics are displayed The English words for each topic are ranked according to pez estimated from the topicspecific English sentences weighted by dnk 33 functional words were removed to highlight the main content of each topic Topic A is about UsChina economic relationships Topic B relates to Chinese companies merging Topic C shows the sports of handicapped people performances over eight Variational EM itera tions of BiTAM1 using both the Null word and the laplace smoothing IBM1 is shown over eight EM iterations for comparison Word Alignment Accuracy Fmeasure and Machine Translation Quality for BiTAM Models comparing with IBM Models and HMMs with a training scheme of 18 h7 43 on the Treebank data listed in Table 1 For each column the highlighted alignment the best one under that model setting is picked up to further evaluate the translation quality Evaluating Word Alignment Accuracies and Machine Translation Qualities for BiTAM Models IBM Models HMMs and boosted BiTAMs using all the training data listed in Table 1 Other experimental conditions are similar to Table 4 An underspecified discourse structure and its five configurations A wRTG modelling Fig 1 Runtime Comparison Experimental data sets Precision at top 200 Results of 1000 sentences Precision at top 50 Results of 3000 sentences Precision at top 100 Results of 2000 sentences Plate diagram representation of the trigram HMM The indexes i and j range over the set of tags and k ranges over the set of characters Hyperparameters have been omitted from the figure for clarity The conditioning structure of the hierarchical PYP with an embedded character language models Simulation comparing the expected table count solid lines versus the approximation under Eq 3 dashed lines for various values of a This data was gen erated from a single PYP with b 1 P0 i 14 and n 100 customers which all share the same tag WSJ performance comparing previous work to our own model The columns display the manyto1 accuracy and the V measure both averaged over 5 inde pendent runs Our model was run with the local sampler HMM the typelevel sampler 1HMM and also with the character LM 1HMMLM Also shown are results using Dirichlet Process DP priors by fixing a 0 The system abbreviations are CGS10 Christodoulopoulos et al 2010 BBDK10 BergKirkpatrick et al 2010 and GG07 Goldwater and Griffiths 2007 Starred entries denote results reported in CGS10 Sorted frequency of tags for WSJ The gold standard distribution follows a steep exponential curve while the induced model distributions are more uniform Cooccurence between frequent gold yaxis and predicted xaxis tags comparing mkcls top and PYP1HMMLM bottom Both axes are sorted in terms of frequency Darker shades indicate more frequent cooc curence and columns represent the induced tags M1 accuracy vs number of samples Manyto1 accuracy across a range of languages comparing our model with mkcls and the best published result BergKirkpatrick et al 2010 and Lee et al 2010 This data was taken from the CoNLLX shared task training sets resulting in listed corpus sizes Fine PoS tags were used for evaluation except for items marked with c which used the coarse tags For each language the systems were trained to produce the same number of tags as the gold standard Selected morphosyntactic categories in the OLiA Reference Model Attributive demonstrative pronouns PDAT in the STTS Annotation Model Individuals for accusative and sin gular in the TIGER Annotation Model Selected morphological features in the OLiA Reference Model The STTS tags PDAT and ART their rep resentation in the Annotation Model and linking with the Reference Model Evaluation setup Confidence scores for diese in ex 1 Recall for morphological hasXY descriptions Examples of DTs and their ICDcodes The dataset shuffled and divided into 3 sets The dataset of DT ICDcode pairs A simplified version in Foma source code of the regular expressions and transducers used to bypass several sources of distortion within the DTs in order to parse variations of unseen input DTs Performance of different FS machines in terms of the percentage of unclassified entries All the classified entries were correctly classified yielding as a result a precision of 100 The screenshot of our webbased system shows a simple quantitative analysis of the frequency of two terms in news articles over time While in the 90s the term Friedensmission peace operation was predominant a reverse tendency can be observed since 2001 with Auslandseinsatz foreign intervention being now frequently used Overview of the complete processing chain Dependency parse of a sentence that contains indirect speech see Sentence 2 10 most used verbs lemma in indirect speech Results of a 10fold crossvalidation for various machine learning algorithms Learning Curves for Confusion Set Disambiguation Representation Size vs Training Corpus Size Complementarity Voting Among Classifiers Active Learning with Large Corpora CommitteeBased Unsupervised Learning Committee Agreement vs Accuracy Comparison of Unsupervised Learning Methods ELUS Segmenter Open test in percentages Closed test in percentages Graphical model of HMBiTAM Graphical model of synonym pair gen erative process The number of vocabularies in the 10k 50k and 100k data sets Comparison of word alignment accuracy The best results are indicated in bold type The additional data set sizes are a 10k b 50k c 100k Three narrative events and the six most likely events to include in the same chain One of the 69 test documents containing 10 narrative events The protagonist is President Bush Results with varying sizes of training data Year 2003 is not explicitly shown because it has an unusually small number of documents compared to other years A narrative chain and its reverse order Results for choosing the correct ordered chain 10 means there were at least 10 pairs of ordered events in the chain An Employment Chain Dotted lines indicate incorrect before relations An automatically learned Prosecution Chain Arrows indicate the before relation Probability of of boundaries f10 m 3 Results of segmentation of entry titles Fscore precisionrecall Acceptance rates for a noun phrase in the course of iteration All models were with backoff mix ing BM Diffs in the course of iteration All models were with backoff mixing BM Effect of matching skip Fscore precisionrecall Features based on the token string Sources of Dictionaries The whole process of retraining the upper case NER Q signifies that the text is converted to upper case before processing Fmeasure on MUC6 and MUC7 test data Improvements in Fmeasure on MUC6 plotted against amount of selected unlabeled data used Improvements in Fmeasure on MUC7 plotted against amount of selected unlabeled data used Trivial and singlefeature baselines using SVM acc unless noted otherwise Combination results using SVMacc Results by relation Lattice representation of the sentence bclm hneim Doublecircles denote token boundaries Lattice arcs correspond to different segments of the token each lattice path encodes a possible reading of the sentence Notice how the token bclm have analyses which include segments which are not directly present in the unsegmented form such as the definite article h 13 and the pronominal suffix which is expanded to the sequence fl hm of them 24 45 Parsing scores of the various systems Two fragments of a hierarchy over word class distributions Error reduction as a function of vocabulary size Evaluation of coarsegrained POS tagging on test data Evaluation of coarsegrained POS tagging on test data Example of a German noun phrase First and last word agree in number gender and case value Syntactic features h and ld mark features from the head and the leftmost daughter dir is a binary fea ture marking the direction of the head with respect to the current token The effect of syntactic features when predicting morphological information mark statistically signifi cantly better models compared to our baseline sentence based ttest with 005 The effect of syntactic features when predicting morphology using lexicons mark statistically signifi cantly better models compared to our baseline sentence based ttest with 005 Syntactic features for featurama Czech mark statistically significantly better models compared to feat urama sentencebased ttest with 005 Agreement counts in morphological annotation compared between the baseline system and the oracle system using gold syntax Simple parser vs full parser syntactic quality Trained on first 5000 sentences of the training set Dependency between amount of training data for syntactic parser and quality of morphological prediction Impact of the improved morphology on the qual ity of the dependency parser for Czech and German Simple parser vs full parser morphological quality The parsing models were trained on the first 5000 sentences of the training data the morphological tagger was trained on the full training set Baseline Outofthebox BerkeleyParser performance on the devset Number of learned splits per POS category after five splitmerge cycles Number of learned splits per NTcategory after five splitmerge cycles A layered POS tag representation A latent layered POS tag representation The lattice for the Hebrew sequence see footnote 19 Devset results when using lattice parsing on top of an external lexiconanalyzer Devset results when incorporating an external lexicon Devset results of using the agreementfilter on top of the lexiconenhanced lattice parser parser does both segmentation and parsing Devset results of using the agreementfilter on top of the lexiconenhanced parser starting from gold segmentation Numbers of parsetree nodes in the 1best parses of the development set that triggered gender or number agreement checks and the results of these checks Testset results of the bestperforming models NP agreement violations that were caught by the agreement filter system a Nouncompound case that was correctly handled b Case involving conjunction that was correctly handled c A case where fixing the agreement violation introduces a PPattachment mistake LDA Phrase pairs extracted from a document pair with an economic topic Topic words extracted from targetside doc uments Corpus statistics 4 Contribution of various caches in our cache based documentlevel SMT system Note that signific ance tests are done against Moses Contribution of combining the three caches Contribution of combining the dynamic Contribution of combining the dynamic and Contribution of employing the dynamic cache on different test documents Impact of the topic cache size Contribution of the static cache on the first sentence of each test document ie with empty dynamic cache Positive and negative examples Descriptions of the 10 corpora Average precisions over the 10 corpora of different window size 3 seeds Precision top N with 3 seeds and window size w 3 Denition of NE in IREX Example of morphological analyses Case frame of haken dispatch Experimental results Fmeasure Comparison with previous work SyntSem tagged corpus extract Optimal context size S and criteria space distribution by partofspeech of Precision P and usage proportion optimal context size using both precision with and without feature precision decrease when omitting non space distribution of most reliable target word frequency F average Evaluation measures tp true positives fp false positives tn true negatives fn false negatives pr precision re recall Classification results with XLE starredness parser exceptions and zero parses Method 1 Classification results with 5gram and fre quency threshold 4 Method 2 Classification results with decision tree on XLE output Method 3 Classification results with decision tree on vectors of frequency of rarest ngrams Method 4 Classification results with decision tree on joined feature set Method 5 Accuracy by sentence length for Method 5 measured on separate grammatical and ungrammat ical data Gr Grammatical AG Agreement RW RealWord EW Extra Word MW Missing Word Training and test set sources genres sizes in terms of numbers of tokens and unigram and bi gram coverage of the training set on the test sets Learning curves of word prediction accu racies of IGT REE trained on TRAIN REUTERS and tested on REUTERS ALICE and BROWN Word prediction speed in terms of the number of classified test examples per second mea sured on the three test sets with increasing training examples Both axes have a logarithmic scale Learning curves in terms of word predic tion accuracy on deciding between the confusable pair there their and theyre by IGT REE trained on TRAIN REUTERS and tested on REUTERS AL ICE and BROWN The top graphs are accuracies at tained by the confusable expert the bottom graphs are attained by the allwords predictor trained on TRAIN REUTERS until 130 million examples and on TRAIN NYT beyond marked by the vertical bar Disambiguation scores on nine confusable set attained by confusable experts trained on ex amples extracted from 1 billion words of text from TRAIN REUTERS plus TRAIN NYT on the three test sets Disambiguation scores on nine confusable set attained by the allwords prediction classifier trained on 30 million examples of TRAIN REUTERS and by confusable experts on the same training set The second column displays the number of exam ples of each confusable set in the 30million word training set the list is ordered on this column Kleene RegularExpression Assignment Examples Four synchronous rules with topic distributions Each subgraph shows a rule with its topic distribution where the Xaxis means topic index and the Yaxis means the topic probability Notably the rule b and rule c shares the same source Chinese string but they have different topic distributions due to the different English translations Example of topictotopic correspondence The last line shows the correspondence probability Each col umn means a topic represented by its top10 topical word s The first column is a targetside topic while the rest three columns are sourceside topics Result of our topic similarity model in terms of BLEU and speed words per second comparing with the traditional hierarchical system Baseline and the topicspecific lexicon translation method TopicLex SimSrc and SimTgt denote similarity by sourceside and targetside ruledistribution respectively while SimSen acti vates the two similarity and two sensitivity features Avg is the average B LEU score on the two test sets Scores marked in bold mean significantly Koehn 2004 better than Baseline p 001 Effects of onetoone and onetomany topic pro jection Illustration of similarities in POS tag statistics across languages a The unigram frequency statistics on five tags for two close languages English and German b Sample sentences in English and German Verbs are shown in blue prepositions in red and noun phrases in green It can be seen that noun phrases follow prepositions An iterative algorithm for minimizing our ob jective in Eq 7 For simplicity we assume that all the weights i and are equal to one It can be shown that the objective monotonically decreases in every iteration The set of typological features that we use for source language selection The first column gives the ID of the feature as listed in WALS The second column describes the feature and the last column enumerates the allowable values for each feature besides these values each feature can also have a value of No dominant order Directed dependency accuracy of our model and the baselines using gold POS tags for the target language The first section of the table is for the direct transfer of the MST parser McDonald et al 2011 The second section is for the weighted mixture parsing model Cohen et al 2011 The first two columns Random and Greedy of each section present the parsing performance with a random or a greedy mapping The third column Petrov shows the results when the mapping of Petrov et al 2011 is used The fourth column Model shows the results when our mapping is used and the fifth column in the first section Best Pair shows the performance of our model when the best source language is selected for every target language The last column Tag Diff presents the difference between our mapping and the mapping of Petrov et al 2011 by showing the percentage of target language tokens for which the two mappings select a different universal tag Objective values for the different mappings used in our experiments for four languages Note that the goal of the optimization procedure is to minimize the objective value Screenshot of ConAno Overview of annotation environment Possible interpretations for the text wAlY Habash and Rambow 2005 Syntactic dependency scheme used in this work Labels that arent selfexplanatory or similar to the labels used by Tratz and Hovy 2011 for English or CATiB for Arabic Habash and Roth 2009 are in bold for completely new relations or italics for similarly named but semantically different relations Counts of the number of files sentences Sent original spacedelimited tokens Tok ATB tree tokens Tree Toks and affixes in the experimental data Counts for the POS tags mentioned in Table 5 Top 10 POS mistakes made more often by either the CTFTM with parsing or the CTFTM without on the ATB part 1 2 and 3 development set Results for the various experiments Exp for both the development and test portions of the data including per token clitic separation tokenization accuracy partofspeech tagging F1 affix boundary detection F1 affix labeling F1 and both unlabeled and labeled attachment scores Chinese parse tree with empty elements marked The meaning of the sentence is Implementation of the law is temporarily suspended English parse tree with empty elements marked a As annotated in the Penn Treebank b With empty elements recongured and slash categories added Recall on different types of empty categories YX Yang and Xue 2010 Ours split 6 Results on Penn Chinese Treebank Results on Penn English Treebank Wall Street Journal sentences with 100 words or fewer Crossdomain B3 Bagga and Baldwin 1998 results for Reconcile with its general feature set The Paired Permutation test Pesarin 2001 was used for statistical significance testing and gray cells represent results that are not significantly different from the best result B3 results for baselines and lexicalized feature sets across four domains B3 results for baselines and lexicalized feature sets on the broadcoverage ACE 2004 data set Crossdomain B3 and MUC results for Reconcile and Sieve with lexical features Gray cells represent results that are not significantly different from the best results in the column at the 005 plevel System Architecture Automatic Evaluations Biography Text Evaluations Weather Text Evaluations Weather Sentence Evaluations Biography Sentence Evaluations Counts of different misspellings of Albert Einsteins name in a web query log Modified Viterbi search stopword treatment Example of trellis of the modified Viterbi search Accuracy and recall as functions of the number of monthly query logs used to train the language model Accuracy of the proposed system on a set which contains misspelled queries that the users had reformulated Accuracy of various instantiations of the system Plate diagram of the basic model with a single feature per token the observed variable f M Z and nj are the number of word types syntactic classes z and features tokens per word type respectively Plate diagram of the extended model with T kinds of tokenlevel features f t variables and a single kind of typelevel feature morphology m Vmeasure VM and manytoone M1 results on the languages in the MULTEXTEast corpus using the gold standard number of classes shown in Table 4 BASE results use 1word context features alone or with morphology ALIGNMENTS adds alignment features reporting the average score across all possible choices of paired language and the scores under the best performing paired language in parens alone or with morphology features Final results on 25 corpora in 20 languages with the number of induced classes equal to the number of gold standard tags in all cases kmeans and SVD2 models could not produce a clustering in the Czech CoNLL corpus due its size Best published results are from Christodoulopoulos et al 2010 BergKirkpatrick et al 2010 and Lee et al 2010 The latter two papers do not report VM scores No best published results are shown for the MULTEXT languages Christodoulopoulos et al 2010 report results based on 45 tags suggesting that clark performs best on these corpora Examples of sentences x domainindependent underspecified logical forms l0 fully specified logical forms y and answers a drawn from the Freebase domain A sample CCG parse Definition of the operations used to transform the structure of the underspecified logical form l0 to match the ontology O The function typec calculates a constant cs type The function freevlf returns the set of variables that are free in lf not bound by a lambda term or quantifier The function subexpslf generates the set of all subexpressions of the lambda calculus expression lf Example derivation for the query how many people visit the public library of new york annu ally Underspecified constants are labelled with the words from the query that they are associated with for readability Constants from O written in typeset are introduced in step c Parameter estimation from QA pairs GeoQuery Results Ablation Results Results on the FQ dataset Example error cases with associated frequencies illustrating system output and gold standard references 5 of the cases were miscellaneous or otherwise difficult to categorize Tradeoff between Margin Threshold and name recognition performance Accuracy of obscure name recognition Coreference factors for name recognition System Flow Results for Mutiple Document System with additional retrieved texts Contributions of features Results for Mutiple Document System Results for Single Document System Results with Coref Rules Alone Baseline Name Tagger Comparison with voted cache Logical form graphs aligned with sur face forms in two languages Logical form graph Encoding local word order Graph for a neoDavidsonian structure DRS and corresponding DRG in tuples and in graph format for A customer did not pay From DRS to DRG labelling Wordaligned DRG for A customer did not pay All alignment information including surface tuples is highlighted Surface composition of embedded structures Wordaligned DRG for the sentence Michelle thinks that Obama smokes Analysis of NP coordination in a distributive left and a collective interpretation right Average ratings and Pearson correlation for rules from the personal stories corpus Lower ratings are better see Fig 2 Instructions for judging of unsharpened factoids Ungrammatical Arabic output of Google Trans late for the English input The car goes quickly The subject should agree with the verb in both gender and number but the verb has masculine inflection For clarity the Arabic tokens are arranged lefttoright Segmentation and tagging of the Arabic token AEJ JJ and they will write it This token has four seg ments with conflicting grammatical features For example the number feature is singular for the pronominal object and plural for the verb Our model segments the raw to ken tags each segment with a morphosyntactic class eg PronFemSg and then scores the class sequences Notation used in this paper The convention eIi indicates a subsequence of a length I sequence Breadthfirst beam search algorithm of Och and Ney 2004 Typically a hypothesis stack H is maintained for each unique source coverage set Procedure for scoring agreement for each hy pothesis generated during the search algorithm of Fig 4 In the extended hypothesis eI1 the index n 1 indicates the start of the new attachment Intrinsic evaluation accuracy development set for Arabic segmentation and tagging Translation quality results BLEU4 for newswire nw sets Avg is the weighted averaged by number of sentences of the individual test set gains All improvements are statistically significant at p 001 Features based on the token string Sources of Dictionaries Comparison of results for MUC6 Training Data Fmeasure after successive addition of each global feature group Graphical model representing M L SLDA Shaded nodes represent observations plates denote repli cation and lines show probabilistic dependencies Two methods for constructing multilingual distributions over words On the left paths to the German word wunsch in GermaNet are shown On the right paths to the English word room are shown Both English and German words are shown some internal nodes in GermaNet have been omitted for space represented by dashed lines Note that different senses are denoted by different internal paths and that internal paths are distinct from the perlanguage expression Topics along with associated regression coefficient from a learned 25topic model on GermanEnglish left and GermanChinese right documents Notice that themerelated topics have regression parameter near zero topics discussing the number of pages have negative regression parameters topics with good great hao good and uberzeugt convinced have positive regression parameters For the GermanChinese corpus note the presence of gut good in one of the negative sentiment topics showing the difficulty of learning collocations Feature templates for POS tagging wi is the ith word in the sentence ti is its POS tag For a word w cj w is its j th character cj w is the last j th character and lw is its length Parse tree binarization Unary rule normalization Nonterminalyield unary chains are collapsed to single unary rules Identity unary rules are added to spans that have no unary rule Feature templates for parsing where X can be word first and last character of word first and last character bigram of word POS tag Xla Xra denotes the firstlast ath X in the span while Xla Xra denotes the ath X leftright to span Xm is the first X of right child and Xm1 is the last X of the left child len lenl lenr denote the length of the span left child and right child respectively wl is the length of word ROOTLEAF means the template can only generate the features for the rootinitial span Complexity Analysis of Algorithm 1 Boundary information is added to states to cal culate the bracket scores in the face of word segmentation errors Left the original parse tree Right the converted parse tree The numbers in the brackets are the indices of the character boundaries based on word segmentation Training development and test data of CTB 5 Results for the joint word segmentation and POS tagging task Word segmentation results Parameters used in our system Results for the joint segmentation tagging and parsing task using pipeline and joint models POS tagging error patterns means the error number of the corresponding pattern made by the pipeline tagging model and mean the error number reduced or increased by the joint model Summary of the results obtained by our algorithm in comparison to Word 2007 Results of the fillintheblank exercise Results of the inflection exercise Solution of the multiple choice exercise Replication of the experiment with a corpus of nonnative speakers CEDEL2 Lozano 2009 The number of OAS types CAS types LUW types and EIW types for our CWS The differences of Fmeasure and ROOV between nearby steps of our CWS The scored results of our CWS in the MSRC track OOV is 0034 for 3rd bakeoff The Fmeasure improvement between the BMMbased CWS and it with WSM in the MSRC track OOV is 0034 using a b and c system dictionary Performance of the statistical approach using a trigram model based on Google Web1T Influence of the ngram model on the perfor mance of the statistical approach Performance of the knowledgebased ap proach using the JiangConrath semantic relatedness measure Performance of knowledgebased approach using different relatedness measures Results obtained by a combination of the best statistical and knowledgebased configuration Best Single is the best precision or recall obtained by a sin gle measure Union merges the detections of both approaches Intersection only detects an error if both methods agree on a detection Examples of learned pronoun probabilities EM input for our example sentence jvalues follow each lexical candidate Comparison to SVM Weights set by maximum entropy Accuracy for all cases all excluding sen tences with quotes and only sentences with quotes Language families in our data set The Other category includes 9 language isolates and 21 language family singletons Average accuracy for EM baseline and model variants across 503 languages First panel results on all languages Second panel results for 30 isolate and singleton languages Third panel results for 27 nonLatin alphabet languages Cyril lic and Greek Standard Deviations across lan guages are about 2 Plurality language families across 20 clusters The columns indicate portion of lan guages in the plurality family number of lan guages and entropy over families Inferred Dirichlet transition hyperparameters for bigram CLUST on threeway classification task with four latent clusters Row gives starting state column gives target state Size of red blobs are proportional to magnitude of corresponding hyperparameters Etrees and Derivation Trees for 2abc Data examined by the two systems for the ATB A correct tree tree1 and an incorrect tree tree2 for BCLM HNEIM indexed by terminal boundaries Erroneous nodes in the parse hypothesis are marked in italics Missing nodes from the hypothesis are marked in bold The morphological segmentation possibilities of BCLM HNEIM Doublecircles are word boundaries Quasimorphological operations Translation results for EnglishGerman Translation results for GermanEnglish Translation results for FrenchEnglish Translation results for EnglishFrench BLEU scores of English to Russian ma chine translation system evaluated on tst2012 us ing baseline GIZA alignment and translitera tion augmentedGIZA OOVTI presents the score of the system trained using TAGIZA af ter transliterating OOVs BLEU scores of English to Russian ma chine translation system evaluated on tst2012 and tst2013 using baseline GIZA alignment and transliteration augmentedGIZA alignment and postprocessed the output by transliterating OOVs Human evaluation in WMT13 is performed on TAGIZA tested on tst2013 marked with Russian to English machine translation system evaluated on tst2012 and tst2013 Human evaluation in WMT13 is performed on the system trained using the original corpus with TAGIZA for alignment marked with TreeTagger and RFTagger outputs Starred word forms are modified during preprocessing Sequence of units built from surface word forms top and POStags bottom Results for FrenchEnglish Results for GermanEnglish Example dependency tree Example coreferent paths Italicized entities generally corefer Example noncoreferent paths Italicized entities do not generally corefer Gender classification performance Example gendernumber probability ANC pronoun resolution accuracy for varying SVMthresholds Resolution accuracy A Dictionary based Word Graph Synthetic Data Set from Xinhua News Quantitative Evaluation of Common Topic Finding crosscollection loglikelihood Effectiveness of Extracting Common Topics Qualitative Evaluation Effectiveness of Latent Topic Extraction from MultiLanguage Corpus Misspellings of receive Classification of corpus token by type Contextsensitive spelling correction denotes also using 60 WSJ 5 corrupted Memorybased learner results Synonyms for chain Synonyms for home Average I NV R for 300 headwords InvR scores ranked by difference Giga word to Web Corpus Feature set for our pronoun resolution systemed feature is only for the singlecandidate model while ed feature is only for the twincandidate mode The performance of different resolution systems Results of different feature groups under the TC model for Npron resolution Computation of probabilities using the language model During training a classified instance in this case for the confusible pair then than are generated from a sentence During testing a similar instance is generated The classifier decides what the corresponding class and hence which word should be the focus word This table shows the performance achieved by the different systems shown in accuracy The Number of cases denotes the number of instances in the testset Stemmed results on 3138utterance test set Asterisked results are significantly better than the baseline p 005 using 1000 iterations of paired bootstrap resampling Koehn 2004 Rank trajectories of 4 LDA inferred topics with incremental topic inference The xaxis indicates the utterance number The yaxis indicates a topics rank at each utterance GermanEnglish translation results Results are cumulative EnglishGerman translation results results are cumulative except for the three alternative PAL configurations Examples of parallel phrases used in word alignment Summary of devtest results and shared task test results for submitted systems and LIU baseline with hier archical reordering confirms that names participating in re Baseline Word Clustering by Relation Reranking by Coreference Reranking by Relation Baseline Word Clustering by Relation Reranking by Coreference System Flow Plate diagram representation of the model ti s wi s and si s denote the tags words and segmentations respectively Gs are various DPs in the model Ej s and j s are the tagspecific emission distributions and their respective Dirichlet prior parameters H is Gamma base distribution S is the base distribution over segments Coupled DP concetrations parameters have been omitted for clarity Loglikelihood of samples plotted against iter ations Dark lines show the average over five runs grey lines in the back show the real samples Tagging part of loglikelihood plotted against Vmeasure Segmentation results on different languages Results are calculated based on word types For each language we report precision recall and F1 measure number of word types in the corpus and number of word types with gold standard segmentation available For each language we report the segmentation result without and with emission likelihood scaling without LLS and with LLS respectively Tagging results for different languages For each language we report median onetoone 11 manytoone m1 and Vmeasure Vm together with standard deviation from five runs where median is taken over Vmeasure Types is the number of word types in each corpus True is the number of gold tags and Induced reports the median number of tags induced by the model together with standard deviation Best Pub lists the best published results so far also 11 m1 and Vm in Christodoulopoulos et al 2011 Blunsom and Cohn 2011 and Lee et al 2010 Tagging and segmentation results on Estonian MultextEast corpus Learned seg and Learned tag com pared to the semisupervised setting where segmentations are fixed to gold standard Fixed seg and tags are fixed to gold standard Fixed tag Finally the segmentatation results from Morfessor system for comparison are pre sented Gibbs sampling algorithm for IBM Model 1 im plemented in the accompanying software Sizes of bilingual dictionaries induced by differ ent alignment methods Distribution of inferred alignment fertilities The four blocks of rows from top to bottom correspond to in order the total number of source tokens source tokens with fertilities in the range 47 source tokens with fertil ities higher than 7 and the maximum observed fertility The first language listed is the source in alignment Sec tion 2 BLEU scores in translation experiments E En glish T Turkish C Czech A Arabic NIL expression forms based on POS attribute NIL expression forms based on word formation Workflow for NIL knowledge engineering component NILE refers to NIL expression which is identified and annotated by human annotator Architecture of NILER system Smoothed precision curves over the five corpora Experimental results for the two methods on the five corpora PRE denotes precision REC denotes recall and F1 denotes F1Measure Smoothed recall curves over the five corpora Smoothed quality curves for SVM method over the five corpora Smoothed quality curves for PM method over the five corpora Smoothed F1Measure curves over the five corpora BLEU scores achieved with different sets of parallel corpora All systems are base line ncode with POS factor models The follow ing shorthands are used to denote corpora N stands for NewsCommentary E for Europarl C for CommonCrawl U for UN and nf for non filtered corpora BLEU scores for different configuration of factored translation models The big prefix de notes experiments with the larger context for n gram translation models BLEU scores for preordering experi ments with a ncode system and the approach pro posed by Neubig et al 2012 Histogram of token movement size ver sus its occurrences performed by the model Neu big on the source english data BLEU scores for the FrenchtoEnglish translation task measured on nt10 with systems tuned on development sets selected according to their original language adapted tuning Perplexity measured on nt08 with the baseline LM std with the LM estimated on the sampled texts generated texts and with the inter polation of both Impact of the use of sampled texts Results for development and test set for the two languages by ME1 Best results For English name lists are used For German partofspeech tags are used List of keywords used in WordNet search for generating WN CLASS features Distribution of SCs in the ACE corpus SC classification accuracies of different methods for the ACE training set and test set Results for feature ablation experiments Accuracies of singlefeature classifiers Resolution accuracies for the ACE test set Coreference results obtained via the MUC scoring program for the ACE test set Metaevaluation results at document level Metaevaluation results at system level Metaevaluation results at document and system level for submitted metrics BLEU scores when testing on the com bined test set newstest2012 PDTB 23 on PDTB section 23 only 2416 sentences 923 con nectives and when randomizing the sense tags PDTB 23 random for the BASELINE system and the two systems using PDTB connective labels SYSTEM 1 complex labels SYSTEM 2 simplified labels When testing on randomized sense labels PDTB 23 random the BLEU scores are statisti cally significantly lower than the ones on the cor rectly labeled test set PDTB 23 which is indi cated by starred values Performance of SYSTEM 2 simplified PDTB tags when manually counting for improved equal and degraded translations compared to the BASELINE in samples from the PDTB section 23 test set Translation outputs for the EN con nective as which was translated more correctly by SYSTEM 2 thanks to the disambiguating sense tags compared to the BASELINE that often just produces the prepositional as jako The erro neous translations are marked in bold The PDTB sense tags indicate the meaning of the CZ trans lations and are encoded as follows Synchrony Sy Asynchrony Asy Contingency Co Cause Ca Parallel Corpus Baseline Results Bayesian Alignment Results Development Sets Results Compound Splitting Results Tuning Results Language Model Results GermantoEnglish Final System Results GermanEnglish Official Test Submis sion English to German Final System Re sults Average of Weights Results Incremental Improvement from Selftraining English Bootstrapping for Name Tagging SelfTraining for Name Tagging Data Description English Name Tagger Chinese Name Tagger Impact of Data Size Chinese Impact of Confidence Measures Impact of Data Selection Chinese Impact of Data Size English Translation results for EnglishFrench Translation results for FrenchEnglish Translation results for GermanEnglish Translation results for EnglishGerman Gibbs Sampling Statistical Information of Corpora Experimental Procedure Features Used for Initial Distribution Ordered List of IncreasedDecreased Number of Correctly Tagged Words Results of Multiple Trials and Compari son to Simulated Annealing Results of POS Guessing of Unknown Words Example of a MUC4 template The topranking feature for each group of features and the classifier of a slot Accuracy of string slots with and without full parsing Systems whose Fmeasures are not signif icantly different from AliceME at the 010 signifi cance level with 099 confidence Accuracy of all slots on the TST3 and TST4 test set Accuracy of string slots on the TST3 and TST4 test set Layers used in our model A standard logical form derivation using CCG The NP notation means that the subject is typeraised and taking the verbphrase as an argumentso is an ab breviation of SSNP This is necessary in part to sup port a correct semantics for quantifiers Example initial lexical entries Using the type model for disambiguation in the derivation of file a suit Type distributions are shown after the variable declarations Both suit and the object of file are lexically ambiguous between different types but after the reduction only one interpretation is likely If the verb were wear a different interpretation would be preferred Most probable terms in some clusters induced by the Type Model Results on widecoverage Question Answer ing task CCGDistributional ranks questionanswer pairs by confidence250 means we evaluate the top 250 of these It is not possible to give a recall figure as the total number of correct answers in the corpus is unknown Accuracy on Section 1 of the FraCaS suite Problems are divided into those with one premise sen tence 44 and those with multiple premises 30 Example problem from the FraCaS suite Example questions correctly answered by CCGDistributional CATiB Annotation example 23 1 4 0 t ml HfydAt AlkA AlkyAt fy AlmdArs AlHkwmy The writers smart granddaughters work for public schools The words in the tree are presented in the Arabic reading direction from right to left Penn Arabic Treebank part 3 v31 data split Parsing performance with each POS tag set on gold and predicted input L AS labeled attachment accuracy dependency relation U AS unlabeled attachment accuracy dependency only L S relation label prediction accuracy L AS diff difference between labeled attachment accuracy on gold and predicted input POS acc POS tag prediction accuracy Prediction accuracy value set sizes descriptions and value examples of features used in this work Accuracy was measured over the development set The set includes a NA values CORE 12 POS tag set with morphological inflectional features Left half Using gold POS tag and feature values In it Top part All Adding all nine inflectional features to CORE 12 Second part Sep Adding each feature separately to CORE 12 Third part Greedy Greedily adding next best feature from Sep and keeping it if improving score Right half Same as left half but with predicted POS tag and feature values Statistical significance tested only on predicted nongold input against the CORE 12 baseline Models with lexical morphosemantic features Top Adding all lexical features together on top of the CORE 12 baseline Center Adding each feature separately Bottom Greedily adding best features from previous part on predicted input Statistical significance tested only on predicted nongold input against the CORE 12 baseline Models with inflectional and lexical morphological features together predicted valueguided heuristic Statistical significance tested only on predicted input against the CORE 12 baseline Models with reengineered DET and PERSON inflectional features Statistical significance tested only on predicted input against the CORE 12 baseline Models with functional features GENDER NUMBER rationality RAT F N functional features based on Alkuhlani and Habash 2011 GN GENDER NUMBER GNR GENDER NUMBER RAT Statistical significance tested only for CORE 12 models on predicted input against the CORE 12 baseline Select models trained using the EasyFirst Parser Statistical significance tested only for CORE 12 models on predicted input significance of the EasyFirst Parser CORE 12 baseline model against its MaltParser counterpart and significance of all other CORE 12 models against the EasyFirst Parser CORE 12 baseline model Alternatives to training on goldonly feature values Top Select MaltParser CORE 12 models retrained on predicted or gold predicted feature values Bottom Similar models to the top half with the EasyFirst Parser Statistical significance tested only for CORE 12 models on predicted input significance of the MaltParser models from the MaltParser CORE 12 baseline model and significance of the EasyFirst Parser models from the EasyFirst Parser CORE 12 baseline 3 7 19 6 7 Figure 2 Error analysis example 82 mrt yAm l AxtfA Alzmyl Almhnd Several days have passed since the disappearance of the colleague the engineer as parsed by the baseline system using only CORE 12 left and as using the best performing model right Bad predictions are marked with The words in the tree are presented in the Arabic reading direction from right to left Training the MaltParser on gold tags accuracy by gold attachment type selected subject object modification of a verb or a noun by a noun modification of a verb or a noun by a preposition idafa and overall results repeated Training the EasyFirst Parser on gold and predicted tags accuracy by gold attachment type selected subject object modification of a verb or a noun by a noun modification of a verb or a noun by a preposition idafa and overall results repeated Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt STTS accuracies of the TnT tagger trained on the STTS tagset the TnT tagger trained on the Tiger tagset and our tagger trained on the Tiger tagset Tagging accuracy on development data depending on context size Tagging accuracies on test data The ANNIS user interface displaying data from the PCC Training set statistics OutOfVocabulary OOV rate is regarding the development sets Example training run of a pruned 1st order model on German showing the fraction of pruned gold se quences sentences during training for training train and development sets dev POS tagging experiments with pruned and unpruned CRFs with different orders n For every language the training time in minutes TT and the POS accuracy ACC are given indicates models significantly better than CRF first line Accuracies for models with and without oracle pruning indicates models significantly worse than the oracle model Test results for POSMORPH tagging Best baseline results are underlined and the overall best results bold indicates a significant difference between the best baseline and a PCRF model Development results for POSMORPH tagging Given are training times in minutes TT and accuracies ACC Best baseline results are underlined and the overall best results bold indicates a significant difference between the best baseline and a PCRF model Test results for POS tagging Best baseline results are underlined and the overall best results bold indicates a significant difference between the best baseline and a PCRF model Development results for POS tagging Given are training times in minutes TT and accuracies ACC Best baseline results are underlined and the overall best results bold indicates a significant difference positive or negative between the best baseline and a PCRF model Statistics on the Italian EVALITA 2009 and English CoNLL 2003 corpora Three kinds of tree kernels Semantic structure of the first sequence Global features in the entity kernel for reranking These features are anchored for each entity instance and adapted to entity categories For example the entity string first feature of the entity United Nations with entity type ORG is ORG United Nations Reranking results of the three tagging kernels on the Italian and English testset Regular expression notation in foma Illustration of a worsening filter for morpheme boundaries OT grammar for devoicing compiled into an FST Violation permutation transducer Devoicing transducer compiled through a rule Example outputs of matching implementation of Finnish OT An nonregular OT approximation Illustrative tableau for a simple constraint sys tem not capturable as a regular relation BLEU scores on the NewsCommentary development test data BLEU scores on the Europarl development test data Characteristics of the parallel corpus used for experiments The role of the standard Basque Batua ana lyzer in filtering out unwanted output candidates created by the induced rule set produced by method 1 Values obtained for Precision Recall and F scores with method 1 by changing the minimum fre quency of the correspondences to construct rules for foma The rest of the options are the same in all three experiments only one rule is applied within a word Values obtained for Precision Recall and F score with method 1 by changing the threshold frequency of the correspondences and applying a postfilter Experiments with the ILP method using a thresh old of 14 times a wordpair is seen to trigger rule learn ing The figures in parentheses are the same results with the added postprocessing unigram filter that given sev eral output candidates of the standard dialect chooses the most frequent one Method 1 Exp1 frequency 2 2 rules applied in parallel without contextual conditioning Exp2 fre quency 1 1 rule applied with contextual conditioning Exp3 frequency 2 2 rules applied in parallel with con textual conditioning Tradeoffs of precision and recall values in the experiments with method 1 using various different pa rameters When the unigram filter is applied the precision is much better but the recall drops The best results per F1 score of the two meth ods The parameters of method 1 included using only those string transformations that occur at least 2 times in the training data and limiting rule application to a maxi mum of 2 times within a word and including a unigram postfilter Rules were contextually conditioned For method 2 all the examples threshold 1 in the training data were used as positive and negative evidence with out a unigram filter Tuple extraction from a sentence pair Translation results in terms of BLEU score and translation edit rate TER estimated on newstest2010 with the NIST scoring script EnglishFrench translation results in terms of BLEU score and TER estimated on newstest2010 with the NIST script All means that the translation model is trained on newscommentary Europarl and the whole GigaWord The rows upper quartile and median corre spond to the use of a filtered version of the GigaWord Architecture of the Structured Output Layer Neural Network language model Translation results from English to French and English to German measured on newstest2010 using a 100best rescoring with SOUL LMs of different orders CCG derivation and unresolved semantics for the sentence I saw nothing suspicious DRS for the sentence I saw nothing suspi cious Results of the second run with postprocessing Results of the first run without postprocessing Results of negated eventproperty detection on gold standard cue and scope annotation The System architecture1 Learning curves using different sam pling strategies Performance comparsion of the rule based robust semantic parser the reversed two stage classification system and our SLU systems TER Topic Error Rate SER Slot Error Rate Performance comparison of two SLU systems through weakly supervised and super vised training on the three test sets TER Topic Error Rate SER Slot Error Rate Learning curves of bootstrapping meth ods for semantic classification on TS1 Proposed discourse structures for Ex 4 a In terms of informational relations b in terms of inten tional relations The Buckwalter Arabic Morphological Analyzers lookup process exemplified for the word lilkitAbi Skeleton of basic lexicon transducer in LEXC generated from BAMA lexicons Overlaid bilingual embeddings English words are plotted in yellow boxes and Chinese words in green reference translations to English are provided in boxes with green borders directly below the original word Vector Matching Alignment AER lower is bet ter Results on Chinese Semantic Similarity Results on Named Entity Recognition NIST08 ChineseEnglish translation BLEU Graphical representation of our model Hyper parameters the stickiness factor and the frame and event initial and transition distributions are not shown for clar ity A partial frame learned by P RO F INDER from the MUC4 data set with the most probable emissions for each event and slot Labels are assigned by the authors for readability Results on MUC4 entity extraction CJ 2011 granularity refers to their experiment in which they mapped one of their templates to five learned clusters rather than one Results on TAC 2010 entity extraction with N best mapping for N 1 and N 5 Intermediate values of N produce intermediate results and are not shown for brevity Syntagmatic vs paradigmatic axes for words in a simple sentence Chandler 2007 Summary of results in terms of the MTO and VM scores Standard errors are given in parentheses when available Starred entries have been reported in the review paper Christodoulopoulos et al 2010 Distributional models use only the identity of the target word and its context The models on the right incorporate orthographic and morphological features MTO is not sensitive to the number of partitions used to discretize the substitute vector space within our experimental range MTO falls sharply for less than 10 SCODE dimensions but more than 25 do not help MTO is fairly stable as long as the Z constant 54 Morphological and orthographic features is within an order of magnitude of the real Z value MTO is not sensitive to the number of random substitutes sampled per word token Hinton diagram comparing most frequent tags and clusters State classification by minimum input consumed for the Finnish dictionary The sizes of error models as automata The sizes of dictionaries as automata Effect of language and error models to quality recall proportion of suggestion sets containing a cor rectly suggested word Effect of language and error models to speed time in seconds per 10000 word forms Effect of text type on error models to speed in seconds per 10000 wordforms Experimental results using different smoothing methods Experimental results using different phrase ta bles OutBp the outofdomain phrase table AdapBp the adapted phrase table Effect of indomain monolingual corpus size on translation quality CORE 12 with inflectional features predicted input Top Adding all nine features to CORE 12 Second part Adding each feature separately comparing difference from CORE 12 Third part Greedily adding best features from second part Feature prediction accuracy and set sizes The set includes a NA value Lexical features Top part Adding each feature separately difference from CORE 12 predicted Bottom part Greedily adding best features from previous part Functional features gender number rationality Inflectionallexical features together Extended inflectional features Results on unseen test set for models which performed best on dev set predicted input Results obtained by applying different types of features in isolation to the Baseline system Results obtained by adding different types of features incrementally to the Baseline system Examples errors introduced by YAGO and FrameNet Features used in baseline system Accuracy of 5fold crossvalidation with sta tisticsbased semantic features Accuracy of 5fold crossvalidation with self extracted semantic features based on different levels of syntacticsemantic relations Accuracy of 5fold crossvalidation with self extracted semantic features Translation of PCC sample commentary Screenshot of Annis Linguistic Database POS Tagging of Unknown Word using Contextual and Lexical features in a Sequential Model The input for capitalized classifier has 2 values and therefore 2 ways to create confusion k sets There are at most F different in puts for the suffix classifier 26 character 10 digits 5 other symbols therefore suffix may k emit up to R R confusion sets POS tagging of unknown words using contextual and lexical Features accuracy in per cent is based only on contextual features T is based on contextual and lexical features SM 2 denotes that follows in the sequential model 2 POS tagging of unknown words using contextual features accuracy in percent is a classifier that uses only contextual features baseline is the same classifier with the addition of the baseline feature NNP or NN Processing time for POS tagging of known words using contextual features In CPU seconds Train training time over sentences Brills learner was interrupted after 12 days of train ing default threshold was used Test average number of seconds to evaluate a single sentence All runs were done on the same machine POS Tagging of known words using con textual features accuracy in percent onevsall denotes training where example serves as positive example to the true tag and as negative example to all the other tags SM R denotes training where 2 example serves as positive example to the true tag Plate diagram depicting the joint model Hyper parameters have been omitted for clarity The Lshaped plate contains the tokens while the square plates contain the morphological analyses The t are latent tags zi is an assignment to a morphological analysis lk sk fk and wi is the observed word T is the number of distinct tags and Kt the number of tables used by tag type t Plate diagram depicting the morphology model adapted from Goldwater et al 2006 Hyperparameters have been omitted for clarity The lefthand plate depicts the base distribution P0 note that the morphological anal yses lk are generated deterministically as tk sk fk The observed words wi are also deterministic given zi k and lk since wi sk fk The posterior distribution of our joint model Because the sequence of words w is deterministic given analyses l and assignments to analyses tables z the joint posterior over all variables Pw t l zt a b s f is equal to Pt l zt a b s f when lzi wi for all i and 0 otherwise We give equations for the nonzero case ns refer to token counts ms to table counts We add two dummy tokens at the start end and between sentences to pad the context history Example sentences in the synthetic languages Words in Category 1 are made of characters ad Cate gory 2 eh Category 3 mp Category 4 ru Suffixes in Language B are separated with periods for illustrative purposes only Log probability of the sampler state over 1000 iterations on Languages A and B Spanish Ornat corpus results Standard devia tions are in parentheses denotes a significant difference from the M ORTAG model English Eve corpus results Standard deviations are in parentheses denotes a significant difference from the M ORTAG model Parse Feature Example for the sentence GM says the addition of OnStar which includes a system that automatically notifies an OnStar operator if the vehicle is involved in a collision complements the Vues top fivestar safety rating for the driver and front passenger in both front and sideimpact crash tests Training Data Sizes for Common ESL Confused Words Spelling correction accuracy impact of combining word cooccurrence CLASSIFIER Logistic Regression trained on 1G words of news text tested on 9months NYT data COMBINED SYSTEM CLASSIFER plus system based on firstorder word cooccurrence Relative increase or decrease in error rate compared to CLASSIFIER As in Bergsma et al 2009 2010 no morphological variants of the words are used in evaluation Spelling correction precision impact of adding parse features SVM trained on 1G words of news text tested on 9months of NYT data Improvement of NGLEXPAR vs NGLEX is statistically significant Improvement of NGLEXPAR vs NG is statistically significant Relative increase or decrease of error rate compared to NGLEX As in Bergsma et al 2009 2010 no morphological variants of the words are used in evaluation The semantic representations of a word W its inverse W inv and its negation W The domain part of the representation remains un changed while the value part will partially be in verted inverse or inverted and scaled negation with 0 1 The separate functional repre sentation also remains unchanged A partially scaled and inverted identity matrix J Such a matrix can be used to trans form a vector storing a domain and value repre sentation into one containing the same domain but a partially inverted value such as W and W de scribed in Figure 1 The parse tree for This car is not blue highlighting the limited scope of the negation Case restoration performance using an MDtrie English Improvement in fscore through restoring case Accuracy on seen and unseen tokens Recognition performance Final results for English and German develop ment and test sets The regular expressions available in Foma from highest to lower precedence Horizontal lines separate precedence classes A relative comparison of running a se lection of regular expressions and scripts against other finitestate toolkits The first and second en tries are short regular expressions that exhibit ex ponential behavior The second results in a FSM with 221 states and 222 arcs The others are scripts that can be run on both XeroxPARC and Foma The file lexiconlex is a LEXC format English dic tionary with 38418 entries North Sami is a large lexicon lexc file for the North Sami language available from httpdivvunno Coreference Definition Differences for MUC and ACE GPE refers to geopolitical entities Dataset characteristics including the number of documents annotated CEs coreference chains annotated CEs per chain average and number of documents in the traintest split We use st to indicate a standard traintest split Impact of Three Subtasks on Coreference Resolution Performance A score marked with a indicates that a 05 threshold was used because threshold selection from the training data resulted in an extreme version of the system ie one that places all CEs into a single coreference chain Correlations of resolution class scores with respect to the average Frequencies and scores for each resolution class Predicted P vs Observed O scores Discourse tree for two sentences in RSTDT Each of the sentences contains three EDUs The second sentence has a wellformed discourse tree but the first sentence does not have one Discourse parsing framework Distributions of six most frequent relations in intrasentential and multisentential parsing scenarios A chainstructured DCRF as our intra sentential parsing model Our parsing model applied to the sequences at different levels of a sentencelevel DT a Only possible se quence at the first level b Three possible sequences at the second level c Three possible sequences at the third level A CRF as a multisentential parsing model Features used in our parsing models Extracting subtrees for S2 Two possible DTs for three sentences Confusion matrix for relation labels on the RSTDT test set Yaxis represents true and Xaxis repre sents predicted relations The relations are TopicChange TC TopicComment TCM Textual Organization T O MannerMeans MM Comparison CMP Evaluation EV Summary SU Condition CND Enablement EN Cause CA Temporal TE Explanation EX Background BA Contrast CO Joint JO SameUnit SU Attribu tion AT and Elaboration EL Parsing results of different models using manual gold segmentation Performances significantly superior to HILDA with p71e05 are denoted by Significant differences between TSP 11 and TSP SW with p001 are denoted by RST Spanish Treebank statistics Rhetorical relations in RST Spanish Treebank Example of the nonrelation SameUnit Interannotator agreement Topic transfer in bilingual LSA model DirichletTree prior of depth two Parallel topics extracted by the bLSA model Top words on the Chinese side are translated into English for illustration purpose English word perplexity PPL on the RT04 test set using a unigram LM Comparison of training log likelihood of English LSA models bootstrapped from a Chinese LSA and from a flat monolingual English LSA Word perplexity with different using manual reference or ASR hypotheses on CCTV BLEU score for those 25 utterances which resulted in different translations after bLSA adaptation manual transcriptions Translation performance of baseline and bLSAAdapted ChineseEnglish SMT systems on manual transcriptions and 1best ASR hypotheses The clausal and topological field structure of a German sentence Notice that the subordinate clause receives its own topology a An example of a document from TuBaDZ b an abbreviated entity grid representation of it and c the feature vector representation of the abbreviated entity grid for transitions of length two Mentions of the entity Frauen are underlined nom nominative acc accusative oth dative oblique and other arguments Accuracy of the permutation de tection experiment with various entity represen tations using manual and automatic annotations of topological fields and grammatical roles The baseline without any additional annotation is un derlined Twotailed sign tests were calculated for each result against the best performing model in each column 1 p 0101 2 p 0053 statis tically significant p 005 very statistically significant p 001 Accuracy of permutation detection experiment with various entity representations us ing manual and automatic annotations of topolog ical fields and grammatical roles on subset of cor pus used by Filippova and Strube 2007a Results of adding coherence features into a natural language generation system VF Acc is the accuracy of selecting the first constituent in main clauses Acc is the percentage of per fectly ordered clauses tau is Kendalls on the constituent ordering The test set contains 2246 clauses of which 1662 are main clauses Sample pairs of similar caseframes by relation type and the similarity score assigned to them by our distributional model A sentence decomposed into its depen dency edges and the caseframes derived from those edges that we consider in black Average sentence cover size the average number of sentences needed to generate the case frames in a summary sentence Study 1 Model summaries are shown in darker bars Peer system numbers that we focus on are in bold The average number of source text sen tences needed to cover a summary sentence The model average is statistically significantly differ ent from all the other conditions p 107 Study 1 Signature caseframe densities for differ ent sets of summarizers for the initial and update guided summarization tasks Study 2 p 0005 Density of signature caseframes Study 2 Examples of signature caseframes found in Study 2 Density of signature caseframes after merging to various threshold for the initial Init and update Up summarization tasks Study 2 The effect on caseframe coverage of adding indomain and outofdomain documents The difference between adding indomain and out ofdomain text is significant p 103 Study 3 Coverage of caseframes in summaries with respect to the source text The model aver age is statistically significantly different from all the other conditions p 108 Study 3 Coverage of summary text caseframes in source text Study 3 An example of the label consistency problem Here we would like our model to encourage entities Albert Einstein and Einstein to get the same label so as to improve the chance that both are labeled PERSON Table showing the number of token sequence token subsequence pairs where the token sequence is assigned a certain entity label and the token subsequence is assigned a certain entity label We show these counts both within documents as well as over the whole corpus Rows correspond to sequences and columns to subsequences These statistics are from the CoNLL 2003 English training set Table showing the number of pairs of different occurrences of the same token sequence where one occurrence is given a certain label and the other occurrence is given a certain label We show these counts both within documents as well as over the whole corpus As we would expect most pairs of the same entity sequence are labeled the sameie the diagonal has most of the density at both the document and corpus levels These statistics are from the CoNLL 2003 English training set Table showing improvements obtained with our additional features over the baseline CRF We also compare our performance against Bunescu and Mooney 2004 and Finkel et al 2005 and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF Performance using FBIS training corpus top and NIST corpus bottom Improvements are significant at the p 005 level except where indicated ns Corpus statistics Filters to improve the dictionary precision Un less otherwise noted the filter was applied if either men tion in the relation satisfied the condition Rules of the baseline system Coreference relations in our dictionary Dataset statistics development dev and test Performance on the test set Scores are on gold mentions Stars indicate a statistically significant difference with respect to the baseline Incremental results for the four sieves using our dictionary on the development set Baseline is the Stanford system without the WordNet sieves Scores are on gold mentions a A standard PTB parse of Example 1a b The MWE part of speech functions syntactically like the ordinary nominal category as shown by this paraphrase c We incorporate the presence of the MWE into the syntactic analysis by flattening the tree dominating part of speech and introducing a new nonterminal label multiword noun MWN for the resulting span The new representation classifies an MWE according to a global syntactic type and assigns a POS to each of the internal tokens It makes no commitment to the internal syntactic structure of the MWE however Semifixed MWEs in French and English The French adverb terme in the end can be modified by a small set of adjectives and in turn some of these adjectives can be modified by an adverb such as trs very Similar restrictions appear in English French grammar development Incremental effects on grammar size and labeled F1 for each of the manual grammar features development set sentences 40 words The baseline is a parentannotated grammar The features tradeoff between maximizing two objectives overall parsing F1 and MWE F1 Unknown word model features for Arabic and French DPTSG notation For consistency we largely follow the notation of Liang Jordan and Klein 2010 Example of two conflicting sites of the same type in a training tree Define the type of a def site tz s ns0 ns1 Sites 1 and 2 have the same type because tz s1 tz s2 The two sites conflict however because the probabilities of setting bs1 and bs2 both depend on counts for the tree fragment rooted at NP Consequently sites 1 and 2 are not exchangeable The probabilities of their assignments depend on the order in which they are sampled Gross corpus statistics for the preprocessed corpora used to train and evaluate our models We compare to the WSJ section of the PTB train Sections 0221 dev Section 22 test Section 23 Due to its flat annotation style the FTB sentences have fewer constituents per sentence In the ATB morphological variation accounts for the high proportion of word types to sentences Frequency distribution of the MWE types in the ATB and FTB training sets French standard parsing experiments test set sentences 40 words FactLex uses basic POS tags predicted by the parser and morphological analyses from Morfette FactLex uses gold morphological analyses Arabic standard parsing experiments test set sentences 40 words SplitPCFG is the same grammar used in the Stanford parser but without the dependency model FactLex uses basic POS tags predicted by the parser and morphological analyses from MADA FactLex uses gold morphological analyses Berkeley and DPTSG results are the average of three independent runs French MWE identification per category and overall results test set sentences 40 words MWI and MWCL do not occur in the test set Arabic MWE identification per category and overall results test set sentences 40 words MWE identification F1 of the parsing models vs the mwetoolkit baseline test set sentences 40 words FactLex uses gold morphological analyses at test time Sample of humaninterpretable French TSG rules Sample of humaninterpretable Arabic TSG rules Recursive rules like MWAA MWA result from memoryless binarization of nary rules This preprocessing step not only increases parsing accuracy but also allows the generation of previously unseen MWEs of a given type An underspecified discourse structure and its five configurations A wRTG modelling the interdependency constraint for Fig 1 A filter RTG corresponding to Ex 2 C2 Figure 3 An underspecified d A RTG integrating the attachment constraint for Contrast from Ex 2 into Fig 3 Texts used for the evaluation Normalization accuracy after training on n tokens and evaluating on 1000 tokens average of 10 random training and evaluation sets compared to the baseline score of the full text without any normalization Tagging accuracy on the goldstandard normalizations OrigP original punctuation ModP modern punctuation NoP no punctu ation Tagging accuracy on the combined TIGERTuba corpus using 10fold CV evaluated with and without capitalization punctuation and sentence boundaries SB POS tagging accuracy on texts without punctuation and capitalization for tagging on the original data the goldstandard normalization and automatic normalizations using the first n tokens as training data Growth of the Wiktionary over the last three years showing total number of entries for all languages and for the 9 languages we consider left axis We also show the corresponding increase in average accuracy right axis achieved by our model across the 9 languages see details below Typelevel top and tokenlevel bottom cov erage for the nine languages in three versions of the Wik tionary Examples of constructing Universal POS tag sets from the Wiktionary Word type coverage by normalized frequency words are grouped by word count highest word count ratio low 0 001 medium 001 01 high 01 1 PTB vs Wiktionary type coverage across sec tions of the Brown corpus The Wiktionary vs tree bank tag sets Around 90 of the Wiktionary tag sets are identical or subsume tree bank tag sets See text for details Tag errors broken down by the word type clas sified into the six classes oov identical superset subset overlap disjoint see text for detail The largest source of error across languages are outofvocabulary oov word types followed by tag set mismatch types subset over lap disjoint Model accuracy across the Brown cor pus sections ST Stanford tagger Wik Wiktionary tagsettrained SHMMME PTBD PTBtagsettrained SHMMME PTB Supervised SHMMME Wik outper forms PTB and PTBD overall Accuracy for Unsupervised Bilingual Wiktionary and Supervised models Avg is the average of all lan guages except English Unsupervised models are trained without dictionary and use an oracle to map tags to clusters Bilingual systems are trained using a dictionary transferred from English into the target language using word align ments The Projection model uses a dictionary build directly from the partofspeech projection The DP model extends the Projection model dictionary by using Label Propagation Supervised models are trained using tree bank information with SHMMME Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary and All TBD uses tree bank tag sets for all words 50 100 and All Sent models are trained in a supervised manner using increasing numbers of training sentences Results on the Arabic Treebank ATB data set We compare our models against Poon et al 2009 PCT09 and the Morfessor system MorfessorCAT For our full model T OKEN S EG and its simplifica tions BASIC POS T OKEN POS we perform five random restarts and show the mean scores The sample standard deviations are shown in brackets The last col umn shows results of a paired ttest against the preceding model significant at 1 significant at 5 not significant test not applicable Segmentation performance on words that have the same final suffix as their preceding words The F1 scores are computed based on all boundaries within the words but the accuracies are obtained using only the final suffixes Segmentation performance on words that begin with prefix Al determiner and end with suffix At plural noun suffix The mean F1 scores are computed using all boundaries of words in this set For each word we also determine if both affixes are recovered while ig noring any other boundaries between them The other two columns report this accuracy at both the typelevel and the tokenlevel Results of the French to English system WMT2012 The marked system corresponds to the system submitted for manual evaluation cs casesensitive ci caseinsensitive Rules for morphological simplification Text normalization for FREN Processing steps for the input sentence dire warnings from pentagon over potential defence cuts Results for French inflection prediction on the WMT2012 test set The marked system corresponds to the system submitted for manual evaluation Russian to English machine translation system evaluated on WMT2012 and WMT2013 Human evaluation in WMT13 is performed on the system trained using the original corpus with TA GIZA for alignment marked with Rules for simplifying the morphological complexity for RU Results on WMT2013 blindtest Russian morphological disambiguation Evaluation of the Russian ngram model The trigram version of our language model rep resented as a graphical model G1w is the unigram model of 22 Evaluation of the Turkish ngram model Evaluation of Turkish predictive text input A complex TurkishEnglish word alignment alignment points in gray EMPYUV black PY US Word alignment experiments on EnglishTurkish entr and EnglishCzech encs data Our alignment model represented as a graphi cal model Derivation with DRSs including conversion for A record date Com binatory rules are indicated by solid lines semantic rules by dotted lines CCG derivation as generated by the CC tools Boxer output for Shared Task Text 2 A typed narrative chain The four top arguments are given The ordering O is not shown Graphical view of an unordered schema automatically built starting from the verb arrest A value that encouraged splitting was used Merging typed chains into a single unordered Narrative Schema Graphical view of an unordered schema automatically built from the verb convict Each node shape is a chain in the schema Six of the top 20 scored Narrative Schemas Events and arguments in italics were marked misaligned by FrameNet definitions indicates verbs not in FrameNet indicates verb senses not in FameNet Results with varying sizes of training data Results of the filtering experiments Comparison of Moses and KIT phrase extraction systems Analysis of context length Translation results for GermanEnglish Translation results for EnglishGerman Translation results for FrenchEnglish Translation results for EnglishFrench Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag Across all languages high performance can be attained by selecting a single tag per word type Graphical depiction of our model and summary of latent variables and parameters The typelevel tag assignments T generate features associated with word types W The tag assignments constrain the HMM emission parameters The tokens w are generated by tokenlevel tags t from an HMM parameterized by the lexicon structure The hyperparameters and represent the concentration parameters of the token and typelevel components of the model respectively They are set to fixed constants Graph of the onetoone accuracy of our full model FEATS under the best hyperparameter setting by iteration see Section 5 Performance typically stabi lizes across languages after only a few number of itera tions Multilingual Results We report tokenlevel onetoone and manytoone accuracy on a variety of languages under several experimental settings Section 5 For each language and setting we report onetoone 11 and many toone m1 accuracies For each cell the first row corresponds to the result using the best hyperparameter choice where best is defined by the 11 metric The second row represents the performance of the median hyperparameter setting Model components cascade so the row corresponding to FEATS also includes the PRIOR component see Section 3 Statistics for various corpora utilized in exper iments See Section 5 The English data comes from the WSJ portion of the Penn Treebank and the other lan guages from the training set of the CoNLLX multilin gual dependency parsing shared task Typelevel English POS Tag Ranking We list the top 5 and bottom 5 POS tags in the lexicon and the predictions of our models under the best hyperparameter setting Typelevel Results Each cell report the type level accuracy computed against the most frequent tag of each word type The statetotag mapping is obtained from the best hyperparameter setting for 11 mapping shown in Table 3 Comparison of our method FEATS to stateoftheart methods Featurebased HMM Model Berg Kirkpatrick et al 2010 The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm Posterior regulariation model Graca et al 2009 The G10 model uses the posterior regular ization approach to ensure tag sparsity constraint Results for morphological processing GermanEnglish Results for morphological processing EnglishGerman Resu EnglishGerman Resu GermanEnglish Number of affected words by OOV preprocessing Results for OOVprocessing and MBR GermanEnglish Results for OOVprocessing and MBR EnglishGerman Example of the effects of OOV processing for GermanEnglish 1 PoCoS Core Scheme Extended Scheme and languagespecific instantiations A summary of the parsing and evaluation sce narios X depicts gold information depicts unknown information to be predicted by the system Overview of participating languages and treebank properties Sents number of sentences Tokens number of raw surface forms Lex size and Avg Length are computed in terms of tagged terminals NT non terminals in constituency treebanks Dep Labels dependency labels on the arcs of dependency treebanks A more comprehensive table is available at httpwwwspmrlorgspmrl2013sharedtaskhtmlProp File formats Trees a and b are aligned constituency and dependency trees for a mockup English example Boxed labels are shared across the treebanks Figure c shows an ambiguous lattice The red part represents the yield of the gold tree For brevity we use empty feature columns but of course lattice arcs may carry any morphological features in the FEATS CoNLL format Dependency parsing LAS scores for full and 5k training sets and for gold and predicted input Results in bold show the best results per language and setting Dependency Parsing MWE results Constituent Parsing ParsEval Fscores for full and 5k training sets and for gold and predicted input Results in bold show the best results per language and setting Constituent Parsing LeafAncestor scores for full and 5k training sets and for gold and predicted input Realistic Scenario Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario Top upper part refers to constituency results the lower part refers to dependency results Realistic Scenario Tedeval Labeled Accuracy and Exact Match for the Raw scenario The upper part refers to constituency results the lower part refers to dependency results The correlation between treebank size label set size and LAS scores x treebank size labels y LAS treebank Correlation between treebank size Non sizenumber terminal labelsof sentences sent and mean F1 The correlation between the non terminals per sentence ratio and Leaf Accuracy macro scores x non terminal sentence y Acc Cross Framework Evaluation Unlabeled TedEval on generalized gold trees in gold scenario trained on 5k sentences and tested on 5k terminals CrossLanguage Evaluation Unlabeled TedEval Results in gold input scenario On a 5ksentences set set and a 5kterminals test set The upper part refers to constituency parsing and the lower part refers to dependency parsing For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics Labeled and Unlabeled TedEval Results for raw Scenarios Trained on 5k sentences and tested on 5k terminals The upper part refers to constituency parsing and the lower part refers to dependency parsing The parts of taxonomic names The Classification Process Abbreviations Comparison to Related Approaches Test results An excerpt from the text with core ferring noun phrases annotated English trans lation in italics Mention Detection Results Features and functions used in clustering algorithm Coreference Resolution Performance Contribution of individual features to overall performance Examples of the ACE Relation Types The RCM structure System Pipeline Test Procedure Chinese system performance with system mentions and system relations 4 Performance of English system with system mentions and system relations 2 Performance of English system with perfect mentions and perfect relations 3 Performance of Chinese system with perfect mentions and perfect relations Annotated RST Tree for example 4 A Generic Morphological Analyzer as a Black Box User Interface with Arabic Script Dis play in Java Mouse clicks on the virtual keyboard or key presses on the physical keyboard are inter cepted converted to Arabic Unicode characters and stored in a buffer which has a start and an end but no inherent ordering The Arabic Canvas Object observes the buffer and contains an Ara bic Scribe object that renders the string of Uni code characters righttoleft as connected Arabic glyphs Information Flow in the Xerox Arabic Demo Input words from the user interface are transmitted across the Internet dotted lines and analyzed by a server typically producing multi ple analysis strings Each analysis string is then generated in fully voweled form combined with English glosses and then reformatted as HTML before being sent back across the Internet to the users browser for display The analyzer and gen erator finitestate transducers FSTs are identical except that the lower side language of the genera tor is limited to contain only fullyvoweled words A morphological analyzergenerator can be implemented elegantly and efficiently as a finitestate transducer By Xerox convention the lowerside language consists of surface strings words and the upperside language consists of strings representing analyses of the lowerside words Such a transducer is a data structure rather than code and the runtime code that applies such a transducer to input strings in either direction is completely languageindependent Creation of a Lexical Transducer The o operator represents the composition operation A Morphophonological Rule Auxiliary Denitions Features with Set Values Subject and Object Agreement Features Verb Lexicon Specic Subject and Object Agreement Rules A Cascade of Compositions Shared Agreement Rules Rules of Referral Denition of Lingala Verbal Morphology Pseudo code of our clustering algorithm The predicates of two sentences white The company has said it plans to restate its earnings for 2000 through 2002 grey The company had announced in January that it would have to restate earnings from the Microsoft Research Paragraph Corpus are aligned by computing clusters with minimum cuts Results for sentencebased predicte alignment in the three benchmark settings MTC Leagues and MSR all numbers in results that significantly differ from Full are marked with asterisks p005 p001 Impact of removing individual measures and us ing a tuned weighting scheme all numbers in results that significantly differ from Full are marked with aster isks p005 p001 Results for GigaPairs all numbers in re sults that significantly differ from Full are marked with asterisks p005 p001 CCG and LTAG supertag sequences Experimental results with individual features compared against Moses and the moseschart baseline Experimental results with combined features Numbers of rules in Hiero or phrasepairs in Moses Source transformed and extracted trees given headline British soldier killed in Afghanistan Types of features extracted for edge e from h to n Filters applied to candidate pair H S Results for the systems and original headline and stand for significantly better than Unsupervised and Our system at 95 confidence respectively Results for two kinds of headlines Results for the unsupervised baseline and the supervised system trained on three kinds of feature sets Translation performance of EM Gibbs sampling and variational Bayes and lower translation performance Section VA after applying alignment combination within and across methods EMCo Arabic English BLEU and TER scores of various a methods EMCo GSCo EMCoGSCo and VBCo Czech English BLEU scores of various al EMCo GSCo EMCoGSCo and VBCo German English BLEU scores of various al EMCo GSCo EMCoGSCo and VBCo Distribution of alignment fertilities for source language tokens Arabic English BLEU scores o schemes in the 1Msentence translation task Intrinsic and extrinsic evaluation of alignments in the small data experiments a Alignment dictionary size normalized by the average of source and target vocabulary sizes b Average alignment fertility of aligned singletons c Percentage of unaligned singletons d Number of symmetric alignments normalized by the average of source and target tokens e Percentage of training set vocabulary covered by singleword phrases in the phrase table f Decodetime rate of input words that are in the training vocabulary but without a translation in the phrase table g Phrase table size normalized by the average of source and target tokens BLEU scores of alignments estimated at different iterations Left EM middle samples from the Gibbs chain right GS viterbi estimates with 1 Sources of conict in crosslingual subjectivity transfer Definitions and synonyms of the fourth sense of the noun argument the fourth sense of verb decide and the first sense of adjective free as provided by the English and Romanian WordNets for Romanian we also provide the manual translation into English Crosslingual bootstrapping Multilingual bootstrapping Macroaccuracy for crosslingual bootstrapping Fmeasure for the objective and subjective classes for crosslingual bootstrapping Macroaccuracy for multilingual bootstrapping versus crosslingual framework Fmeasure for the objective and subjective classes for multilingual bootstrapping versus crosslingual framework Sample of subjective features appearing in the top 100 discriminant attributes selected with Information Gain on the 3rd fold training data at iteration 10 The words in italics in the multilingual features represent equivalent translations in English and Romanian Structure and data preprocessing of the initial dataset and the cleaned one after preprocessing Schematic owchart of the workow we followed regarding the datasets the training techniques and the operations Number of tweets mentioning party leaders Example tweet assigned with feature scores Performance of the training algorithms supervised against semisupervised techniques The semisupervised precision is evaluated indirectly by using the predicted dataset as trainset against the humanannotated manual small dataset Ironic tweets that received every party and their election results The uctuation describes the difference between the May 2012 election results and the previous Performance measures of the training algorithms Green indicates the best performance while red the worst Examples of ambiguity for the English word play together with different translations depending on the context Related research integrating context into wordbased SMT WBSMT models Related research integrating context into alternative SMT models Related research using English as source language Example of CCG supertags CCG supertags are combined under the operations of forward and backward applications into a parse tree Example of LTAG supertags LTAG supertags are combined under the operations of substitution and adjunction into a parse tree The dependency parse tree of the English sentence Can you play my favourite old record and the dependency features extracted from it for the SMT phrase play my favourite The semantic graph of an English sentence and the semantic features extracted from it for an SMT phrase Some of the possible Spanish translations of the English phrase make with their memorybased con textdependent translation probabilities rightmost column compared against contextindependent transla tion probabilities of the baseline system Weights of different loglinear features of the CCG1 system Experiments with words and partsofspeech as contextual features Experiments with dependency relations Experiments combining dependency relations words and partofspeech Distances found between phrase boundaries with linked modifier words and with parent words Experiments applying individual features in EnglishtoHindi translation Experiments applying combinations of features in EnglishtoHindi translation Experimental results on the WMT 2010 test set Experimental results on the WMT 2009 test set Results on largescale DutchtoEnglish translation Results on EnglishtoDutch translation employing homogeneous features Experimental results for largescale EnglishtoChinese translation Experimental results for largescale EnglishtoJapanese translation BLEU learning curves left and difference curves right comparing the Moses baseline against two IGTree LTAG1 and PR and TRIBL SuperPair1 and PR classifiers Average number of target phrase distribution sizes for source phrases for TRIBL and IGTree com pared to the Moses baseline BLEU difference curves of four contextinformed models using TRIBL DutchtoEnglish Learning curves lefthand side graphs and difference curves righthand side graphs comparing the Moses baseline against four contextinformed models PR OE POS2 and Word2 These curves are plotted with scores obtained using three evaluation metrics BLEU top METEOR centre and TER bottom EnglishtoDutch Learning curves lefthand side graphs and difference curves righthand side graphs comparing the Moses baseline against four contextinformed models CCG1 LTAG1 PR PSAL POS2 and Word2 These curves are plotted with scores obtained using three evaluation met rics BLEU top METEOR centre and TER bottom Sample seeds used for each semantic relation and sample outputs from Espresso The number in the parentheses for each relation denotes the total number of seeds System performance on the production relation on the CHEM dataset System performance on the reaction relation on the CHEM dataset System performance on the isa relation on the TREC9 dataset System performance on the isa relation on the CHEM dataset System performance on the partof relation on the TREC9 dataset System performance on the partof relation on the CHEM dataset System performance on the succession relation on the TREC9 dataset Overall steps of proposed method The result of preprocessing Pseudocode to extract UW Weightmeasure of cooccurring words Pseudocode to extract UW Extracted UW and noun set Result of normalization Related noun group and sum of Bayesian Weight of cooccurring words TercomTERs of invWERoracles and in paren theses oracle BLEU scores of confusion networks gen erated with tercom and ITG alignments The best results per row are shown in bold Comparison of average perdocument ter comTER with invWER on the EVAL07 GALE Newswire NW and Weblogs WB data sets Nave FSA with duplicated paths FSA for the pattern hit a e Overgenerating FSA 4tape representation for the Hebrew word htpqdut FSRA for the pattern hit a e FSRA2 for Arabic nominative definite and indefinite nouns FSRA for a given CNF formula Participleforming combinations in German Interdigitation FSRA general Reduplication for n 4 Reduplication general case FSRA for Arabic nominative definite nouns Time comparison between FSAs and FSRAs Space comparison between FSAs and FSRAs Four of the five logically possible schemes for annotating coordination show up in humanproduced depen dency treebanks The other possibility is a reverse Melcuk scheme These treebanks also differ on other conventions With the English tree and alignment provided by a parser and aligner at test time the Chinese parser finds the correct dependencies see 6 A monolingual parsers incor rect edges are shown with dashed lines Precision and recall of direct dependency projection via onetoone links alone Adapting a parser to a new annotation style We learn to parse in a target style wide column label given some number narrow column label of supervised targetstyle training sentences As a font of additional features all training and test sentences have already been augmented with parses in some source style row label either goldstandard parses an oracle experiment or else the output of a parser trained on 18k source trees more realistic If we have 0 training sentences we simply output the sourcestyle parse But with 10 or 100 targetstyle training sentences each offdiagonal block learns to adapt mostly closing the gap with the diagonal block in the same column In the diagonal blocks source and target styles match and the QG parser degrades performance when acting as a stacked parser Test accuracy with unsupervised training methods Parser projection with target trees Using the true or 1best parse trees in the source language is equivalent to having twice as much data in the target language Note that the penalty for using automatic alignments instead of gold alignments is negligible in fact using Source text alone is often higher than Gold alignments Using gold source trees however significantly outperforms using 1best source trees BLEU scores for each translation direction trained on directional condition on target and generate source and symmetrised alignments growdiagfinaland Observe that the plots are on different scales This means that results cannot directly be compared across plots Properties of the manually aligned corpus Examples of output of the phrasebased and syntaxbased systems Distribution of generated paraphrases per Lev enshtein distance NIST scores per Levenshtein distance Illustration of Pareto Frontier Ten hypotheses are plotted by their scores in two metrics Hypotheses indicated by a circle o are paretooptimal while those indicated by a plus are not The line shows the convex hull which attains only a subset of paretooptimal points The triangle 4 is a point that is weakly paretooptimal but not paretooptimal Task characteristics sentences in TrainDev of features and metrics used Our MT models are trained with standard phrasebased Moses software Koehn and others 2007 with IBM M4 alignments 4gram SRILM lexical ordering for PubMed and distance ordering for the NIST system The decoder generates 50best lists each iteration We use SVMRank Joachims 2006 as opti mization subroutine for PRO which efficiently handle all pairwise samples without the need for sampling PubMed Results The curve represents the Pareto Frontier of all results collected after multiple runs NIST Results Average number of Pareto points Training time usage in PMOPRO Algo 2 Avg runtime per sentence of FindPareto Learning Curve on RIBES comparing single objective optimization and PMO Fuzzy hierarchical clustering for Paraphrase Extraction Collecting paraphrases using a Paraphrase Recognizer Algorithm for Fuzzy Agglomerative Clustering based on verbs a Binary merging of clusters b Merging of multiple clusters Simplied Lesk algorithm 21 Example of fuzzy divisive clustering Algorithm for fuzzy divisive clustering based on nouns Performance of Paraphrase Recognition system on MSRPC 25 Dependency parse and triples for the sentence Mr Burke said it was a textbook landing considering the circumstances Statistics of paraphrase pairs retrieved from MSRPC Precision of existing and proposed approaches Relative recall evaluation Comparative performance on MSRPC Performance of proposed system on MSRVDC Dataset 1 Performance evaluation of proposed and existing systems Performance of existing approaches on MSRVDC Dataset 1 Performance evaluation on MSRVDC Dataset 2 Correlation between cohesiondriven functions Gibbs sampling for word alignment Performance of Final Translation BLEU4 Performance of Word Alignment Distribution of annotated data Results of UniGraph BiGraph and Bi Graph Results of the gloss classifier Comparison to onlySL and onlyGraph Results of an iterative approach Accuracy and frequency of the top 5 for each iteration Part of a sample headline cluster with subclusters Examples of correct above and incorrect below alignments Precision and Recall for both methods Structure of a threepass machine translation system with the new regeneration pass The original Nbest translations list N best1 is expanded to generate a new Nbest translations list Nbest2 before the rescoring pass Structure of a typical twopass ma chine translation system Nbest translations are generated by the decoder and the 1best transla tion is returned after rescored with additional feature functions Example of original hypotheses and 3 grams collected from them New generated hypotheses through n gram expansion and one reference Expanding a partial hypothesis via a matching ngram Example of creating a confusion net work from the word alignments and new hy potheses generated through the confusion net work The sentence in bold is the alignment ref erence Statistics of training development and test data for NIST task Statistics of training development and test data for IWSLT task Translations output by system RESC2 and COMB on IWSLT task caseinsensitive Translation performances BLEU and NIST scores of NIST task decoder 1best rescoring on original 2400 Nbest RESC1 and 4000 Nbest hypotheses RESC2 redecoding RD ngram expansion NE confusion network CN and combination of all hypotheses COMB Number of translations generated by each method in the final translation output of system COMB decoder Orig redecoding RD ngram expansion NE and confusion network CN Tot is the size of the devtest set Similarity graph after its sparsification Data about our evaluation corpora Evaluation of topic segmentation for the French corpus Pk and WD as percentages Evaluation of topic identification Evaluation of topic segmentation for the English corpus Pk and WD as percentages Morphological AnalysisGeneration as a Relation between Analyses and Words Aymara utamankapxasamachiwa it appears that they are in your house Inuktitut Parimunngaujumaniralauqsimanngittunga I never said I wanted to go to Paris Multiple Analyses for suis Compilation of a Regular Expression into an fst that Maps between Two Regular Languages Creation of a Lexical Transducer A Path in a Transducer for English After the Application of Compile Replace Networks Illustrating Steps 2 and 3 of the CompileReplace Algorithm A Network with a RegularExpression Substring on the Lower Side Two Paths in the Initial Malay Transducer Defined via Concatenation The Malay fst After the Application of CompileReplace to the LowerSide Language A Template Network and Two Filler Networks Intermediate Result Initial paths After Applying CompileReplace to the Lower Side A translation forest which is the running example throughout this paper The reference translation is the gunman was killed by the police 1 Solid hyperedges denote a reference derivation tree t1 which exactly yields the reference translation 2 Replacing e3 in t1 with e4 results a competing nonreference derivation t2 which fails to swap the order of X34 3 Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 Generally this is done by deleting a node X01 Lexicalize and generalize operators over t1 part in Figure 1 Although here only shows the nodes we also need to change relative edges actually 1 Applying lexicalize operator on the nonterminal node X01 in a results a new derivation shown in b 2 When visiting bei in b the generalize operator changes the derivation into c Corpus statistics of Chinese side where Sent Avg Lon and Len are short for sentence longest average and length respectively RT RAIN denotes the reachable given rule table without added rules subset of T RAIN data Effect of our method comparing with MERT and perceptron in terms of B LEU We also compare our fast generation method with different data only reachable or full data Data is the size of data for training the feature weights means significantly Koehn 2004 better than MERT p 001 gooda15 gloss and examples gooda15 SentiWN scores Jumping POS in WordNet Total number of synsets classified by sentiment Results for Positive and Negative Classes Accuracy Trends on MicroWnOp Corpus Measuring Accuracy Weights learned for discount features Nega tive weights indicate bonuses positive weights indicate penalties Adding new features with MIRA significantly improves translation accuracy Scores are caseinsensitive IBM B scores or significantly better than MERT baseline p 005 or 001 respectively Weights learned for inserting target English words with rules that lack Chinese words Weights learned for employing rules whose En glish sides are rooted at particular syntactic categories Weights learned for generating syntactic nodes of various types anywhere in the English translation Using over 10000 wordcontext features leads to overfitting but its detrimental effects are modest Scores on the tuning set were obtained from the 1best output of the online learning algorithm whereas scores on the test set were obtained using averaged weights Weights learned for wordcontext features which fire when English word e is generated aligned to Chinese word f with Chinese word f1 to the left or f1 to the right Glosses for Chinese words are not part of features Improved syntaxbased translations due to MIRAtrained weights Comparison of a confusion network and a lat tice An example of alignment units Different cases of null insertion A toy instance of lattice construction Results on the MT02 and MT05 test sets Results on the MT06 and MT08 test sets Effect of dictionary scale A real translation example Effect of semantic alignments Distribution of Class Labels in the WSJ Section of the Penn TreeBank Accuracies for Different Context Types and Sizes Accuracies for WordExtraction Us Litkowski and Hargraves 2007 selected exam ing MALT Parser or Heuristics ples based on a search for governors8 most anno Accuracies for LeaveOne Out LOO and OnlyOne WordExtractionRule Evaluation none includes all words and serves for comparison Important words reduce accuracy for LOO but rank high when used as only rule Accuracies for Coarse and FineGrained PSD Using MALT and Heuristics Sorted by preposition Precision Recall and F1 Results for CoarseGrained Classification Comparison to OHara and Wiebe 2009 Classes ordered by frequency Terminologies Illustration of the paraphrase degree calculation Illustration of the coordinate degree calculation Another example of some discovered paraphrases Performance of our method for paraphrase acquisition An example of some discovered paraphrases Input sentences Subparts and features extracts from the Akkadian project Plan to the experiments described in this paper A multiword expression in HeiST Filtering out objective phrases HeiST baseline crosslingual projection SVM Comparison figures on subsets of the Stanford Sentiment Treebank Lexiconbased phrase labeling Incorporating additional information Rule types in SSTb and HeiST Precision of rules with nonneutral parent label ID daughters and parent have identical labels This loglog plot shows that there are many rare features and few common features The probability that a feature occurs in x number of N best lists behaves according to the powerlaw x where 228 Feature growth rate For Nbest list i in the table we have NewFt number of new fea tures introduced since Nbest i 1 SoFar Total number of features defined so far and Ac tive number of active features for Nbest i Eg we extracted 7535 new features from Nbest 2 combined with the 3900 from Nbest 1 the total features so far is 11435 BLEU difference of 1000 bootstrap sam ples 95 confidence interval is 15 90 The proposed approach therefore seems to be a stable method Results for different feature sets with corresponding feature size and traintest BLEUPER All multitask features give statistically significant improvements over the baselines boldfaced eg Shared Subspace 291 BLEU vs Baseline 286 BLEU Combinations of multitask features with high frequency features also give significant improvements over the high frequency features alone Additional features designed to improve model of longrange reordering Effect of different sets of reference translations used during tuning Effect of discriminatively learned penalties for OOV words Effect of supplementing recasing model training data with the test set source Part of a sample headline cluster with aligned paraphrases Examples of generated paraphrased head lines Correlations between human judge ments and automatic evaluation metrics for vari ous edit distances Automatic evaluation and sentence Levenshtein scores Ten relation instances extracted by our system that did not appear in Freebase The 23 largest Freebase relations we use with their size and an instance of each relation Dependency parse with dependency path from Edwin Hubble to Marshfield highlighted in boldface Features for Astronomer Edwin Hubble was born in Marshfield Missouri Examples of highweight features for several relations Key SYN syntactic feature LEX lexical feature x reversed NE named entity tag of entity Automatic evaluation with 50 of Freebase relation data held out and 50 used in training on the 102 largest relations we use Precision for three different feature sets lexical features syntactic features and both is reported at recall levels from 10 to 100000 At the 100000 recall level we classify most of the instances into three relations 60 as locationcontains 13 as personplaceofbirth and 10 as personnationality Estimated precision on humanevaluation experiments of the highestranked 100 and 1000 results per relation using stratified samples Average gives the mean precision of the 10 relations Key Syn syntactic features only Lex lexical features only We use stratified samples because of the overabundance of locationcontains instances among our highconfidence results Number of Sentences for bilingual training de velopment and test and monolingual forum data sets Few examples of the untranslatable tokens in forum posts Evaluation results for all combinations of mixture adapted language and translation models Baselinebl scores are italicized best scores are in bold Bayesian network and are vectors of hy perparameters and i for i 1 nc and are distributions u is a vector of underlying forms generated from and si for i nu is a set of observed surface forms generated from the hidden variable ui according to i Sample dataset constructed by hand Finnish verbs with inflection for person and number Posterior likelihood at each of the first 100 iter ations from 4 runs with different random seeds on 10 of the Morphochallenge dataset i6j 0001 ij 100 01 indicating convergence within the first 15 iterations Accuracy of underlying segment hypotheses Resampling probabilities for alternations after 1000 iterations Parse likelihood Nave FSA with duplicated paths Overgenerating FSA 4tape representation for the Hebrew word htpqdut FSRA2 for Arabic nominative definite and indefinite nouns Interdigitation FSRA general Reduplication for n 4 Time comparison between FSAs and FSRAs Space comparison between FSAs and FSRAs Optimized TERp Edit Costs Optimization Test Set Pearson Correlation Results Average Metric Rank in NIST Metrics MATR 2008 Official Results MT06 Dev Optimization Test Set Spearman Correlation Results Optimized Edit Costs Results on the NIST MATR 2008 test set for several variations of paraphrase usage The Ensemble Semantics framework for information extraction Feature space describing each candidate instance S indicates the set of seeds for a given class Average precision AP and coverage Cov results for our proposed system ESall and the baselines indicates AP statistical signifi cance at the 095 level wrt all baselines Number of extracted instances and the sample sizes P and N indicate positive and neg ative annotations Overall AP results of the different feature configurations compared to two baselines in dicates statistical significance at the 095 level wrt B3 indicates statistical significance at 095 level wrt both B3 and B4 Precision at rank for the different sys tems on the Athletes class Ablation study of the web w query log q and table t features bold letters indicate whole feature families Listing of all seeds used for KEdis and KEpat as well as the top10 entities discovered by ESall on one of our test folds Example consensus network with votes on word arcs Three confusion networks with prior prob abilities Mixedcase TER and BLEU and lower case METEOR scores on Arabic NIST MT05 Mixedcase TER and BLEU and lowercase METEOR scores on Arabic NIST MT03MT04 Mixedcase TER and BLEU and lower case METEOR scores on Chinese NIST MT05 Mixedcase TER and BLEU and lowercase METEOR scores on Chinese NIST MT03MT04 Terminologies An example for grouped entity tuples Entity tuples in big frame are those suitable for the template X direct Y whereas entity tuples in small frame are those held the same relation A realworld situation An example of the mutual reinforcement between P rparati tj and P rcoordek eg Input sentences Performance of our method for paraphrase acquisition Another example of some discovered paraphrases An example of some discovered paraphrases Structure of a term in the original documents Total documents Modified query Results Evaluation A BCN x Baseline WCN x Baseline Topic 141 Results Evaluation B Topics with MWEs Ranking for Topic 141 CN Ranking for Topic 141 Baseline Table 1 Data sets used for our alignment quality experiments The total number of sentences in the respective corpora are given along with the number of sentences and goldstandard Sure and Possible alignment links in the corresponding test set Results of our alignment quality experiments All timing and accuracy figures use means from five independently initial ized runs Note that lower is better for AER higher is better for F05 All experiments are run on a system with two Intel Xeon E5645 CPUs running at 24 GHz in total 12 physical 24 virtual cores shows the result of varying the number of samplers and iterations for all Data used for training SMT models all counts in millions Parallel data sets refer to the bitexts aligned to English and their token counts include both languages Table 5 Timings from the word alignments for our SMT evaluation The values are averaged over both alignment directions For these experiments we used systems with 8core Intel E52670 processors running at 26 GHz Results from our SMT evaluation The BLEU scores are the maximum over the Moses parameters explored for the given word alignment conguration Upper triangle of the sentencesimilarity matrix An example of a hierarchical cluster tree The porposed TSHAC algorithm An example confusion network construc tion An example packed forest representing hy potheses in Figure 1a The deductive system for Earleys genera tion algorithm WMT10 system combination tuningtesting data Oracle lowercase BLEU Translation results in lowercase BLEU CN for confusion network and CF for confusion forest with different vertical v and horizontal h Markovization order Average minmax hypothesis length pro ducible by each method h 1 for CF Hypegraph size measured by the average number of hyperedges h 1 for CF lattice is the average number of edges in the original CN Aligned parsed sentence GermanEnglish results for hierarchical and syntactic models in BLEU Training tuning and test conditions Example input and best output found Reachability of 1000 training sentences can they be translated with the model Source span lengths Chunked sentence Derivation with Hierarchical model EnglishGerman results in BLEU Effect on BLEU of varying number of nonterminals Derivation with soft syntax model Chunk Length and count of glue rules used decoding test set Translated chunked sentence Proportion of OOV words in some corpora used for real world applications Numbers in parentheses exclude words whose first letters are capitalized because they are likely to refer to named entities Examples of positive and negative words Accuracy for SOPMI with different data set sizes the spin model the label propagation model and the random walks model for 10fold crossvalidation and 14 seeds Accuracy for adjectives only for the spin model the bootstrap method and the random walk model The effect of varying the number of samples k on accuracy The effect of varying the maximum number of steps m on accuracy k 1000 Accuracy for words with high confidence measure The effect of varying the number of seeds on accuracy Accuracy for three classes on a general purpose list of 2000 words Accuracy of foreign word polarity identification Accuracy of different methods in predicting OOV words polarity The effect of varying the number of extracted related words on accuracy Overview of experiments applying WSMs to determine semantic compositionality of word expressions BNC British National Corpus GR grammatical relations GNC German newspaper corpus TREC TREC corpus SY substitutabilitybased methods CT componentbased methods CTn componentbased methods comparing WSM neighbors of expressions and their components CY compositionalitybased methods NVAP c noun verb adjective adverb combinations NN nounnoun VP verbparticles AN adjectivenoun VO verbobject SV subjectverb PV phrasalverb PNV prepositionnounverb dicts dictionaries of idioms WN Wordnet MA use of manually annotated data S Spearman correlation PC Pearson correlation CR Spearman and Kendall correlations APD average point difference CL classification PR PrecisionRecall PRc PrecisionRecall curves Fm F measure R2 goodness A sample of manually annotated expressions from DiscoEnGold with their numerical scores Ns and coarse scores Cs The values of AP Spearman and Kendall correlations between the LSAbased and PMIbased model respectively and the Gold data with regards to the expression type Every zero value in the table corresponds to the theoretically achieved mean value of correlation calculated from the infinite number of correlation values between the ranking of scores assigned by the annotators and the rankings of scores being obtained by a random number genarator ReddyWSM stands for the best performing WSM in the DISCO task Reddy et al 2011b StatMix stands for the best performing system based upon association measures Chakraborty et al 2011 Only All and All are available for the models explored by Reddy et al 2011b and Chakraborty et al 2011 Smoothed graphs depicting the dependency of Precision upon Recall using the LSA and PMIbased models ordering the expressions in TrainValD left and TestD right according to their noncompositionality Smoothed graphs depicting the dependency of Precision left and Recall right upon the nBest selected noncompositional candidates from the ordered list of expressions in TestD created by the LSA and PMIbased models Sources of conflict in crosslingual subjectivity transfer Definitions and synonyms of the fourth sense of the noun argument the fourth sense of verb decide and the first sense of adjective free as provided by the English and Romanian WordNets for Romanian we also provide the manual translation into English Crosslingual bootstrapping Multilingual bootstrapping Macroaccuracy for crosslingual bootstrapping Fmeasure for the objective and subjective classes for crosslingual bootstrapping Fmeasure for the objective and subjective classes for multilingual bootstrapping versus crosslingual framework Macroaccuracy for multilingual bootstrapping versus crosslingual framework Sample of subjective features appearing in the top 100 discriminant attributes selected with Information Gain on the 3rd fold training data at iteration 10 The words in italics in the multilingual features represent equivalent translations in English and Romanian MT system combination Each 1best outputs are aligned to create as many Confusion Networks which are connected together to form a lattice This lattice is then decoded with a tokenpass decoder using a Language Model to produce 1best andor nbest hypotheses Incremental alignment with TERp resulting in a confusion network Results of system combination on Dev7 development corpus and Test09 the ocial test corpus of IWSLT09 evaluation campaign each line of the 6best translations and BLEU scores with 1best translation selected by the current param eter This shows the shapes of BLEU and 1slack SVM objective function for one parameter These lines were calculated by 800 development sentences randomly selected from dev06 for development data when the hyperparameter Q is fixed 10000 BLEU scores on the test08 and news08 test data obtained by models trained by MERT and SVM Tuninig test for hyperparameter Q of structural SVM fixed 10 by increasing it The average improvements of BLEU scores on the test08 and news08 outofdomain when we trained the paramenters using only 400 development sentences with MERT and SVMbased algorithms four times BLEU scores of two open test sets obtained when training by MERT SslackSVM and 1slackSVM using four development sets containing 400 sentences randomly se lecting from WMT08 dev2006 BLEU scores as a function of development data size The polarity classification positive and negative based on product aspect framework Result for microblog classification Cell phone experiment result 17 aspects Comparison of the news and reports corpora Words with the highest association scores in decreasing order for the word cigarette as extracted automatically Integration of confidence measures recallprecision curves figures in the legend correspond to resp 1 and 2 Integration of paradigmatic relations recallprecision curves Integration of semantic relations news corpus best F1measures Integration of confidence measures and interpolation recallprecision curves Interpolation recallprecision curves Best F1measure values for all possible combination Schematic of our proposed method English MWEs and their components with their translation in Persian Direct matches between the trans lation of a MWE and its components are shown in bold partial matches are underlined The 10 best languages for R EDDY using LCS The 10 best languages for the verb component of BANNARD using LCS The 10 best languages for the particle compo nent of BANNARD using LCS Results for the classification task S TRING S IM MEAN is our method using Mean for f1 Correlation after combining Reddy et als method and our method with Mean for f1 S TRING S IM MEAN The correlation using Reddy et als method is 0714 Most frequent phrase dependencies with at least 2 words in one of the phrases dependencies in which one phrase is entirely punctuation are not shown indicates the root of the tree Key notation Most probable child phrases for the parent phrase made up for each direction sorted by the con ditional probability of the child phrase given the parent phrase and direction Stringtotree configurations each is associated with a feature that counts its occurrences in a derivation UrduEnglish Results BLEU ChineseEnglish Results BLEU a Moses translation output along with and a An English gloss is shown above the Chinese sentence and above the gloss is shown the dependency parse from the Stanford parser b QPDG system output with additional structure c reference translations Average feature values across best translations of sentences in the MT03 tuning set both before MERT column 2 and after column 3 Same versions of tree totree configuration features are shown the rarer swap features showed a similar trend Results when using unsupervised dependency parsers Cells contain averaged BLEU on the three test sets and BLEU on tuning data MT03 in parentheses Examples of SMT errors due to MWEs Framework for MWE acquisition from corpora MWE acquisition applied to lexicography Evaluation of translation of phrasal verbs in test set Network after pairwise TER alignment Network after incremental TER alignment Results on the Arabic GALE Phase 2 evaluation set with one reference translation Results on the Arabic GALE Phase 2 system combination tuning set with four reference translations NIST BLEU scores on the GermanEnglish de en and FrenchEnglish fren Europarl test2008 set Features used by the polyglot ranking system 0 Figure 1 The computation of DKL Pve i kPe0i using a toy corpus for e looking forward to Note that the sec Particles and prepositions allowed in phrasal verbs gathered from Wiktionary Our boosted ranker combining monolingual and bilingual features bottom compared to three base lines top gives comparable performance to the human curated upper bound The solid line shows recallat1220 when com bining the k bestperforming bilingual statistics and three monolingual statistics The dotted line shows the indi vidual performance of the kth bestperforming bilingual statistic when applied in isolation to rank candidates An ablation of monolingual statistics shows that they are useful in addition to the 50 bilingual statistics combined and no single statistic provides maximal per formance The highest ranked phrasal verb candidates from our full system that do not appear in either Wiktionary set Candidates are presented in decreasing rank pat on is the second highest ranked candidate Composition Gold Standards Sample of Gold Standard entries Results tested against gsso Diff results tested against gsswaco Results tested against gsswacosubjective LL results tested against gsswaco Detailed DIFF results Glue Semantics proof for 80 Swedish Directed Motion Construction Glue Semantics proof for 83 English Way Construction means interpretation Glue Semantics proof for 83 English Way Construction manner interpretation Glue Semantics proof for 86 English Way Construction means interpretation Bilingual training size vs BLEU score mid dle line left axis and phrase table composition top line right axis on Arabic Development Set The baseline BLEU score bottom line is included for comparison AER results IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM We choose t 1 5 and 30 for the fertility HMM AER comparison encn Training time comparison The training time for each model is calculated from scratch For example the training time of IBM Model 4 includes the training time of IBM Model 1 the HMM and IBM Model 3 AER comparison cn en BLEU results Segmentation algorithm Known topic changes found in 90 generated texts using a block size of six Position of news story boundaries in a CNN news summary in relation to troughs found by the algorithm CNN news summaries segmented using different block sizes Example candidate space of dimensionality 2 Note I 1 2 J1 J2 1 2 3 We also show a local scoring function hw i j where w 2 1 and a local gold scoring function gi j Result of synthetic data learning experiment for MERT and PRO with and without added noise As the dimensionality increases MERT is unable to learn the original weights but PRO still performs adequately Machine translation performance for the experiments listed in this paper Scores are casesensitive IBM B LEU For every choice of system language pair and feature set PRO performs comparably with the other methods Data sizes for the experiments reported in this paper English words shown Summary of features used in experiments in this paper Comparison of MERT PRO and MIRA on tuning UrduEnglish SBMT systems and test results at every iteration PRO performs comparably to MERT and MIRA Tune and test curves of five repetitions of the same UrduEnglish PBMT baseline feature experiment PRO is more stable than MERT Comparison of SWSD systems SO classifier with and without SWSD NP classifier with and without SWSD SO classifier with learned SWSD integration NP classifier with learned SWSD integration Polarity classifier with and without SWSD Example of a long jump alignment grid All possible deletion insertion identity and substitution op erations are depicted Only long jump edges from the best path are drawn Corpus statistics of the MATR MT06 corpus that was used for experimental evaluation of the proposed measures Example of worddependent substitution costs Pearsons r and Kendalls absolute between adequacy and automatic evaluation measures on different levels of the MATR MT06 data Examples of TERp alignment output In each example R H and H denote the reference the original hypothesis and the hypothesis after shifting respectively Shifted words are bolded and other edits are in brackets Number of edits shown TERp TER Metric correlations with adequacy on the MetricsMATR 2008 development set Correlations are significantly different if the center point of one correlation does not lie within the confidence interval of the other correlation TERp edit costs optimized for adequacy Pearson correlation of TERp with selective features Numbers of expressions of all the differ ent types from the DISCO and Reddy datasets All the parameters of Measures for de termining semantic compositionality described in Section 3 used in our experiments All the parameters of WSMs described in Section 2 used in all our experiments Semicolon denotes OR All the examined combinations of parameters are implied from reading the diagram from left to right Parameters of WSMs Section 2 which combined with particular Measures achieved the highest average correlation in TrValD Parameters of Measures Section 3 which combined with particular WSMs achieved the highest average correlation in TrValD Selection and weighting of words from the collocation network Automaton for topic shift detection Pk for C99 corpus Pk for Le Monde corpus Precisionrecall for Le Monde corpus Error rates for Le Monde corpus a RM and large margin solution comparison and b the spread of the projections given by each RM and large margin solutions are shown with a darker dotted line and a darker solid line respectively RM update with margin and bounding con straints The diagonal dotted line depicts costmargin equi librium The vertical gray dotted line depicts the bound B White arrows indicate updates triggered by constraint viola tions Squares are data points in the kbest list not selected for update in this round Corpus statistics Active sparse feature templates Performance on ArEn with basic left and sparse right feature sets on MT05 and MT08 RM gain over other optimizers averaged over all test sets Performance on ZhEn with basic left and sparse right feature sets on MT03 and MT05 Notation used in this article Examples of word alignment patterns in GermanEnglish that require the increased expressive power of synchronous tree adjoining grammar Examples of dependency trees with word alignment Arrows are drawn from children to parents A child word is a modifier of its parent Each word has exactly one parent and is a special wall symbol that serves as the parent of all root words in the tree ie those with no other parent Example of a sentence pair containing a frequentlyobserved sibling relationship in GermanEnglish data in the theus dependency the aligned German words are siblings in the source dependency tree This occurs due to differences in treebank and head rule conventions between the two data sets The German parser produces flat PPs with little internal structure so when the dependency tree is generated each word in the PP attaches to the P the head of the phrase Key definitions for our model Example output of our model for ChineseEnglish translation The wordsegmented Chinese sentence and dependency tree are inputs Our models outputs include the English translation phrase segmentations for each sentence a box surrounds each phrase a onetoone alignment between the English and Chinese phrases and a projective dependency tree on the English phrases Note that the Chinese dependency tree is on words whereas the English dependency tree is on phrases Most frequent phrase dependencies in DEEN data shown with their counts and attachment directions Child phrases point to their parents To focus on interesting phrase dependencies we only show those in which one phrase has at least two tokens and neither phrase is entirely punctuation The words forming the longest lexical dependency in each extracted phrase dependency are shown in bold these are used for backoff features Top 60 most frequent root phrases in DEEN data with at least two words shown with their counts Shown in bold are the actual root words in the lexical dependency trees from which these phrases were extracted these are extracted along with the phrases and used for backoff features Most frequent Brown cluster phrase dependencies extracted from DEEN data shown with their counts As in Table 4 we only show those in which one phrase has at least two tokens and neither phrase is entirely punctuation Each cluster is shown as a set of words large enough to cover 95 of the token counts in the cluster up to a maximum of four words It is characteristic of Brown clustering that very frequent tokens eg function words often receive their own clusters Examples of illustrative sentence pairs and frequently extracted rules that model verb movement between German and English An ellipsis indicates that there must be material between the two phrases for the rule to apply a Example of movement of the finite verb to the end of a dependent clause b Example of movement of an infinitive to the end of an independent clause following a modal verb mochte would like Discussion of the features used to score these stringtotree rules is given in Section 52 Stringtotree configurations each is associated with a feature that counts its occurrences in a derivation Quasisynchronous treetotree configurations from Smith and Eisner 2006 There are additional configurations involving NULL alignments and an other category for those that do not fit into any of the named categories Lattice dependency parsing using an arcfactored dependency model Lone indices like p and i denote nodes in the lattice and an ordered pair like i j denotes the lattice edge from node i to node j S TART is the single start node in the lattice and F INAL is a set of final nodes We use edgeScorei j to denote the model score of crossing lattice edge i j which only includes the phrasebased features h 0 We use arcScorei j l m to denote the score of building the dependency arc from lattice edge i j to its parent l m arcScore only includes the QPD features h 00 BLEU on tune and test sets for ZHEN translation showing the contribution of feature sets in our QPD model Both QPD models are significantly better than the best Moses numbers on test sets 1 and 2 but not on test set 3 The full QPD model is significantly better than the version with only T GT T REE features on test set 1 but statistically indistinguishable on the other two test sets Hiero is significantly better than the full QPD model on test set 2 but not on the other two BLEU on tune and test sets for DEEN translation comparing the baselines to our QPD model with target syntactic features T GT T REE and then also with source syntax T REE T O T REE Here merely using the additional round of tuning with the SSVM reranker improves the BLEU score to 199 which is statistically indistinguishable from the two QPD feature sets Differences between Hiero and the three 199 numbers are at the border of statistical significance the first two are statistically indistinguishable from Hiero but the third is different at p 004 BLEU on tune and test sets for UREN translation using our unsupervised Urdu parser to incorporate source syntactic features The two QPD rows are statistically indistinguishable on both test sets Both are significantly better than all Moses results but Hiero is significantly better than all others BLEU on tune and test sets for ENMG translation using a supervised English parser and an unsupervised Malagasy parser The 156 BLEU reached by the full QPD model is statistically significantly better than all other results on the test set All other test set numbers are statistically indistinguishable BLEU on tune and test sets when comparing parsers for ZHEN translation QPD uses all features including T GT T REE and T REE T O T REE The table first pairs supervised English parsing with supervised unsupervised and random Chinese parsing then pairs unsupervised English parsing with supervised and unsupervised Chinese parsing significantly better than supsup significantly worse than supsup Feature ablation experiments for UREN translation with stringtotree features showing the drop in BLEU when separately removing word W ORD cluster C LUST and configuration C FG feature sets significantly worse than T GT T REE Removing word features causes no significant difference Removing cluster features results in a significant difference on both test sets and removing configuration features results in a significant difference on test 2 only Results of human evaluation performed via Amazon Mechanical Turk The percentages represent the portion of sentences for which one system had more preference judgments than the other system If a sentence had an equal number of judgments for the two systems it was counted in the final row neither preferred BLEU on tune and test sets for UREN translation comparing several settings for maximum dependency lengths in the decoder x is for the source side and y is for the target side The upper table shows Moses BLEU scores for comparison The lower table compares two max dependency length settings during tuning and several for decoding on the test sets showing both BLEU scores and average decoding times per sentence See text for discussion Relation weights Method 2 Graph of Word Senses Results of SVM and Mincuts with different settings of feature Accuracy with different sizes of unlabeled data random selection Accuracy for Different PartOfSpeech Learning curve with different sizes of labeled data Accuracy with different sizes of labeled data Accuracy with different sizes of unlabeled data from WordNet relation a An example in which an English sentence is parsed into a tree structure with 12 PCFG rules b an instance in which a Chinese sentence both Chinese characters and Chinese Pinyin are provided and note that we will use Chinese Pinyin throughout the paper is converted into an English tree using 6 STSG rules The symbol to the upper right of a node indicates that this node is constructed using rule Lexicalized training example POS of target headword is not explicitly given Examples of rules used during decoding Two real translation examples Example MERT values along one coordi nate first unregularized When regularized with 2 the piecewise constant function becomes piecewise quadratic When using 0 the function remains piecewise constant with a point discontinuity at 0 Datasets for the two experimental conditions Comparison of rate of convergence between coordinate ascent and our expected BLEU direction finder D 500 Noisy refers to the noisy experimental setting BLEU scores for GBM features Model parameters were optimized on the Tune set For PRO and regularized MERT we optimized with different hyperparameters regularization weight etc and retained for each experimental condition the model that worked best on Dev The table shows the performance of these retained models BLEU scores for SparseHRM features Notes in Table 2 also apply here Equivalent Left LM State Computation BLEU scores after discriminative hypergraph reranking Only the language model LM or the transla tion model TM or both LMTM may be discrimina tively trained to prefer the oraclebest hypotheses Baseline and oraclebest 4gram BLEU scores with 4 references for NIST ChineseEnglish MT datasets Speed of oracle extraction from hypergraphs The basic dynamic program Sec 21 improves signifi cantly by collapsing equivalent oracle states Sec 22 Experimental results over the 120 evalu ation sentences Alignment error rates in both di rections are provided here Key notation Feature factorings are elaborated in Tab 2 Factoring of global feature collections g into f xji denotes hxi xj i in sequence x hx1 i Decoding as lattice parsing with the highestscoring translation denoted by black lattice arcs others are grayed out and thicker blue arcs forming a dependency tree over them Eq 9 Loglikelihood Eq 10 Pseudolikelihood In both cases we maximize wrt Eqs 1113 Recursive DP equations for summing over t and a Comparison of size of kbest list for cube decoding with various feature sets Feature set comparison BLEU QG configuration comparison The name of each configuration following Smith and Eisner 2006 refers to the relationship between at j and aj in s Total corpus sizes in sentences and number of Sure and Possible alignment links in their respective evaluation sets Results from the empirical evaluation including the Bayesian model without PoS tags Base line the alternating alignmentannotation algorithm AAA the corresponding method but with super vised PoS taggers for both languages Supervised and comparable previous results on the same data The number of alignment links A of which A S are considered Sure and A P Possible are reported For convenience precision P recall R F1 score F and Alignment Error Rate AER are also given Kendalls correlation over WMT 2013 all en for the full dataset and also the subset of the data containing a noun compound in both the reference and the MT output Pearsons r correlation results over the WMT allen dataset and the subset of the dataset that contains noun compounds Two examples from the allen dataset Each example shows a reference translation and the outputs of two machine translation systems In each case the output of MT system 1 is annotated as the better translation The total costs for the three MTurk subtasks in volved with the creation of our Dialectal ArabicEnglish parallel corpus One possible breakdown of spoken Arabic into dialect groups Maghrebi Egyptian Levantine Gulf and Iraqi Habash 2010 gives a breakdown along mostly the same lines We used this map as an illustration for annotators in our dialect classification task Section 31 with Arabic names for the dialects instead of English Statistics about the trainingtuningtest datasets used in our experiments The token counts are calculated before MADA segmentation Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal Arabic web text The morphological segmentation uniformly improves translation quality but the improvements are more dramatic for MSA than for Dialectal Arabic when comparing similarlysized training corpora A comparison of translation quality of Egyptian Levantine and MSA web text using various training corpora The highest BLEU scores are achieved using the full set of dialectal data which combines Levantine and Egyptian since the Egyptian alone is sparse For Levantine adding Egyptian has no effect In both cases adding MSA to the dialectal data results in marginally worse translations Examples of ambiguous words that are trans lated incorrectly by the MSAEnglish system but cor rectly by the Dialectal ArabicEnglish system Examples of improvement in MT output when training on our Dialectal ArabicEnglish parallel corpus instead of an MSAEnglish parallel corpus The most frequent OOVs with counts 10 of the dialectal test sets against the MSA training data Learning curves showing the effects of increas ing the size of dialectal training data when combined with the 150Mword MSA parallel corpus and when used alone Adding the MSA training data is only use ful when the dialectal data is scarce 200k words Results on a truly independent test set consisting of data harvested from Egyptian Facebook pages that are entirely distinct from the our dialectal training set The improvements over the MSA baseline are still considerable 29 BLEU points when no Facebook data is available for tuning and 27 with a Facebook tuning set A comparison of the effectiveness of performing LevantinetoMSA mapping before translating into English versus translating directly from Levantine into English The mapping from Levantine to MSA was done manually so it is an optimistic estimate of what might be done automatically Although initially helpful to the MSA baseline system the usefulness of pivoting through MSA drops as more dialectal data is added eventually hurting performance The MEDLINE semantic categories Filtered 5gram dataset statistics Performance relationship between WMEB and BASILISK on Sgold UNION Variation in precision with random gold seed sets Bagging with 50 gold seed sets Bagging with 50 unsupervised seed sets Semantic drift in CELL n20 m20 Final accuracy with drift detection Semantic drift detection results Arabic Verbal Inflection Arabic Nominal Inflection Broken Plural Adjective Full Inflection Verb Stem Alternation Noun Stem Alternation Derivation by Means of Adding a Suffix Arabic Clitics Example 1 Syncretism Example 2 Syncretism Example 1 Arabic Pronoun Dropping Compounding in Russian Arabic Clitics Example 2 Challenge in Elliptical Constructions Arabic Equational Sentences ZeroCopula in Russian Arabic OrderFree Structure Six Accepted Word Orders in Russian Arabic idafa Construct Phonetic Stress in Russian Phonetic Stress in Russian Fake Homograph Arabic Vocalization Problem Arabic Diglossia A snippet from Russian and Czech tag comparison WordNet representation Splitting Compounds in Russian Arabic Tokenization Schemes Arabic Tokenization Schemes Performance comparison between original and universal tagsets Arabic POS Studies with Different Tagsets Different Arabic Transliterations of Los Angeles Shallow parsing chunking Extracted from httpkontextfraunhoferde An example CCG parse obtained from 60 Relation phrase compliance with semanticlexical constraints 32 Dependency parsing and clause constituents Relation extraction rules used by Gamallo etal36 A comparison between QA semantic parsing approaches12 a Parsing of input sentence78 Transliteration Scheme Distribution of dialogue acts in our dataset The 7 speakers from ICSIMRDA dataset used in our experiments The table lists the Speaker ID orig inal speaker tag the type of meeting selected for this speaker the number of meetings this speaker participated and the total number of dialogue acts by this speaker Effect of samespeaker data on dialogue act recognition We compare two approaches 1 when a recognizer is trained on the same person and tested on new utterances from the same person and 2 when the recognizer was trained on another speaker same test set We vary the amount of training data to be 200 500 1000 1500 and 2000 dialogue acts In all cases using speakerspecific recognizer outperforms recognizer from other speakers The average results among all 7 speakers when train with different combinations of speaker specific data and other speakers data are displayed In both Constant adaptation and Reweighted adaptation models the num ber of speaker specific data are varied from 200 500 1000 1500 to 2000 In Generic model only all other speakers data are used for training data Average results of Reweighting among all 7 speakers when the amount of speaker specific data is 0 500 2000 a A related work section extracted from Wu and Oard 2008 b An associated topic hierar chy tree of a c An associated topic tree annotated with key wordsphrases The demographics of RWSData No RW RA SbL WbL TS and TD are labeled as Number of Related Works Referenced Articles Sentencebased Length of Word based Length of Tree Size and Tree Depth respectively A context modeling example Evaluation results for ReWoS variants and baselines Clustering an 11nodes graph with CW in two iterations The middle node gets the grey or the black class Small numbers denote edge weights Percentage of obtaining two clusters when applying CW on nbipartite cliques oscillating states in matrix CW for an unweighted graph The 10bipartite clique Rate of obtaining two clusters for mix tures of SWgraphs dependent on merge rate r normalized Mutual Information values for three graphs and different iterations in Bipartite neighboring cooccurrence graph a and secondorder graph on neighboring cooccurrences b clustered with CW Disambiguation results in dependent on frequency Disambiguation results in dependent on word class nouns verbs adjectives the largest clusters from partitioning the second order graph with CW Graphical model for the Bayesian QueryFocused Summarization Model Empirical results for the baseline models as well as BAYE S UM when all query fields are used Performance with noisy relevance judg ments The Xaxis is the Rprecision of the IR engine and the Yaxis is the summarization per formance in MAP Solid lines are BAYE S UM dot ted lines are KLRel Bluestars indicate title only redcircles indicated titledescriptionsummary and blackpluses indicate all fields Empirical results for the positionbased model the KLbased models and BAYE S UM with different inputs A portion of the local cooccurrence graph for mouse from the SemEval2010 Task 14 corpus VMeasure and paired FScore results for different partitionings of the dendrogram The dashed vertical line indicates SP D Performance results on the SemEval2010 WSI Task with rank shown in parentheses Refer ence scores of the best submitted systems are shown in the bottom Pipeline architecture for dialogue act recognition and reranking component Here the input is a list of dialogue acts with confidence scores and the output is the same list of dialogue acts but with recomputed confidence scores A dialogue act is represented as DialogueActTypeattributevalue pairs Bayesian network for probabilistic rea soning of locations variable from desc which incorporates ASR Nbest information in the vari ablefrom desc nbest and dialogue history in formation in the remaining random variables Bayesian dialogue act recognisers show ing the impact of ASR Nbest information Annotation statistics Results for baseline BAS system standard multiclass SVM Results for basic DAC system perclass feature optimization followed by maximum confidence based choice ER refers to error reduction in percent over standard multiclass SVM Table 2 Results for cascading minoritypreference DAC system DACCMP consult classifiers in reverse order of frequency of class ER refers to error reduction in percent over standard multiclass SVM Table 2 Results for ODP system using various sources of DA tags Posthoc analysis on the models built by the DAC system some of the top features with corresponding feature weights in parentheses for each individual tagger POS tags are capitalized BOS stands for Beginning Of Sentence Speech Act Categories and Kappa values An example discussion thread Statistics for each Speech Act Category Some of the top selected features by Infor mation Gain SA classification results Thread Classification Results Example patterns in student discussion threads Results from Direct Thread Classification The NLMWSD test set and some of its sub sets Note that the test set used by Joshi et al 2005 comprises the set union of the terms used by Liu et al 2004 and Leroy and Rindflesch 2005 while the com mon subset is formed from their intersection Results from WSD system applied to various sections of the NLMWSD data set using a variety of fea tures and machine learning algorithms Results from baseline and previously published approaches are included for comparison Perword performance of best reported systems Example of semantic trees Two STs composing a STN ROUGE Fscores for different systems The set of types and subtypes of relations used in the 2004 ACE evaluation The Division of LDC annotated data into training and development test sets Comparing Fmeasure precision and recall of different voting schemes for English relation extraction Comparing Fmeasure precision and recall of different voting schemes for Chinese relation extraction Comparing Fmeasure precision and recall of different voting schemes for Arabic relation ex traction Comparing the best Fmeasure obtained by AtLeastN Voting with Majority Voting Summing and the single best classifier LMR Tagging Learning curves on the development dataset of the Beijing Univ corpus Learning curves on the development dataset of the HK City Univ corpus Official Bakeoff Outcome Fscore on development data Opinion PageRank Opinion HITS model Opinion Question Answering System Sentiment lexicon description Opinion PageRank Performance with varying parameter 02 Sentiment Lexicon Performance Opinion PageRank Performance with varying parameter 05 Questionspecific popular topic words and opinion words generated by Opinion HITS Opinion HITS Performance with vary ing parameter Comparison results with TAC 2008 Three Top Ranked Systems system 13 demonstrate top 3 systems in TAC Length distribution of entities in the train ing set of the shared task in 2004 JNLPBA Modification of O other labels to transfer information on a preceding named entity The framework of our system We first enumerate all possible candidate states and then filter out low probability states by using a lightweight classifier and represent them by using feature forest Features used in the naive Bayes Classi fier for the entity candidate ws ws1 we spi is the result of shallow parsing at wi Example of feature forest representation of linear chain CRFs Feature functions are as signed to and nodes Example of packed representation of semiCRFs The states that have the same end po sition and preventity label are packed Filtering results using the naive Bayes classifier The number of entity candidates for the training set was 4179662 and that of the develop ment set was 418628 Feature templates used for the chunk s ws ws1 we where ws and we represent the words at the beginning and ending of the target chunk respectively pi is the part of speech tag of wi and sci is the shallow parse result of wi Comparison with other systems Performance with filtering on the development data 10 1012 means the threshold probability of the filtering is 10 1012 Performance of our system on the evalu ation set Overall performance on the evaluation set L is the upper bound of the length of possible chunks in semiCRFs Basic Statistics of DUC2007 Update Data Set System Comparison Experiment Results Bahktins characterization of dialogue Bahktin 1986 describes a discourse along the three major properties style situation and topic Current information retrieval systems focus on the topical as pect which might be crucial in written documents Furthermore since throughout text analysis is still a hard problem information retrieval has mostly used keywords to characterize topic Many features that could be extracted are therefore ignored in a tradi tional keyword based approach Information access hierarchy Oral com munications take place in very different formats and the first step in the search is to determine the database or subdatabase of the rejoinder The next step is to find the specific rejoinder Since re joinders can be very long the rejoinder has to seg mented and a segment has to be selected Distribution of activity types Both databases contain a lot of discussing informing and storytelling activities however the meeting data contains a lot more planning and advising Intercoder agreement for activities The meeting dialogues and Santa Barbara corpus have been annotated by a seminaive coder and the first author of the paper The coefficient is determined as in Carletta et al 1997 and mutual information measures how much one label informs the other see Sec 3 For CallHome Spanish 3 dialogues were coded for activities by two coders and the result seems to indicate that the task was easier Activity detection Activities are detected on the Santa Barbara Corpus SBC and the meet ing database meet either without clustering the activities all or clustering them according to their interactivity interactive see Sec 2 for details TV show types The distribution of show types in a large database of TV shows 1067 shows that has been recorded over the period of a couple of months until April 2000 in Pittsburgh PA Detection accuracy summary The detec tion of highlevel genre as exemplified by the differ entiation of corpora can be done with high accuracy using simple features Ries 1999 Similar it was fairly easy to discriminate between male and female speakers on Switchboard Ries 1999 Discrimi nating between subgenre such as TVshow types Sec 4 can be done with reasonable accuracy How ever it is a lot harder to discriminate between ac tivities within one conversation for personal phone calls CallHome Ries et al 2000 or for general rejoinders Santa and meetings Sec 2 Show type detection Using the neural net work described in Sec 2 the show type was detected If there is a number in the word column the word feature is being used The number indicates how many wordpart of speech pairs are in the vocabu lary additionally to the parts of speech A graphical representation of the HMM ap proach for speaker role labeling This is a simple first order HMM Automatic role labeling results using the HMM and Maxent classifiers Impact of role sequence information on the HMM and Maxent classifiers The combination results of the HMM and Maxent are also provided Statistics about the results of our word sense discovery algorithm Average precision of discovered senses for English in relation with WordNet Senses found by our algorithm from first order cooccurrences LM1 and LAT1 Coarse overview From multilingual in put to typed relations and instances Some examples for MEDLINE tagset Number of lex entries per tag and sample words The chunking results for the six systems associated with the project shared task CoNLL 2000 The baseline results have been obtained by selecting the most frequent chunk tag associ ated with each partofspeech tag The best results at CoNLL2000 were obtained by Support Vector Machines A majority vote of the six LCG sys tems does not perform much worse than this best result A majority vote of the five best systems outperforms the best result slightly error re duction The NP chunking results for six sys tems associated with the project The baseline results have been obtained by selecting the most frequent chunk tag associated with each partof speech tag The best results for this task have been obtained with a combination of seven learn ers five of which were operated by project mem bers The combination of these five performances is not far off these best results The results for three systems associ ated with the project for the NP bracketing task the shared task at CoNLL99 The baseline re sults have been obtained by finding NP chunks in the text with an algorithm which selects the most frequent chunk tag associated with each partof speech tag The best results at CoNLL99 was obtained with a bottomup memorybased learner An improved version of that system MBL deliv ered the best project result The MDL results have been obtained on a different data set and therefore combination of the three systems was not feasible Derivational entropy of Gq and cross entropies for three different corpora A Sample Network MUC7 Level Distribution of the Facts Combined MUC7 Level Distribution of Each of the Facts MUC7 Level Distribution of Each of the Facts MUC7 Level Distribution of Each of the Facts MUC7 Level Distribution of Each of the Facts MUC4 Level Distribution of the Five Facts Combined MUC4 Level Distribution of Each of the Five Facts MUC5 Level Distribution of the Five Facts Combined MUC5 Level Distribution of Each of the Five Facts MUC6 Level Distribution of Each of the Six Facts Domain Numbers of MUC4 MUC5 MUC6 and MUC7 MUC6 Level Distribution of the Six Facts Combined Types of message speech acts in corpus Categories of Message Speech Act Thread length distribution Frequency of speech acts Gold standard length distribution Sample poster scores System Performance Comparison SA strength scores Example queries for abbreviation BSA Properties of abbreviations corpus retrieved from Medline Performance of WSD system using various combinations of learning algorithms and features Performance of WSD system over individual ab breviations in three reduced corpora Maximally Accurate Assignment Numeric Assignment Experimental Results Word prediction from a partial parse Dependency structure of a sentence A conceptual gure of the lexicalization Corpus Relation between cross entropy and pars Cross entorpy and accuracy of each model Number of feedback items per speaker Distribution of isolated vs initial posi tion for the most frequent lexical items Duration in seconds of each lexical type Distribution of the lexical items Dendrogram of the participants cluster based on their feedback profile The density of the F1 scores with the three approaches The prior used is a symmetric Dirichlet with 01 Size of cooccurrence databases WSI and WSD Pipeline Results for the submitted runs Representation of Bigram Counts Experimental Results Decision Tree and Stump Characteristics Comparison with other approaches F1measures with in 0 3 F1measure with in 01 Example sentence and extracted features from the SENSEVAL 2 word church Empiricallyderived classifier similarity Individual Classifier Properties crossvalidation on SENSEVAL training data Accuracy for different EMweighted probability interpolation models for SENSEVAL 2 Training set characteristics Individual feature type contribution to perfor mance Fields marked with indicate that the difference in performance was not statistically significant at a level paired McNemar test Classifier combination accuracy over 5 base classifiers NB BR TBL DL MMVC Best perform ing methods are shown in bold Individual basic classifiers contribution to the final classifier combination performance Final Performance Frozen Systems on SENSEVAL Lexical Sample WSD Test Data Example of semantic trees Two STs composing a STN ROUGE2 measures in kmeans learning ROUGEW in empirical approach ROUGEW measures in kmeans learning ROUGEW measures in EM learning ROUGESU in kmeans learning Fmeasures for different systems ROUGESU in empirical approach ROUGE2 in empirical approach ROUGE2 measures in EM learning ROUGESU measures in EM learning A corpus of two trees A derivation for Mary likes Susan Another derivation yielding same tree All binary trees for NNS VBD JJ NNS Investors suffered heavy losses Some subtrees from trees in figure 4 Fscores of UMLDOP compared to previous models on the same data Fscores of UDOP UMLDOP and a supervised treebank PCFG MLPCFG for a random 9010 split of WSJ10 and WSJ40 Baseline results for human word lists Data 700 positive and 700 negative reviews Results for baseline using introspection and simple statistics of the data including test data Average threefold crossvalidation accuracies in percent Boldface best performance for a given setting row Recall that our baseline results ranged from 50 to 69 Event descriptions spread across two sentences Counts of matches between MUC and Soderland data Matches between MUC and Soderland data at field level Corpus Excerpt with Dialogue Act Annotation Figure 2 Automatically detected posture points H headDepth M midTorsoDepth L lowerTorsoDepth reports the average classification accuracies from the fivefold cross validation The majority baseline accuracy for our data is 347 when the classifier always chooses the most frequent dialog act A The first group of rows in Table 3 report the accuracies of individual feature classes All of the individual features performed better than the baseline The improvement from the baseline was significant except for D with CRF The most powerful feature class was dialogue context class when the full set was used The second group in Table 3 shows the effects of incrementally combining the feature classes Adding dialogue act features to the lexical features L D brought significant improvement in the classification accuracy for ME and CRF Adding posture features L D T P also improved the accuracy of ME by a statistically significant margin The last group shows similar results for ME when the previous tutor dialogue act was excluded from the dialogue context except that the improvement achieved by adding the posture features L D T P was not significant Stages of the proposed method Graph of words for the target word paper Numbers inside vertices correspond to their degree Running example of graph creation Two dendrograms for the graph in Figure 3 Sensetagged corpus for the example in Figure 3 A current configuration for internal node Dk and its associated subtrees B first alternative configuration C second alternative configuration Note that swapping st1 st2 in A results in an equivalent tree Hence this configuration is excluded Parameter values used in the evaluation Performance analysis of HRGs CWU CWW HAC for different parameter combinations Table 2 A All combinations of p1 p2 and p3 005 B All combinations of p1 p2 and p3 009 Performance of HRGs and HAC for different parameter combinations Table 2 All combinations of p1 p2 and p3 013 HRGs against recent methods baselines Contingency table for the children of canine in the subject position of run Contingency table for the children of liquid in the object position of drink Example levels of generalization for different values of Extent of generalization for different values of and sample sizes Results for the pseudodisambiguation task Results for the pseudodisambiguation task with onefifth training data Disambiguation results for G2 and X2 Four ambiguous words their senses and frequency Mutual information between feature subset and class label with f req based feature ranking Mutual information between feature subset and class label with 2 based feature ranking Average accuracy over three procedures in Figure 1 as a function of context window size horizontal axis for 4 datasets Results for three procedures over 4 datases The horizontal axis corresponds to the context window size Solid line represents the result of F SGM M binary dashed line denotes the result of CGDSV D idf and dotted line is the result of CGDterm idf Square marker denotes 2 based feature ranking while cross marker denotes f req based feature ranking Average accuracy of three procedures with various settings over 4 datasets Automatically determined mixture component num A tree showing head information Perplexity results for two previous grammarbased language models A nounphrase with substructure Perplexity results for the immediate bihead model Perplexity results for the immediate trihead model Precisionrecall for sentences in which trigramgrammar models performed best Accuracy in Lexical Sample Tasks System Pairwise Agreement Number of synset relations TS of partyn1 first 10 out of 12890 total words First ten words with weigths and number of senses in WN of the Topic Signature for airportn1 obtained from BNC using InfoMap Minimum distances from airportn1 Sense disambiguated TS for airportn1 obtained from BNC using InfoMap and SSIDijkstra Size and percentage of overlapping relations between KnowNet versions and WNXWN Percentage of overlapping relations between KnowNet versions P R and F1 finegrained results for the resources evaluated at Senseval3 English Lexical Sample Task Definitions for the top four senses of law according to WordNet Example confounders for festival and laws and their similarities Pseudoword discrimination performance The error frequency distributions for confusing the correct sense with another sense of the given similarity when using a 5word cooccurrence window as context Dashed lines indicate the null models Unsupervised and Supervised scores on the SemEval2010 WSI Task for each feature and clustering models with reference scores for the top performing systems for each evaluation shown below Question tracking interface to a summa rization system LexRank example sentence similarity graph with a cosine threshold of 015 Corpus of complex news stories Development testing evaluation Average scores by cluster baseline versus LR020095 Training phase effect of similarity thresh old a on Ave MRR and TRDR Training phase effect of question bias d on Ave MRR and TRDR Training phase systems outperforming the baseline in terms of TRDR score Top ranked sentences using baseline system on the question What caused the Kursk to sink Testing phase baseline vs LR020095 Top ranked sentences using the LR020095 system on the question What caused the Kursk to sink Visualisation examples Top named en tity recognition middle dependency syntax bot tom verb frames Screenshot of the main BRAT userinterface showing a connection being made between the annotations for moving and Citibank Incomplete T RANSFER event indicated to the annotator The BRAT search dialog Total annotation time portion spent se lecting annotation type and absolute improve ment for rapid mode Example annotation from the BioNLP Shared Task 2011 Epigenetics and Posttranslational Modifications event extraction task Three representations of NP modifications a the original treebank representation b Selective leftcorner representation and c a flat structure that is unambiguously equivalent to b Conditioning features for the probabilistic CFG used in the reported empirical trials Corpus sizes Parser performance on BrownE baselines Note that the Gildea results are for sentences 40 words in length Parser performance on WSJ23 baselines Note that the Gildea results are for sentences 40 words in length All others include all sentences Parser performance on WSJ23 baselines Parser performance on BrownE supervised adaptation Parser performance on WSJ23 unsupervised adaptation For all trials the base training is BrownT the held out is BrownH plus the parser output for WSJ24 and the mixing parameter A is 020e cA Parser performance on WSJ23 supervised adaptation All models use BrownTH as the outofdomain treebank Baseline models are built from the fractions of WSJ221 with no outofdomain treebank A brief description of the tested parsers Note that the Tune data is not the data used to train the individual parsers Higher numbers in the right column reflect just the fact that the Test part is slightly easier to parse Comparison of various groups of parsers All percentages refer to the share of the total words in test data attached correctly The single parser part shows shares of the data where a single parser is the only one to know how to parse them The sizes of the shares should correlate with the uniqueness of the individual parsers strategies and with their contributions to the overall success The at least rows give clues about what can be got by majority voting if the number represents over 50 of parsers compared or by hypothetical oracle selection if the number represents 50 of the parsers or less an oracle would generally be needed to point to the parsers that know the correct attachment Results of voting experiments Voting under handinvented schemes Contexts where ec is better than mcdz J are coordination conjunctions is the root V are verbs Nn are nouns in case n R are preposi tions Z are punctuation marks An are adjectives The decision tree for ecmcz learned by C5 Besides pairwise agreement be tween the parsers only morphological case and negativeness matter Contextsensitive voting Contexts trained on the Tune data set accuracy figures apply to the Test data set Contextfree results are given for the sake of comparison Unbalanced vs balanced combining All runs ignored the context Evaluated on the Test data set Combined systems Basque in cross validation best recall in bold Only vectorf was used for combination Single systems Basque in cross validation sorted by recall Single systems English in cross validation sorted by recall Combined systems English in cross validation best recall in bold Ocial results for the English and Basque lexical tasks recall 