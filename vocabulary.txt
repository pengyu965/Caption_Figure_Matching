Table 1 : Morph Examples and Motivations . Table 2 : Distributions of Morph Examples Figure 1 : Overview of Morph Resolution Figure 2 : Cross-source Comparable Data Example ( each morph and target pair is shown in the same color ) Figure 3 : Network Schema of Morph-Related Het- erogeneous Information Network Figure 4 : Example of Morph-Related Heteroge- neous Information Network Table 5 : The System Performance Based on Com- binations of Surface and Semantic Features . Table 4 : The System Performance Based on Each Single Feature Set . Table 3 : Description of feature sets . ∗ Glob only uses the same set of similarity measures when combined with other semantic features . Table 8 : The Effects of Temporal Constraint Table 9 : Accuracy of Target Candidate Detection Table 7 : The Effects of Social Features . Table 6 : The System Performance of Integrating Cross Source and Cross Genre Information . Table 11 : Effects of Popularity of Morphs Table 10 : Performance of Two Categories Figure 1 Architecture of the translation approach based on a log-linear modeling approach . Figure 2 Example of a ( symmetrized ) word alignment ( Verbmobil task ) . Figure 3 Algorithm phrase-extract for extracting phrases from a word-aligned sentence pair . Here quasi-consecutive ( TP ) is a predicate that tests whether the set of words TP is consecutive , with the possible exception of words that are not aligned . Figure 4 Examples of alignment templates obtained in training . Figure 6 Dependencies in the alignment template model . Figure 5 Example of segmentation of German sentence and its English translation into alignment templates . Figure 7 Algorithm for breadth-first search with pruning . Figure 8 Algorithm min-jumps to compute the minimum number of needed jumps D ( cJ1 , j ) to complete the translation . Table 2 Statistics for Verbmobil task : training corpus ( Train ) , conventional dictionary ( Lex ) , development corpus ( Dev ) , test corpus ( Test ) ( Words* : words without punctuation marks ) . Table 3 Effect of alignment template length on translation quality . Table 4 Effect of pruning parameter tp and heuristic function on search efficiency for direct-translation model ( Np = 50,000 ) . Table 5 Effect of pruning parameter tp and heuristic function on error rate for direct-translation model ( Np = 50,000 ) . Table 6 Effect of pruning parameter Np and heuristic function on search efficiency for direct-translation model ( tp = 10−12 ) . Table 7 Effect of pruning parameter Np and heuristic function on error rate for direct-translation model ( tp = 10−12 ) . Table 9 Corpus statistics for Hansards task ( Words* : words without punctuation marks ) . Table 11 Corpus statistics for Chinese–English corpora—large data track ( Words* : words without punctuation marks ) . Table 10 Translation results on the Hansards task . Table 13 Example translations for Chinese–English MT . Table 1 : Feature set for coreference resolution ( Feature 22 , 23 and features involving Cj are not used in the single-candidate model ) Table 3 : Results for the non-pronoun resolution Table 2 : Results for the pronoun resolution Table 4 : Results for the coreference resolution Table 1 : List of semantic labels . Table 3 : Examples of aggregated instances . Table 2 : Results of semantic classification . Table 4 : Confusion matrix of acquired nouns . Table 5 : Results of ablation experiments . Table 1 : Linking FrameNet frames and VerbNet classes . Table 2 : Mapping algorithm - refining step . Table 3 : Results of the mapping algorithm . Table 5 : F1s of some individual FN role classifiers and the overall multiclassifier accuracy ( 454 roles ) . Table 4 : F1s of some individual ILC classifiers and the overall multiclassifier accuracy ( 180 classes on PB and 133 on FN ) . Figure 1 : Semantic role learning curve . Figure 2 : Graph representing transliteration pairs and cooccurence relations . Table 1 : Ten highest-scoring matches for the Xin- hua corpus for 8/13/01 . The final column is the −log P estimate for the transliteration . Starred entries are incorrect . Table 2 : MRRs of the frequency correlation meth- ods . Table 4 : MRRs on the augmented candidate list . Table 3 : Effectiveness of combining the two scor- ing methods . Figure 3 : Propagation : Core items Table 5 : Effectiveness of score propagation . Figure 4 : Propagation : All items Figure 1 : An example of MOD feature extraction . An oval in the dependency tree denotes a bunsetsu . Table 7 : # of features . Table 5 : Precision for 200 candidates ( Ev.Rec ) . Table 6 : Precision for each phrase type ( Ev.Ling ) . Figure 1 . Anaphora resolution preferences . Figure 2 . Description of the unrestricted corpora used in the evaluation . Figure 3 . Results obtained in the detection of zero-pronouns . Figure 4 . Classification of third person zero- pronouns . Figure 1 : Syntactic tree kernel ( STK ) . Figure 2 : Integrating Brown cluster information Table 1 : Overview of the ACE 2005 data . Figure 3 : Distribution of relations in ACE 2005 . Table 2 : For each domain the percentage of target domain words ( types ) that are unseen in the source together with the most frequent OOV words . Table 3 : Comparison to previous work on the 7 re- lations of ACE 2004 . K : kernel-based ; F : feature- based ; yes/no : models argument order explicitly . Table 4 : Brown clusters in tree kernels ( cf . Fig 2 ) . Table 6 : F1 per coarse relation type ( ACE 2005 ) . SYS is the final model , i.e . last row ( PET+PET WC+PET LSA ) of Table 5 . Table 5 : In-domain ( first column ) and out-of-domain performance ( columns two to four ) on ACE 2005 . PET and BOW are abbreviated by P and B , respectively . If not specified BOW is marked . Figure 1 : Bell tree representation for three mentions : numbers in [ ] denote a partial entity . In-focus entities are marked on the solid arrows , and active mentions are marked by * . Solid arrows signify that a mention is linked with an in-focus partial entity while dashed arrows indicate starting of a new entity . Table 1 : Basic features used in the maximum entropy model . Table 2 : Statistics of three test sets . Table 4 : Impact of feature categories . Numbers after are the standard deviations . * indicates that the result # '' is significantly ( pair-wise t-test ) different from the line above at . Figure 2 : Performance vs. log start penalty Table 5 : Results on the MUC6 formal test set . ACE-value ; ECM-F : Entity-constrained Mention F-measure . MP uses & & Table 3 : Coreference results on true mentions : MP – mention-pair model ; EM – entity-me features wh features . None of the ECM-F differences between MP and EM is statistically significant at Figure 1 The generic beam-search algorithm . Table 1 Feature templates for the word segmentor . Table 2 Feature templates of a typical character-based word segmentor . Figure 3 Speed/accuracy tradeoff of the segmentor . Table 3 Training , development , and test data for word segmentation on CTB5 . Table 4 The accuracies of various word segmentors over the first SIGHAN bakeoff data . Table 5 The accuracies of various word segmentors over the second SIGHAN bakeoff data . Figure 4 The extended generic beam-search algorithm with multiple beams . Table 6 Comparison between three different decoders for word segmentation . Table 7 POS feature templates for the joint segmentor and POS -tagger . Figure 6 The influence of beam-sizes , and the convergence of the perceptron for the joint segmentor and POS -tagger . Table 9 The speeds of joint word segmentation and POS-tagging by 10-fold cross validation . Table 8 The accuracies of joint segmentation and POS-tagging by 10-fold cross validation . Table 10 The comparison of overall accuracies of various joint segmentor and POS-taggers by 10-fold cross validation using CTB . Table 11 Training , development , and test data from CTB5 for joint word segmentation and POS-tagging . Table 12 Accuracy comparisons between various joint segmentors and POS-taggers on CTB5 . Figure 7 The accuracy/speed tradeoff graph for the joint segmentor and POS-taggers and the two-stage baseline . Figure 8 An example Chinese dependency tree . Table 13 Transition-based feature templates for the dependency parser . Figure 9 Transition-based feature context for the dependency parser . Table 14 Graph-based feature templates for the dependency parser . Table 15 The training , development , and test data for English dependency parsing . Figure 10 The accuracy/speed tradeoff graph for the transition-based and combined dependency parsers . Table 16 Accuracy comparisons between various dependency parsers on English data . Table 17 Training , development , and test data for Chinese dependency parsing . Table 18 Test accuracies of various dependency parsers on CTB5 data . Table 19 The combined segmentation , POS-tagging , and dependency parsing F-scores using different pipelined systems . Figure 11 An example Chinese lexicalized phrase-structure parse tree . Table 20 Feature templates for the phrase-structure parser . Table 21 The standard split of CTB2 data for phrase-structure parsing . Table 22 Accuracies of various phrase-structure parsers on CTB2 with gold-standard POS-tags . Table 23 Accuracies of various phrase-structure parsers on CTB2 with automatically assigned tags . Table 24 Accuracies of our phrase-structure parser on CTB5 using gold-standard and automatically assigned POS-tags . Figure 13 The accuracy/speed tradeoff graph for the phrase-structure parser . Table 25 Comparison of dependency accuracies between phrase-structure parsing and dependency parsing using CTB5 data . Figure 2 : Composition of two FSTs maintaining separate transitions . Figure 1 : Finite-state cascades for five natural language problems . Figure 3 : Changing a decision in the derivation lattice . All paths generate the observed data . The bold path rep- resents the current sample , and the dotted path represents a sidetrack in which one decision is changed . Figure 4 : Multiple EM restarts for POS tagging . Each point represents one random restart ; the y-axis is tag- ging accuracy and the x-axis is EM ’ s objective function , − log P ( data ) . Figure 5 : Multiple Bayesian learning runs ( using anneal- ing with temperature decreasing from 2 to 0.08 ) for POS tagging . Each point represents one run ; the y-axis is tag- ging accuracy and the x-axis is the − log P ( derivation ) of the final sample . Figure 6 : Multiple Bayesian learning runs ( using averag- ing ) for POS tagging . Each point represents one run ; the y-axis is tagging accuracy and the x-axis is the average − log P ( derivation ) over all samples after burn-in . Table 1 : Gibbs sampling for Bayesian inference outperforms both EM and Variational Bayesian EM . ∗ The output of EM alignment was used as the gold standard . Figure 2 : Comparison of Min- , Simple- , Full-and Dynamic-Expansions : More Examples Figure 1 : Dynamic-Expansion Tree Span Scheme Table 1 : Comparison of different context-sensitive Table 3 : Comparison of tree span schemes with antecedents in different sentences apart Table 2 : Feature templates used in R-phase . Ex- ample used is “ 32 ddd ” . Table 1 : Tags used in LMR Tagging scheme . Figure 1 : Example of LMR Tagging . Table 3 : Additional feature templates used in C- phase . Example used is “ 32 ddd ” with tagging results after R-phase as “ SSLMR ” . Table 5 : Official BakeOff2005 results . Keys : F - Regular Tagging only , all training data are used P1 - Regular Tagging only , 90 % of training data are used P2 - Regular Tagging only , 70 % of training data are used S - Regular and Correctional Tagging , Separated Mode I - Regular and Correctional Tagging , Integrated Mode Table 4 : Experimental results of CityU corpus measured in F-measure . Figure 2 : Dependency representation of example ( 2 ) from Talbanken05 . Figure 1 : Animacy classification scheme ( Zaenen et al. , 2004 ) . Table 1 : The animacy data set from Talbanken05 ; number of noun lemmas ( Types ) and tokens in each class . Table 2 : Accuracy for MBL and SVM classifiers on Talbanken05 nouns in accumulated frequency bins by Parole frequency . Table 3 : Precision , recall and F-scores for the two classes in MBL-experiments with a general feature space . Table 4 : Confusion matrix for the MBL-classifier with a general feature space on the > 10 data set on Talbanken05 nouns . Table 5 : Overall results in experiments with au- tomatic features compared to gold standard fea- tures , expressed as unlabeled and labeled attach- ment scores . Table 1 . Usefulness evaluation result Table 2 . Correctness evaluation result Figure 1 : Example of a prediction for English to French translation . s is the source sentence , h is the part of its translation that has already been typed , x∗ is what the translator wants to type , and x is the prediction . Figure 2 : Probability that a prediction will be ac- cepted versus its gain . Figure 3 : Time to read and accept or reject proposals versus their length Table 1 : Approximate times in seconds to generate predictions of maximum word sequence length M , on a 1.2GHz processor , for the MEMD model . Table 3 : Results for different user simulations . Numbers give % reductions in keystrokes . Table 2 : Results for different predictor configura- tions . Numbers give % reductions in keystrokes . Table 1 : Snapshot of the supersense-annotated data . The 7 article titles ( translated ) in each domain , with total counts of sentences , tokens , and supersense mentions . Overall , there are 2,219 sentences with 65,452 tokens and 23,239 mentions ( 1.3 tokens/mention on average ) . Counts exclude sentences marked as problematic and mentions marked ? . Figure 1 : Illustration of the alignment of steps . Table 2 : Statistics of datasets . Figure 2 : F1 scores ( in % ) of SegTagDep on CTB- 5c-1 w.r.t . the training epoch ( x-axis ) and parsing feature weights ( in legend ) . Figure 3 : Performance of baseline and joint models w.r.t . the average processing time ( in sec . ) per sen- tence . Each point corresponds to the beam size of 4 , 8 , 16 , 32 , ( 64 ) . The beam size of 16 is used for SegTag in SegTag+Dep and SegTag+TagDep . Table 5 : Final results on CTB-5j Table 3 : F1 scores and speed ( in sentences per sec . ) of SegTagDep on CTB-5c-1 w.r.t . the beam size . Table 6 : Final results on CTB-6 and CTB-7 Table 4 : Segmentation , POS tagging , and ( unlabeled attachment ) dependency F1 scores averaged over five trials on CTB-5c . Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01 ) . Figure 1 : Examples of the semantic role features Figure 2 : Decoding algorithm for the standard Tree-to-String transducer . lef tw/rightw denote the left/right boundary word of s. c1 , c2 denote the descendants of v , ordered based on RHS of t . Figure 3 : An example showing the combination of the se- mantic role sequences of the states . Above/middle is the state information before/after applying the TTS template , and bot- tom is the used TTS template and the triggered SRFs during the combination . Figure 4 : An example showing how to compute the target side position of a semantic role by using the median of its aligning points . Figure 5 : Decoding algorithm using semantic role features . Sema ( c1 .role , c2 .role , t ) denotes the triggered semantic role features when combining two children states , and ex- amples can be found in Figure 3 . Figure 7 : Computing the partition function of the conditional probability P r ( S|T ) . Sema ( s1 , s2 , t ) denotes all the seman- tic role features generated by combining s1 and s2 using t . Figure 8 : Examples of the MT outputs with and without SRFs . The first and second example shows that SRFs improve the completeness and the ordering of the MT outputs respectively , the third example shows that SRFs improve both properties . The subscripts of each Chinese phrase show their aligned words in English . Table 2 : Distribution of the sentences where the semantic role features give no/positive/negative impact to the sentence fluency in terms of the completeness and ordering of the semantic roles . Table 1 : BLEU-4 scores of different systems Figure 1 Abridged grammatical representation for the example sentence ( 9 ) . Figure 1 : ( a ) An undirected graph G representing the similarity matrix ; ( b ) The bipartite graph showing three clusters on G ; ( c ) The induced clusters U ; ( d ) The new graph G1 over clusters U ; ( e ) The new bipartite graph over G1 Table 3 : Performance on T3 using a pre-defined tree structure . Table 2 : Performance on T2 using a pre-defined tree structure . Table 1 : Comparison against Stevenson and Joanis ( 2003 ) ’ s result on T1 ( using similar features ) . Table 4 : NMI of unconstrained HGFC when trees for T2 and T3 are inferred automatically . Table 1 : Effect of Factors Table 2 : Performance of Algorithms Table 1 : Notation and signatures for our framework . Table 3 : Additional notation and signatures for CAM Table 2 : CoreLex ’ s basic types with their corresponding WordNet anchors . CAM adopts these as meta senses . Table 4 : Sample of experimental items for the meta alternation anm-fod . ( Abbreviations are listed in Table 2 . ) Table 6 : Sample targets for meta alternations with high AP and mid-coherence values . Figure 2 : Average Precision and Coherence ( κ ) for each meta alternation . Correlation : r = 0.743 ( p < 0.001 ) Table 5 : Meta alternations and their average precision values for the task . The random baseline performs at 0.313 while the frequency baseline ranges from 0.255 to 0.369 with a mean of 0.291 . Alternations for which the model outperforms the frequency baseline are in boldface ( mean AP : 0.399 , standard deviation : 0.119 ) . Figure 1 : Illustration of dictionary based segmenta- tion finite state transducer Table 1 : Performance of the mention detection sys- tem using lexical features only . Table 2 : Performance of the mention detection sys- tem using lexical , syntactic , gazetteer features as well as features obtained by running other named-entity classifiers Table 3 : Performance of the mention detection sys- tem including all ACE ’ 04 subtasks Table 4 : Effect of Arabic stemming features on coref- erence resolution . The row marked with “ Truth ” represents the results with “ true ” mentions while the row marked with “ System ” represents that mentions are detected by the system . Numbers under “ ECM- F ” are Entity-Constrained-Mention F-measure and numbers under “ ACE-Val ” are ACE-values . Figure 1 : Organisation of the hierarchical graph of concepts Figure 2 : ( a ) An undirected graph G representing the similarity matrix ; ( b ) The bipartite graph showing three clusters on G ; ( c ) The induced clusters U ; ( d ) The new graph G1 over clusters U ; ( e ) The new bipartite graph over G1 Figure 4 : Salient features for fire and the violence cluster Figure 3 : Discovered metaphorical associations Figure 5 : Identified metaphorical expressions for the mappings FEELING IS FIRE and CRIME IS A DISEASE Figure 6 : Metaphors tagged by the system ( in bold ) Figure 1 : Outline of word segmentation process Table 2 : Segmentation results by a pure subword-based IOB tagging . The upper numbers are of the character- based and the lower ones , the subword-based . Table 1 : Our segmentation results by the dictionary- based approach for the closed test of Bakeoff 2005 , very low R-oov rates due to no OOV recognition applied . Table 4 : Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score Table 3 : Effects of combination using the confidence measure . The upper numbers and the lower numbers are of the character-based and the subword-based , respec- tively Table 1 : Feature set for the baseline pronoun res- olution system Figure 1 : structured-features for the instance i { “ him ” , “ the man ” } Table 2 : Results of the syntactic structured fea- tures Table 6 : Results using different parsers Table 5 : Comparison of the structured feature and the flat features extracted from parse trees Figure 2 : Learning curves of systems with different features Table 1 Feature templates used for CRF in our system Table 2 performance each step of our system achieves Figure 1 A framework for jointly identifying and aligning bilingual NEs . Table 2 Initial type-sensitive Chinese/English NER performance . Table 3 NEA type-insensitive ( type-sensitive ) performance on the test set . Table 5 NEA type-insensitive ( type-sensitive ) performance with the same Chinese NE recognizer ( Wu ’ s system ) and different English NE recognizers . Table 4 NER type-insensitive ( type-sensitive ) performance of different English NE recognizers . Table 6 NER type-insensitive ( type-sensitive ) performance of different Chinese NE recognizers . Table 7 NEA type-insensitive ( type-sensitive ) performance with the same English NE recognizer ( Mallet system ) and different Chinese NE recognizers . Table 8 NEA type-insensitive ( type-sensitive ) performance with a different English NE recognizer and another Chinese NE recognizer . Table 9 Initial NE recognition type-insensitive ( type-sensitive ) performance across various domains . Table 10 The superiority of our joint model on three different domains indicated by type-insensitive ( type-sensitive ) performance ( those signiﬁcant entries are marked in comparison with baseline ) . Table 11 Comparison between a ME framework and the derived model on the same test set . Table 12 Distribution of various error categories ( type-insensitive ) . Table 13 Distribution of Category ( VI ) error classes ( type-insensitive ) . Table 14 Top four worst-case statistics of features for NE boundary errors . Table 16 Distribution of the NE type errors ( MERT-W ) . Table 15 Effect of adjacent contextual ( non-NE ) bigrams on the test set . Table 18 Type-sensitive improvement for Chinese/English NER . Table 17 Type-insensitive improvement for Chinese/English NER . Table 19 English NE recognition on test data after semi-supervised learning . Table 20 NE alignment on test data after semi-supervised learning . Figure 1 : Syntactical Variations of “ activate ” Figure 2 : Extraction of Raw Pattern Figure 3 : Division of Raw Pattern into Combina- tion Pattern Components ( Entity-Main-Entity ) Figure 5 : Example Demonstrating Advantages of Full Parsing Table 2 : Features for SVM Learning of Prediction Model Figure 4 : Results of IE Experiment Figure 7 : Effect of Training Corpus Size ( 2 ) Figure 6 : Effect of Training Corpus Size ( 1 ) Table 4 : Causes of Error for FPs Figure 2 : Comparison for Head and Tail datasets Figure 1 : Illustration of entity-relationship graphs Table 1 : Summary for graphs and test datasets obtained from each seed pair Figure 3 : How to get the ordered set B t ( i , j , θ ) Table 2 : MRR of baseline and reinforced matrices Figure 5 : Distribution over number of hits Figure 6 : Matched translations over |Ve | and |Vc | Figure 4 : Parameter setup for λ , θ , and δ Table 4 : Precision , Recall , and F1-score of Engkoo , Google , and Ours with head and tail datasets Table 3 : Precision , Recall , and F1-score of Baseline , Engkoo , Google , and Ours over test sets Ti Figure 1 : The first set of features in our model . All of them are binary . The final feature set includes two sets : the set here , and a set obtained by its conjunction with the verb ’ s lemma . Figure 2 : An example parse tree for the ‘ second head word ’ feature . Table 1 : Accuracy and error reduction ( ER ) results ( in percents ) for our model and the MF baseline . Error reduction is computed as M ODEL−M 100−M F F . Results are given for the WSJ and GENIA corpora test sets . The top table is for a model receiving gold standard parses of the test data . The bottom is for a model using ( Charniak and Johnson , 2005 ) state-of-the-art parses of the test data . In the main scenario ( left ) , instances were always mapped to VN classes , while in the OIP one ( right ) it was possible ( during both training and test ) to map instances as not belonging to any existing class . For the latter , no results are displayed for polysemous verbs , since each verb can be mapped both to ‘ other ’ and to at least one class . Table 1 Taxonomy of Chinese words used in developing MSRSeg . Figure 1 ( a ) A Chinese sentence . Slashes indicate word boundaries . ( b ) An output of our word segmentation system . Square brackets indicate word boundaries . + indicates a morpheme boundary . Figure 2 Taxonomy of morphologically derived words ( MDWs ) in MSRSeg . Table 3 Words in the MSR gold test set . Table 2 Domain/style distribution in the MSR test corpus . Table 4 Standards and corpora . Table 5 Evaluation measures for Chinese word segmenter . Table 6 Context model , word classes , class models , and feature functions . Figure 4 The perceptron training algorithm for Chinese word segmentation . Figure 5 Overall architecture of MSRSeg . Table 7 Generative patterns of ONA , where sij denotes the j-th character of the i-th word of ON ( Sun , Zhou and Gao 2003 ) . Table 8 FT detection results on the MSR gold test set . The ‘ All ’ column shows the results of detecting all 10 types of factoids , as described in Table 1 , which amount to 6630 factoids , as shown in Table 3 . Table 12 NWI results on HK and AS corpora , NWI as post-processor versus unified approach . Table 9 NW 11 identification results on PK test set . Table 10 NW 21 identification results on PK test set . Table 11 NWI results on PK and CTB corpora , NWI as post-processor versus unified approach . Figure 6 Word internal structure and class-type transformation templates . Table 13 Comparison scores for PK open and CTB open . Table 15 Size of training data set and the adaptation results on AS open . Table 14 Comparison scores for HK open and AS open . Table 16 Methods of resolving OAs in word segmentation , on the MSR test set . ý Figure 7 ( a ) A Chinese OAS . ( b ) Two sentences in the training set , which contain t whose OASs have been replaced with the single tokens < OAS > . ( Li et al . 2003 ) . Figure 8 Results of 70 high-frequency two-character CASs . ‘ Voting ’ indicates the accuracy of the baseline method that always chooses the more frequent case of a given CAS . ‘ ME ’ indicates the accuracy of the maximum-entropy classifier . ‘ VSM ’ indicates the accuracy of the method of using VSM for disambiguation . Table 17 Comparison of performance of MSRSeg : The versions that are trained using ( semi- ) supervised iterative training with different initial training sets ( Rows 1 to 8 ) versus the version that is trained on annotated corpus of 20 million words ( Row 9 ) . Table 20 Precision of organization name recognition on the MSR test set , using Viterbi iterative training , initialized by four seed sets with different sizes . Table 19 Precision of location name recognition on the MSR test set , using Viterbi iterative training , initialized by four seed sets with different sizes . Table 18 Precision of person name recognition on the MSR test set , using Viterbi iterative training , initialized by four seed sets with different sizes . Table 21 MSRSeg system results for the MSR test set . Table 23 Comparisons against other segmenters : In Column 1 , SXX indicates participating sites in the 1st SIGHAN International Chinese Word Segmentation Bakeoff , and CRFs indicates the word segmenter reported in ( Peng et al . 2004 ) . In Columns 2 to 5 , entries contain the F-measure of each segmenter on different open runs , with the best performance in bold . Column Site-Avg is the average F-measure over the data sets on which a segmenter reported results of open runs , where a bolded entry indicates the segmenter outperforms MSRSeg . Column Our-Avg is the average F-measure of MSRSeg over the same data sets , where a bolded entry indicates that MSRSeg outperforms the other segmenter . Table 22 Cross-system comparison results . Table 1 : Results for the acquisition of subcategori- sation frames . Table 3 : Clustering evaluation for the experiment with Named Entities Table 2 : Clustering evaluation for the experiment without Named Entities Figure 1 . Entity detection and tracking system flow . Figure 2 . The procedure of TBL entity track- ing/coreference model Table 8 . EDT and mention detection results . Table 7 . Statistics of the ACE corpus . Table 6 . Examples of transformation rules of Table 5 . Templates for feedback . Table 1 : Examples of zero anaphora Table 2 : Examples of zero anaphora Figure 1 : ( a ) A Chinese sentence . Slashes indicate word boundaries . ( b ) An output of our word segmentation system . Square brackets indicate word boundaries . + indicates a morpheme boundary . Table 1 Subtypes of the ArgM modifier tag . Figure 1 Split constituents : In this case , a single semantic role label points to multiple nodes in the original treebank tree . Table 4 Confusion matrix among subtypes of ArgM , defined in Table 1 . Entries are fraction of all ArgM labels . Entries are a fraction of all ArgM labels ; true zeros are omitted , while other entries are rounded to zero . Table 3 Confusion matrix for argument labels , with ArgM labels collapsed into one category . Entries are a fraction of total annotations ; true zeros are omitted , while other entries are rounded to zero . Table 5 Comparison of frames . Table 6 Most frequent semantic roles for each syntactic position . Table 7 Most frequent syntactic positions for each semantic role . Table 8 Semantic roles of verbs ’ subjects , for the verb classes of Merlo and Stevenson ( 2001 ) . Table 8 ( cont . ) Table 9 Semantic roles for different frame sets of kick . Figure 2 In this example , the path from the predicate ate to the argument NP He can be represented as VBjVPjS , NP , with j indicating upward movement in the parse tree and , downward movement . Figure 3 Back-off lattice with more specific distributions towards the top . Table 10 Accuracy of semantic-role prediction ( in percentages ) for known boundaries ( the system is given the constituents to classify ) . Table 11 Accuracy of semantic-role prediction ( in percentages ) for unknown boundaries ( the system must identify the correct constituents as arguments and give them the correct roles ) . Table 12 Common values ( in percentages ) for parse tree path in PropBank data , using gold-standard parses . Table 13 Accuracy of semantic-role prediction for unknown boundaries ( the system must identify the correct constituents as arguments and give them the correct roles ) . Table 14 Summary of results for unknown-boundary condition . Figure 1 : The directional matching relationships between a hypothesis ( h ) , an entailment rule ( r ) and a text ( t ) in the Contextual Preferences framework . Table 2 : Recall ( R ) , Precision ( P ) and Mean Average Pre- cision ( MAP ) when also using rules for matching . Table 1 : Recall ( R ) , Precision ( P ) and Mean Average Pre- cision ( MAP ) when only matching template hypotheses directly . Figure 2 : Recall-Precision curves for ranking using : ( a ) only the prior ( baseline ) ; ( b ) allCP ; ( c ) allCP+pr . Table 3 : MAP ( % ) , under the ‘ 50 rules , All ’ setup , when adding component match scores to Precision ( P ) or prior- only MAP baselines , and when ranking with allCP or allCP+pr methods but ignoring that component scores . Table 1 . Unigram , bigram and trigram counts of the ligature corpus Table 2 . Unigram , bigram and trigram counts of the word corpus Table 3 . Results changing beam width k of the tree Table 2 Sample analysis of an English sentence . Input : Do we have to reserve rooms ? . Table 3 Resolution of ambiguity on the Verbmobil corpus . Table 4 Candidates for equivalence classes . Figure 1 Training and test with hierarchical lexicon . “ ( Inverse ) restructuring , ” “ analyze , ” and “ annotation ” all require morpho-syntactic analysis of the transformed sentences . Figure 2 Disambiguation of conventional dictionaries . “ Learn phrases , ” “ analyze , ” and “ annotation ” require morpho-syntactic analysis of the transformed sentences . Figure 3 Training with scarce resources . “ Restructuring , ” “ learn phrases , ” and “ annotation ” all require morpho-syntactic analysis of the transformed sentences . Table 5 Statistics of corpora for training : Verbmobil and Nespole ! Singletons are types occurring only once in training . Table 7 The official vocabularies in Verbmobil . Table 8 Statistics for the test sets for German to English translation : Verbmobil Eval-2000 ( Test and Develop ) and Nespole ! Table 6 Conventional dictionary used to complement the training corpus . Figure 4 Impact of corpus size ( measured in number of running words in the corpus ) on vocabulary size ( measured in number of different full-form words found in the corpus ) for the German part of the Verbmobil corpus . Table 11 Results for hierarchical lexicon model Nespole ! “ Restructuring ” entails treatment of question inversion and separated verb prefixes as well as merging of phrases in both languages . The same conventional dictionary was used as in the experiments the Verbmobil . The language model was trained on a combination of the English parts of the Nespole ! corpus and the Verbmobil corpus . Table 10 Examples of the effect of the hierarchical lexicon . Table 6 . The performance on the set of unknown Table 4 . Experimental result of total unknown Table 5 . The performance on the set of unknown Figure 2 : Example of a so-called semi-formal text , where one can see that here more time points are available , and that those can be complemen- tary to the time points to be extracted from formal texts . So , already at this level , a unification or merging of extracted time points is necessary . Figure 1 : The value of the penalized loss based on the number of iterations : DPLVMs vs. CRFs on the MSR data . Table 1 : Details of the corpora . W.T . represents word types ; C.T . represents character types ; S.C. represents simpliﬁed Chinese ; T.C . represents traditional Chinese . Table 3 : Error analysis on the latent variable seg- menter . The errors are grouped into four types : over- generalization , errors on named entities , errors on idioms and errors from data-inconsistency . Table 1 . Numbers of relations on the ACE RDC 2004 : break down by relation types and subtypes Figure 2 . Stratefied Sampling for initial seeds Table 2 . The initial performance of applying various sampling strategies to selecting the initial Table 3 . The highest performance of applying various sampling strategies in selecting the initial seed set on the ACE RDC 2004 corpus Figure 5 . Bootstrapping time for different p values Figure 4 . Performance for different p values Figure 3 . Comparison of two augmentation strategies over different sampling strategies in selecting the initial seed set . Table 4 . Comparison of semi-supervised relation classification systems on the ACE RDC 2003 corpus Figure 1 . A pruned phrase tokenization lattice . Edges are tokenizations of phrases , e.g . e5 represents tokenizing 质疑 ‘ question ’ into a word and e7 represents tokenizing 疑他 ‘ doubt him ’ into a partial word 疑 ‘ doubt ’ followed by a word 他 ‘ him ’ . Figure 2 . The pseudo code of Algorithm 1 . Table 3 . The Riv over the bakeoff-2 data . Table 2 . The Roov over the bakeoff-2 data . Table 1 . The F-score over the bakeoff-2 data . Figure 1 A focused entailment graph . For clarity , edges that can be inferred by transitivity are omitted . The single strongly connected component is surrounded by a dashed line . Table 1 Positive and negative examples for entailment in the training set . The direction of entailment is from the left template to the right template . Table 2 The similarity score features used to represent pairs of templates . The columns specify the corpus over which the similarity score was computed , the template representation , the similarity measure employed , and the feature representation ( as described in Section 4.1 ) . Table 3 Scenarios in which we added hard constraints to the ILP . Table 4 Results when tuning for performance over the development set . Table 5 Results when the development set is not used to estimate λ and K . Table 6 Results with prior estimated on the development set , that is η = 0.1 , which is equivalent to λ = 2.3 . Figure 2 Recall-precision curve comparing ILP-Global with Greedy-Global and ILP-Local . Table 7 Results per concept for the ILP-Global . Table 8 Results of all distributional similarity measures when tuning K over the development set . We encode the description of the measures presented in Table 2 in the following manner— h = health-care corpus ; R = RCV1 corpus ; b = binary templates ; u = unary templates ; L = Lin similarity measure ; B = BInc similarity measure ; pCt = pair of CUI tuples representation ; pC = pair of CUIs representation ; Ct = CUI tuple representation ; C = CUI representation ; Lin & Pantel = similarity lists learned by Lin and Pantel . Table 9 Comparing disagreements between ILP-Global and ILP-Local against the gold-standard graphs . Figure 4 A comparison between ILP-Global and ILP-Local for two fragments of the test-set concept seizure . Figure 3 A comparison between ILP-Global and ILP-local for two fragments of the test-set concept diarrhea . Table 10 Comparing disagreements between ILP-Global and Greedy-Global against the gold-standard graphs . Figure 5 A comparison between ILP-Global and Greedy-Global . Parts A1–A3 depict the incremental progress of Greedy Global for a fragment of the headache graph . Part B depicts the corresponding fragment in ILP-Global . Nodes surrounded by a bold oval shape are strongly connected components . Figure 6 Distribution of probabilities given by the classiﬁer over all node pairs of the test-set graphs . Table 11 Error analysis for false positives and false negatives . Figure 7 A scenario where ILP-Global makes a mistake , but ILP-Local is correct . Table 12 The set of new features . The last two columns denote the number and percentage of examples for which the value of the feature is non-zero in examples generated from the 23 gold-standard graphs . Table 13 Macro-average recall , precision , and F1 on the development set and test set using the parameters that maximize F1 of the learned edges over the development set . Table 14 Results of feature analysis . The second column denotes the proportion of manually annotated examples for which the feature value is non-zero . A detailed explanation of the other columns is provided in the body of the article . Figure 8 A hierarchical summary of propositions involving nausea as an argument , such as headache is related to nausea , acupuncture helps with nausea , and Lorazepam treats nausea . Table 1 Example distributions of German verbs . Table 2 Data similarity measures . Table 3 k-means experiment baseline and upper bound . Table 4 Comparing distributions on D1 and D2 . Table 5 Comparing similarity measures on D1 and D2 . Table 7 Comparing clustering initializations on D2 . Table 6 Comparing clustering initializations on D1 . Table 8 Comparing feature descriptions . Table 10 Comparing selectional preference frame definitions . Table 9 Comparing selectional preference slot definitions . Figure 1 Varying the number of clusters ( evaluation : Randadj ) . Table 11 Large-scale clustering on D1 . Table 13 Large-scale clustering on D3 with n/na/nd/nad/ns-dass . Table 12 Large-scale clustering on D2 . Figure 3 : Learning curves on the development dataset of the Beijing Univ . corpus . Figure 2 : Learning curves on the development dataset of the HK City Univ . corpus . Table 3 : Official Bakeoff Outcome Table 2 : F-score on development data Figure 2 Summariser and VPA Architecture Figure 3 Positional sentence weight for varying Figure 4 Precision by Named Entity Class Figure 5 Recall by Named Entity Class Figure 6 Average Precision and Recall Figure 7 Summaries Recall and Precision Figure 1 . Relation Feature Spaces of the Example Sentence “ …… to stop the merger of an estimated Table 1 . Performance of seven relation feature spaces over the 5 ACE major types using parse tree information only Table 3 . Performance comparison , the numbers in parentheses report the performance over the 24 ACE subtypes while the numbers outside paren- theses is for the 5 ACE major types Table 1 Distribution of antecedent NP types in the other-anaphora data set . Table 2 Overview of the results for all baselines for other-anaphora . Table 3 Descriptive statistics for WordNet hyp/syn relations for other-anaphora . Table 4 Patterns and instantiations for other-anaphora . Table 5 Descriptive statistics for Web scores and BNC scores for other-anaphora . Table 6 Properties of the variations for the corpus-based algorithms for other-anaphora . Table 7 Web results for other-anaphora . Table 8 BNC results for other-anaphora . Table 9 Overview of the results for the best algorithms for other-anaphora . Table 10 Occurrences of error types for the best other-anaphora algorithm algoWebv4 . Table 11 Distribution of antecedent NP types for definite NP anaphora . Table 12 Overview of the results for all baselines for coreference . Table 13 Descriptive statistics for WordNet hyp/syn relations on the coreference data set . Table 14 Overview of the results for all WordNet algorithms for coreference . Table 15 Overview of the results for all Web algorithms for coreference . Table 16 Overview of the results for all BNC algorithms for coreference . Table 17 Occurrences of error types for the best coreference algorithm algoWebv4n . Table 1 : Word segmentation on NIST data sets Figure 1 : Example of 1-to-n word alignments be- tween English words and Chinese characters Table 2 : Word segmentation on IWSLT data sets Figure 2 : Example of a word lattice Table 6 : BS on IWSLT 2007 task Table 5 : BS on IWSLT 2006 task Table 3 : Corpus statistics for Chinese ( Zh ) character segmentation and English ( En ) Table 4 : BS on NIST task Table 10 : Scalability of BS on NIST task Table 7 : Vocabulary size of NIST task ( 40K ) Table 9 : Scale-up to 160K on IWSLT data sets Table 8 : Vocabulary size of IWSLT task ( 40K ) Figure 3 : The search graph on development set of IWSLT task Table 11 : BS on IWSLT data sets using MTTK Figure 2 : Word alignment based translation model P ( J , A|E ) ( IBM Model 4 ) Figure 1 : Example of word alignment Figure 3 : Example of chunk-based alignment Figure 4 : Chunk-based translation model . The words in bold are head words . Table 1 : Basic Travel Expression Corpus Table 2 : Experimental results for Japanese–English Figure 5 : Examples of viterbi chunking and chunk alignment for English-to-Japanese translation model . Chunks are bracketed and the words with ∗ to the left are head words . Table 1 : An example of English , Chinese and French terms consisting of the same morphemes Table 2 : Example of first and second order features using a predefined n-gram size of 2 . Figure 1 : Example of a term construction rule as a branch in a decision tree . Figure 2 : F-Score of the RF and SVM , GIZA++ and Levenshtein distance-based classifier on the first order dataset Table 3 : Best observed performance of RF , SVM and GIZA++ and Levenshtein Distance Figure 3 : F-Score of the RF and SVM , GIZA++ and Levenshtein distance-based classifier on the second order dataset Table 1 . OBI vs. BI ; where the lost of F > 1 % , such as SC-B , is caused by incorrect English segments that will be discussed in the section 4 . Table 2 . Baseline vs . Submitted Results Figure 1 : An example of annotation projection for relation detection of a bitext in English and Korean Table 1 : Numbers of projected instances Table 1 : Rules and patterns for the four syntactico-semantic structures . Regular expression notations : ‘ * ’ matches the preceding element zero or more times ; ‘ + ’ matches the preceding element one or more times ; ‘ ? ’ indicates that the preceding element is optional ; ‘ | ’ indicates or . Abbreviations : Ec ( m ) : coarse-grained entity type of mention m ; Ld : labels in dependency path between the headword of two mentions . We use square brackets ‘ [ ’ and ‘ ] ’ to denote mention boundaries . The ‘ / ’ in the Formulaic row denotes the occurrence of a lexical ‘ / ’ in text . Table 3 : Additional RE features . Table 5 : Micro-averaged ( across the 5 folds ) RE results using predicted mentions . Table 4 : Micro-averaged ( across the 5 folds ) RE results using gold mentions . Table 6 : Recall and precision of the patterns . Figure 4 : Improvement in ( predicted mention ) RE . Figure 3 : Improvement in ( gold mention ) RE . Table 2 : Comparison of our system with the best-reported systems on MUC-6 and MUC-7 Table 2 : A second example of disagreement in segmentation guidelines Table 1 : Examples of disagreement in segmentation guidelines Table 3 : Analysis of results of segmentation on LDC training and test data for all CWS schemes Table 4 : BLEU scores for CWS schemes Table 5 : Correlation between F-score and BLEU Table 7 : Feature blending of translation models Table 6 : Feature interpolation of translation models : A=ICTCLAS , B=dict-hybrid , C=dict-PKU-LDC , D=dict-CITYU , E=CRF-AS Table 1 Common grammatical relations of Minipar involving nouns . Table 2 The top 20 most similar words for country ( and their ranks ) in the similarity list of LIN , followed by the next four words in the similarity list that were judged as entailing at least in one direction . Table 3 The top 10 ranked features for country produced by MI , the weighting function employed in the LIN method . Table 4 Lexical entailment precision values for top-n similar words by the Bootstrapped LIN and the original LIN method . Figure 1 Percentage of correct entailments within the top 40 candidate pairs of each of the methods , LIN and Bootstrapped LIN ( denoted as LINB in the ﬁgure ) , when using varying numbers of top-ranked features in the feature vector . The value of “ All ” corresponds to the full size of vectors and is typically in the range of 300–400 features . Table 5 Comparative precision values for the top 20 similarity lists of the three selected similarity measures , with MI and Bootstrapped feature weighting for each . Table 6 Top 30 features of town by bootstrapped weighting based on LIN , WJ , and COS as initial similarities . The three sets of words are almost identical , with relatively minor ranking differences . Table 8 Top 10 features of country by the Bootstrapped feature weighting . Table 7 LIN ( MI ) weighting : The top 10 common features for country–state and country–party , along with their corresponding ranks in each of the two feature vectors . The features are sorted by the sum of their feature weights with both words . Table 10 Top 20 most similar words for country and their ranks in the similarity list by the Bootstrapped LIN measure . Table 9 Bootstrapped weighting : top 10 common features for country–state and country–party along with their corresponding ranks in the two ( sorted ) feature vectors . Figure 2 Comparison between the acfr-ratio for MI and Bootstrapped LIN methods , when using varying numbers of common top-ranked features in the words ’ feature vectors . Table 11 The comparative error rates of the pseudo-disambiguation task for the three examined similarity measures , with and without applying the bootstrapped weighting for each of them . Figure 1 : Wrong assignment due to missing sense : from the Hound of the Baskervilles , Ch . 14 Figure 2 : Sample Minipar parse and extracted gram- matical function features Table 1 : Experiment 1 : Results for label unknown sense , WSD confidence level approach . θ : confi- dence threshold . σ : std . dev . Figure 3 : Outlier detection by comparing distances between nearest neighbors Table 2 : Experiment 2 : Results for label unknown sense , NN-based outlier detection , θ = 1.0. σ : stan- dard deviation Table 3 : Experiment 2 : Results by training set size , θ = 1.0 Table 5 : Experiment 3 : Results for label unknown sense , NN-based outlier detection , θ = 1.0. σ : stan- dard deviation Table 4 : Extending training sets : an example Figure 4 : “ Acceptance radius ” of an outlier within the training set ( left ) and a more “ normal ” training set object ( right ) Table 6 : Experiment 3 : Results by training set size , θ = 1.0 Table 7 : Experiments 2 and 3 : Results by the num- ber of senses of a lemma , condition All , θ = 1.0 Figure 1 : Example entries for the Transfer of a Message - levels 1 and 2 classes Figure 2 : Clusters for transitive , unaccusative , and ditransitive Figure 3 : Outcome of clustering procedure Figure 1 : Average Precision , Recall and F1 at dif- ferent top K rule cutoff points . Table 3 : Distribution of reasons for false negatives ( missed argument mentions ) by BInc at K=20 . Table 2 : Distribution of reasons for false positives ( incorrect argument extractions ) by BInc at K=20 . Table 1 : Rule type distribution of a sample of 200 rules that extracted incorrect mentions . The corre- sponding numbers of incorrect mentions extracted by the sampled rules is shown in parentheses . Figure 1 : The Maytag interface Figure 1 : Translation extraction from comparable corpora using cross-lingual WSI and WSD . Table 1 : Entries from the English-Slovene sense cluster inventory . Table 2 : Disambiguation results . Table 4 : Comparison of different configurations . Table 3 : Results of the experiment . Figure 1 : GC examples . Table 1 : Devtest Set Statistics by Language Table 2 : F-measure ( % ) Breakdown by Mention Type : NAM ( e ) , NOM ( inal ) , PRE ( modifier ) and PRO ( noun ) . Chinese data does not have the PRE type . Table 3 : Impact of Syntactic Features on English Sys- tem After Taking out Distance Features . Numbers are F-measures ( % ) . Table 5 : An example where syntactic features help to link the PRO mention Ñë ( hm ) with its antecedent , the NAM Table 4 : Distribution of Pronoun Mentions and Fre- quency of c-command Features Table 6 : Summary Results on the 2004 ACE Evaluation Data . Figure 2 : A Portion of the Syntactic Tree . Table 1 : An example of NE and non-NE Table 2 : Possibility combination of neighboring tokens within the corpus for PER Figure 1 : A NE detection window Figure 2 : All possible NEs identified in a test article c. Unification : When the tokens of two NEs are Fig . 3 : NEs after agents-based modification Table 3 : Results of MET2 under different configurations Table 2 : Priority Order for Second Person ADs Table 3 : Priority Order for Third Person ADs Table 1 : Priority Order for First Person ADs Table 1 : Basic Features for CRF-based Segmenter Table 4 : Performance of our system in the compe- tition Table 3 : Effectiveness of post-processing rules Figure 1 . Overview of the WEBRE algorithm ( Illustrated with examples sampled from experiment results ) . The tables and rec- tangles with a database sign show knowledge sources , shaded rectangles show the 2 phases , and the dotted shapes show the sys- tem output , a set of Type A relations and a set of Type B relations . The orange arrows denote resources used in phase 1 and the green arrows show the resources used in phase 2 . Table 4 . Pairwise precision/recall/F1 of WEBRE and SNE . Table 1 : Overall results by Vieira and Poesio Table 2 : Discourse-new prediction results by Bean and Riloff Table 3 : Evaluation of the three anaphoric resolvers discussed by Ng and Cardie . Table 4 : Results of Uryupina ’ s discourse new clas- sifier Table 5 : Results of Uryupina ’ s uniqueness classifier Table 8 : Using an oracle Table 7 : Evaluation of the GUITAR system without DN detection off raw text Table 6 : Evaluation of the GUITAR system without DN detection over a hand-annotated treebank Table 1 : Relation types and subtypes in the ACE training data Table 2 : Contribution of different features over 43 relation subtypes in the test data Table 6 : Distribution of errors Table 5 : Comparison of our system with other best-reported systems on the ACE corpus Table 4 : Performance of different relation types and major subtypes in the test data Table 3 : Distribution of relations over # words and # other mentions in between in the training data Table 2 : Results of the baseline model : best guess Table 1 : Word distribution in the extended Cilin Table 3 : Results of the baseline model : best 5 guesses Table 7 : Results of combining the character-category association and rule-based models : best guess Table 4 : Results of the character-category association model : best guess Table 5 : Results of the character-category association model : best 5 guesses Table 6 : Results of the rule-based model : best guess Table 12 : Results of the corpus-based model on words with different frequency Table 11 : Results of the corpus-based model Table 10 : Parameter settings of the corpus-based model Table 9 : Results of the combined model for classify- ing unknown words into major and medium catego- ries : best guess Figure 1 : Re-ordering for the German verbgroup . Table 1 : DP algorithm for statistical machine translation . Table 2 : Coverage set hypothesis extensions for the IBM re-ordering . Table 4 : Multi-reference word error rate ( mWER ) Table 6 : Example Translations for the Verbmobil task . Table 1 : Examples of non-phonetic translations . Figure 1 : Dissimilarity of temporal distributions of ‘ WTO ’ in English and Chinese corpora . Figure 2 : Framework overview . Table 2 : Evidence cardinality in the corpora . Figure 3 : Network of relations . Edges indicate that the relations have a non-empty support inter- section , and edge labels show the size of the inter- section . Figure 4 : Relation clusters and a few individual relations . Edge labels show the size of the inter- section . Figure 6 : MRR with decreasing comparability . Table 4 : Example translations from the different methods . Boldface indicates correct translations . Table 3 : Evaluation results of the methods . Figure 5 : Example of similar document pairs . Table 1 : Automatically generated training set examples . Table 3 : Impact of scaling techinques ( ILP− /ILPscale ) . Table 2 : micro-average F1 and AUC for the algorithms . Figure 2 : Precision-recall curve for the algorithms . Figure 1 : A pair of comparable , non-parallel documents Figure 2 : A pair of comparable sentences . Figure 3 : A Parallel Fragment Extraction System Figure 4 : Translated fragments , according to the lexicon . Figure 5 : Our approach for detecting parallel fragments . The lower part of the figure shows the source and target sentence together with their alignment . Above are displayed the initial signal and the filtered signal . The circles indicate which fragments of the target sentence are selected by the procedure . Table 2 : Sizes of the extracted datasets . Table 1 : Sizes of our comparable corpora Figure 6 : SMT performance results Table 2 : Incremental evaluations , by incrementally adding new features ( word features and high dimensional edge features ) , new word detection , and ADF training ( replacing SGD training with ADF training ) . Number of passes is decided by empirical convergence of the training methods . Figure 2 : F-score curves on the MSR , CU , and PKU datasets : ADF learning vs. SGD and LBFGS training methods . Table 3 : Comparing our method with the state-of-the-art CWS systems . Table 1 . Test corpora details Table 2 . Evaluation closed results on all data sets Table 4 . Evaluation open results on all test sets Table 3 . Comparison our closed results with the top three in all test sets Figure 1 : Architecture of the translation approach based on Bayes ’ decision rule . Table 1 : Some training events for the English word “ which ” . The symbol “ ” is the placeholder of the English word “ which ” in the English context . In the German part the placeholder ( “ ” ) corresponds to the word aligned to “ which ” , in the first example the German word “ die ” , the word “ das ” in the second and the word “ was ” in the third . The considered English and German contexts are separated by the double bar “ p ” .The last number in the rightmost position is the number of occurrences of the event in the whole corpus . Table 2 : Meaning of different feature categories where s represents a specific target word and t repre- sents a specific source word . Table 3 : The 10 most important features and their respective category and values for the English word “ which ” . f Table 4 : Number of features used according to different cut-off threshold . In the second column of the table are shown the number of features used when only the English context is considered . The third column correspond to English , German and Word-Classes contexts . Table 5 : Corpus characteristics for translation task . Table 7 : Training and Test perplexities us- ing different contextual information and different thresholds  . The reference perplexities obtained with the basic translation model 5 are TrainPP = 10.38 and TestPP = 13.22 . Table 6 : Corpus characteristics for perplexity quality experiments . Table 8 : Preliminary translation results for the Verbmobil Test-147 for different contextual infor- mation and different thresholds using the top-10 translations . The baseline translation results for model 4 are WER=54.80 and PER=43.07 . Table 9 : Four examples showing the translation obtained with the Model 4 and the ME model for a given German source sentence . Table 1 : Monolingual and Crosslingual Baseline Slot Filling Pipelines Table 2 : Baseline Pipeline Results Table 3 : Distribution of Spurious Errors Table 4 : Validation Features for Crosslingual Slot Filling Table 5 : Using Basic Features to Filter Answers Table 7 : Fact vs. Statistical Cross-Doc Features Table 1 . The count of the types of anaphora per corpus . Table 2 . Bibliome system scores at Bacteria Biotope Task in BioNLP shared tasks 2011 . Figure 1 : Architecture of the translation approach based on Bayes ’ decision rule . Table 1 : Candidates for equivalence classes . Table 2 : Corpus statistics : Verbmobil training . Singletons are types occurring only once in train- ing . Table 3 : Statistics of the Verbmobil test corpus for German-to-English translation . Unknowns are word forms not contained in the training corpus . Table 5 : Effect of the introduction of equivalence classes . For the baseline we used the original in- flected word forms . Figure 3 : Examples for the effect of the combined lexica . Table 4 : Effect of two-level lexicon combination . For the baseline we used the conventional one-level full form lexicon . Figure 4 : Examples for the effect of equivalence classes resulting from dropping morpho-syntactic tags not relevant for translation . First the translation using the original representation , then the new representation , its reduced form and the resulting translation . Table 1 : The incompleteness of Freebase ( * are must- have attributes for a person ) . Figure 1 : Plate diagram of our model . Table 2 : False negative matches on the Riedel ( Riedel et al. , 2010 ) and KBP dataset ( Surdeanu et al. , 2012 ) . All numbers are on bag ( pairs of entities ) level . BD* are the numbers before downsampling the negative set to 10 % and 5 % in Riedel and KBP dataset , respectively . Figure 2 : Performance on the KBP dataset . The figures on the left , middle and right show MIML , Hoffmann , and Mintz++ compared to the same MIML-Semi curve , respectively . MIML-Semi is shown in red curves ( lighter curves in black and white ) while other algorithms are shown in black curves ( darker curves in black and white ) . Figure 1 : Dependency parse tree for the sentence ( in the ACE corpus ) : “ [ Toujan Faisal ] , 54 , { said } [ she ] was { informed } of the refusal by an [ Interior Min- istry committee ] overseeing election preparations . ” Table 2 : Under-sampled system for the task of rela- tion detection . The proportion of positive examples in the training and test corpus is 50.0 % and 20.6 % respectively . Table 5 : System for the task of relation classifica- tion . The two classes are INR and COG , and we evaluate using accuracy ( Acc. ) . The proportion of INR relations in training and test set is 49.7 % and 49.63 % respectively . Figure 2 : Clustering-based stratified seed sampling Table 4 : Performance of various clustering-based seed sampling strategies on the held-out test data with the optimal cluster number for each clustering algorithm Table 3 : Performance in F1-score over different cluster numbers with intra-stratum sampling on the develop- ment data Table 1 : 50-document corpora averages Table 4 : LO Dice configuration scores Table 3 : Best LO and LL configurations scores Table 2 : Best MAP in Experiment 1 Table 5 : LO sentence configuration scores Table 6 : LO cosine sentence configuration scores Figure 5 : Average rank of correct translation according to average source term frequency Table 7 : LO cosine sentence configuration scores Table 1 : Feature set for our pronoun resolution system ( *ed feature is only for the single-candidate model while **ed feature is only for the twin-candidate mode ) Table 2 : The performance of different resolution systems Table 4 : Results of different feature groups under the TC model for N-pron resolution Table 1 : Relationship types and their argument type con- straints . Table 2 : Count of relationships in 77 gold standard documents . Figure 1 : The relationship extraction system . Table 3 : Feature sets used for learning relationships . The size of a set is the number of features in that set . Table 4 : Variation in performance by feature set . Features sets are abbreviated as in Table 3 . For the first seven columns , features were added cumulatively to each other . The next two columns , allgen and notok , are as de- scribed in Table 3 . The final two columns give inter annotator agreement and corrected inter annotator agreement , for comparison . Table 5 : Variation in performance , by number of sentence boundaries ( n ) , and by training corpus size . Table 2 Mechanical evaluation of translation Figure 1 Japanese-to-English Display of NICT- ATR Speech-to-Speech Translation System Table 1 Evaluation of speech recognition Table 3 Human Evaluation of translation Figure 1 : A fragment of an entailment graph ( a ) , its SCC graph ( b ) and its reduced graph ( c ) . Nodes are predicates with typed variables ( see Section 5 ) , which are omitted in ( b ) and ( c ) for compactness . Figure 4 : Three types of transitivity constraint violations . Figure 5 : Run-time in seconds for various −λ values . Table 1 : Predictive power of admissible and almost admissible heuristic functions . Table 3 : Training corpus statistics ( * without punctuation marks ) . Table 4 : Test corpora statistics . Table 7 : Average search time [ s ] per sentence . Table 5 : Search Success Rate ( 1 million hypothe- ses ) [ % ] . Table 6 : Search errors [ % ] . Table 2 : Effect of observation pruning on the translation quality ( average over all test sets ) . Table 9 : A* ( E+ ) Success Rate for 12- and 14-word sentences [ % ] . Table 8 : Translation quality . Figure 1 : Proposed method : data flow . Table 1 : Feature set used in the Stage 2 classifier , and their number for the causal relation experiments . Figure 2 : Precision of acquired relations ( causality ) . L and S denote lenient and strict evaluation . Figure 3 : Precision of acquired relations ( prevention ) . L and S denote lenient and strict evaluation . Figure 4 : Precision of acquired relations ( material ) . L and S denote lenient and strict evaluation . Figure 5 : Frequencies of patterns in the evaluation data ( causation ) . Figure 8 : Contribution of feature sets ( material ) . Figure 7 : Contribution of feature sets ( prevention ) . Figure 6 : Contribution of feature sets ( causality ) . Table 1 : Example of the context of “ 水 ” in “ 吃水 果 ( Eat fruits ) ” and the context of “ 篮 ” in “ 打篮球 ( Play basketball ) ” Table 2 : An example for the “ BMES ” representa- tion . The sentence is “ 我爱北京天安门 ” ( I love Bei- jing Tian-an-men square ) , which consists of 4 Chi- nese words : “ 我 ” ( I ) , “ 爱 ” ( love ) , “ 北京 ” ( Beijing ) , and “ 天安门 ” ( Tian-an-men square ) . Table 8 : Comparison of f-scores when changing the size of labeled data . ( 1/10 , 1/4 , 1/2 and all labeled data . The size of unlabeled data is ﬁxed as 5 million characters . ) Table 4 : Details of the PKU data Table 5 : Details of the unlabeled data . Table 10 : Comparison of our approach with the state-of-art systems Table 9 : Comparison of our approach with using only the Gigaword corpus Figure 1 : A Motivating Example Table 1 : Processed Data Statistics Table 2 : Sample I ( s ) Values Table 3 : Human Assessment of Errors Table 4 : Slot Value Translation Assessment from Ran- dom Sample of 1000 Figure 2 : Performance of Unsupervised Name Mining Figure 3 : Example of Learned Name Pairs with Gloss Translations in Parentheses Table 5 : Name Pairs Mined Using Previous Methods Figure 1 : Average word accuracy for transduced sentences . Figure 2 : Fraction of the sentences that were transduced . Figure 4 : Sizes of the automata . Figure 3 : Time consumption of transduction . Table 1 : Ten relation instances extracted by our system that did not appear in Freebase . Table 2 : The 23 largest Freebase relations we use , with their size and an instance of each relation . Figure 1 : Dependency parse with dependency path from ‘ Edwin Hubble ’ to ‘ Marshfield ’ highlighted in boldface . Table 3 : Features for ‘ Astronomer Edwin Hubble was born in Marshfield , Missouri ’ . Table 4 : Examples of high-weight features for several relations . Key : SYN = syntactic feature ; LEX = lexical feature ; x = reversed ; NE # = named entity tag of entity . Figure 2 : Automatic evaluation with 50 % of Freebase relation data held out and 50 % used in training on the 102 largest relations we use . Precision for three different feature sets ( lexical features , syntactic features , and both ) is reported at recall levels from 10 to 100,000 . At the 100,000 recall level , we classify most of the instances into three relations : 60 % as location-contains , 13 % as person-place-of-birth , and 10 % as person-nationality . Table 5 : Estimated precision on human-evaluation experiments of the highest-ranked 100 and 1000 results per relation , using stratified samples . ‘ Average ’ gives the mean precision of the 10 relations . Key : Syn = syntactic features only . Lex = lexical features only . We use stratified samples because of the overabundance of location-contains instances among our high-confidence results . Figure 1 . Linking FrameNet frames and VerbNet classes Table 1 . Results of the mapping algorithm Figure 2 . Mapping algorithm – refining step Table 3 . F1 and accuracy of the argument classifiers and the overall multiclassifier for FrameNet semantic roles Figure 3 . Semantic Role learning curve Table 1 . Examples of pseudo features Table 2 . Sources of the training data Table 3 . Number of candidates for each target language . Table 4 . Examples of the top-3 candidates in the transliteration of English – Chinese Table 5 . Examples of the top-3 candidates in the transliteration of English-Korean Table 8 . Number of evaluated English Name Table 7 . MRRs of the phonetic transliteration Table 6 . Size of the test data Table 10 . MRRs for the phonetic transliteration 2 Table 9 . MRRs of the phonetic transliteration Figure 1 : Dependency tree for the sentence “ PROT1 contains a sequence motif binds to PROT2 . ” Table 2 : Comparison with other PPI extraction systems in the AIMed corpus Table 4 : Comparison of contributions of different features to relation detection across multiple domains Table 3 : Comparison of performance across the five PPI corpora Figure 2 : Mixed Membership MEDLDA Table 1 : Relation types for ACE 05 corpus Table 2 : Overall performance of the 3 systems Table 3 : Multi-class Classification Results with PlusCOMP for SVM , LLDA and MEDLDA for the six ACE 05 categories and NO-REL Figure 4 : LLDA Fmeausres for 3 feature conditions Figure 3 : SVM Fmeausres for 3 feature conditions Figure 5 : MEDLDA Fmeausres for 3 feature conditions Table 4 : F-measures for every kernel in ( Khayyamian et al. , 2009 ) and MEDLDA Table 1 : Features used by paraphrase classifier . Figure 1 : Illustration of features f8-12 . Figure 2 : Bidirectional checking of entailment relation ( → ) of p1 → p2 and p2 → p1 . p1 is “ reduces bone mass ” in s1 and p2 is “ decreases the quantity of bone ” in s2 . p1 and p2 are exchanged between s1 and s2 to generate corresponding paraphrased sentences s01 and s02 . p1 → p2 ( p2 → p1 ) is verified if s1 → s01 ( s2 → s02 ) holds . In this case , both of them hold . English is used for ease of explanation . Table 2 : Number of extracted paraphrases . Table 3 : Examples of correct and incorrect paraphrases extracted by our supervised method with their rank . Figure 3 : Precision curves of paraphrase extraction . Table 2 : Top-7 Chinese long-form candidates for the En- glish acronym TAA , according to the LH score . Figure 2 : The performances of the transliteration models and their comparison on EMatch . Table 3 : The BLEU score of self-trained h4 translitera- tion models under four selection strategies . nt ( n=1..5 ) stands for the n-th iteration . Table 4 : The BLEU score of self-trained cascaded trans- lation model under five initial training sets . Table 1 : 25 noun lexicographer files in W ORD N ET Table 2 : Example nouns and their supersenses Table 3 : 2 billion word corpus statistics Table 4 : Grammatical relations from S EXTANT Table 5 : Hand-coded rules for supersense guessing Table 7 : Breakdown of results by supersense Table 6 : Summary of supersense tagging accuracies Table 1 The best two performing systems of each type ( according to fine-grained recall ) in Senseval-2 and -3 . Table 3 Polysemous word types in the Senseval-2 and -3 English all-words tasks test documents with no data in SemCor ( 0 columns ) , or with very little data ( ≤ 1 and ≤ 5 occurrences ) . Note that there are no annotations for adverbs in the Senseval-3 documents . Table 2 Words ( excluding multiwords ) in WordNet 1.7.1 and the BNC without any data in SemCor . Table 4 Most frequent sense analysis for Senseval-2 and -3 polysemous lemmas occurring more than once in a document ( adverb data is only from Senseval-2 ) . Table 5 Most frequent sense analysis for all polysemous lemmas in the Senseval-2 and -3 test data , broken down by their frequencies of occurrence in SemCor ( adverb data is only from Senseval-2 ) . Figure 1 The prevalence ranking process for the noun star . Table 6 Example dss and sss scores for star and its neighbors . Table 7 Grammatical contexts used for acquiring the BNC thesaurus . Table 8 Thesaurus coverage of polysemous words ( excluding multiwords ) in WordNet 1.6 . Table 9 Evaluation on SemCor , polysemous words only . Table 10 Simplified prevalence score , evaluation on SemCor , polysemous words only . Table 11 Results of the error analysis for the sample of 80 words . Table 12 SemCor results for Nouns using jcn . Table 13 Evaluating predominant sense information for polysemous nouns on the Senseval-2 all-words task data . Table 14 Senseval-2 results , polysemous nouns only , broken down by their frequencies of occurrence in SemCor . Figure 2 “ TYPE ” precision on finding the predominant sense for the Senseval-2 English all-words test data for nouns having a frequency less than or equal to various thresholds . Figure 3 WSD precision on the Senseval-2 English all-words test data for nouns having a frequency less than or equal to various thresholds . Table 15 Most frequent SFC labels for all senses of polysemous words in WordNet , by part of speech . Figure 4 Distribution of domain labels of predominant senses for 38 polysemous nouns ranked using the SPORTS and FINANCE corpora . Figure 5 Distribution of domain labels of predominant senses for 20 polysemous verbs ranked using the SPORTS and FINANCE corpora . Table 16 WSD using predominant senses , training , and testing on all domain combinations ( hand-classified corpora ) . Table 17 WSD using predominant senses , training , and testing on all domain combinations ( automatically classified corpora ) . Figure 1 : Example of a word with internal structure . Figure 2 : Example of telescopic compound ( a ) and sepa- rable word ( b ) . Figure 3 : Structure of the out-of-vocabulary word 戽䊂 䠽吼 ‘ English People ’ . Figure 4 : Two words that differ only in one character , but have different internal structures . The character 吼 ‘ people ’ is part of a personal name in tree ( a ) , but is a suffix in ( b ) . Figure 5 : An example word which has very complex structures . Figure 8 : The actual output of our parser trained with a fully annotated treebank . Figure 7 : Proposed output for the new Chinese word seg- mentation paradigm . Figure 6 : Difference between our output ( a ) of parsing the word 碾碜扨 ‘ olive oil ’ and the output ( b ) of Luo ( 2003 ) . In ( c ) we have a true flat word , namely the loca- tion name 䧢哫膝 ‘ Los Angeles ’ . Figure 9 : Example word structure annotation . We add an ‘ f ’ to the POS tags of words with no further structures . Figure 10 : Example of parser error . Tree ( a ) is correct , and ( b ) is the wrong result by our parser . Table 1 : Labeled precision and recall for the three types of labels . The line labeled ‘ Flat* ’ is for unlabeled met- rics of flat words , which is effectively the ordinary word segmentation accuracy . Table 1 : List of keywords used in WordNet search for generating WN CLASS features . Table 2 : Distribution of SCs in the ACE corpus . Table 3 : SC classification accuracies of different methods for the ACE training set and test set . Table 4 : Results for feature ablation experiments . Table 5 : Accuracies of single-feature classifiers . Table 7 : Resolution accuracies for the ACE test set . Table 6 : Coreference results obtained via the MUC scoring program for the ACE test set . Table 1 : Statistics of relation types and subtypes in the training data of the ACE RDC 2003 corpus ( Note : According to frequency , all the subtypes are divided into three bins : large/ middle/ small , with 400 as the lower threshold for the large bin and 200 as the upper threshold for the small bin ) . Figure 2 : Learning curve of the hierarchical strategy and its comparison with the flat strategy for some major relation subtypes ( Note : FS for the flat strategy and HS for the hierarchical strategy ) Table 4 : Comparison of the hierarchical and flat learning strategies on the relation subtypes of differ- ent training data sizes . Notes : the figures in the parentheses indicate the cosine similarities between the weight vectors of the linear discriminative functions learned using the two strategies . Table 5 : Comparison of our system with other best-reported systems Figure 1 : The combined sequence and parse tree representation of the relation instance “ leader of a minority government. ” The non-essential nodes for “ a ” and for “ minority ” are removed based on the algorithm from Qian et al . ( 2008 ) . Table 1 : Examples of similar syntactic structures across different relation types . The head words of the first and the second arguments are shown in italic and bold , respectively . Table 2 : Examples of unigram and bigram features extracted from Figure 1 . Table 3 : Comparison of different methods on ACE 2004 data set . P , R and F stand for precision , recall and F1 , respectively . Table 4 : The average performance of TL-comb with different λTµ . ( λkµ = 104 and λν = 1 . ) Figure 2 : Performance of TL-comb and TL-auto as H changes . Figure 3 : Performance of TL-NE , BL and BL-A as the number of seed instances S of the target type increases . ( H = 500. λTµ was set to 104 and 102 ) . Table 5 : Average F1 using different hypothesized type-specific features . Figure 1 : General architecture of LINGUA Table 1 : Success rate of anaphora resolution Table 2 : Complexity of the evaluation data Table 3 : Summary of LINGUA performance Table 2 : An example of words and their bit string representations obtained in this paper . Words in bold are head words that appeared in Table 1 . Table 3 : Lexical features for relation extraction . Table 4 : Cluster features ordered by importance . Table 5 : Performance comparison on the ACE 2004 data over the 7 relation types . Table 7 : Performance 12 of the baseline and using different cluster features with PC4 over the 7 types . Table 9 : Performance of each individual relation type based on 5-fold cross-validation . Table 8 : Performance over the 7 relation types with different sizes of training data . Prefix10 uses the single prefix length 10 to generate word clusters as used by Chan and Roth ( 2010 ) . Table 1 : The F1-Measure value is shown for every kernel on each ACE-2005 main relation type . For every relation type the best result is shown in bold font . Table 1 : Example of DIRT algorithm output . Most confident paraphrases of X put emphasis on Y Table 2 : Example of inference rules needed in RTE Table 3 : Lexical variations creating new rules based on DIRT rule X face threat of Y → X at risk of Y Figure 1 : Dependency structure of text . Tree skeleton in bold Table 6 : Precision on full RTE data Table 4 : Coverage/precision with various rule collections Table 5 : Precision on the covered RTE data Table 1 : Frequency of Relation SubTypes in the ACE training and devtest corpus . Table 2 : The Performance of SVM and LP algorithm with different sizes of labeled data for relation detection on relation subtypes . The LP algorithm is run with two similarity measures : cosine similarity and JS divergence . Table 3 : The performance of SVM and LP algorithm with different sizes of labeled data for relation detection and classification on relation subtypes . The LP algorithm is run with two similarity measures : cosine similarity and JS divergence . Table 5 : Comparison of the performance of previous methods on ACE RDC task . Table 4 : Comparison of the performance of the bootstrapped SVM method from ( Zhang , 2004 ) and LP method with 100 seed labeled examples for relation type classification task . Figure 2 . Growing Algorithm for Language Model Pruning Figure 1 . Language Model Pruning Algorithm Figure 3 . Calculation of `` Importance '' of Bigrams Figure 4 . Step-by-step Growing Algorithm Table 1 . Comparison of Number of Bigrams at F-Measure 96.33 % Figure 5 . Performance Comparison of Different Pruning Methods Figure 7 . Performance Comparison of Combined Model and KLD Model Table 2 . Correlation between Perplexity Figure 6 . Perplexity Comparison of Different Pruning Methods Figure 1 . Overview of the method Table 1 . Evaluation results within sets Table 2 . Evaluation results for links Figure 2 . Extracted NE pair instances and context Figure 3 . High TF/ITF words in “ Com-Com ” ( Numbers are TF/ITF score , frequency in the collec- tion ( TF ) , frequency in the corpus ( TF ) and word ) Table 2 : Examples and number of them in Semcor , for sense approach and for class approach Table 1 : BLC for WN1.6 using all or hyponym relations Table 3 : Average polysemy on SE2 and SE3 Table 4 : Results for nouns Figure 2 : Learning curve of BLC20 on SE3 Figure 1 : Learning curve of BLC20 on SE2 Table 5 : Results for verbs Figure 4 : Learning curve of SuperSense on SE3 Figure 3 : Learning curve of SuperSense on SE2 Figure 1 : Architecture of Name-aware Machine Translation System . Table 2 : Translation Performance ( % ) . Table 1 : Statistics and Name Distribution of Test Data Sets . A u to m a tic M e tr ic s H u m a n E v a lu a tio n Figure 2 : Scores based on Automatic Metrics and Human Evaluation . Table 3 : Impact of Joint Bilingual Name Tagging on Word Alignment ( % ) . # name tokens/ # all tokens ( % ) Figure 3 : Word alignment gains according to the percentage of name words in each sentence . Table 2 : accuracy using non-averaged and averaged perceptron . Figure 3 : learning curves of the averaged and non- averaged perceptron algorithms Table 3 : the influence of agenda size . Table 6 : the accuracies over the second SIGHAN bakeoff data . Table 5 : the accuracies over the first SIGHAN bake- off data . Table 4 : the influence of features . ( F : F-measure . Feature numbers are from Table 1 ) Figure 1 . Different representations of a relation instance in the example sentence “ …provide bene- Table 1. five different tree kernel setups on the ACE 2003 five major types using the parse tree structure information only ( regardless of any entity-related information ) Table 4 . Performance comparison on the ACE 2004 data over both 7 major types ( the numbers outside parentheses ) and 23 subtypes ( the num- bers in parentheses ) Table 3 . Performance comparison on the ACE 2003/2003 data over both 5 major types ( the numbers outside parentheses ) and 24 subtypes ( the numbers in parentheses ) Table 5 . Error distribution of major types on both the 2003 and 2004 data for the compos- ite kernel by polynomial expansion Table 1 . Nouns and verbs supersense labels , and short description ( from the Wordnet documentation ) . Table 2 . The noun “ box ” in Wordnet : each line lists one synset , the set of synonyms , a definition , an optional example sentence , and the supersense label . Table 3 . Statistics of the datasets . The row “ Super- senses ” lists the number of instances of supersense labels , partitioned , in the following two rows , between verb and noun supersense labels . The lowest four rows summarize average polysemy figures at the synset and supersense level for both nouns and verbs . Table 4 . Summary of results for random and first sense baselines and supersense tagger , σ is the standard error computed on the five trials results . Table 5 . Summary of results of baseline and tagger on selected subsets of labels : NER categories evaluated on Semcor ( upper section ) , and 5 most frequent verb ( middle ) and noun ( bottom ) categories evaluated on Senseval . Table 1 : Context Clustering with Spectral-based Clustering technique . Table 2 : Frequency of Major Relation SubTypes in the ACE training and devtest corpus . Table 5 : Performance of our proposed method ( Spectral- based clustering ) compared with other unsupervised methods : ( ( Hasegawa et al. , 2004 ) ) ’ s clustering method and K-means clustering . Table 4 : Different Context Window Size Setting Fig . 1 . System architecture overview Fig . 2 . Procedure to mine key lexicons for each semantic type Table 1 . Some key lexicons and verbs for two semantic types Table 3 . Salience grading for candidate antecedents Fig . 3 . Procedure to find semantic types for antecedent candidates Table 4 . Statistics of anaphor and antecedent pairs Table 6 . Feature impact experiments Table 5 . F-Score of Medstract and 100-Medlines Table 8 . Comparisons among different strategies on Medstract Table 7 . Impacts of the mined semantic lexicons and the use of PubMed Table 1 : Context Clustering with Spectral-based Clustering technique . Table 2 : Frequency of Major Relation SubTypes in the ACE training and devtest corpus . Table 4 : Different Context Window Size Setting Table 6 : Comparison of the existing efforts on ACE RDC task . Figure 1 : Outline of word segmentation process Table 3 : Three different vocabulary sizes used in subword- based tagging . s1 contains all the characters . s2 and s3 contains some common words . Table 1 : Corpus statistics in Sighan Bakeoff 2005 Table 2 : Segmentation results of dictionary-based segmentation in closed test of Bakeoff 2005 . A “ / ” separates the results of unigram , bigram and trigram . Table 4 : Segmentation results by the pure subword-based IOB tagging . The separator “ / ” divides the results by three lexicon sizes as illustrated in Table 3 . The first is character-based ( s1 ) , while the other two are subword-based with different lexicons ( s2/s3 ) . Figure 2 : R-iv and R-oov varing as the confidence threshold , t . Table 5 : Effects of combination using the confidence measure . Here we used α = 0.8 and confidence threshold t = 0.7 . The separator “ / ” divides the results of s1 , s2 , and s3 . Table 6 : Effects of using CRF . The separator “ / ” divides the results of s1 , and s3 . Table 7 : List of results in Sighan Bakeoff 2005 Figure 1 . NPs in a sample from the Catalan training data ( left ) and the English translation ( right ) . Figure 1 : Evolution of τA means relative to the length of the n-best sequence Figure 2 : The MCPG algorithm . Figure 3 : Comparison of paraphrase generators . Top : the MOSES baseline ; middle and bold : the “ true-score ” MCPG ; down : the “ translator ” MCPG . The use of “ true-score ” improves the MCPG per- formances . MCPG reaches MOSES performance level . Table 1 : Verb classes ( see Section 3.1 ) , their Levin class numbers , and the number of experimental verbs in each ( see Section 3.2 ) . Table 2 : Experimental Results . C5.0 is supervised accuracy ; Base is on random clusters . set ; Ling is manually selected subset ; Seed is seed-verb-selected set . See text for further description . Table 3 : Feature counts for Ling and Seed feature sets . Table 1 : Relation extraction results on the JDPA Corpus test set , broken down by document source . Table 2 : Selected document statistics for three JDPA Corpus document sources . Table 1 : Size of Seed Lexicons Table 2 : Performance on Bilingual Lexicon Extraction Table 4 : Seeds with the Highest Weight Table 3 : Translation Candidates for 躁鬱病 ( manic- depression ) Table 1 : Examples of templates suggested by DIRT and TEASE as having an entailment relation , in some direction , with the input template ‘ X change Y ’ . The entailment direction arrows were judged manually and added for readability . Table 2 : Rule evaluation examples and their judgment . Table 4 : Average Precision ( P ) and Yield ( Y ) at the rule and template levels . Table 3 : Examples for disagreement between the two judges . Table 2 : Entity type constraints . Table 3 : BasicRE gives the performance of our basic RE system on predicting fine-grained relations , obtained by performing 5-fold cross validation on only the news wire corpus of ACE-2004 . Each sub- sequent row +Hier , +Hier+relEntC , +Coref , +Wiki , and +Cluster gives the individual contribution from using each knowledge . The bottom row +ALL gives the performance improvements from adding +Hier+relEntC+Coref+Wiki+Cluster . ∼ indicates no change in score . Figure 1 A general architecture for paraphrasing approaches leveraging the distributional similarity hypothesis . Figure 2 Two different dependency tree paths ( a and b ) that are considered paraphrastic because the same words ( John and problem ) are used to ﬁll the corresponding slots ( shown co-indexed ) in both the paths . The implied meaning of each dependency path is also shown . Figure 3 Using Chinese translations as the distributional elements to extract a set of English paraphrastic patterns from a large English corpus . Figure 4 A bootstrapping algorithm to extract phrasal paraphrase pairs from monolingual parallel corpora . Figure 5 The merging algorithm . ( a ) How the merging algorithm works for two simple parse trees to produce a shared forest . Note that for clarity , not all constituents are expanded fully . Leaf nodes with two entries represent paraphrases . ( b ) The word lattice generated by linearizing the forest in ( a ) . Figure 6 A paraphrase generation lattice for the sentence He ate lunch at a cafe near Paris . Alternate paths between various nodes represent phrasal replacements . The probability values associated with each edge are not shown for the sake of clarity . Figure 7 An example showing the generalization of the word lattice ( a ) into a slotted lattice ( b ) . The word lattice is produced by aligning seven sentences . Nodes having in-degrees > 1 occur in more than one sentence . Nodes with thick incoming edges occur in all sentences . Figure 8 Extracting consistent bilingual phrasal correspondences from the shown sentence pairs . ( i1 , j1 ) × ( i2 , j2 ) denotes the correspondence fi1 . . . fj1 , ei2 . . . ej2 . Not all extracted correspondences are shown . Figure 9 An example of syntactically motivated paraphrastic patterns that can be extracted from the paraphrase corpus constructed by Cohn , Callison-Burch , and Lapata ( 2008 ) . Table 1 : Test verbs and their monosemous/polysemic gold standard senses Figure 1 : Connected components nearest neighbour ( NN ) clustering . D is the Kullback-Leibler distance . Figure 2 : Information Bottleneck ( IB ) iterative clustering . D is the Kullback-Leibler distance . Table 2 : Clustering performance on the predominant senses , with and without prepositions . The last entry presents the per- formance of random clustering with K = 25 , which yielded the best results among the three values K=25 , 35 and 42 . Table 5 : The fraction of verb pairs clustered together , as a function of the number of shared senses ( results of the NN algo- rithm ) Table 3 : Evaluation against the monosemous ( Pred . ) and pol- ysemous ( Multiple ) gold standards . The figures in parentheses are results of evaluation on randomly polysemous data + sig- nificance of the actual figure . Results were obtained with fine- grained SCFs ( including prepositions ) . Table 4 : The fraction of verb pairs clustered together , as a function of the number of different senses between pair mem- bers ( results of the NN algorithm ) Figure 1 : Addition method . Table 2 : Evaluation of the manual annotation improvement - summarization ratio : 30 % . Table 1 : Evaluation of the manual annotation improvement - summarization ratio : 15 % . Table 4 : Evaluation of the GUITAR improvement - summarization ratio : 30 % . Table 3 : Evaluation of the GUITAR improvement - summarization ratio : 15 % . Table 1 : Performance of our system versus a baseline Figure 1 : The greedy binding problem . ( a ) The correct binding , ( b ) the greedy binding , ( c ) the result . Figure 1 . The Lattice of the 8 Patterns . Table 1 . Segmentation accuracy of different seg- menters . Table 3 . Results on three query categories . Table 2 . MAP of different IR systems with differ- ent segmenters . Table 1 : The feature set for coreference resolution . Non-relational features describe a mention and in most cases take on a value of YES or NO . Relational features describe the relationship between the two mentions and indicate whether they are COMPATIBLE , INCOMPATIBLE or NOT APPLICABLE . Table 2 : Statistics for the ACE 2005 corpus Table 4 : MUC , CEAF , and B3 coreference results using system mentions . Table 3 : MUC , CEAF , and B3 coreference results using true mentions . Table 1 : Two characteristic topics for the Y slot of ‘ acquire ’ , along with their topic-biased Lin sim- ilarities scores Lint , compared with the original Lin similarity , for two rules . The relevance of each topic to different arguments of ‘ acquire ’ is illus- trated by showing the top 5 words in the argument y vector vacquire for which the illustrated topic is the most likely one . Table 2 : Context-sensitive similarity scores ( in bold ) for the Y slots of four rule applications . The components of the score calculation are shown for the topics of Table 1 . For each rule application , the table shows a couple of the topic-biased scores Lint of the rule ( as in Table 1 ) , along with the topic relevance for the given context p ( t|dv , w ) , which weighs the topic-biased scores in the LinW T cal- culation . The context-insensitive Lin score is shown for comparison . Table 4 : MAP values on corresponding test set ob- tained by each method . Figures in parentheses in- dicate optimal number of LDA topics . Table 3 : Sizes of rule application test set for each learned rule-set . Table 5 : MAP results for the two split Lin test- sets . Figure 1 : How the IBM models model the translation process . This is a hypothetical example and not taken from any actual training or decoding logs . Figure 2 : Runtimes for sentences of length 10–80 . The graph shows the average runtimes ( ) of 10 different sample sentences of the respective length with swap op- erations restricted to a maximum swap segment size of 5 and a maximum swap distance of 2 . Figure 3 : A decoding trace using improvement caching and tiling ( ICT ) . The search in the second and later iterations is limited to areas where a change has been applied ( marked in bold print ) — note that the number of alignment checked goes down over time . The higher number of alignments checked in the second iteration is due to the insertion of an additional word , which increases the number of possible swap and insertion operations . Decoding without ICT results in the same translation but requires 11 iterations and checks a total of 17701 alignments as opposed to 5 iterations with a total of 4464 alignments with caching . Figure 4 : Number of search iterations ( left ) and total number of alignments considered ( right ) during search in depen- dence of input length . The data is taken from the translation of the Chinese testset from the TIDES MT evaluation in June 2002 . Translations were performed with a maximum swap distance of 2 and a maximum swap segment size of 5 . Figure 5 : BLEUscores for the Chinese test set ( de- coding ) in dependence of maximum swap distance and maximum swap segment size . Table 1 : Decoder performance on the June 2002 TIDES MT evluation test set with multiple searches from randomized starting points ( MSD=2 , MSSS=5 ) . sentence length Figure 6 : Time consumption of the various change types in Figure 2 : FDG Analyser ’ s output example Table 1 : Evaluation results from DSO-WSJ Figure 2 . PAT-Tree Instantiation for Figure 1 . In the extraction process , the PAT-tree is Table 1 . Results for feature combination . Figure 3 . Results for α value setting . Figure 4 . Results for initial ranking manner . Figure 5 . Results for webpage snippet number . 7.3 Experiment on Multiple Feature Fusion To verify the effectiveness for multiple feature fusion , the test on the feature combination for OOV term translation is implemented . As shown in Table 1 , the highest accuracy ( the percentage of the correct translations in all the extracted translations ) of 83.1367 % can be ac- Table 2 . OOV term translation examples . Figure 6 . Results for English-Chinese CLIR com- bining our OOV term translation model . Table 1 : Results for 4-fold site-wise cross-validation us- ing the DP corpus Table 2 : DP corpus comparison for OPUS features based on frequent vs. domain-relevant verbs Figure 1 : Results on the Bitter Lemons corpus Figure 1 : Architecture of the translation approach based on Bayes decision rule . Figure 2 : Word-to-word alignment . Figure 3 : Example of a word alignment and of ex- tracted alignment templates . Table 1 : Bilingual training corpus , recognition lex- icon and translation lexicon ( PM = punctuation mark ) . Figure 4 : Illustration of search in statistical trans- lation . Figure 5 : Illustration of bottom-to-top search . Table 2 : Comparison of three statistical translation approaches ( test on text input : 251 sentences = 2197 words + 430 punctuation marks ) . Table 4 : Sentence error rates of end-to-end evalua- tion ( speech recognizer with WER=25 % ; corpus of 5069 and 4136 dialogue turns for translation Ger- man to English and English to German , respec- tively ) . Table 3 : Disambiguation examples ( ∗ : using morpho-syntactic analysis ) . Table 1 . Accuracy of our system in each period ( M = 10 ) Table 2 . Precision and recall for different values of Table 3 . Rank of correct translation for period Dec 01 – Dec 15 and Dec 16 – Dec 31 . ‘ Cont . rank ’ is the context rank , ‘ Trans . Rank ’ is the transliteration rank . ‘ NA ’ means the word can not be transliterated . ‘ insuff ’ means the correct translation appears less than 10 times in the English part of the comparable corpus . ‘ comm ’ means the correct translation is a word ap- pearing in the dictionary we used or is a stop word . ‘ phrase ’ means the correct translation contains multi- ple English words . Table 2 : Pronouns as Opinion Targets Table 4 : Results of AR for Opinion Targets Table 3 : Op . Target - Op . Word Pair Extraction Figure 1 : An example sequence representation . The subgraph on the left represents a bigram feature . The subgraph on the right represents a unigram feature that states the entity type of arg 2 . Figure 3 : An example dependency parse tree rep- resentation . The subgraph represents a dependency relation feature between arg 1 “ Palestinians ” and “ of ” . Table 1 : Comparison among the three feature sub- spaces and the effect of including larger features . Figure 1 : CTB 10-fold CV word segmentation F- measure for our word segmenter Figure 2 : Comparison of word segmentation F- measure for SIGHAN bakeoff3 tasks Figure 3 : POS tagging accuracy using one-at-a- time , word-based POS tagger Figure 4 : POS tagging accuracy using one-at-a- time , character-based POS tagger Table 1 : Summary table on the various methods investigated for POS tagging Figure 5 : CTB 10-fold CV word segmentation F- measure using an all-at-once approach Figure 6 : CTB 10-fold CV POS tagging accuracy using an all-at-once approach Table 1 Some of the words extracted from the small corpus . Table 2 Experiments on the threshold–precision relationship of the small corpus . Table 3 Experiments on the word length–precision relationship of the small corpus . Table 4 Experiments on the threshold–partial recall relationship of the small corpus . Table 5 Some words extracted from the large corpus . Table 6 Experiments on the threshold–precision relationship of the large corpus . Table 9 Experiments on the threshold–partial recall relationship of the large corpus . Table 8 Experiments on the word length–precision relationship of the large corpus with threshold nine . Table 7 Experiments on the word length–precision relationship of the large corpus with threshold three . Table 10 Numeric-type compounds extracted . Table 12 Precision and partial recall of word lengths two to seven of the second experiment on IT and AV . Table 11 Precision and partial recall of word lengths two to four of the first experiment on IT and AV . Figure 1 : Semantic expansion example . Note that the expanded queries that were generated in the first two retrieved texts ( listed under ‘ matched query ’ ) do not contain the original query . Table 1 : Examples for correct templates that were learned by TEASE for input templates . Table 2 : Analysis of the number of relevant documents out of the top 10 and the total number of retrieved documents ( up to 100 ) for a sample of queries . Figure 1 Architecture of the statistical translation approach based on Bayes ’ decision rule . Figure 2 Regular alignment example for the translation direction German to English . For each German source word there is exactly one English target word on the alignment path . Figure 3 Illustration of the transitions in the regular and in the inverted alignment model . The regular alignment model ( left figure ) is used to generate the sentence from left to right ; the inverted alignment model ( right figure ) is used to generate the sentence from bottom to top . Table 1 DP-based algorithm for solving traveling-salesman problems due to Held and Karp . The outermost loop is over the cardinality of subsets of already visited cities . Figure 4 Illustration of the algorithm by Held and Karp for a traveling salesman problem with J = 5 cities . Not all permutations of cities have to be evaluated explicitly . For a given subset of cities the order in which the cities have been visited can be ignored . Table 2 DP-based algorithm for statistical MT that consecutively processes subsets C of source sentence positions of increasing cardinality . Figure 5 Word reordering for the translation direction German to English : The reordering is restricted to the German verb group . Figure 6 Order in which the German source positions are covered for the German-to-English reordering example given in Figure 5 . Figure 7 Word reordering for the translation direction English to German : The reordering is restricted to the English verb group . Figure 8 Order in which the English source positions are covered for the English-to-German reordering example given in Figure 7 . Table 3 Procedural description to compute the set Succ of successor hypotheses by which to extend a partial hypothesis ( S , C , j ) . Figure 9 Illustration of the IBM-style reordering constraint . Figure 10 Number of processed arcs for the pseudotranslation task as a function of the input sentence length J ( y-axis is given in log scale ) . The complexity for the four different reordering constraints MON , GE , EG , and S3 is given . The complexity of the S3 constraint is close to J4 . Table 4 Two-list implementation of a DP-based search algorithm for statistical MT . Table 5 Training and test conditions for the German-to-English Verbmobil corpus ( *number of words without punctuation ) . Table 7 Example translations for the translation direction German to English using three different reordering constraints : MON , GE , and S3 . Table 12 Translation results for the translation direction English to German on the TEST-331 test set . The results are given in terms of computing time , WER , and PER for three different reordering constraints : MON , EG , and S3 . Table 11 Demonstration of the combination of the two pruning thresholds tC = 5.0 and tc = 12.5 to speed up the search process for the two reordering constraints GE and S3 ( no = 50 ) . The translation performance is shown in terms of mWER on the TEST-331 test set . Table 14 Training and test conditions for the Hansards task ( *number of words without punctuation ) . Table 13 Example translations for the translation direction English to German using three different reordering constraints : MON , EG , and S3 . Table 17 Example translations for the translation direction French to English using the S3 reordering constraint . Figure 1 : Outline of the segmentation process Table 3 : Scores for UPUC corpus Table 2 : Scores for MSRA corpus Table 1 : Scores for CityU corpus Figure 1 : An example graph modeling relations between mentions . Table 2 : Number of clustering decisions made ac- cording to mention type ( rows anaphor , columns antecedent ) and percentage of wrong decisions . Table 1 : Results of different systems on the CoNLL ’ 12 English data sets . Table 4 : Number of recall errors according to mention type ( rows anaphor , columns antecedent ) . Table 3 : Precision statistics for pronouns . Rows are pronoun surfaces , columns number of cluster- ing decisions and percentage of wrong decisions for all and only anaphoric pronouns respectively . Figure 1 . Removal and reduction of constituents using dependencies Figure 2 . Different setups for entity-related se- mantic tree ( EST ) Table 1 . Contribution of constituent dependen- cies in respective mode ( inside parentheses ) and accumulative mode ( outside parentheses ) Table 3 . Improvements of different tree setups Table 1 : Combinatorial Search Problems in Decoding Figure 1 : Average decoding time Table 2 : P r ( f , ã|e ) for IBM Models Table 1 : 5-fold cross-validation results on training data . Table 2 : Results obtained on the official test set of the 2011 DDI Extraction challenge . LII filtering refers to the techniques proposed in Chowdhury and Lavelli ( 2012b ) for reducing skewness in RE data distribution . stat . sig . in- dicates that the improvement of F-score , due to usage of Stage 1 classifier , is statistically significant ( verified using Approximate Randomization Procedure ( Noreen , 1989 ) ; number of iterations = 1,000 , confidence level = 0.01 ) . Table 1 : Examples of phrase “ meaningfulness ” ( Note that the comments are not presented to Turkers ) . Table 2 : Examples given in the description of Task 2 . Figure 1 : Left : An entailment graph . For clarity , edges that can be inferred by transitivity are omitted . Right : A hierarchical summary of propositions involving nausea as an argument , such as headache is related to nausea , acupuncture helps with nausea , and Lorazepam treats nausea . Table 1 : Results for all experiments Figure 3 : Subgraph of Local∗1 output for “ headache ” Figure 2 : Subgraph of tuned-LP output for “ headache ” Table 2 : Comparing disagreements between the best local and global algorithms against the gold standard Figure 1 Relation between number of classes and alternations . Table 2 Estimation of model parameters . Table 3 Estimation of F ( c , f , v ) and F ( v , c ) . Table 5 Ten most frequent classes using equal distribution of verb frequencies . Table 4 Estimation of F ( v , c ) for the verb feed . Table 6 Ten most frequent classes using unequal distribution of verb frequencies . Table 8 Model accuracy using equal distribution of verb frequencies for the estimation of P ( c ) . Table 9 Model accuracy using unequal distribution of verb frequencies for the estimation of P ( c ) . Table 11 Model accuracy using unequal distribution of verb frequencies for the estimation of P ( c ) . Table 10 Model accuracy using equal distribution of verb frequencies for the estimation of P ( c ) . Table 12 Semantic preferences for verbs with the double-object frame . Table 13 Features for collocations . Figure 4 Word sense disambiguation accuracy for “ NP1 V NP2 to NP3 ” frame . Figure 2 Word sense disambiguation accuracy for “ NP1 V NP2 ” frame . Figure 3 Word sense disambiguation accuracy for “ NP1 V NP2 NP3 ” frame . Figure 5 Word sense disambiguation accuracy for “ NP1 V NP2 for NP3 ” frame . Figure 7 Word sense disambiguation accuracy for “ NP1 V NP2 NP3 ” frame . Figure 6 Word sense disambiguation accuracy for “ NP1 V NP2 ” frame . Figure 8 Word sense disambiguation accuracy for “ NP1 V to NP2 NP3 ” frame . Figure 9 Word sense disambiguation accuracy for “ NP1 V for NP2 NP3 ” frame . Figure 2 : Examples of context- free and context-sensitive sub- trees related with Figure 1 ( b ) . Note : the bold node is the root for a sub-tree . Figure 1 : Different tree span categories with SPT ( dotted circle ) and an ex- ample of the dynamic context-sensitive tree span ( solid circle ) Table 1 : Evaluation of context-sensitive convolution tree kernels using SPT on the major relation types of the ACE RDC 2003 ( inside the parentheses ) and 2004 ( outside the parentheses ) corpora . Table 2 : Comparison of dynamic context-sensitive tree span with SPT using our context-sensitive convolution tree kernel on the major relation types of the ACE RDC 2003 ( inside the parentheses ) and 2004 ( outside the parentheses ) corpora . 18 % of positive instances in the ACE RDC 2003 test data belong to the predicate-linked category . Table 5 : Comparison of difference systems on the ACE RDC 2004 corpus over both 7 types ( outside the parentheses ) and 23 subtypes ( inside the parentheses ) Table 4 : Comparison of difference systems on the performs the state-of-the-art Collins and Duffy ’ s con- ACE RDC 2003 corpus over both 5 types ( outside the volution tree kernel . It also shows that feature-based parentheses ) and 24 subtypes ( inside the parentheses ) Table 1 Possible Relations between ARG-1 and ARG-2 Table 2 Evaluation on Structure Features Table 5 Evaluation of Correction and Inference Mechanisms Table 6 Evaluation of Feature and Their Combinations Table 4 Evaluation of Two Detection and Classification Modes Table 3 Imbalance Training Class Problem Table 1 : IBM Model 3 Table 1 : Linguistic levels as feature sets . Table 2 : Semantic features . Table 3 : Accuracy results for binary decisions . Table 4 : Accuracy results for combined decisions . Table 5 : Comparison of accuracy scores across linguistic levels . Table 6 : Levels all and morph against the Gold Standard . Table 7 : Results for ensemble classifier . Figure 1 : Example of BLC selection Table 3 : Most frequent monosemic words in BG Table 1 : Most frequent BLC–20 semantic classes on WordNet 3.0 Table 2 : Number of training examples Table 4 : Results of task # 17 Table 1 : The ‘ B , I , E , S ’ Tag Set Figure 1 : Example of Lattice Used in the Markov Model-Based Method Figure 2 : Example of the Character Tagging Method : Word boundaries are indicated by vertical lines ( ‘ | ’ ) . Figure 3 : Example of the Hybrid Method Table 4 : Calculated Values of λi Table 3 : Statistical Information of Corpora Table 6 : Performance of Japanese Word Segmentation Table 5 : Performance of Chinese Word Segmentation Fig . 1 GETARUNS AR algorithm Table 1 . Expletive “ it ” compared results Table 3 . GETARUNS pronouns collapsed at structural level Table 2 . Overall results Coverage/Accuracy Table 1 : Examples of new alternations Table 2 : New Verb Classes Table 3 : Average results for 35 verbs Table 1 : Association frequencies for target verb . Table 2 : Association overlap for target verbs . Table 3 : Coverage of verb association features by grammar/window resources . Table 4 : Accuracy for induced verb classes . Figure 1 : Latent Dependency coupling for the RE task . The D-C ONNECT factor expresses ternary connection re- lations because the shared head word of the proposed re- lation is unknown . As is convention , variables are repre- sented by circles , factors by rectangles . Table 1 : Relation Extraction Results . Models using hidden constituency syntax provide significant gains over the syntactically-uniformed baseline model in both languages , but the advantages of the latent syntax were mitigated on the smaller Chinese data set . Figure 2 : A tiered graphic representing the three different SRL model configurations . The baseline system is described in the bottom ( c & d ) , the separate panels highlighting the independent predictions of this model : sense labels are assigned in an entirely separate process from argument prediction . Pruning in the model takes place primarily in this tier , since we observe true predicates we only instantiate over these indices . The middle tier ( b . ) illustrates the syntactic representation layer , and the connective factors between syntax and SRL . In the observed syntax model the Link variables are clamped to their correct values , with no need for a factor to coordinate them to form a valid tree . Finally , the hidden model comprises all layers , including a combinatorial syntactic constraint ( a . ) over syntactic variables . In this scenario all labels in ( b . ) are hidden at both training and test time . Figure 3 : Examining the learned hidden representation for SRL . In this example the syntactic dependency arcs derived from gold standard syntactic annotations ( left ) are entirely disjoint from the correct predicate/arguments pairs ( shown in the heatmaps by the squares outlined in black ) , and the observed syntax model fails to recover any of the correct predictions . In contrast , the hidden model structure ( right ) learns a representation that closely parallels the desired end task predictions , helping it recover three of the four correct SRL predictions ( shaded arcs : red corresponds to a correct prediction , with true labels GA , KARA , etc . ) , and providing some evidence towards the fourth . The dependency tree corresponding to the hidden structure is derived by edge-factored decoding : dependency variables whose beliefs > 0.5 are classified as true ( though some arcs not relevant to the SRL predictions are omitted for clarity ) . Table 2 : SRL Results . The hidden model excels on the unlabeled prediction results , often besting the scores obtained using the parses distributed with the CoNLL data sets . These gains did not always translate to the labeled task where poor sense prediction hindered absolute performance . Figure 1 : Illustration on temporality Table 1 : Symmetry of window size Table 2 : Optimality of window size Figure 2 : The translation examples where shaded cells indicate the correctly translated pairs . Table 3 : MRR , Precision , Recall , and F1-score Table 1 : Supersense evaluation results . Values are the percentage of correctly assigned supersenses . k indicates the number of nearest neighbours considered . Table 2 : Pseudo-disambiguation : Percentage of correct choices made . L-bound denotes the Web1T lower bound on the ( a1 , n ) bigram , size the number of decisions made . Table 4 : Results on the unseen plausibility dataset . Table 2 : Chinese character usage in 3 corpora . The numbers in brackets indicate the percentage of characters that are shared by at least 2 corpora . Table 1 : Number of entries in 3 corpora Table 3 : Number of unique entries in training and test sets , categorized by semantic attributes Table 4 : Language detection accuracies ( % ) using a 4-gram language model for the letter sequence of the source name in Latin script . Table 7 : The effect of language and gender in- Table 6 : MRR performance of phonetic translit- eration for 3 corpora using unigram and bigram language models . Table 5 : Gender detection accuracies ( % ) using a 4-gram language model for the letter sequence of the source name in Latin script . Table 10 : Overall transliteration performance Table 8 : The effect of language detection Table 9 : The effect of gender detection schemes Figure 1 : An example of alignment for Japanese and English sentences Figure 2 : Translation Model ( IBM Model 4 ) Figure 3 : string insertion operator for left-to-right decoding method . A string e0 was appended after the partial output string , e , and the last word in e 0 was aligned from f j . Figure 4 : string insertion operation for right-to-left decoding method . A string e0 was prepended before the partial output string , e , and the first word in e 0 was aligned from f j . Figure 5 : Merging left-to-right and right-to-left hypotheses ( ef and eb ) in bidirectional decoding method . Figure 5 ( a ) merge two open hypotheses , while Figure 5 ( b ) merge them with inserted zero fer- tility words . Table 3 : Comparison of the three decoders by the ratio each decoder produced search errors . Table 1 : Statistics on a travel conversation corpus Figure 2 . Consistently formatted sentence trans- lation pairs Figure 1 . Consistently formatted term translation pairs Figure 3 . The framework of our approach Table 1 . Example segmentations ( „ |‟ indicates the separator between adjacent snippets ) Table 3 . Performance of different settings Table 4 . Contribution of every feature Table 2 : Missing argument examples of biological interactions Table 1 : A protein domain-referring phrase example Table 3 : Statistics of anaphoric expressions Table 4 : A subjective pronoun resolution example Table 7 : Non-anaphoric DNP examples Table 5 : Possessive pronoun resolution examples Table 6 : Example patterns for parallelism Table 10 : An example antecedent of a nominal in- teraction keyword Table 9 : Example patterns of nominal interaction keywords Table 8 : Example patterns of proteins and their do- mains Table 13 : An annotation example for the necessity of species information Table 11 : Term variation examples Table 12 : Protein name grounding examples Table 15 : Experimental results of test corpus Table 14 : An example result of BioAR Table 16 : Incorrect resolution example of pronoun resolution module Table 2 : The templates for generating potentially deter- ministic constraints of English POS tagging . Table 1 : Morph features of frequent words and rare words as computed from the WSJ Corpus of Penn Treebank . Table 3 : Comparison of raw input and constrained input . Table 4 : Comparison of raw input and constrained input . Table 5 : Character- and word-based features of a possi- ble word wi over the input character sequence c. Suppose that wi = ci0 ci1 ci2 , and its preceding and following char- acters are cl and cr respectively . Table 7 : Deterministic constraints for POS tagging . Table 6 : POS tagging with deterministic constraints . The maximum in each column is bold . Table 8 : Character tagging with deterministic constraints . Table 11 : ILP problem size and segmentation speed . Table 1 : Substitution/insertion/deletion patterns for phonemes based on English second-language learner ’ s data reported in ( Swan and Smith , 2002 ) . Each row shows an input phoneme class , possi- ble output phonemes ( including null ) , and the positions where the substitution ( or deletion ) is likely to occur . Table 2 : Examples of features and associated costs . Pseudofeatures are shown in boldface . Exceptional denotes a situation such as the semivowel [ j ] substituting for the affricate [ dZ ] . Substitutions between these two sounds actually occur frequently in second-language error data . Table 3 : Substitution/deletion/insertion costs for /g/ . Table 4 : Examples of the three top candidates in the transliteration of English/Arabic , English/Hindi and English/Chinese . The second column is the rank . Table 5 : Language-pair datasets . Table 6 : Number of evaluated English NEs . Figure 1 : CoreMRR scores with different α values using score combination . A higher α puts more weight on the phonetic model . Table 7 : MRRs and CorrRate for the pronunciation method ( top ) and time correlation method ( middle ) . The bottom table shows the scores for the combination ( CoreMRR ) . Table 1 : Part-of-speech tags of the Penn Chinese Treebank that are referenced in this paper . Please see ( Xia , 2000 ) for the full list . Table 2 : Categories of multi-character words that are considered ‘ strings without internal structures ’ ( see Section 4.1 ) . Each category is illustrated with one example from our corpus . Table 3 : Categories of multi-character words that are considered ‘ strings with internal structures ’ ( see Section 4.2 ) . Each category is illustrated with an example from our corpus . Both the individual characters and the compound they form receive a POS tag . Table 5 : POS annotations of a couplet , i.e. , a pair of two verses , in a classical Chinese poem . See Table 1 for the meaning of the POS tags . Table 4 : POS annotations of an example sentence with a string , wan lai ‘ evening ’ , that has internal structure . See Section 4.2 for two possible translations , and Table 1 for the meaning of the POS tags . Table 6 : Part-of-speech annotations of the three- character strings 細柳營 xi liu ying ‘ Little Willow military camp ’ and 新豐市 xin feng shi ‘ Xinfeng city ’ . Both are ‘ strings with internal structures ’ , with nested structures that perfectly match at all three levels . They are the noun phrases that end both verses in the couplet 忽過 新豐市 , 還歸細柳營 . Table 1 : The semantic roles of cases beside C-3 verb cluster Table 2 : The semantic roles of cases beside C-1 verb cluster Figure 1 : Rule expansion with minimal context ( Example 3 ) Table 1 : Sample of extracted entailment rules . Table 3 : Accuracy ( % ) of the extracted sets of rules . Table 2 : Resulting sets of entailment rules Figure 1 : Segmentation recall relative to gold word frequency . Table 1 : Baseline performance . Figure 2 : Segmentation precision/recall relative to gold word length in training data . Table 2 : Word length statistics on test sets . Table 3 : F-score of two segmenters , with ( − ) and without ( + ) word token/type features . Table 4 : Upper bound for combination . The error reduction ( ER ) rate is a comparison between the F-score produced by the oracle combination sys- tem and the character-based system ( see Tab . 1 ) . Table 5 : Segmentation performance presented in previous work and of our combination model . Figure 3 : Precision/Recall/F-score of different models . Table 1 : Accuracy of maximum entropy system using different subsets of features for S ENSEVAL -2 verbs . Table 2 : Overall accuracy of maximum entropy sys- tem using different subsets of features for Penn Chi- nese Treebank words ( manually segmented , part-of- speech-tagged , parsed ) . Table 4 : Overall accuracy of maximum entropy sys- tem using different subsets of features for People ’ s Daily News words ( manually segmented , part-of- speech-tagged ) . Table 3 : Overall accuracy of maximum entropy sys- tem using different subsets of features for People ’ s Daily News words ( automatically segmented , part- of-speech-tagged , parsed ) . Figure 1 : Overview of the system . Table 1 : The pool of features for all languages . Table 2 : Experiment results ( as F1 scores ) where IM is identification of mentions and S - Setting . Table 3 : Final system results ( as F1 scores ) where IM is identification of mentions and S - Setting . For more details cf . ( Recasens et al. , 2010 ) . Figure 1 . Inter-annotator agreement of ACE 2005 relation annotation . Numbers are the distinct relation mentions whose both arguments are in the list of adjudicated entity mentions . Figure 2 . Percentage of examples of major syntactic classes . Figure 3 : cumulative distribution of frequency ( CDF ) of the relative ranking of model-predicted probability of being positive for false negatives in a pool mixed of false negatives and true negatives ; and the CDF of the relative ranking of model-predicted probability of being negative for false positives in a pool mixed of false positives and true positives . Table 1 . Categories of spurious relation mentions in fp1 ( on a sample of 10 % of relation mentions ) , ranked by the percentage of the examples in each category . In the sample text , red text ( also marked with dotted underlines ) shows head words of the first arguments and the underlined text shows head words of the second arguments . Table 2 . Performance of RDC trained on fp1/fp2/adj , and tested on adj . Figure 4 . TSVM optimization function for non-separable case ( Joachims , 1999 ) Table 4 . Performance with SVM trained on a fraction of adj . It shows 5 fold cross validation results . Table 3 . 5-fold cross-validation results . All are trained on fp1 ( except the last row showing the unchanged algorithm trained on adj for comparison ) , and tested on adj . McNemar 's test show that the improvement from +purify to +tSVM , and from +tSVM to ADJ are statistically significant ( with p < 0.05 ) . Figure 1 ESA and input/output data . Figure 2 A character sequence and its subsequence pairs . Figure 4 The path of Selection . Figure 5 The binary tree of Selection . Figure 6 The initial frequencies of character sequences . Figure 7 The adjusted frequencies of character sequences . Figure 8 The hierarchical form of a result . Table 1 The scales of corpora . Table 2 The results of setting 1 ( Punctuation and other encoding information are not used ; the maximum length is 30 ) . Table 3 The results of setting 2 ( Punctuation and other encoding information are not used ; the maximum length is 10 ) . Table 4 The results of setting 3 ( Punctuation is used ; the maximum length is 30 ) . Table 5 The results of setting 4 ( Punctuation and other encoding information are used ; the maximum length is 30 ) . Figure 9 The difference between the results of four settings . Table 6 The results brought by different maximum lengths . Table 7 The empirical formulae for the prediction ( linear model ) . Figure 10 The correlation between the scales and the proper exponents . Figure 11 The four types of changes . Figure 12 Convergence of results . Figure 13 The time complexity in practice ( 4 samples : 10 , 30 , 50 , and 100 ) . Table 8 The comparison between NPYLM and ESA . Table 9 The comparison between DLG , AV , BE , and ESA . Table 11 The comparison between IWSLRR ( I ) , SS ( S ) , TONGO ( O ) , TH ( T ) , and ESA . Table 2 : Outputs from each algorithm at different sorted ranks Table 4 Performance on Internet data Table 5 : Equation 1 settings Figure 1 : Precision-recall curve for rescoring Figure 2 : Syntactic frames for VerbNet classes Figure 3 : An excerpt from SemLink Figure 4 : Training instances obtained from Verb- Net ( upper ) and VerbNet+SemLink ( lower ) Figure 5 : Example of features for “ sway ” Table 1 : 14 classes used in Joanis et al . ( 2008 ) and their corresponding Levin class numbers Figure 6 : Corpus size vs. accuracy Table 3 : Accuracy and KL-divergence for the all- class task ( the VerbNet+SemLink setting ) Table 4 : Accuracy and KL-divergence for the all- class task ( the VerbNet only setting ) Table 2 : Accuracy for the 14-class task Figure 7 : Corpus size vs. KL-divergence Table 5 : Contribution of features Table 1 : Results of paraphrases evaluation for 100 sentences in French using English as the pivot lan- guage . Comparison between the baseline system MOSES and our algorithm MCPG . Table 2 : Chunk feature templates : i f & @ jFk : : - is the chunk label plus the tag of its right most child if the j2r t tree is a chunk ; Otherwise i f is the con- stituent label of the j r t tree . Table 3 : Extend feature templates : G fA & @ k : : : l - is the root constituent label of th Table 4 : Check feature templates : G f & @ j Figure 1 : Learning curves : word-segmentation F- measure and parsing label F-measure vs. percentage of training data . Table 5 : Language-dependent lexical features . A word list can be collected to encode different Table 6 : WS : word-segmentation . Baseline : language-independent features . LexFeat : plus lex- ical features . Numbers are averaged over the 10 ex- periments in Figure 2 . Figure 2 : Parsing and word segmentation F- measures vs. the experiment numbers . Lines with triangles : segmentation ; Lines with circles : label ; Dotted-lines : language-independent features only ; Solid lines : plus lexical features . Figure 3 : Usefulness of syntactic information : ( black ) dash-dotted line – word boundaries only , ( red ) dashed line – POS info , and ( blue ) solid line – full parse trees . Table 1 IV and OOV recall in ( Zhang et al. , 2006a ) Table 2 Corpora statistics of Bakeoff 2005 Table 3 Feature templates used for CRF in our experiments Table 5 Error analysis of confidence measure with and without EIV tag Table 4 Results of CT when MP is less than 0.875 Table 6 Results of different approach used in our experiments ( White background lines are the results we repeat Zhang‟s methods and they have some trivial difference with Table 1 . ) Table 1 : A text segment from MUC-6 data set Table 2 : Feature set for the baseline pronoun resolution system Table 3 : Backward features used to capture the coreferential information of a candidate Table 4 : Results of different systems for pronoun resolution on MUC-6 and MUC-7 ( *Here we only list backward feature assigner for pronominal candidates . In RealResolve-1 to RealResolve-4 , the backward features for non-pronominal candidates are all found by DTnon−pron . ) Figure 2 : The pronoun resolution algorithm by incorporating coreferential information of can- didates Figure 3 : The classifier refining algorithm Table 5 : New training and testing procedures Figure 1 : A Generic Morphological Analyzer as a Black Box Figure 2 : User Interface with Arabic Script Dis- play in Java . Mouse clicks on the virtual keyboard or key presses on the physical keyboard are inter- cepted , converted to Arabic Unicode characters , and stored in a buffer , which has a start and an end but no inherent ordering . The Arabic Canvas Object observes the buffer and contains an Ara- bic Scribe object that renders the string of Uni- code characters right-to-left as connected Arabic glyphs . Figure 3 : Information Flow in the Xerox Arabic Demo . Input words from the user interface are transmitted across the Internet ( dotted lines ) and analyzed by a server , typically producing multi- ple analysis strings . Each analysis string is then generated in fully voweled form , combined with English glosses and then reformatted as HTML before being sent back across the Internet to the user ’ s browser for display . The analyzer and gen- erator finite-state transducers ( FSTs ) are identical except that the lower side language of the genera- tor is limited to contain only fully-voweled words . Figure 5 : A morphological analyzer-generator can be implemented elegantly and efficiently as a finite-state transducer . By Xerox convention , the lower-side language consists of surface strings ( words ) , and the upper-side language consists of strings representing analyses of the lower-side words . Such a transducer is a data structure rather than code , and the runtime code that applies such a transducer to input strings , in either direction , is completely language-independent . Figure 4 : Creation of a Lexical Transducer . The .o . operator represents the composition operation . Figure 1 : An Initial Learning Curve for Confusable Disambiguation Figure 2 . Learning Curves for Confusable Disambiguation Table 2 : Distribution of Error Types Figure 1 : Turning distributional similarity into a weighted inference rule Table 1 : Results on the RTE-1 Test Set . Table 2 : Results on the STS video dataset . Table 2 . A Morphophonological Rule Table 7 . Features with Set Values Table 8 . Subject and Object Agreement Features Table 10 . Speciﬁc Subject and Object Agreement Rules Table 11 . A Cascade of Compositions Table 13 . Shared Agreement Rules Table 14 . Rules of Referral Table 16 . Deﬁnition of Lingala Verbal Morphology Table 2 : Distribution of dialogue acts in our dataset . Table 1 : The 7 speakers from ICSI-MRDA dataset used in our experiments . The table lists : the Speaker ID , orig- inal speaker tag , the type of meeting selected for this speaker , the number of meetings this speaker participated and the total number of dialogue acts by this speaker . Figure 1 : Effect of same-speaker data on dialogue act recognition . We compare two approaches : ( 1 ) when a recognizer is trained on the same person and tested on new utterances from the same person , and ( 2 ) when the recognizer was trained on another speaker ( same test set ) . We vary the amount of training data to be 200 , 500 , 1000 , 1500 and 2000 dialogue acts . In all cases , using speaker-specific recognizer outperforms recognizer from other speakers . Figure 2 : The average results among all 7 speakers when train with different combinations of speaker specific data and other speakers ’ data are displayed . In both Constant adaptation and Reweighted adaptation models the num- ber of speaker specific data are varied from 200 , 500 , 1000 , 1500 to 2000 . In Generic model , only all other speakers ’ data are used for training data . Figure 4 : Average results of Reweighting among all 7 speakers when the amount of speaker specific data is 0 , 500 , 2000 Figure 2 : Beam search algorithm for joint tagging and de- pendency parsing of input sentence x with weight vector w and beam parameters b1 and b2 . The symbols h.c , h.s and h.f denote , respectively , the configuration , score and feature representation of a hypothesis h ; h.c.A denotes the arc set of h.c . Figure 1 : Transitions for joint tagging and dependency parsing extending the system of Nivre ( 2009 ) . The stack Σ is represented as a list with its head to the right ( and tail σ ) and the buffer B as a list with its head to the left ( and tail β ) . The notation f [ a → b ] is used to denote the function that is exactly like f except that it maps a to b . Figure 3 : Specialized feature templates for tagging . We use Σi and Bi to denote the ith token in the stack Σ and buffer B , respectively , with indexing starting at 0 , and we use the following functors to extract properties of a token : πi ( ) = ith best tag ; s ( πi ( ) ) = score of ith best tag ; π ( ) = finally predicted tag ; w ( ) = word form ; pi ( ) = word prefix of i characters ; si ( ) = word suffix of i characters . Score differences are binned in discrete steps of 0.05 . Table 1 : Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and the score threshold α. Beam parameters fixed at b1 = 40 , b2 = 4 . Table 2 : Accuracy scores for the CoNLL 2009 shared task test sets . Rows 1–2 : Top performing systems in the shared CoNLL Shared Task 2009 ; Gesmundo et al . ( 2009 ) was placed first in the shared task ; for Bohnet ( 2010 ) , we include the updated scores later reported due to some improvements of the parser . Rows 3–4 : Baseline ( k = 1 ) and best settings for k and α on development set . Rows 5–6 : Wider beam ( b1 = 80 ) and added graph features ( G ) and cluster features ( C ) . Second beam parameter b2 fixed at 4 in all cases . Table 3 : Accuracy scores for WSJ-PTB converted with head rules of Yamada and Matsumoto ( 2003 ) and labeling rules of Nivre ( 2006 ) . Best dev setting : k = 3 , α = 0.4 . Results marked with † use additional information sources and are not directly comparable to the others . Table 5 : Selected entries from the confusion matrix for parts of speech in German with F-scores for the left-hand- side category . ADJ* ( ADJD or ADJA ) = adjective ; ADV = adverb ; ART = determiner ; APPR = preposition ; NE = proper noun ; NN = common noun ; PRELS = relative pronoun ; VVFIN = finite verb ; VVINF = non-finite verb ; VAFIN = finite auxiliary verb ; VAINF = non-finite auxil- iary verb ; VVPP = participle ; XY = not a word . We use α* to denote the set of categories with α as a prefix . Table 4 : Accuracy scores for Penn Chinese Treebank converted with the head rules of Zhang and Clark ( 2008 ) . Best dev setting : k = 3 , α = 0.1 . MSTParser results from Li et al . ( 2011 ) . UAS scores from Li et al . ( 2011 ) and Ha- tori et al . ( 2011 ) recalculated from the separate accuracy scores for root words and non-root words reported in the original papers . Table 6 : Selected entries from the confusion matrix for parts of speech in English with F-scores for the left-hand- side category . DT = determiner ; IN = preposition or sub- ordinating conjunction ; JJ = adjective ; JJR = compara- tive adjective ; NN = singular or mass noun ; NNS = plural noun ; POS = possessive clitic ; RB = adverb ; RBR = com- parative adverb ; RP = particle ; UH = interjection ; VB = base form verb ; VBD = past tense verb ; VBG = gerund or present participle ; VBN = past participle ; VBP = present tense verb , not 3rd person singular ; VBZ = present tense verb , 3rd person singular . We use α* to denote the set of categories with α as a prefix . Figure 2 : Pseudo code of our clustering algorithm Figure 1 : The predicates of two sentences ( white : “ The company has said it plans to restate its earnings for 2000 through 2002. ” ; grey : “ The company had announced in January that it would have to restate earnings ( . . . ) ” ) from the Microsoft Research Paragraph Corpus are aligned by computing clusters with minimum cuts . Table 1 : Results for sentence-based predicte alignment in the three benchmark settings MTC , Leagues and MSR ( all numbers in % ) ; results that significantly differ from Full are marked with asterisks ( * p < 0.05 ; ** p < 0.01 ) . Table 3 : Impact of removing individual measures and us- ing a tuned weighting scheme ( all numbers in % ) ; results that significantly differ from Full are marked with aster- isks ( * p < 0.05 ; ** p < 0.01 ) . Table 2 : Results for GigaPairs ( all numbers in % ) ; re- sults that significantly differ from Full are marked with asterisks ( * p < 0.05 ; ** p < 0.01 ) . Figure 1 : Example of the transfer of a verbal chunk . Table 1 : Examples of translations Figure 1 : An example of the label consistency problem excerpted from a document in the CoNLL 2003 English dataset . Table 2 : Features used by the CRF for the two tasks : named entity recognition ( NER ) and template filling ( TF ) . Table 3 : Counts of the number of times multiple occurrences of a token sequence is labeled as different entity types in the same document . Taken from the CoNLL training set . Table 6 : F1 scores of the local CRF and non-local models on the CMU Seminar Announcements dataset . We also provide the results from Sutton and McCallum ( 2004 ) for comparison . Table 5 : F1 scores of the local CRF and non-local models on the CoNLL 2003 named entity recognition dataset . We also provide the results from Bunescu and Mooney ( 2004 ) for comparison . Table 1 : Summary of the previous work on coreference resolution that employs the learning algorithms , the clustering algorithms , the feature sets , and the training instance creation methods discussed in Section 3.1 . Table 2 : Statistics for the ACE corpus . Table 3 : Results for the three ACE data sets obtained via the MUC scoring program . Table 4 : Results for the three ACE data sets obtained via the B-CUBED scoring program . Table 5 : The coreference systems that achieved the highest F-measure scores for each test set and scorer combination . The average rank of the candidate partitions produced by each system for the corresponding test set is also shown . Table 1 : Overview of the tasks investigated in this paper ( n : size of n-gram ; POS : parts of speech ; Ling : linguistic knowledge ; Type : type of task ) Table 2 : Meaning of diacritics indicating statistical sig- nificance ( χ2 tests ) Table 4 : Performance comparison with the literature for candidate selection for MT Table 3 : Performance of Altavista counts and BNC counts for candidate selection for MT ( data from Prescher et al . 2000 ) Table 6 : Performance comparison with the literature for context sensitive spelling correction Table 5 : Performance of Altavista counts and BNC counts for context sensitive spelling correction ( data from Cucerzan and Yarowsky 2002 ) Table 9 : Performance comparison with the literature for compound bracketing Table 8 : Performance of Altavista counts and BNC counts for compound bracketing ( data from Lauer 1995 ) Table 7 : Performance of Altavista counts and BNC counts for adjective ordering ( data from Malouf 2000 ) Table 11 : Performance comparison with the literature for compound interpretation Table 10 : Performance of Altavista counts and BNC counts for compound interpretation ( data from Lauer 1995 ) Table 13 : Performance comparison with the literature for noun countability detection Table 12 : Performance of Altavista counts and BNC counts for noun countability detection ( data from Bald- win and Bond 2003 ) Table 1 : Syntactic Seeding Heuristics Figure 1 : Caseframe Network Examples Figure 2 : Lexical Caseframe Expectations Figure 3 : Semantic Caseframe Expectations Figure 4 : General Knowledge Sources Table 5 : Individual Performance of KSs for Disasters Table 4 : Individual Performance of KSs for Terrorism Table 3 : General + Contextual Role Knowledge Sources Table 2 : General Knowledge Sources Figure 1 : a ) A related work section extracted from ( Wu and Oard , 2008 ) ; b ) An associated topic hierar- chy tree of a ) ; c ) An associated topic tree , annotated with key words/phrases . Table 1 : The demographics of RWSData . No , RW , RA , SbL , WbL , TS , and TD are labeled as ( N ) umber ( o ) f , ( R ) elated ( W ) orks , ( R ) eferenced ( A ) rticles , ( S ) entence- ( b ) ased ( L ) ength of , ( W ) ord- ( b ) ased ( L ) ength of , ( T ) ree ( S ) ize , and ( T ) ree ( D ) epth , respectively . Figure 3 : A context modeling example . Table 2 : Evaluation results for ReWoS variants and baselines . Figure 1 : CCG and LTAG supertag sequences . Table 1 : Experimental results with individual features , compared against Moses and the moses-chart baseline . Table 2 : Experimental results with combined features . Table 3 : Numbers of rules in Hiero or phrase-pairs in Moses . Figure 1 : A verse written in the BAD web application . Figure 2 : The response of the rhyme search engine . Figure 3 : The BAD application before entering a verse , showing two possible rhyme patterns . Figure 1 : Extract of a French-English sentence pair segmented into bilingual units . The original ( org ) French sentence appears at the top of the figure , just above the reordered source s and target t. The pair ( s , t ) decomposes into a sequence of L bilingual units ( tuples ) u1 , ... , uL . Each tuple ui contains a source and a target phrase : si and ti . Table 1 : Experimental results in terms of BLEU scores measured on the newstest2011 and newstest2012 . For newstest2012 , the scores are provided by the organizers . Figure 1 : Source , transformed and extracted trees given headline British soldier killed in Afghanistan Table 1 : Types of features extracted for edge e from h to n Table 2 : Filters applied to candidate pair ( H , S ) Table 4 : Results for the systems and original headline : † and ‡ stand for significantly better than Unsupervised and Our system at 95 % confidence , respectively Table 3 : Results for two kinds of headlines Table 5 : Results for the unsupervised baseline and the supervised system trained on three kinds of feature sets Figure 1 : Percentage of major punctuation marks in the Chinese corpus 4 Table 1 : Syntax of Chinese Dash Table 2 : Rhetorical pattern of C-Question Table 3 : Rhetorical pattern of C-Exclamation Table 6 : Rhetorical pattern of C-Ellipses Table 5 : Rhetorical pattern of C-Semicolon Table 7 : Rhetorical pattern of C-Dash Table 4 : Rhetorical pattern of C-Colon Figure 2 : Rhetorical Function of Exclamation Mark in Chinese and German corpora Table 1 : POS/morphological feature accuracies on the development sets . Table 2 : PARSEVAL scores on the development sets . Figure 1 : Architecture of the dependency ranking system . Table 3 : Baseline performance and n-best oracle scores ( UAS/LAS ) on the development sets . mate ’ uses the prepro- cessing provided by the organizers , the other parsers use the preprocessing described in Section 2 . Table 6 : Unlabeled TedEval scores ( accuracy/exact match ) for the test sets in the predicted segmentation set- ting . Only sentences of length ≤ 70 are evaluated . Table 4 : Feature sets for the dependency ranker for each language . default denotes the default ranker feature set . Table 5 : Performance ( UAS/LAS ) of the reranker on the development sets . Baseline denotes our baseline . Ranked-dflt and Ranked denote the default and optimized ranker feature sets , respectively . Oracle denotes the oracle scores . Table 7 : Final PARSEVAL F1 scores for constituents on the test set for the predicted setting . ST Baseline denotes the best baseline ( out of 2 ) provided by the Shared Task organizers . Our submission is underlined . Table 8 : Final UAS/LAS scores for dependencies on the test sets for the predicted setting . Other denotes the highest scoring other participant in the Shared Task . ST Baseline denotes the MaltParser baseline provided by the Shared Task organizers . Figure 2 : Clustering an 11-nodes graph with CW in two iterations Figure 3 : The middle node gets the grey or the black class . Small numbers denote edge weights . Figure 7 : Percentage of obtaining two clusters when applying CW on n-bipartite cliques Figure 5 : oscillating states in matrix CW for an unweighted graph Figure 6 : The 10-bipartite clique . Figure 8 : Rate of obtaining two clusters for mix- tures of SW-graphs dependent on merge rate r . Table 1 : normalized Mutual Information values for three graphs and different iterations in % . Figure 9 : Bi-partite neighboring co-occurrence graph ( a ) and second-order graph on neighboring co-occurrences ( b ) clustered with CW . Table 4 : Disambiguation results in % dependent on frequency Table 3 : Disambiguation results in % dependent on word class ( nouns , verbs , adjectives ) Table 2 : the largest clusters from partitioning the second order graph with CW . Figure 1 : Graphical model for the Bayesian Query-Focused Summarization Model . Table 1 : Empirical results for the baseline models as well as BAYE S UM , when all query fields are used . Figure 2 : Performance with noisy relevance judg- ments . The X-axis is the R-precision of the IR engine and the Y-axis is the summarization per- formance in MAP . Solid lines are BAYE S UM , dot- ted lines are KL-Rel . Blue/stars indicate title only , red/circles indicated title+description+summary and black/pluses indicate all fields . Table 2 : Empirical results for the position-based model , the KL-based models and BAYE S UM , with different inputs . Fig . 2 . Translation performance of EM , Gibbs sampling , and variational Bayes and lower translation performance ( Section V-A ) . after applying alignment combination within and across methods : EM ( Co ) , Fig . 4 . Arabic English BLEU and TER scores of various a methods : EM ( Co ) , GS ( Co ) , EM ( Co ) +GS ( Co ) , and VB ( Co ) . Fig . 5 . Czech English BLEU scores of various al EM ( Co ) , GS ( Co ) , EM ( Co ) +GS ( Co ) , and VB ( Co ) . Fig . 6 . German English BLEU scores of various al EM ( Co ) , GS ( Co ) , EM ( Co ) +GS ( Co ) , and VB ( Co ) . Fig . 8 . Distribution of alignment fertilities for source language tokens . Fig . 7 . Arabic English BLEU scores o schemes in the 1M-sentence translation task . Fig . 9 . Intrinsic and extrinsic evaluation of alignments in the small data experiments . ( a ) Alignment dictionary size normalized by the average of source and target vocabulary sizes . ( b ) Average alignment fertility of aligned singletons . ( c ) Percentage of unaligned singletons . ( d ) Number of symmetric alignments normalized by the average of source and target tokens . ( e ) Percentage of training set vocabulary covered by single-word phrases in the phrase table . ( f ) Decode-time rate of input words that are in the training vocabulary but without a translation in the phrase table . ( g ) Phrase table size normalized by the average of source and target tokens . Fig . 11 . BLEU scores of alignments estimated at different iterations . Left : EM , middle : samples from the Gibbs chain , right : GS viterbi estimates with Table 1 : The results of different systems for coreference resolution Table 2 : Top patterns chosen under different scoring schemes Figure 2 : The decision tree ( Nwire ) for the system using the single semantic relatedness feature Figure 2 : Sequence of POS-tagged units used to estimate the bilingual n-gram LM . Table 1 : Statistics for the training , tune and test data sets . Figure 3 : Size ( in words ) of reorderings ( % ) ob- served in training bi-texts . Figure 4 : Size of translation unit n-grams ( % ) seen in test for different n-gram models . Table 3 : Reordering accuracy ( BLEU ) results . Table 2 : Translation accuracy ( BLEU ) results . Figure 2 : Example context for the spelling confusion set { piece , peace } and extracted features Figure 1 : Example context for WSD S ENSEVAL-2 target word bar ( inventory of 21 senses ) and extracted features Table 1 : The increase in performance for successive variants of Bayes and Mixture Model as evaluated by 5-fold cross vali- dation on S ENSEVAL -2 English data Figure 3 : A WSD example that shows the influence of syntactic , collocational and long-distance context features , the ® probability estimates used by Naïve Bayes and MM and their associated weights ( ) , and the posterior probabilities of the true sense as computed by the two models . ©° ©2Á Figure 4 : WSD example showing the utility of the MVC method . A sense with a high variational coefficient is preferred to the mode of the MM distribution ( the fields corresponding to the true sense are highlighted ) Figure 5 : MM and MMVC performance by performing 5- fold cross validation on S ENSEVAL -2 data for 4 languages Table 2 : Results using 5-fold cross validation on S ENSEVAL- 2 English lexical-sample training data Table 5 : The contribution of MMVC in a rank-based classi- fier combination on S ENSEVAL -1 and S ENSEVAL -2 English as computed by 5-fold cross validation over training data Table 4 : Results on the standard 14 CSSC data sets Table 3 : Results using 5-fold cross validation on S ENSEVAL- 1 training data ( English ) Figure 6 : Learning Curve for MM and MMVC on S ENSEVAL -2 English ( cross-validated on heldout data ) ings of the 13th International Conference , pages 182–190 . Table 6 : Accuracy on S ENSEVAL-1 and S ENSEVAL-2 En- A. R. Golding and D. Roth . 1999 . A winnow-based appro glish test data ( only the supervised systems with a coverage of to context-sensitive spelling correction . Machine Learni at least 97 % were used to compute the mean and variance ) 34 ( 1-3 ) :107–130 . Table 1 Sources of conﬂict in cross-lingual subjectivity transfer . Definitions and synonyms of the fourth sense of the noun argument , the fourth sense of verb decide , and the first sense of adjective free as provided by the English and Romanian WordNets ; for Romanian we also provide the manual translation into English . Fig . 1 . Cross-lingual bootstrapping . Fig . 2 . Multilingual bootstrapping . Fig . 3 . Macro-accuracy for cross-lingual bootstrapping . Fig . 4 . F-measure for the objective and subjective classes for cross-lingual bootstrapping . Fig . 5 . Macro-accuracy for multilingual bootstrapping ( versus cross-lingual framework ) . Fig . 6 . F-measure for the objective and subjective classes for multilingual bootstrapping ( versus cross-lingual framework ) . Table 2 Sample of subjective features appearing in the top 100 discriminant attributes selected with Information Gain on the 3rd fold training data at iteration 10 . The words in italics in the multilingual features represent equivalent translations in English and Romanian . Fig . 1 . Structure and data preprocessing of the initial dataset and the cleaned one after preprocessing . Fig . 2 . Schematic ﬂowchart of the workﬂow we followed , regarding the datasets , the training techniques and the operations . Table 2 Number of tweets mentioning party leaders . Fig . 3 . Example tweet assigned with feature scores . Fig . 4 . Performance of the training algorithms , supervised against semi-supervised techniques . The semi-supervised precision is evaluated indirectly by using the predicted dataset as train-set against the human-annotated manual small dataset . Table 4 Ironic tweets that received every party and their election results . The ﬂuctuation describes the difference between the May 2012 election results and the previous . Table 3 Performance measures of the training algorithms . Green indicates the best performance , while red the worst . Figure 1 : A portion of the local co-occurrence graph for “ mouse ” from the SemEval-2010 Task 14 corpus Figure 2 : V-Measure and paired FScore results for different partitionings of the dendrogram . The dashed vertical line indicates SP D Table 1 : Performance results on the SemEval-2010 WSI Task , with rank shown in parentheses . Refer- ence scores of the best submitted systems are shown in the bottom . Table 1 : Manual analysis of suggested corrections on CLC data . Figure 2 : Precision and recall for articles . Figure 1 : Precision and recall for prepositions . Figure 4 : Using different amounts of annotated training data for the article meta-classifier . Figure 3 : Using different amounts of annotated training Figure 1 : The Stanford parser ( Klein and Manning , 2002 ) is unable to recover the verbal reading of the unvocalized surface form an ( Table 1 ) . Table 1 : Diacritized particles and pseudo-verbs that , after orthographic normalization , have the equivalent surface form an . The distinctions in the ATB are linguistically justified , but complicate parsing . Table 8a shows that the best model recovers SBAR at only 71.0 % F1 . Table 2 : Frequency distribution for sentence lengths in the WSJ ( sections 2–23 ) and the ATB ( p1–3 ) . English parsing evaluations usually report results on sentences up to length 40 . Arabic sentences of up to length 63 would need to be evaluated to account for the same fraction of the data . We propose a limit of 70 words for Arabic parsing evaluations . Table 4 : Gross statistics for several different treebanks . Test set OOV rate is computed using the following splits : ATB ( Chiang et al. , 2006 ) ; CTB6 ( Huang and Harper , 2009 ) ; Ne- gra ( Dubey and Keller , 2003 ) ; English , sections 2-21 ( train ) and section 23 ( test ) . Figure 2 : An ATB sample from the human evaluation . The ATB annotation guidelines specify that proper nouns should be specified with a flat NP ( a ) . But the city name Sharm Al- Sheikh is also iDafa , hence the possibility for the incorrect annotation in ( b ) . Table 5 : Evaluation of 100 randomly sampled variation nu- clei types . The samples from each corpus were indepen- dently evaluated . The ATB has a much higher fraction of nuclei per tree , and a higher type-level error rate . Figure 3 : Dev set learning curves for sentence lengths ≤ 70 . All three curves remain steep at the maximum training set size of 18818 trees . Table 7 : Test set results . Maamouri et al . ( 2009b ) evaluated the Bikel parser using the same ATB split , but only reported dev set results with gold POS tags for sentences of length ≤ 40 . The Bikel GoldPOS configuration only supplies the gold POS tags ; it does not force the parser to use them . We are unaware of prior results for the Stanford parser . Figure 4 : The constituent Restoring of its constructive and effective role parsed by the three different models ( gold segmen- tation ) . The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals . Like verbs , maSdar takes arguments and assigns case to its objects , whereas it also demonstrates nominal characteristics by , e.g. , taking determiners and heading iDafa ( Fassi Fehri , 1993 ) . In the ATB , +2 3 asta ’ adah is tagged 48 times as a noun and 9 times as verbal noun . Consequently , all three parsers prefer the nominal reading . Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify . None of the models attach the attributive adjectives correctly . Table 9 : Dev set results for sentences of length ≤ 70 . Cov- erage indicates the fraction of hypotheses in which the char- acter yield exactly matched the reference . Each model was able to produce hypotheses for all input sentences . In these experiments , the input lacks segmentation markers , hence the slightly different dev set baseline than in Table 6 . Table 8 : Per category performance of the Berkeley parser on sentence lengths ≤ 70 ( dev set , gold segmentation ) . ( a ) Of the high frequency phrasal categories , ADJP and SBAR are the hardest to parse . We showed in §2 that lexical ambiguity explains the underperformance of these categories . ( b ) POS tagging accuracy is lowest for maSdar verbal nouns ( VBG , VN ) and adjectives ( e.g. , JJ ) . Richer tag sets have been suggested for modeling morphologically complex distinctions ( Diab , 2007 ) , but we find that linguistically rich tag sets do not help parsing . ( c ) Coordination ambiguity is shown in dependency scores by e.g. , S S S R and NP NP NP R . NP NP PP R and NP NP ADJP R are both iDafa attachment . Fig . 1 Examples of ambiguity for the English word play , together with different translations depending on the context Table 1 Related research integrating context into word-based SMT ( WB-SMT ) models Table 4 Related research integrating context into alternative SMT models Table 6 Related research using English as source language Fig . 2 Example of CCG supertags . CCG supertags are combined under the operations of forward and backward applications into a parse tree Fig . 3 Example of LTAG supertags . LTAG supertags are combined under the operations of substitution and adjunction into a parse tree Fig . 4 The dependency parse tree of the English sentence Can you play my favourite old record ? and the dependency features extracted from it for the SMT phrase play my favourite Fig . 5 The semantic graph of an English sentence and the semantic features extracted from it for an SMT phrase Table 7 Some of the possible Spanish translations of the English phrase make with their memory-based con- text-dependent translation probabilities ( rightmost column ) compared against context-independent transla- tion probabilities of the baseline system Table 8 Weights of different log-linear features of the CCG±1 system Table 10 Experiments with words and parts-of-speech as contextual features Table 11 Experiments with dependency relations Table 12 Experiments combining dependency relations , words and part-of-speech Fig . 6 Distances found between phrase boundaries with linked modifier words and with parent words Table 13 Experiments applying individual features in English-to-Hindi translation Table 14 Experiments applying combinations of features in English-to-Hindi translation Table 16 Experimental results on the WMT 2010 test set Table 15 Experimental results on the WMT 2009 test set Table 17 Results on large-scale Dutch-to-English translation Table 18 Results on English-to-Dutch translation employing homogeneous features Table 21 Experimental results for large-scale English-to-Chinese translation Table 20 Experimental results for large-scale English-to-Japanese translation Fig . 7 BLEU learning curves ( left ) and difference curves ( right ) comparing the Moses baseline against two IGTree ( LTAG±1 and PR ) and TRIBL ( Super-Pair±1 and PR ) classifiers Fig . 8 Average number of target phrase distribution sizes for source phrases for TRIBL and IGTree com- pared to the Moses baseline Fig . 9 BLEU difference curves of four context-informed models using TRIBL Fig . 10 Dutch-to-English Learning curves ( left-hand side graphs ) and difference curves ( right-hand side graphs ) comparing the Moses baseline against four context-informed models ( PR , OE , POS±2 and Word±2 ) . These curves are plotted with scores obtained using three evaluation metrics : BLEU ( top ) , METEOR ( centre ) and TER ( bottom ) Fig . 11 English-to-Dutch Learning curves ( left-hand side graphs ) and difference curves ( right-hand side graphs ) comparing the Moses baseline against four context-informed models ( CCG±1 , LTAG±1 , PR , PS-AL , POS±2 and Word±2 ) . These curves are plotted with scores obtained using three evaluation met- rics : BLEU ( top ) , METEOR ( centre ) and TER ( bottom ) Figure 1 : BiTAM models for Bilingual document- and sentence-pairs . A node in the graph represents a random variable , and a hexagon denotes a parameter . Un-shaded nodes are hidden variables . All the plates represent replicates . The outmost plate ( M -plate ) represents M bilingual document-pairs , while the inner N -plate represents the N repeated choice of topics for each sentence-pairs in the document ; the inner J-plate represents J word-pairs within each sentence-pair . ( a ) BiTAM-1 samples one topic ( denoted by z ) per sentence-pair ; ( b ) BiTAM-2 utilizes the sentence-level topics for both the translation model ( i.e. , p ( f |e , z ) ) and the monolingual word distribution ( i.e. , p ( e|z ) ) ; ( c ) BiTAM-3 samples one topic per word-pair . Table 1 : Training and Test Data Statistics Table 2 : Topic-specific translation lexicons are learned by a 3-topic BiTAM-1 . The third lexicon ( Topic-3 ) prefers to translate the word Korean into ChaoXian ( m : North Korean ) . The co-occurrence ( Cooc ) , IBM-1 & 4 and HMM only prefer to translate into HanGuo ( ¸I : South Korean ) . The two candidate translations may both fade out in the learned translation lexicons . Table 3 : Three most distinctive topics are displayed . The English words for each topic are ranked according to p ( e|z ) estimated from the topic-specific English sentences weighted by { φdnk } . 33 functional words were removed to highlight the main content of each topic . Topic A is about Us-China economic relationships ; Topic B relates to Chinese companies ’ merging ; Topic C shows the sports of handicapped people . Figure 2 : performances over eight Variational EM itera- tions of BiTAM-1 using both the “ Null ” word and the laplace smoothing ; IBM-1 is shown over eight EM iterations for comparison . Table 4 : Word Alignment Accuracy ( F-measure ) and Machine Translation Quality for BiTAM Models , comparing with IBM Models , and HMMs with a training scheme of 18 h7 43 on the Treebank data listed in Table 1 . For each column , the highlighted alignment ( the best one under that model setting ) is picked up to further evaluate the translation quality . Table 5 : Evaluating Word Alignment Accuracies and Machine Translation Qualities for BiTAM Models , IBM Models , HMMs , and boosted BiTAMs using all the training data listed in Table . 1 . Other experimental conditions are similar to Table . 4 . Table 1 . Sample seeds used for each semantic relation and sample outputs from Espresso . The number in the parentheses for each relation denotes the total number of seeds . Table 8 . System performance on the production relation on the CHEM dataset . Table 7 . System performance on the reaction relation on the CHEM dataset . Table 2 . System performance on the is-a relation on the TREC-9 dataset . Table 3 . System performance on the is-a relation on the CHEM dataset . Table 4 . System performance on the part-of relation on the TREC-9 dataset . Table 5 . System performance on the part-of relation on the CHEM dataset . Table 6 . System performance on the succession relation on the TREC-9 dataset . Figure 1 : An underspecified discourse structure and its five configurations Figure 2 : A wRTG modelling Fig . 1 Table 1 . Experimental data sets Table 7 . Precision at top 200 Table 2 . Results of 1000 sentences Table 5 . Precision at top 50 Table 4 . Results of 3000 sentences Table 6 . Precision at top 100 Table 3 . Results of 2000 sentences Table 1 . Overall steps of proposed method Table 2 . The result of pre-processing Table 3 . Pseudo-code to extract UW Table 6 . Weight-measure of co-occurring words Table 4 . Pseudo-code to extract UW Table 5 . Extracted UW and noun set Table 10 . Result of normalization Table 8 . Related noun group and sum of Bayesian Table 7 . Weight of co-occurring words Table 2 : TercomTERs of invWER-oracles and ( in paren- theses ) oracle BLEU scores of confusion networks gen- erated with tercom and ITG alignments . The best results per row are shown in bold . Table 1 : Comparison of average per-document ter- comTER with invWER on the EVAL07 GALE Newswire ( “ NW ” ) and Weblogs ( “ WB ” ) data sets . Figure 1 Naı̈ve FSA with duplicated paths . Figure 3 FSA for the pattern hit a e . Figure 4 4-tape representation for the Hebrew word htpqdut . Figure 7 FSRA for the pattern hit a e . Figure 8 FSRA-2 for Arabic nominative definite and indefinite nouns . Figure 10 FSRA for a given CNF formula . Figure 11 Participle-forming combinations in German . Figure 12 Interdigitation FSRA – general . Figure 14 Reduplication for n = 4 . Figure 15 Reduplication – general case . Figure 16 FSRA* for Arabic nominative definite nouns . Table 2 Time comparison between FSAs and FSRAs . Table 1 Space comparison between FSAs and FSRAs . Figure 1 : Four of the five logically possible schemes for annotating coordination show up in human-produced depen- dency treebanks . ( The other possibility is a reverse Mel ’ čuk scheme . ) These treebanks also differ on other conventions . Figure 2 : With the English tree and alignment provided by a parser and aligner at test time , the Chinese parser finds the correct dependencies ( see §6 ) . A monolingual parser ’ s incor- rect edges are shown with dashed lines . Table 2 : Precision and recall of direct dependency projection via one-to-one links alone . Table 1 : Adapting a parser to a new annotation style . We learn to parse in a “ target ” style ( wide column label ) given some number ( narrow column label ) of supervised target-style training sentences . As a font of additional features , all training and test sentences have already been augmented with parses in some “ source ” style ( row label ) : either gold-standard parses ( an oracle experiment ) or else the output of a parser trained on 18k source trees ( more realistic ) . If we have 0 training sentences , we simply output the source-style parse . But with 10 or 100 target-style training sentences , each off-diagonal block learns to adapt , mostly closing the gap with the diagonal block in the same column . In the diagonal blocks , source and target styles match , and the QG parser degrades performance when acting as a “ stacked ” parser . Table 3 : Test accuracy with unsupervised training methods Figure 3 : Parser projection with target trees . Using the true or 1-best parse trees in the source language is equivalent to having twice as much data in the target language . Note that the penalty for using automatic alignments instead of gold alignments is negligible ; in fact , using Source text alone is often higher than +Gold alignments . Using gold source trees , however , significantly outperforms using 1-best source trees . Figure 1 : Plate diagram representation of the trigram HMM . The indexes i and j range over the set of tags and k ranges over the set of characters . Hyper-parameters have been omitted from the figure for clarity . Figure 2 : The conditioning structure of the hierarchical PYP with an embedded character language models . Figure 3 : Simulation comparing the expected table count ( solid lines ) versus the approximation under Eq . 3 ( dashed lines ) for various values of a . This data was gen- erated from a single PYP with b = 1 , P0 ( i ) = 14 and n = 100 customers which all share the same tag . Table 1 : WSJ performance comparing previous work to our own model . The columns display the many-to-1 accuracy and the V measure , both averaged over 5 inde- pendent runs . Our model was run with the local sampler ( HMM ) , the type-level sampler ( 1HMM ) and also with the character LM ( 1HMM-LM ) . Also shown are results using Dirichlet Process ( DP ) priors by fixing a = 0 . The system abbreviations are CGS10 ( Christodoulopoulos et al. , 2010 ) , BBDK10 ( Berg-Kirkpatrick et al. , 2010 ) and GG07 ( Goldwater and Griffiths , 2007 ) . Starred entries denote results reported in CGS10 . Figure 4 : Sorted frequency of tags for WSJ . The gold standard distribution follows a steep exponential curve while the induced model distributions are more uniform . Figure 6 : Cooccurence between frequent gold ( y-axis ) and predicted ( x-axis ) tags , comparing mkcls ( top ) and PYP-1HMM-LM ( bottom ) . Both axes are sorted in terms of frequency . Darker shades indicate more frequent cooc- curence and columns represent the induced tags . Figure 5 : M-1 accuracy vs. number of samples . Table 2 : Many-to-1 accuracy across a range of languages , comparing our model with mkcls and the best published result ( ? Berg-Kirkpatrick et al . ( 2010 ) and † Lee et al . ( 2010 ) ) . This data was taken from the CoNLL-X shared task training sets , resulting in listed corpus sizes . Fine PoS tags were used for evaluation except for items marked with c , which used the coarse tags . For each language the systems were trained to produce the same number of tags as the gold standard . Figure 1 : Pipeline architecture for dialogue act recognition and re-ranking component . Here , the input is a list of dialogue acts with confidence scores , and the output is the same list of dialogue acts but with recomputed confidence scores . A dialogue act is represented as DialogueActType ( attribute-value pairs ) . Figure 2 : Bayesian network for probabilistic rea- soning of locations ( variable “ from desc ” ) , which incorporates ASR N-best information in the vari- able “ from desc nbest ” and dialogue history in- formation in the remaining random variables . Figure 3 : Bayesian dialogue act recognisers show- ing the impact of ASR N-best information . Figure 2 : Selected morphosyntactic categories in the OLiA Reference Model Figure 1 : Attributive demonstrative pronouns ( PDAT ) in the STTS Annotation Model Figure 3 : Individuals for accusative and sin- gular in the TIGER Annotation Model Figure 4 : Selected morphological features in the OLiA Reference Model Figure 5 : The STTS tags PDAT and ART , their rep- resentation in the Annotation Model and linking with the Reference Model . Table 1 : Confidence scores for diese in ex . ( 1 ) Table 3 : Recall for morphological hasXY ( ) descriptions Table 1 : Examples of DTs and their ICD-codes . Table 3 : The data-set shuffled and divided into 3 sets Table 2 : The data-set of ( DT , ICD-code ) pairs . Figure 1 : A simplified version in Foma source code of the regular expressions and transducers used to bypass several sources of distortion within the DTs in order to parse variations of unseen input DTs . Table 4 : Performance of different FS machines in terms of the percentage of unclassified entries . All the classified entries were correctly classified , yielding , as a result , a precision of 100 % . Figure 1 : BLEU scores for each translation direction trained on ( ← ) directional ( condition on target and generate source ) and ( ↔ ) symmetrised alignments ( grow-diag-final-and ) . Observe that the plots are on different scales . This means that results can not directly be compared across plots . Figure 1 : The screenshot of our web-based system shows a simple quantitative analysis of the frequency of two terms in news articles over time . While in the 90s the term Friedensmission ( peace operation ) was predominant a reverse tendency can be observed since 2001 with Auslandseinsatz ( foreign intervention ) being now frequently used . Figure 2 : Overview of the complete processing chain . Figure 3 : Dependency parse of a sentence that contains indirect speech ( see Sentence 2 ) . Figure 4 : 10 most used verbs ( lemma ) in indirect speech . Table 1 : Results of a 10-fold cross-validation for various machine learning algorithms . Figure 1 . Learning Curves for Confusion Set Disambiguation Figure 2 . Representation Size vs. Training Corpus Size Figure 3 . Voting Among Classifiers Figure 4 . Active Learning with Large Corpora Table 3 . Committee-Based Unsupervised Learning Table 2 . Committee Agreement vs . Accuracy Table 4 . Comparison of Unsupervised Learning Methods Table 2 Open test , in percentages ( % ) Table 1 Closed test , in percentages ( % ) Figure 1 : Graphical model of HM-BiTAM Figure 2 : Graphical model of synonym pair gen- erative process Table 2 : The number of vocabularies in the 10k , 50k and 100k data sets . Table 1 : Comparison of word alignment accuracy . The best results are indicated in bold type . The additional data set sizes are ( a ) 10k , ( b ) 50k , ( c ) 100k . Figure 1 : Three narrative events and the six most likely events to include in the same chain . Figure 2 : One of the 69 test documents , containing 10 narrative events . The protagonist is President Bush . Figure 3 : Results with varying sizes of training data . Year 2003 is not explicitly shown because it has an unusually small number of documents compared to other years . Figure 4 : A narrative chain and its reverse order . Figure 5 : Results for choosing the correct ordered chain . ( ≥ 10 ) means there were at least 10 pairs of ordered events in the chain . Figure 7 : An Employment Chain . Dotted lines indicate incorrect before relations . Figure 6 : An automatically learned Prosecution Chain . Arrows indicate the before relation . Table 2 : Results for baseline ( BAS ) system ( standard multiclass SVM ) Table 3 : Results for basic DAC system ( per-class feature optimization followed by maximum confidence based choice ) ; “ ER ” refers to error reduction in percent over standard multiclass SVM ( Table 2 ) Table 5 : Results for cascading minority-preference DAC system — DACCMP ( consult classifiers in reverse order of frequency of class ) ; “ ER ” refers to error reduction in percent over standard multiclass SVM ( Table 2 ) Table 7 : Results for ODP system using various sources of DA tags Table 6 : Post-hoc analysis on the models built by the DAC system : some of the top features with corresponding feature weights in parentheses , for each individual tagger . ( POS tags are capitalized ; BOS stands for Beginning Of Sentence ) Figure 1 : Probability of # of boundaries f10 ( m′ ; 3 ) . Table 1 : Results of segmentation of entry titles ( F-score ( precision/recall ) ) . Figure 3 : Acceptance rates for a noun phrase in the course of iteration . All models were with back-off mix- ing ( +BM ) . Figure 2 : Diffs in the course of iteration . All models were with back-off mixing ( +BM ) . Table 2 : Effect of matching skip ( F-score ( precision/recall ) ) . Table 1 : Features based on the token string Table 2 : Sources of Dictionaries Figure 2 : The whole process of re-training the upper case NER . Q signifies that the text is converted to upper case before processing . Table 3 : F-measure on MUC-6 and MUC-7 test data Figure 3 : Improvements in F-measure on MUC-6 plotted against amount of selected unlabeled data used Figure 4 : Improvements in F-measure on MUC-7 plotted against amount of selected unlabeled data used Table 1 : Properties of the manually aligned corpus Table 2 : Examples of output of the phrase-based and syntax-based systems Figure 2 : Distribution of generated paraphrases per Lev- enshtein distance Figure 1 : NIST scores per Levenshtein distance Table 1 : Trivial and single-feature baselines ( using SVM- acc unless noted otherwise ) Table 2 : Combination results ( using SVMacc ) Table 3 : Results by relation Figure 1 : Lattice representation of the sentence bclm hneim . Double-circles denote token boundaries . Lattice arcs correspond to different segments of the token , each lattice path encodes a possible reading of the sentence . Notice how the token bclm have analyses which include segments which are not directly present in the unsegmented form , such as the definite article h ( 1-3 ) and the pronominal suffix which is expanded to the sequence fl hm ( “ of them ” , 2-4 , 4-5 ) . Table 1 : Parsing scores of the various systems Table 1 . Speech Act Categories and Kappa values Figure 1 . An example discussion thread Table 2 . Statistics for each Speech Act Category Table 4 . Some of the top selected features by Infor- mation Gain Table 5 . SA classification results Table 6 . Thread Classification Results Figure 2 . Example patterns in student discussion threads Table 7 . Results from Direct Thread Classification Figure 1 : Two fragments of a hierarchy over word class distributions Figure 2 : Error reduction as a function of vocabulary size Table 1 : Evaluation of coarse-grained POS tagging on test data Table 2 : Evaluation of coarse-grained POS tagging on test data Figure 1 : Example of a German noun phrase . First and last word agree in number , gender , and case value . Table 2 : Syntactic features . h and ld mark features from the head and the left-most daughter , dir is a binary fea- ture marking the direction of the head with respect to the current token . Table 3 : The effect of syntactic features when predicting morphological information . * mark statistically signifi- cantly better models compared to our baseline ( sentence- based t-test with α = 0.05 ) . Table 4 : The effect of syntactic features when predicting morphology using lexicons . * mark statistically signifi- cantly better models compared to our baseline ( sentence- based t-test with α = 0.05 ) . Table 6 : Syntactic features for featurama ( Czech ) . * mark statistically significantly better models compared to feat- urama ( sentence-based t-test with α = 0.05 ) . Table 5 : Agreement counts in morphological annotation compared between the baseline system and the oracle system using gold syntax . Table 7 : Simple parser vs full parser – syntactic quality . Trained on first 5,000 sentences of the training set . Figure 2 : Dependency between amount of training data for syntactic parser and quality of morphological prediction . Table 9 : Impact of the improved morphology on the qual- ity of the dependency parser for Czech and German . Table 8 : Simple parser vs full parser – morphological quality . The parsing models were trained on the first 5,000 sentences of the training data , the morphological tagger was trained on the full training set . Table 1 Baseline : Out-of-the-box BerkeleyParser performance on the dev-set . Table 2 Number of learned splits per POS category after five split-merge cycles . Table 3 Number of learned splits per NT-category after five split-merge cycles . Figure 1 A layered POS tag representation . Figure 2 A latent layered POS tag representation . Figure 3 The lattice for the Hebrew sequence ! ‫ ( בצ‌לם הנ‌עים‬see footnote 19 ) . Table 5 Dev-set results when using lattice parsing on top of an external lexicon/analyzer . Table 4 Dev-set results when incorporating an external lexicon . Table 7 Dev-set results of using the agreement-filter on top of the lexicon-enhanced lattice parser ( parser does both segmentation and parsing ) . Table 6 Dev-set results of using the agreement-filter on top of the lexicon-enhanced parser ( starting from gold segmentation ) . Table 8 Numbers of parse-tree nodes in the 1-best parses of the development set that triggered gender or number agreement checks , and the results of these checks . Table 9 Test-set results of the best-performing models . Figure 5 NP agreement violations that were caught by the agreement filter system . ( a ) Noun-compound case that was correctly handled . ( b ) Case involving conjunction that was correctly handled . ( c ) A case where fixing the agreement violation introduces a PP-attachment mistake . Figure 1 : The NLM-WSD test set and some of its sub- sets . Note that the test set used by ( Joshi et al. , 2005 ) comprises the set union of the terms used by ( Liu et al. , 2004 ) and ( Leroy and Rindflesch , 2005 ) while the “ com- mon subset ” is formed from their intersection . Table 1 : Results from WSD system applied to various sections of the NLM-WSD data set using a variety of fea- tures and machine learning algorithms . Results from baseline and previously published approaches are included for comparison . Table 2 : Per-word performance of best reported systems . Figure 1 : Example of semantic trees Figure 2 : Two STs composing a STN Table 1 : ROUGE F-scores for different systems Table 1 : Phrase pairs extracted from a document pair with an economic topic Table 2 : Topic words extracted from target-side doc- uments Table 4 : Contribution of various caches in our cache- based document-level SMT system . Note that signific- ance tests are done against Moses . Figure 5 : Contribution of combining the three caches Figure 4 : Contribution of combining the dynamic Figure 3 : Contribution of combining the dynamic and Figure 2 : Contribution of employing the dynamic cache on different test documents Table 5 : Impact of the topic cache size Figure 6 : Contribution of the static cache on the first sentence of each test document ( i.e . with empty dynamic cache ) Table 6 : Positive and negative examples Table 1 . Descriptions of the 10 corpora Table 3 . Average precisions over the 10 corpora of different window size ( 3 seeds ) Table 2 . Precision @ top N ( with 3 seeds , and window size w = 3 ) Table 1 : Deﬁnition of NE in IREX . Figure 1 : Example of morphological analyses . Table 2 : Case frame of “ haken ( dispatch ) . ” Table 3 : Experimental results ( F-measure ) . Table 4 : Comparison with previous work . Table 1 : The set of types and subtypes of relations used in the 2004 ACE evaluation . Table 2 : The Division of LDC annotated data into training and development test sets . Figure 1 : Comparing F-measure , precision , and recall of different voting schemes for English relation extraction . Figure 2 : Comparing F-measure , precision , and recall of different voting schemes for Chinese relation extraction . Figure 3 : Comparing F-measure , precision , and recall of different voting schemes for Arabic relation ex- traction . Table 3 : Comparing the best F-measure obtained by At-Least-N Voting with Majority Voting , Summing and the single best classifier . Figure 1 : Illustration of Pareto Frontier . Ten hypotheses are plotted by their scores in two metrics . Hypotheses indicated by a circle ( o ) are pareto-optimal , while those indicated by a plus ( + ) are not . The line shows the convex hull , which attains only a subset of pareto-optimal points . The triangle ( 4 ) is a point that is weakly pareto-optimal but not pareto-optimal . Table 1 : Task characteristics : # sentences in Train/Dev , # of features , and metrics used . Our MT models are trained with standard phrase-based Moses software ( Koehn and others , 2007 ) , with IBM M4 alignments , 4gram SRILM , lexical ordering for PubMed and distance ordering for the NIST system . The decoder generates 50-best lists each iteration . We use SVMRank ( Joachims , 2006 ) as opti- mization subroutine for PRO , which efficiently handle all pairwise samples without the need for sampling . Figure 2 : PubMed Results . The curve represents the Pareto Frontier of all results collected after multiple runs . Figure 6 : Average number of Pareto points Table 2 : Training time usage in PMO-PRO ( Algo 2 ) . Figure 5 : Avg . runtime per sentence of FindPareto Figure 4 : Learning Curve on RIBES : comparing single- objective optimization and PMO . Table 1 : SyntSem tagged corpus extract . Table 2 : Optimal context size ( S ) and criteria Figure 1 : space distribution by part-of-speech of Table 3 : Precision ( P % ) and usage proportion Table 7 : optimal context size using both Table 6 : precision with and without feature Table 5 : precision decrease when omitting non- Table 4 : space distribution of most reliable Table 8 : target word frequency ( F ) , average Figure 1 : Evaluation measures : tp = true positives , fp = false positives , tn = true negatives , fn = false negatives , pr = precision , re = recall Table 1 : Classification results with XLE starredness , parser exceptions and zero parses ( Method 1 ) Table 2 : Classification results with 5-gram and fre- quency threshold 4 ( Method 2 ) Table 3 : Classification results with decision tree on XLE output ( Method 3 ) Table 4 : Classification results with decision tree on vectors of frequency of rarest n-grams ( Method 4 ) Table 5 : Classification results with decision tree on joined feature set ( Method 5 ) Figure 2 : Accuracy by sentence length for Method 5 measured on separate grammatical and ungrammat- ical data : Gr = Grammatical , AG = Agreement , RW = Real-Word , EW = Extra Word , MW = Missing Word Fig . 1 . Fuzzy hierarchical clustering for Paraphrase Extraction . Fig . 2 . Collecting paraphrases using a Paraphrase Recognizer . Fig . 3 . Algorithm for Fuzzy Agglomerative Clustering based on verbs . Fig . 5 . ( a ) Binary merging of clusters . ( b ) Merging of multiple clusters . Fig . 4 . Simpliﬁed Lesk algorithm [ 21 ] . Fig . 7 . Example of fuzzy divisive clustering . Fig . 6 . Algorithm for fuzzy divisive clustering based on nouns . Table 1 Performance of Paraphrase Recognition system on MSRPC [ 25 ] . Fig . 8 . Dependency parse and triples for the sentence – “ Mr Burke said it was a textbook landing considering the circumstances ” . Table 2 Statistics of paraphrase pairs retrieved from MSRPC . Table 3 Precision of existing and proposed approaches . Table 4 Relative recall evaluation . Table 5 Comparative performance on MSRPC . Table 6 Performance of proposed system on MSRVDC Dataset 1 . Fig . 9 . Performance evaluation of proposed and existing systems . Table 7 Performance of existing approaches on MSRVDC Dataset 1 . Table 8 Performance evaluation on MSRVDC Dataset 2 . Table 1 : Training and test set sources , genres , sizes in terms of numbers of tokens , and unigram and bi- gram coverage ( % ) of the training set on the test sets . Figure 1 : Learning curves of word prediction accu- racies of IGT REE trained on TRAIN - REUTERS , and tested on REUTERS , ALICE , and BROWN . Figure 2 : Word prediction speed , in terms of the number of classified test examples per second , mea- sured on the three test sets , with increasing training examples . Both axes have a logarithmic scale . Figure 3 : Learning curves in terms of word predic- tion accuracy on deciding between the confusable pair there , their , and they ’ re , by IGT REE trained on TRAIN - REUTERS , and tested on REUTERS , AL - ICE , and BROWN . The top graphs are accuracies at- tained by the confusable expert ; the bottom graphs are attained by the all-words predictor trained on TRAIN - REUTERS until 130 million examples , and on TRAIN - NYT beyond ( marked by the vertical bar ) . Table 3 : Disambiguation scores on nine confusable set , attained by confusable experts trained on ex- amples extracted from 1 billion words of text from TRAIN - REUTERS plus TRAIN - NYT , on the three test sets . Table 2 : Disambiguation scores on nine confusable set , attained by the all-words prediction classifier trained on 30 million examples of TRAIN - REUTERS , and by confusable experts on the same training set . The second column displays the number of exam- ples of each confusable set in the 30-million word training set ; the list is ordered on this column . Table 1 : Kleene Regular-Expression Assignment Examples . Figure 1 : Four synchronous rules with topic distributions . Each sub-graph shows a rule with its topic distribution , where the X-axis means topic index and the Y-axis means the topic probability . Notably , the rule ( b ) and rule ( c ) shares the same source Chinese string , but they have different topic distributions due to the different English translations . Table 1 : Example of topic-to-topic correspondence . The last line shows the correspondence probability . Each col- umn means a topic represented by its top-10 topical word- s. The first column is a target-side topic , while the rest three columns are source-side topics . Table 2 : Result of our topic similarity model in terms of BLEU and speed ( words per second ) , comparing with the traditional hierarchical system ( “ Baseline ” ) and the topic-specific lexicon translation method ( “ TopicLex ” ) . “ SimSrc ” and “ SimTgt ” denote similarity by source-side and target-side rule-distribution respectively , while “ Sim+Sen ” acti- vates the two similarity and two sensitivity features . “ Avg ” is the average B LEU score on the two test sets . Scores marked in bold mean significantly ( Koehn , 2004 ) better than Baseline ( p < 0.01 ) . Table 4 : Effects of one-to-one and one-to-many topic pro- jection . Figure 1 : Illustration of similarities in POS tag statistics across languages . ( a ) The unigram frequency statistics on five tags for two close languages , English and German . ( b ) Sample sentences in English and German . Verbs are shown in blue , prepositions in red and noun phrases in green . It can be seen that noun phrases follow prepositions . Figure 2 : An iterative algorithm for minimizing our ob- jective in Eq . ( 7 ) . For simplicity we assume that all the weights αi and λ are equal to one . It can be shown that the objective monotonically decreases in every iteration . Table 1 : The set of typological features that we use for source language selection . The first column gives the ID of the feature as listed in WALS . The second column describes the feature and the last column enumerates the allowable values for each feature ; besides these values each feature can also have a value of ‘ No dominant order ’ . Table 2 : Directed dependency accuracy of our model and the baselines using gold POS tags for the target language . The first section of the table is for the direct transfer of the MST parser ( McDonald et al. , 2011 ) . The second section is for the weighted mixture parsing model ( Cohen et al. , 2011 ) . The first two columns ( Random and Greedy ) of each section present the parsing performance with a random or a greedy mapping . The third column ( Petrov ) shows the results when the mapping of Petrov et al . ( 2011 ) is used . The fourth column ( Model ) shows the results when our mapping is used and the fifth column in the first section ( Best Pair ) shows the performance of our model when the best source language is selected for every target language . The last column ( Tag Diff . ) presents the difference between our mapping and the mapping of Petrov et al . ( 2011 ) by showing the percentage of target language tokens for which the two mappings select a different universal tag . Figure 3 : Objective values for the different mappings used in our experiments for four languages . Note that the goal of the optimization procedure is to minimize the objective value . Figure 2 : Opinion HITS model Figure 3 : Opinion Question Answering System Table 1 : Sentiment lexicon description Figure 6 : Opinion PageRank Performance with varying parameter µ ( λ = 0.2 ) Figure 4 : Sentiment Lexicon Performance Figure 5 : Opinion PageRank Performance with varying parameter λ ( µ = 0.5 ) Table 2 : Question-specific popular topic words and opinion words generated by Opinion HITS Figure 7 : Opinion HITS Performance with vary- ing parameter γ Table 3 : Comparison results with TAC 2008 Three Top Ranked Systems ( system 1-3 demonstrate top 3 systems in TAC ) Figure 3 : Screenshot of ConAno Figure 4 : Overview of annotation environment Figure 1 . Correlation between cohesion-driven functions . Table 1 : Possible interpretations for the text wAlY ( Habash and Rambow , 2005 ) . Table 2 : Syntactic dependency scheme used in this work . Labels that aren ’ t self-explanatory or similar to the labels used by Tratz and Hovy ( 2011 ) for English or CATiB for Arabic ( Habash and Roth , 2009 ) are in bold ( for completely new relations ) or italics ( for similarly named but semantically different relations ) Table 3 : Counts of the number of files , sentences ( Sent ) , original space-delimited tokens ( Tok ) , ATB tree tokens ( Tree Toks ) , and affixes in the experimental data . Table 6 : Counts for the POS tags mentioned in Table 5 . Table 5 : Top 10 POS mistakes made more often by either the CTF-TM with parsing or the CTF-TM without on the ATB part 1 , 2 , and 3 development set . Table 4 : Results for the various experiments ( Exp ) for both the development and test portions of the data , including per- token clitic separation ( tokenization ) accuracy , part-of-speech tagging F1 , affix boundary detection F1 , affix labeling F1 , and both unlabeled and labeled attachment scores . Table 1 . Gibbs sampling for word alignment . Table 3 . Performance of Final Translation ( BLEU-4 ) . Table 2 . Performance of Word Alignment . Table 1 : Distribution of annotated data . Table 2 : Results of UniGraph , BiGraph , and Bi- Graph* . Table 4 : Results of the gloss classifier . Table 6 : Comparison to onlySL and onlyGraph . Table 5 : Results of an iterative approach . Table 7 : Accuracy and frequency of the top 5 % for each iteration Table 1 : Part of a sample headline cluster , with sub-clusters Table 3 : Examples of correct ( above ) and incorrect ( below ) alignments Table 2 : Precision and Recall for both methods Figure 1 : Chinese parse tree with empty elements marked . The meaning of the sentence is , “ Implementation of the law is temporarily suspended . ” Figure 2 : English parse tree with empty elements marked . ( a ) As annotated in the Penn Treebank . ( b ) With empty elements reconﬁgured and slash categories added . Table 3 : Recall on different types of empty categories . YX = ( Yang and Xue , 2010 ) , Ours = split 6× . Table 2 : Results on Penn ( Chinese ) Treebank . Table 1 : Results on Penn ( English ) Treebank , Wall Street Journal , sentences with 100 words or fewer . Table 1 : Length distribution of entities in the train- ing set of the shared task in 2004 JNLPBA Figure 1 : Modification of “ O ” ( other labels ) to transfer information on a preceding named entity . Figure 2 : The framework of our system . We first enumerate all possible candidate states , and then filter out low probability states by using a light-weight classifier , and represent them by using feature forest . Table 2 : Features used in the naive Bayes Classi- fier for the entity candidate : ws , ws+1 , ... , we . spi is the result of shallow parsing at wi . Figure 3 : Example of feature forest representation of linear chain CRFs . Feature functions are as- signed to “ and ” nodes . Figure 4 : Example of packed representation of semi-CRFs . The states that have the same end po- sition and prev-entity label are packed . Table 4 : Filtering results using the naive Bayes classifier . The number of entity candidates for the training set was 4179662 , and that of the develop- ment set was 418628 . Table 3 : Feature templates used for the chunk s : = ws ws+1 ... we where ws and we represent the words at the beginning and ending of the target chunk respectively . pi is the part of speech tag of wi and sci is the shallow parse result of wi . Table 8 : Comparison with other systems Table 5 : Performance with filtering on the development data . ( < 1.0 × 10−12 ) means the threshold probability of the filtering is 1.0 × 10−12 . Table 7 : Performance of our system on the evalu- ation set Table 6 : Overall performance on the evaluation set . L is the upper bound of the length of possible chunks in semi-CRFs . Table 1 . Basic Statistics of DUC2007 Update Data Set Figure 2 : Bahktin ’ s characterization of dialogue : Bahktin ( 1986 ) describes a discourse along the three major properties style , situation and topic . Current information retrieval systems focus on the topical as- pect which might be crucial in written documents . Furthermore , since throughout text analysis is still a hard problem , information retrieval has mostly used keywords to characterize topic . Many features that could be extracted are therefore ignored in a tradi- tional keyword based approach . Figure 1 : Information access hierarchy : Oral com- munications take place in very different formats and the first step in the search is to determine the database ( or sub-database ) of the rejoinder . The next step is to find the specific rejoinder . Since re- joinders can be very long the rejoinder has to seg- mented and a segment has to be selected . Table 1 : Distribution of activity types : Both databases contain a lot of discussing , informing and story-telling activities however the meeting data contains a lot more planning and advising . Table 2 : Intercoder agreement for activities : The meeting dialogues and Santa Barbara corpus have been annotated by a semi-naive coder and the first author of the paper . The κ-coefficient is determined as in Carletta et al . ( 1997 ) and mutual information measures how much one label “ informs ” the other ( see Sec . 3 ) . For CallHome Spanish 3 dialogues were coded for activities by two coders and the result seems to indicate that the task was easier . Table 3 : Activity detection : Activities are detected on the Santa Barbara Corpus ( SBC ) and the meet- ing database ( meet ) either without clustering the activities ( all ) or clustering them according to their interactivity ( interactive ) ( see Sec . 2 for details ) . Table 4 : TV show types : The distribution of show types in a large database of TV shows ( 1067 shows ) that has been recorded over the period of a couple of months until April 2000 in Pittsburgh , PA Figure 3 : Detection accuracy summary : The detec- tion of high-level genre as exemplified by the differ- entiation of corpora can be done with high accuracy using simple features ( Ries , 1999 ) . Similar it was fairly easy to discriminate between male and female speakers on Switchboard ( Ries , 1999 ) . Discrimi- nating between sub-genre such as TV-show types ( Sec . 4 ) can be done with reasonable accuracy . How- ever it is a lot harder to discriminate between ac- tivities within one conversation for personal phone calls ( CallHome ) ( Ries et al. , 2000 ) or for general rejoinders ( Santa ) and meetings ( Sec . 2 ) . Table 5 : Show type detection : Using the neural net- work described in Sec . 2 the show type was detected . If there is a number in the word column the word feature is being used . The number indicates how many word/part of speech pairs are in the vocabu- lary additionally to the parts of speech . Table 1 : Cross-domain B3 ( Bagga and Baldwin , 1998 ) results for Reconcile with its general feature set . The Paired Permutation test ( Pesarin , 2001 ) was used for statistical significance testing and gray cells represent results that are not significantly different from the best result . Table 2 : B3 results for baselines and lexicalized feature sets across four domains . Table 3 : B3 results for baselines and lexicalized feature sets on the broad-coverage ACE 2004 data set . Table 4 : Cross-domain B3 and MUC results for Reconcile and Sieve with lexical features . Gray cells represent results that are not significantly different from the best results in the column at the 0.05 p-level . Figure 1 : System Architecture . Figure 2 : Automatic Evaluations . Figure 3 : Biography Text Evaluations . Figure 4 : Weather Text Evaluations . Figure 6 : Weather Sentence Evaluations . Figure 5 : Biography Sentence Evaluations . Figure 2 : Structure of a three-pass machine translation system with the new regeneration pass . The original N-best translations list ( N- best1 ) is expanded to generate a new N-best translations list ( N-best2 ) before the rescoring pass . Figure 1 : Structure of a typical two-pass ma- chine translation system . N-best translations are generated by the decoder and the 1-best transla- tion is returned after rescored with additional feature functions . Figure 3 : Example of original hypotheses and 3- grams collected from them . Figure 5 : New generated hypotheses through n- gram expansion and one reference . Figure 4 : Expanding a partial hypothesis via a matching n-gram . Figure 6 : Example of creating a confusion net- work from the word alignments , and new hy- potheses generated through the confusion net- work . The sentence in bold is the alignment ref- erence . Table 2 : Statistics of training , development and test data for NIST task . Table 1 : Statistics of training , development and test data for IWSLT task . Table 5 : Translations output by system RESC2 and COMB on IWSLT task ( case-insensitive ) . Table 4 : Translation performances ( BLEU % and NIST scores ) of NIST task : decoder ( 1-best ) , rescoring on original 2,400 N-best ( RESC1 ) and 4,000 N-best hypotheses ( RESC2 ) , re-decoding ( RD ) , n-gram expansion ( NE ) , confusion network ( CN ) and combination of all hypotheses ( COMB ) . Table 7 : Number of translations generated by each method in the final translation output of system COMB : decoder ( Orig . ) , re-decoding ( RD ) , n-gram expansion ( NE ) and confusion network ( CN ) . “ Tot. ” is the size of the dev/test set . Figure 1 : Similarity graph after its sparsification Table 1 : Data about our evaluation corpora Table 3 : Evaluation of topic segmentation for the French corpus ( Pk and WD as percentages ) Table 2 : Evaluation of topic identification Table 4 : Evaluation of topic segmentation for the English corpus ( Pk and WD as percentages ) Table 1 . Counts of different ( mis ) spellings of Albert Einstein ’ s name in a web query log . Figure 2 . Modified Viterbi search – stop-word treatment Figure 1 . Example of trellis of the modified Viterbi search Figure 3 . Accuracy and recall as functions of the number of monthly query logs used to train the language model Table 3 . Accuracy of the proposed system on a set which contains misspelled queries that the users had reformulated Table 2 . Accuracy of various instantiations of the system Figure 1 : A graphical representation of the HMM ap- proach for speaker role labeling . This is a simple first order HMM . Table 1 : Automatic role labeling results ( % ) using the HMM and Maxent classifiers . Table 2 : Impact of role sequence information on the HMM and Maxent classifiers . The combination results of the HMM and Maxent are also provided . Figure 1 : Plate diagram of the basic model with a single feature per token ( the observed variable f ) . M , Z , and nj are the number of word types , syntactic classes z , and features ( = tokens ) per word type , respectively . Figure 2 : Plate diagram of the extended model with T kinds of token-level features ( f ( t ) variables ) and a single kind of type-level feature ( morphology , m ) . Table 3 : V-measure ( VM ) and many-to-one ( M-1 ) results on the languages in the MULTEXT-East corpus using the gold standard number of classes shown in Table 4 . BASE results use ±1-word context features alone or with morphology . ALIGNMENTS adds alignment features , reporting the average score across all possible choices of paired language and the scores under the best performing paired language ( in parens ) , alone or with morphology features . Table 4 : Final results on 25 corpora in 20 languages , with the number of induced classes equal to the number of gold standard tags in all cases . k-means and SVD2 models could not produce a clustering in the Czech CoNLL corpus due its size . Best published results are from ∗ Christodoulopoulos et al . ( 2010 ) , † Berg-Kirkpatrick et al . ( 2010 ) and ‡ Lee et al . ( 2010 ) . The latter two papers do not report VM scores . No best published results are shown for the MULTEXT languages ; Christodoulopoulos et al . ( 2010 ) report results based on 45 tags suggesting that clark performs best on these corpora . Figure 1 : Examples of sentences x , domain-independent underspecified logical forms l0 , fully specified logical forms y , and answers a drawn from the Freebase domain . Figure 2 : A sample CCG parse . Figure 4 : Definition of the operations used to transform the structure of the underspecified logical form l0 to match the ontology O . The function type ( c ) calculates a constant c ’ s type . The function freev ( lf ) returns the set of variables that are free in lf ( not bound by a lambda term or quantifier ) . The function subexps ( lf ) generates the set of all subexpressions of the lambda calculus expression lf . Figure 3 : Example derivation for the query ‘ how many people visit the public library of new york annu- ally. ’ Underspecified constants are labelled with the words from the query that they are associated with for readability . Constants from O , written in typeset , are introduced in step ( c ) . Figure 5 : Parameter estimation from Q/A pairs . Figure 6 : Results on the FQ dataset . Figure 9 : Example error cases , with associated frequencies , illustrating system output and gold standard references . 5 % of the cases were miscellaneous or otherwise difficult to categorize . Figure 3 : Morphological Analysis/Generation as a Relation between Analyses and Words Figure 1 : Aymara : utamankapxasamachiwa = ” it appears that they are in your house ” Figure 2 : Inuktitut : Parimunngaujumaniralauqsimanngittunga = “ I never said I wanted to go to Paris ” Figure 5 : Multiple Analyses for suis Figure 4 : Compilation of a Regular Expression into an fst that Maps between Two Regular Languages Figure 6 : Creation of a Lexical Transducer Figure 7 : A Path in a Transducer for English Figure 9 : After the Application of Compile- Replace Figure 10 : Networks Illustrating Steps 2 and 3 of the Compile-Replace Algorithm Figure 8 : A Network with a Regular-Expression Substring on the Lower Side Figure 11 : Two Paths in the Initial Malay Transducer Defined via Concatenation Figure 12 : The Malay fst After the Application of Compile-Replace to the Lower-Side Language Figure 13 : A Template Network and Two Filler Networks Figure 14 : Intermediate Result . Figure 17 : After Applying Compile-Replace to the Lower Side Figure 1 : A translation forest which is the running example throughout this paper . The reference translation is “ the gunman was killed by the police ” . ( 1 ) Solid hyperedges denote a “ reference ” derivation tree t1 which exactly yields the reference translation . ( 2 ) Replacing e3 in t1 with e4 results a competing non-reference derivation t2 , which fails to swap the order of X3,4 . ( 3 ) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally , this is done by deleting a node X0,1 . Figure 2 : Lexicalize and generalize operators over t1 ( part ) in Figure 1 . Although here only shows the nodes , we also need to change relative edges actually . ( 1 ) Applying lexicalize operator on the non-terminal node X0,1 in ( a ) results a new derivation shown in ( b ) . ( 2 ) When visiting bei in ( b ) , the generalize operator changes the derivation into ( c ) . Table 1 : Corpus statistics of Chinese side , where Sent. , Avg. , Lon. , and Len . are short for sentence , longest , average , and length respectively . RT RAIN denotes the reachable ( given rule table without added rules ) subset of T RAIN data . Table 2 : Effect of our method comparing with MERT and perceptron in terms of B LEU . We also compare our fast generation method with different data ( only reachable or full data ) . # Data is the size of data for training the feature weights . * means significantly ( Koehn , 2004 ) better than MERT ( p < 0.01 ) . Table 2 : good # a # 15 gloss and examples . Table 1 : good # a # 15 SentiWN scores . Table 3 : Jumping POS in WordNet . Table 4 : Total number of synsets classified by sentiment . Table 5 : Results for Positive and Negative Classes . Figure 1 : Accuracy Trends on MicroWnOp Corpus . Table 6 : Measuring Accuracy . Figure 1 : Tradeoff between Margin Threshold and name recognition performance Table 2 Accuracy ( % ) of ‘ obscure ’ name recognition Table 3 Coreference factors for name recognition Table 8 Results for Mutiple Document System with additional retrieved texts Table 7 Results for Mutiple Document System Table 6 Results for Single Document System Table 5 Results with Coref Rules Alone Table 9 . Comparison with voted cache Figure 2 : Logical form graphs aligned with sur- face forms in two languages . Figure 1 : Logical form graph . Figure 3 : Encoding local word order . Figure 4 : Graph for a neo-Davidsonian structure . Figure 5 : DRS and corresponding DRG ( in tuples and in graph format ) for “ A customer did not pay . ” Figure 6 : From DRS to DRG : labelling . Figure 7 : Word-aligned DRG for “ A customer did not pay. ” All alignment information ( including surface tuples ) is highlighted . Figure 8 : Surface composition of embedded structures . Figure 9 : Word-aligned DRG for the sentence “ Michelle thinks that Obama smokes . ” Figure 10 : Analysis of NP coordination , in a distributive ( left ) and a collective interpretation ( right ) . Table 1 : Average ratings and Pearson correlation for rules from the personal stories corpus . Lower ratings are better ; see Fig . 2 . Figure 2 : Instructions for judging of unsharpened factoids . Figure 1 : Ungrammatical Arabic output of Google Trans- late for the English input The car goes quickly . The subject should agree with the verb in both gender and number , but the verb has masculine inflection . For clarity , the Arabic tokens are arranged left-to-right . Figure 2 : Segmentation and tagging of the Arabic token AîEñJ . JºJð ‘ and they will write it ’ . This token has four seg- ments with conflicting grammatical features . For example , the number feature is singular for the pronominal object and plural for the verb . Our model segments the raw to- ken , tags each segment with a morpho-syntactic class ( e.g. , “ Pron+Fem+Sg ” ) , and then scores the class sequences . Figure 3 : Notation used in this paper . The convention eIi indicates a subsequence of a length I sequence . Figure 4 : Breadth-first beam search algorithm of Och and Ney ( 2004 ) . Typically , a hypothesis stack H is maintained for each unique source coverage set . Figure 5 : Procedure for scoring agreement for each hy- pothesis generated during the search algorithm of Fig . 4 . In the extended hypothesis eI1 , the index n + 1 indicates the start of the new attachment . Table 1 : Intrinsic evaluation accuracy [ % ] ( development set ) for Arabic segmentation and tagging . Table 2 : Translation quality results ( BLEU-4 [ % ] ) for newswire ( nw ) sets . Avg is the weighted averaged ( by number of sentences ) of the individual test set gains . All improvements are statistically significant at p ≤ 0.01 . Table 2 : Weights learned for discount features . Nega- tive weights indicate bonuses ; positive weights indicate penalties . Table 1 : Adding new features with MIRA significantly improves translation accuracy . Scores are case-insensitive IBM B scores . ∗ or ∗∗ = significantly better than MERT baseline ( p < 0.05 or 0.01 , respectively ) . Table 3 : Weights learned for inserting target English words with rules that lack Chinese words . Table 4 : Weights learned for employing rules whose En- glish sides are rooted at particular syntactic categories . Table 5 : Weights learned for generating syntactic nodes of various types anywhere in the English translation . Figure 2 : Using over 10,000 word-context features leads to overfitting , but its detrimental effects are modest . Scores on the tuning set were obtained from the 1-best output of the online learning algorithm , whereas scores on the test set were obtained using averaged weights . Table 6 : Weights learned for word-context features , which fire when English word e is generated aligned to Chinese word f , with Chinese word f−1 to the left or f+1 to the right . Glosses for Chinese words are not part of features . Figure 1 : Improved syntax-based translations due to MIRA-trained weights . Table 1 : Features based on the token string Table 2 : Sources of Dictionaries Table 5 : Comparison of results for MUC-6 Table 3 : F-measure after successive addition of each global feature group Figure 1 : Comparison of a confusion network and a lat- tice . Figure 2 : An example of alignment units Figure 3 : Different cases of null insertion Figure 4 : A toy instance of lattice construction Table 1 : Results on the MT02 and MT05 test sets Table 2 : Results on the MT06 and MT08 test sets Table 4 : Effect of dictionary scale Table 3 : A real translation example Table 5 : Effect of semantic alignments Figure 1 : Distribution of Class Labels in the WSJ Section of the Penn TreeBank . Table 2 : Accuracies ( % ) for Different Context Types and Sizes Table 1 : Accuracies ( % ) for Word-Extraction Us- Litkowski and Hargraves ( 2007 ) selected exam- ing MALT Parser or Heuristics . ples based on a search for governors8 , most anno- Table 3 : Accuracies ( % ) for Leave-One- Out ( LOO ) and Only-One Word-Extraction-Rule Evaluation . none includes all words and serves for comparison . Important words reduce accuracy for LOO , but rank high when used as only rule . Table 4 : Accuracies ( % ) for Coarse and Fine-Grained PSD , Using MALT and Heuristics . Sorted by preposition . Table 5 : Precision , Recall and F1 Results ( % ) for Coarse-Grained Classification . Comparison to O ’ Hara and Wiebe ( 2009 ) . Classes ordered by frequency Figure 1 : Graphical model representing M L SLDA . Shaded nodes represent observations , plates denote repli- cation , and lines show probabilistic dependencies . Figure 2 : Two methods for constructing multilingual distributions over words . On the left , paths to the German word “ wunsch ” in GermaNet are shown . On the right , paths to the English word “ room ” are shown . Both English and German words are shown ; some internal nodes in GermaNet have been omitted for space ( represented by dashed lines ) . Note that different senses are denoted by different internal paths , and that internal paths are distinct from the per-language expression . Figure 4 : Topics , along with associated regression coefficient η from a learned 25-topic model on German-English ( left ) and German-Chinese ( right ) documents . Notice that theme-related topics have regression parameter near zero , topics discussing the number of pages have negative regression parameters , topics with “ good , ” “ great , ” “ hǎo ” ( good ) and “ überzeugt ” ( convinced ) have positive regression parameters . For the German-Chinese corpus , note the presence of “ gut ” ( good ) in one of the negative sentiment topics , showing the difficulty of learning collocations . Figure 1 Illustration of the paraphrase degree calculation . Figure 2 Illustration of the coordinate degree calculation . Table 5 Another example of some discovered paraphrases . Table 3 Performance of our method for paraphrase acquisition . Table 4 An example of some discovered paraphrases . Table 2 : Feature templates for POS tagging . wi is the ith word in the sentence , ti is its POS tag . For a word w , cj ( w ) is its j th character , c−j ( w ) is the last j th character , and l ( w ) is its length . Figure 1 : Parse tree binarization Figure 2 : Unary rule normalization . Nonterminal-yield unary chains are collapsed to single unary rules . Identity unary rules are added to spans that have no unary rule . Table 3 : Feature templates for parsing , where X can be word , first and last character of word , first and last character bigram of word , POS tag . Xl+a /Xr−a denotes the first/last ath X in the span , while Xl−a /Xr+a denotes the ath X left/right to span . Xm is the first X of right child , and Xm−1 is the last X of the left child . len , lenl , lenr denote the length of the span , left child and right child respectively . wl is the length of word . ROOT/LEAF means the template can only generate the features for the root/initial span . Table 4 : Complexity Analysis of Algorithm 1 . Figure 3 : Boundary information is added to states to cal- culate the bracket scores in the face of word segmentation errors . Left : the original parse tree , Right : the converted parse tree . The numbers in the brackets are the indices of the character boundaries based on word segmentation . Table 5 : Training , development , and test data of CTB 5 . Table 8 : Results for the joint word segmentation and POS tagging task . Table 7 : Word segmentation results . Table 6 : Parameters used in our system . Table 10 : Results for the joint segmentation , tagging , and parsing task using pipeline and joint models . Table 11 : POS tagging error patterns . # means the error number of the corresponding pattern made by the pipeline tagging model . ↓ and ↑ mean the error number reduced or increased by the joint model . Table 1 : Summary of the results obtained by our algorithm in comparison to Word 2007 Table 5 : Results of the fill-in-the-blank exercise Table 4 : Results of the inflection exercise Table 3 : Solution of the multiple choice exercise Table 2 : Replication of the experiment with a corpus of non-native speakers ( CEDEL2 , Lozano , 2009 ) Table 5 . The number of OAS ( types ) , CAS ( types ) , LUW ( types ) and EIW ( types ) for our CWS . Table 4 . The differences of F-measure and ROOV between near-by steps of our CWS . Table 2 . The scored results of our CWS in the MSR_C track ( OOV is 0.034 ) for 3rd bakeoff . Table 3 . The F-measure improvement between the BMM-based CWS and it with WSM in the MSR_C track ( OOV is 0.034 ) using a , b , and c system dictionary . Figure 1 : Subparts and features Figure 3 : extracts from the Akkadian project Table 1 : Performance of the statistical approach using a trigram model based on Google Web1T . Table 2 : Influence of the n-gram model on the perfor- mance of the statistical approach . Table 3 : Performance of the knowledge-based ap- proach using the JiangConrath semantic relatedness measure . Table 4 : Performance of knowledge-based approach using different relatedness measures . Table 5 : Results obtained by a combination of the best statistical and knowledge-based configuration . ‘ Best- Single ’ is the best precision or recall obtained by a sin- gle measure . ‘ Union ’ merges the detections of both approaches . ‘ Intersection ’ only detects an error if both methods agree on a detection . Table 1 : Statistics about the results of our word sense discovery algorithm Table 4 : Average precision of discovered senses for English in relation with WordNet Table 3 : Senses found by our algorithm from first order cooccurrences ( LM-1 and LAT-1 ) Table 1 : Examples of learned pronoun probabilities . Figure 1 : EM input for our example sentence . j-values follow each lexical candidate . Table 4 : Comparison to SVM . Table 3 : Weights set by maximum entropy . Table 2 : Accuracy for all cases , all excluding sen- tences with quotes , and only sentences with quotes . Table 1 : Language families in our data set . The Other category includes 9 language isolates and 21 language family singletons . Table 2 : Average accuracy for EM baseline and model variants across 503 languages . First panel : results on all languages . Second panel : results for 30 isolate and singleton languages . Third panel : results for 27 non-Latin alphabet languages ( Cyril- lic and Greek ) . Standard Deviations across lan- guages are about 2 % . Table 3 : Plurality language families across 20 clusters . The columns indicate portion of lan- guages in the plurality family , number of lan- guages , and entropy over families . Figure 4 : Inferred Dirichlet transition hyperparameters for bigram CLUST on three-way classification task with four latent clusters . Row gives starting state , column gives target state . Size of red blobs are proportional to magnitude of corresponding hyperparameters . Figure 1 : Etrees and Derivation Trees for ( 2abc ) . Table 1 : Data examined by the two systems for the ATB Figure 1 : A correct tree ( tree1 ) and an incorrect tree ( tree2 ) for “ BCLM HNEIM ” , indexed by terminal boundaries . Erroneous nodes in the parse hypothesis are marked in italics . Missing nodes from the hypothesis are marked in bold . Figure 2 : The morphological segmentation possibilities of BCLM HNEIM . Double-circles are word boundaries . Figure 1 : Coarse overview : From multilingual in- put to typed relations and instances Figure 2 : Some examples for MEDLINE tagset : Number of lex . entries per tag and sample words . Table 2 : Translation results for English-German Table 1 : Translation results for German-English Table 4 : Translation results for French-English Table 3 : Translation results for English-French Figure 1 : Plan to the experiments described in this paper Figure 2 : A multiword expression in HeiST Table 1 : Filtering out objective phrases Table 2 : HeiST baseline , cross-lingual projection , SVM . Table 3 : Comparison figures on subsets of the Stanford Sentiment Treebank Table 4 : Lexicon-based phrase labeling Table 5 : Incorporating additional information Table 6 : Rule types in SSTb and HeiST Table 7 : Precision of rules with non-neutral parent label ( ID : daughters and parent have identical labels ) Table 1 : BLEU scores of English to Russian ma- chine translation system evaluated on tst2012 us- ing baseline GIZA++ alignment and translitera- tion augmented-GIZA++ . OOV-TI presents the score of the system trained using TA-GIZA++ af- ter transliterating OOVs Table 2 : BLEU scores of English to Russian ma- chine translation system evaluated on tst2012 and tst2013 using baseline GIZA++ alignment and transliteration augmented-GIZA++ alignment and post-processed the output by transliterating OOVs . Human evaluation in WMT13 is performed on TA-GIZA++ tested on tst2013 ( marked with * ) Table 3 : Russian to English machine translation system evaluated on tst2012 and tst2013 . Human evaluation in WMT13 is performed on the system trained using the original corpus with TA-GIZA++ for alignment ( marked with * ) Figure 1 : This log-log plot shows that there are many rare features and few common features . The probability that a feature occurs in x number of N- best lists behaves according to the power-law x−α , where α = 2.28 . Table 1 : Feature growth rate : For N-best list i in the table , we have ( # NewFt = number of new fea- tures introduced since N-best i − 1 ) ; ( # SoFar = Total number of features defined so far ) ; and ( # Ac- tive = number of active features for N-best i ) . E.g. , we extracted 7535 new features from N-best 2 ; combined with the 3900 from N-best 1 , the total features so far is 11435 . Figure 2 : BLEU difference of 1000 bootstrap sam- ples . 95 % confidence interval is [ .15 , .90 ] The proposed approach therefore seems to be a stable method . Table 2 : Results for different feature sets , with corresponding feature size and train/test BLEU/PER . All multitask features give statistically significant improvements over the baselines ( boldfaced ) , e.g . Shared Subspace : 29.1 BLEU vs Baseline : 28.6 BLEU . Combinations of multitask features with high frequency features also give significant improvements over the high frequency features alone . Table 1 : TreeTagger and RFTagger outputs . Starred word forms are modified during preprocessing . Figure 1 : Sequence of units built from surface word forms ( top ) and POS-tags ( bottom ) . Table 3 : Results for French-English Table 2 : Results for German-English Figure 1 : Example dependency tree . Table 1 : Example coreferent paths : Italicized entities generally corefer . Table 2 : Example non-coreferent paths : Italicized entities do not generally corefer Table 3 : Gender classification performance ( % ) Table 4 : Example gender/number probability ( % ) Figure 2 : ANC pronoun resolution accuracy for varying SVM-thresholds . Table 5 : Resolution accuracy ( % ) Figure 1 : A Dictionary based Word Graph Table 2 : Synthetic Data Set from Xinhua News Table 4 : Quantitative Evaluation of Common Topic Finding ( “ cross-collection ” log-likelihood ) Table 3 : Effectiveness of Extracting Common Topics Table 5 : Effectiveness of Latent Topic Extraction from Multi-Language Corpus Table 2 : Misspellings of receive Table 1 : Classification of corpus token by type Table 3 : Context-sensitive spelling correction ( * denotes also using 60 % WSJ , 5 % corrupted ) Table 4 : Memory-based learner results Table 8 : Synonyms for chain Table 7 : Synonyms for home Table 5 : Average I NV R for 300 headwords Table 6 : InvR scores ranked by difference , Giga- word to Web Corpus Table 2 : Additional features designed to improve model of long-range reordering . Table 4 : Effect of different sets of reference translations used during tuning . Table 3 : Effect of discriminatively learned penalties for OOV words . Table 5 : Effect of supplementing recasing model training data with the test set source . Figure 1 : Computation of probabilities using the language model . Figure 2 : During training , a classified instance ( in this case for the confusible pair { then , than } ) are generated from a sentence . During testing , a similar instance is generated . The classifier decides what the corresponding class , and hence , which word should be the focus word . Table 1 : This table shows the performance achieved by the different systems , shown in accuracy ( % ) . The Number of cases denotes the number of instances in the testset . Table 1 : The chunking results for the six systems associated with the project ( shared task CoNLL- 2000 ) . The baseline results have been obtained by selecting the most frequent chunk tag associ- ated with each part-of-speech tag . The best results at CoNLL-2000 were obtained by Support Vector Machines . A majority vote of the six LCG sys- tems does not perform much worse than this best result . A majority vote of the five best systems outperforms the best result slightly ( error re- duction ) . Table 2 : The NP chunking results for six sys- tems associated with the project . The baseline results have been obtained by selecting the most frequent chunk tag associated with each part-of- speech tag . The best results for this task have been obtained with a combination of seven learn- ers , five of which were operated by project mem- bers . The combination of these five performances is not far off these best results . Table 3 : The results for three systems associ- ated with the project for the NP bracketing task , the shared task at CoNLL-99 . The baseline re- sults have been obtained by finding NP chunks in the text with an algorithm which selects the most frequent chunk tag associated with each part-of- speech tag . The best results at CoNLL-99 was obtained with a bottom-up memory-based learner . An improved version of that system ( MBL ) deliv- ered the best project result . The MDL results have been obtained on a different data set and therefore combination of the three systems was not feasible . Figure 1 : Part of a sample headline cluster , with aligned paraphrases Table 1 : Examples of generated paraphrased head- lines Figure 2 : Correlations between human judge- ments and automatic evaluation metrics for vari- ous edit distances Table 3 : Automatic evaluation and sentence Levenshtein scores Table 1 : Stemmed results on 3,138-utterance test set . Asterisked results are significantly better than the baseline ( p ≤ 0.05 ) using 1,000 iterations of paired bootstrap re-sampling ( Koehn , 2004 ) . Figure 1 : Rank trajectories of 4 LDA inferred topics , with incremental topic inference . The x-axis indicates the utterance number . The y-axis indicates a topic ’ s rank at each utterance . Figure 1 : Derivational entropy of Gq and cross- entropies for three different corpora . Figure 1 : A Sample Network Figure 2 : MUC-7 : Level Distribution of the Facts Combined Figure 3 : MUC-7 : Level Distribution of Each of the Facts Figure 4 : MUC-7 : Level Distribution of Each of the Facts Figure 5 : MUC-7 : Level Distribution of Each of the Facts Figure 6 : MUC-7 : Level Distribution of Each of the Facts Figure 8 : MUC-4 : Level Distribution of the Five Facts Combined Figure 7 : MUC-4 : Level Distribution of Each of the Five Facts Figure 10 : MUC-5 : Level Distribution of the Five Facts Combined Figure 9 : MUC-5 : Level Distribution of Each of the Five Facts Figure 11 : MUC-6 : Level Distribution of Each of the Six Facts Figure 13 : Domain Numbers of MUC-4 , MUC-5 , MUC-6 , and MUC-7 Figure 12 : MUC-6 : Level Distribution of the Six Facts Combined Table 1 : German–English translation results . Results are cumulative . Table 2 : English–German translation results , results are cumulative except for the three alternative PAL- configurations . Figure 1 : Examples of parallel phrases used in word alignment . Table 3 : Summary of devtest results and shared task test results for submitted systems and LIU baseline with hier- archical reordering . Table 1 . Types of message speech acts in corpus . Figure 1 . Categories of Message Speech Act . Table 2 . Thread length distribution . Table 3 . Frequency of speech acts . Table 4 . Gold standard length distribution . Table 5 . Sample poster scores . Table 7 . System Performance Comparison . Table 6 . SA strength scores . Figure 1 : Example queries for abbreviation “ BSA ” Table 1 : Properties of abbreviations corpus retrieved from Medline Table 2 : Performance of WSD system using various combinations of learning algorithms and features . Table 3 : Performance of WSD system over individual ab- breviations in three reduced corpora Table 1 confirms that names participating in re- Table 6 Baseline + Word Clustering by Relation + Re-ranking by Coreference + Re-ranking by Relation Table 5 Baseline + Word Clustering by Relation + Re-ranking by Coreference Figure 1 : Plate diagram representation of the model . ti - s , wi -s and si -s denote the tags , words and segmentations respectively . G-s are various DP-s in the model , Ej -s and βj -s are the tag-specific emission distributions and their respective Dirichlet prior parameters . H is Gamma base distribution . S is the base distribution over segments . Coupled DP concetrations parameters have been omitted for clarity . Figure 2 : Log-likelihood of samples plotted against iter- ations . Dark lines show the average over five runs , grey lines in the back show the real samples . Figure 3 : Tagging part of log-likelihood plotted against V-measure Table 2 : Segmentation results on different languages . Results are calculated based on word types . For each language we report precision , recall and F1 measure , number of word types in the corpus and number of word types with gold standard segmentation available . For each language we report the segmentation result without and with emission likelihood scaling ( without LLS and with LLS respectively ) . Table 1 : Tagging results for different languages . For each language we report median one-to-one ( 1-1 ) , many-to-one ( m-1 ) and V-measure ( V-m ) together with standard deviation from five runs where median is taken over V-measure . Types is the number of word types in each corpus , True is the number of gold tags and Induced reports the median number of tags induced by the model together with standard deviation . Best Pub . lists the best published results so far ( also 1-1 , m-1 and V-m ) in ( Christodoulopoulos et al. , 2011 ) ∗ , ( Blunsom and Cohn , 2011 ) ? and ( Lee et al. , 2010 ) † . Table 3 : Tagging and segmentation results on Estonian Multext-East corpus ( Learned seg and Learned tag ) com- pared to the semisupervised setting where segmentations are fixed to gold standard ( Fixed seg ) and tags are fixed to gold standard ( Fixed tag ) . Finally the segmentatation results from Morfessor system for comparison are pre- sented . Table 1 : Gibbs sampling algorithm for IBM Model 1 ( im- plemented in the accompanying software ) . Table 4 : Sizes of bilingual dictionaries induced by differ- ent alignment methods . Table 3 : Distribution of inferred alignment fertilities . The four blocks of rows from top to bottom correspond to ( in order ) the total number of source tokens , source tokens with fertilities in the range 4–7 , source tokens with fertil- ities higher than 7 , and the maximum observed fertility . The first language listed is the source in alignment ( Sec- tion 2 ) . Table 2 : BLEU scores in translation experiments . E : En- glish , T : Turkish , C : Czech , A : Arabic . Table 1 : Number of Sentences for bilingual training , de- velopment and test and monolingual forum data sets Table 2 : Few examples of the untranslatable tokens in forum posts Table 3 : Evaluation results for all combinations of mixture adapted language and translation models : Baseline ( bl ) scores are italicized , best scores are in bold Figure 2 : Bayesian network : α and β are vectors of hy- perparameters , and θ i ( for i ∈ { 1 , . . . , nc } ) and φ are distributions . u is a vector of underlying forms , generated from φ , and si ( for i ∈ nu ) is a set of observed surface forms generated from the hidden variable ui according to θi Figure 1 : Sample dataset ( constructed by hand ) : Finnish verbs , with inflection for person and number . Figure 4 : Posterior likelihood at each of the first 100 iter- ations , from 4 runs ( with different random seeds ) on 10 % of the Morphochallenge dataset ( αi6=j = 0.001 , αi=j = 100 , β = 0.1 ) , indicating convergence within the first 15 iterations . Figure 7 : Accuracy of underlying segment hypotheses . Figure 5 : Resampling probabilities for alternations , after 1000 iterations . Figure 1 Naı̈ve FSA with duplicated paths . Figure 4 4-tape representation for the Hebrew word htpqdut . Figure 8 FSRA-2 for Arabic nominative definite and indefinite nouns . Figure 12 Interdigitation FSRA – general . Figure 14 Reduplication for n = 4 . Table 2 Time comparison between FSAs and FSRAs . Table 1 Space comparison between FSAs and FSRAs . Table 2 : NIL expression forms based on POS attribute . Table 1 : NIL expression forms based on word formation . Figure 1 : Workflow for NIL knowledge engineering component . NILE refers to NIL expression , which is identified and annotated by human annotator . Figure 2 : Architecture of NILER system . Figure 3 : Smoothed precision curves over the five corpora . Table 3 : Experimental results for the two methods on the five corpora . PRE denotes precision , REC denotes recall , and F1 denotes F1-Measure . Figure 4 : Smoothed recall curves over the five corpora . Figure 7 : Smoothed quality curves for SVM method over the five corpora . Figure 6 : Smoothed quality curves for PM method over the five corpora . Figure 5 : Smoothed F1-Measure curves over the five corpora . Table 1 : Optimized TERp Edit Costs Table 2 : Optimization & Test Set Pearson Correlation Results Table 4 : Average Metric Rank in NIST Metrics MATR 2008 Official Results Table 3 : MT06 Dev . Optimization & Test Set Spearman Correlation Results Table 6 : Optimized Edit Costs Table 5 : Results on the NIST MATR 2008 test set for several variations of paraphrase usage . Figure 2 : Maximally Accurate Assignment Figure 2 : Word prediction from a partial parse Figure 1 : Dependency structure of a sentence . Figure 3 : A conceptual gure of the lexicalization Figure 4 : Relation between cross entropy and pars- Table 2 : Cross entorpy and accuracy of each model . Figure 3 : Number of feedback items per speaker Figure 1 : Distribution of isolated vs. initial posi- tion for the most frequent lexical items Figure 2 : Duration ( in seconds ) of each lexical type Figure 4 : Distribution of the lexical items Figure 5 : Dendrogram of the participants cluster based on their feedback profile Figure 1 : The Ensemble Semantics framework for information extraction . Table 1 : Feature space describing each candidate instance ( S indicates the set of seeds for a given class ) . Table 3 : Average precision ( AP ) and coverage ( Cov ) results for our proposed system ES-all and the baselines . ‡ indicates AP statistical signifi- cance at the 0.95 level wrt all baselines . Table 2 : Number of extracted instances and the sample sizes ( P and N indicate positive and neg- ative annotations ) . Table 4 : Overall AP results of the different feature configurations , compared to two baselines . † in- dicates statistical significance at the 0.95 level wrt B3 . ‡ indicates statistical significance at 0.95 level wrt both B3 and B4 . Figure 2 : Precision at rank for the different sys- tems on the Athletes class . Table 5 : Ablation study of the web ( w ) , query- log ( q ) and table ( t ) features ( bold letters indicate whole feature families ) . Table 6 : Listing of all seeds used for KEdis and KEpat , as well as the top-10 entities discovered by ES-all on one of our test folds . Figure 1 : Example consensus network with votes on word arcs . Figure 2 : Three confusion networks with prior prob- abilities . Table 2 : Mixed-case TER and BLEU , and lower- case METEOR scores on Arabic NIST MT05 . Table 1 : Mixed-case TER and BLEU , and lower-case METEOR scores on Arabic NIST MT03+MT04 . Table 4 : Mixed-case TER and BLEU , and lower- case METEOR scores on Chinese NIST MT05 . Table 3 : Mixed-case TER and BLEU , and lower-case METEOR scores on Chinese NIST MT03+MT04 . Table 1 : BLEU scores achieved with different sets of parallel corpora . All systems are base- line n-code with POS factor models . The follow- ing shorthands are used to denote corpora , : ” N ” stands for News-Commentary , ” E ” for Europarl , ” C ” for CommonCrawl , ” U ” for UN and ( nf ) for non filtered corpora . Table 2 : BLEU scores for different configuration of factored translation models . The big prefix de- notes experiments with the larger context for n- gram translation models . Table 3 : BLEU scores for pre-ordering experi- ments with a n-code system and the approach pro- posed by ( Neubig et al. , 2012 ) Figure 1 : Histogram of token movement size ver- sus its occurrences performed by the model Neu- big on the source english data . Table 5 : BLEU scores for the French-to-English translation task measured on nt10 with systems tuned on development sets selected according to their original language ( adapted tuning ) . Figure 2 : Perplexity measured on nt08 with the baseline LM ( std ) , with the LM estimated on the sampled texts ( generated texts ) , and with the inter- polation of both . Table 4 : Impact of the use of sampled texts . Table 1 : Results for development and test set for the two languages by ME1 Table 2 : Best results : For English , name lists are used . For German , part-of-speech tags are used Table 1 : Meta-evaluation results at document level Table 2 : Meta-evaluation results at system level Table 3 : Meta-evaluation results at document and system level for submitted metrics Figure 1 : The density of the F1 -scores with the three approaches . The prior used is a symmetric Dirichlet with α = 0.1 . Table 1 : BLEU scores when testing on the com- bined test set ( newstest2012 + PDTB 23 ) ; on PDTB section 23 only ( 2416 sentences , 923 con- nectives ) ; and when randomizing the sense tags ( PDTB 23 random ) , for the BASELINE system and the two systems using PDTB connective labels : SYSTEM 1 : complex labels , SYSTEM 2 : simplified labels . When testing on randomized sense labels ( PDTB 23 random ) , the BLEU scores are statisti- cally significantly lower than the ones on the cor- rectly labeled test set ( PDTB 23 ) , which is indi- cated by starred values . Table 2 : Performance of SYSTEM 2 ( simplified PDTB tags ) when manually counting for improved , equal and degraded translations compared to the BASELINE , in samples from the PDTB section 23 test set . Table 3 : Translation outputs for the EN con- nective as , which was translated more correctly by SYSTEM 2 thanks to the disambiguating sense tags compared to the BASELINE that often just produces the prepositional as – jako . The erro- neous translations are marked in bold . The PDTB sense tags indicate the meaning of the CZ trans- lations and are encoded as follows : Synchrony ( Sy ) , Asynchrony ( Asy ) , Contingency ( Co ) , Cause ( Ca ) . Fig . 1 . An example for grouped entity tuples . Entity tuples in big frame are those suitable for the template X direct Y , whereas entity tuples in small frame are those held the same relation . Fig . 2 . A real-world situation Fig . 3 . An example of the mutual reinforcement between P r ( para ( ti , tj ) ) and P r ( coord ( ek , eg ) ) . Table 2 . Input sentences . Table 3 . Performance of our method for paraphrase acquisition . Table 5 . Another example of some discovered paraphrases . Table 4 . An example of some discovered paraphrases . Table 1 : Size of co-occurrence databases Figure 1 : WSI and WSD Pipeline Table 2 : Results for the submitted runs Figure 1 : Structure of a term in the original documents Figure 2 : Modified query . Table 2 : Results — Evaluation A . Table 3 : BCN x Baseline Table 4 : WCN x Baseline Figure 3 : Topic # 141 Table 6 : Results - Evaluation B Table 5 : Topics with MWEs Table 8 : Ranking for Topic # 141 - CN Table 7 : Ranking for Topic # 141 - Baseline Table 1 : Parallel Corpus . Table 2 : Baseline Results . Table 3 : Bayesian Alignment Results . Table 4 : Development Sets Results . Table 7 : Compound Splitting Results . Table 5 : Tuning Results . Table 6 : Language Model Results . Table 9 : German-to-English Final System Results . Table 11 : German-English Official Test Submis- sion . Table 10 : English to German Final System Re- sults . Table 8 : Average of Weights Results . Table 1 . Data sets used for our alignment quality experiments . The total number of sentences in the respective corpora are given along with the number of sentences and gold-standard ( S ) ure and ( P ) ossible alignment links in the corresponding test set . Table 2 : Results of our alignment quality experiments . All timing and accuracy figures use means from five independently initial- ized runs . Note that lower is better for AER , higher is better for F0.5 . All experiments are run on a system with two Intel Xeon E5645 CPUs running at 2.4 GHz , in total 12 physical ( 24 virtual ) cores . Table 2 shows the result of varying the number of samplers and iterations for all Table 3 . Data used for training SMT models ( all counts in millions ) . Parallel data sets refer to the bitexts aligned to English and their token counts include both languages . Table 5 . Timings from the word alignments for our SMT evaluation . The values are averaged over both alignment directions . For these experiments we used systems with 8-core Intel E5-2670 processors running at 2.6 GHz . Table 4 . Results from our SMT evaluation . The BLEU scores are the maximum over the Moses parameters explored for the given word alignment conﬁguration . Table 1 . Incremental Improvement from Self-training ( English ) Figure 1 . Bootstrapping for Name Tagging Figure 2 . Self-Training for Name Tagging Table 3 . English Name Tagger Table 4 . Chinese Name Tagger Figure 4 . Impact of Data Size ( Chinese ) Table 6 . Impact of Confidence Measures Table 5 . Impact of Data Selection ( Chinese ) Figure 3 . Impact of Data Size ( English ) Figure 1 . Upper triangle of the sentence-similarity matrix . Figure 2 . An example of a hierarchical cluster tree . Figure 3 . The porposed TSHAC algorithm . Figure 1 : An example confusion network construc- tion Figure 2 : An example packed forest representing hy- potheses in Figure 1 ( a ) . Figure 3 : The deductive system for Earley ’ s genera- tion algorithm Table 1 : WMT10 system combination tuning/testing data Table 3 : Oracle lower-case BLEU Table 2 : Translation results in lower-case BLEU . CN for confusion network and CF for confusion forest with different vertical ( v ) and horizontal ( h ) Markovization order . Table 5 : Average min/max hypothesis length pro- ducible by each method ( h = 1 for CF ) . Table 4 : Hypegraph size measured by the average number of hyperedges ( h = 1 for CF ) . “ lattice ” is the average number of edges in the original CN . Figure 1 : Aligned parsed sentence Table 2 : German–English results for hierarchical and syntactic models , in % BLEU Table 1 : Training , tuning , and test conditions Table 4 : Example input and best output found Table 3 : Reachability of 1000 training sentences : can they be translated with the model ? Figure 2 : Source span lengths Figure 5 : Derivation with Hierarchical model Table 6 : English–German results in % BLEU Table 5 : Effect on % BLEU of varying number of non-terminals Figure 6 : Derivation with soft syntax model Figure 9 : Chunk - Length and count of glue rules used decoding test set Figure 8 : Translated chunked sentence Table 4 : Translation results for English→French Table 3 : Translation results for French→English Table 1 : Translation results for German→English Table 2 : Translation results for English→German Table 1 : Statistical Information of Corpora Table 2 : Features Used for Initial Distribution Table 4 : Ordered List of Increased/Decreased Number of Correctly Tagged Words Table 5 : Results of Multiple Trials and Compari- son to Simulated Annealing Table 3 : Results of POS Guessing of Unknown Words Figure 1 : Representation of Bigram Counts Table 2 : Decision Tree and Stump Characteristics Table 1 Proportion of OOV words in some corpora used for real world applications . ( Numbers in parentheses exclude words whose first letters are capitalized because they are likely to refer to named entities . ) Table 2 Examples of positive and negative words . Table 3 Accuracy for SO-PMI with different data set sizes , the spin model , the label propagation model , and the random walks model for 10-fold cross-validation and 14 seeds . Table 4 Accuracy for adjectives only for the spin model , the bootstrap method , and the random walk model . Figure 2 The effect of varying the number of samples ( k ) on accuracy . Figure 1 The effect of varying the maximum number of steps ( m ) on accuracy ( k = 1,000 ) . Figure 3 Accuracy for words with high confidence measure . Figure 4 The effect of varying the number of seeds on accuracy . Table 5 Accuracy for three classes on a general purpose list of 2,000 words . Figure 6 Accuracy of foreign word polarity identification . Figure 7 Accuracy of different methods in predicting OOV words polarity . Figure 8 The effect of varying the number of extracted related words on accuracy . Table 1 : Overview of experiments applying WSMs to determine semantic compositionality of word expressions . BNC - British National Corpus , GR - grammatical relations , GNC - German newspaper corpus , TREC - TREC corpus ; SY - substitutability-based methods , CT - component-based methods , CTn - component-based methods comparing WSM neighbors of expressions and their components , CY - compositionality-based methods ; NVAP c. - noun , verb , adjective , adverb combinations , NN - noun-noun , VP - verb-particles , AN - adjective-noun , VO - verb-object , SV - subject-verb , PV - phrasal-verb , PNV - preposition-noun-verb ; dicts . - dictionaries of idioms , WN - Wordnet , MA - use of manually annotated data , S - Spearman correlation , PC - Pearson correlation , CR - Spearman and Kendall correlations , APD - average point difference , CL - classification , P/R - Precision/Recall , P/Rc - Precision/Recall curves , Fm - F measure , R2 - goodness . Table 2 : A sample of manually annotated expressions from Disco-En-Gold with their numerical scores ( Ns ) and coarse scores ( Cs ) . Table 3 : The values of AP , Spearman ( ρ ) and Kendall ( τ ) correlations between the LSA-based and PMI-based model respectively and the Gold data with regards to the expression type . Every zero value in the table corresponds to the theoretically achieved mean value of correlation calculated from the infinite number of correlation values between the ranking of scores assigned by the annotators and the rankings of scores being obtained by a random number genarator . Reddy-WSM stands for the best performing WSM in the DISCO task ( Reddy et al. , 2011b ) . StatMix stands for the best performing system based upon association measures ( Chakraborty et al. , 2011 ) . Only ρ-All and τ -All are available for the models explored by Reddy et al . ( 2011b ) and Chakraborty et al . ( 2011 ) . Figure 2 : Smoothed graphs depicting the dependency of Precision upon Recall using the LSA and PMI-based models ordering the expressions in TrainValD ( left ) and TestD ( right ) according to their non-compositionality . Figure 1 : Smoothed graphs depicting the dependency of Precision ( left ) and Recall ( right ) upon the nBest selected non-compositional candidates from the ordered list of expressions in TestD created by the LSA and PMI-based models . Figure 2 : Example of a MUC-4 template Table 1 : The top-ranking feature for each group of features and the classifier of a slot Table 6 : Accuracy of string slots with and without full parsing Table 5 : Systems whose F-measures are not signif- icantly different from Alice-ME at the 0.10 signifi- cance level with 0.99 confidence Table 4 : Accuracy of all slots on the TST3 and TST4 test set Table 3 : Accuracy of string slots on the TST3 and TST4 test set Figure 2 : Layers used in our model . Figure 1 : A standard logical form derivation using CCG . The NP↑ notation means that the subject is type-raised , and taking the verb-phrase as an argument—so is an ab- breviation of S/ ( S\NP ) . This is necessary in part to sup- port a correct semantics for quantifiers . Figure 3 : Example initial lexical entries Figure 4 : Using the type model for disambiguation in the derivation of file a suit . Type distributions are shown after the variable declarations . Both suit and the object of file are lexically ambiguous between different types , but after the β -reduction only one interpretation is likely . If the verb were wear , a different interpretation would be preferred . Table 1 : Most probable terms in some clusters induced by the Type Model . Table 2 : Results on wide-coverage Question Answer- ing task . CCG-Distributional ranks question/answer pairs by confidence— @ 250 means we evaluate the top 250 of these . It is not possible to give a recall figure , as the total number of correct answers in the corpus is unknown . Table 4 : Accuracy on Section 1 of the FraCaS suite . Problems are divided into those with one premise sen- tence ( 44 ) and those with multiple premises ( 30 ) . Figure 5 : Example problem from the FraCaS suite . Table 3 : Example questions correctly answered by CCG-Distributional . Table 1 . Comparison with other approaches Figure 1 . F1-measures with  in [ 0 3 ] Figure 2 . F1-measure with  in [ 0,1 ] Figure 1 : Example sentence and extracted features from the SENSEVAL 2 word church Figure 2 : Empirically-derived classifier similarity Figure 3 : Individual Classifier Properties ( cross-validation on SENSEVAL training data ) Table 4 : Accuracy for different EM-weighted probability interpolation models for SENSEVAL 2 Table 1 : Training set characteristics Table 2 : Individual feature type contribution to perfor- mance . Fields marked with £ indicate that the difference in performance was not statistically significant at a level ( paired McNemar test ) . Table 3 : Classifier combination accuracy over 5 base classifiers : NB , BR , TBL , DL , MMVC . Best perform- ing methods are shown in bold . Figure 4 : Individual basic classifiers ’ contribution to the final classifier combination performance . Table 5 : Final Performance ( Frozen Systems ) on SENSEVAL Lexical Sample WSD Test Data Figure 1 : Example of semantic trees Figure 2 : Two STs composing a STN Table 1 : ROUGE-2 measures in k-means learning Table 8 : ROUGE-W in empirical approach Table 2 : ROUGE-W measures in k-means learning Table 5 : ROUGE-W measures in EM learning Table 3 : ROUGE-SU in k-means learning Table 10 : F-measures for different systems Table 9 : ROUGE-SU in empirical approach Table 7 : ROUGE-2 in empirical approach Table 4 : ROUGE-2 measures in EM learning Table 6 : ROUGE-SU measures in EM learning Figure 1 CATiB Annotation example . & ( - ) 23 1+ , 4 ! $ % ./0 tς ml HfydAt AlkA AlðkyAt fy AlmdArs AlHkwmy ( ‘ The writer ’ s smart granddaughters work for public schools ’ ) . The words in the tree are presented in the Arabic reading direction ( from right to left ) . Table 1 Penn Arabic Treebank part 3 v3.1 data split . Table 2 Parsing performance with each POS tag set , on gold and predicted input . L AS = labeled attachment accuracy ( dependency + relation ) . U AS = unlabeled attachment accuracy ( dependency only ) . L S = relation label prediction accuracy . L AS diff = difference between labeled attachment accuracy on gold and predicted input . POS acc = POS tag prediction accuracy . Table 3 Prediction accuracy , value set sizes , descriptions , and value examples of features used in this work . Accuracy was measured over the development set . * = The set includes a “ N/A ” value ( s ) . Table 4 CORE 12 POS tag set with morphological inflectional features . Left half : Using gold POS tag and feature values . In it : Top part ( All ) : Adding all nine inflectional features to CORE 12 . Second part ( Sep ) : Adding each feature separately to CORE 12 . Third part ( Greedy ) : Greedily adding next best feature from Sep , and keeping it if improving score . Right half : Same as left half , but with predicted POS tag and feature values . Statistical significance tested only on predicted ( non-gold ) input , against the CORE 12 baseline . Table 5 Models with lexical morpho-semantic features . Top : Adding all lexical features together on top of the CORE 12 baseline . Center : Adding each feature separately . Bottom : Greedily adding best features from previous part , on predicted input . Statistical significance tested only on predicted ( non-gold ) input , against the CORE 12 baseline . Table 6 Models with inflectional and lexical morphological features together ( predicted value-guided heuristic ) . Statistical significance tested only on predicted input , against the CORE 12 baseline . Table 7 Models with re-engineered DET and PERSON inflectional features . Statistical significance tested only on predicted input , against the CORE 12 baseline . Table 8 Models with functional features : GENDER , NUMBER , rationality ( RAT ) . F N * = functional feature ( s ) based on Alkuhlani and Habash ( 2011 ) ; GN = GENDER + NUMBER ; GNR = GENDER + NUMBER + RAT . Statistical significance tested only for CORE 12+ . . . models on predicted input , against the CORE 12 baseline . Table 9 Select models trained using the Easy-First Parser . Statistical significance tested only for CORE 12. . . models on predicted input : significance of the Easy-First Parser CORE 12 baseline model against its MaltParser counterpart ; and significance of all other CORE 12+ . . . models against the Easy-First Parser CORE 12 baseline model . Table 10 Alternatives to training on gold-only feature values . Top : Select MaltParser CORE 12+ . . . models re-trained on predicted or gold + predicted feature values . Bottom : Similar models to the top half , with the Easy-First Parser . Statistical significance tested only for CORE 12+ . . . models on predicted input : significance of the MaltParser models from the MaltParser CORE 12 baseline model , and significance of the Easy-First Parser models from the Easy-First Parser CORE 12 baseline . 3 . ( 7 $ % 19 : 6 7 ( Figure 2 Error analysis example . . . . ) 82 mrt ÂyAm ς lý ǍxtfA ’ Alzmyl Almhnd . . . ( ‘ Several days have passed since the disappearance of the colleague the engineer . . . ’ ) , as parsed by the baseline system using only CORE 12 ( left ) and as using the best performing model ( right ) . Bad predictions are marked with < < < . . . > > > . The words in the tree are presented in the Arabic reading direction ( from right to left ) . Table 15 Training the MaltParser on gold tags , accuracy by gold attachment type ( selected ) : subject , object , modification ( of a verb or a noun ) by a noun , modification ( of a verb or a noun ) by a preposition , idafa , and overall results ( repeated ) . Table 17 Training the Easy-First Parser on gold and predicted tags , accuracy by gold attachment type ( selected ) : subject , object , modification ( of a verb or a noun ) by a noun , modification ( of a verb or a noun ) by a preposition , idafa , and overall results ( repeated ) . Figure 1 . A corpus of two trees Figure 2 . A derivation for Mary likes Susan Figure 3 . Another derivation yielding same tree Figure 4 . All binary trees for NNS VBD JJ NNS ( Investors suffered heavy losses ) Figure 5 . Some subtrees from trees in figure 4 Table 1 . F-scores of UML-DOP compared to previous models on the same data Table 2 . F-scores of U-DOP , UML-DOP and a supervised treebank PCFG ( ML-PCFG ) for a random 90/10 split of WSJ10 and WSJ40 . Table 1 : Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt . Table 3 : STTS accuracies of the TnT tagger trained on the STTS tagset , the TnT tagger trained on the Tiger tagset , and our tagger trained on the Tiger tagset . Figure 2 : Tagging accuracy on development data depending on context size Table 4 : Tagging accuracies on test data . Figure 1 : The ANNIS user interface , displaying data from the PCC Table 1 : Sources of conflict in cross-lingual subjectivity transfer . Definitions and synonyms of the fourth sense of the noun argument , the fourth sense of verb decide , and the first sense of adjective free as provided by the English and Romanian WordNets ; for Romanian we also provide the manual translation into English . Figure 3 : Macro-accuracy for cross-lingual bootstrapping Figure 4 : F-measure for the objective and subjective classes for cross-lingual bootstrapping Figure 6 : F-measure for the objective and subjective classes for multilingual bootstrapping ( versus cross-lingual framework ) Figure 5 : Macro-accuracy for multilingual bootstrapping ( versus cross-lingual framework ) Table 2 : Sample of subjective features appearing in the top 100 discriminant attributes selected with Information Gain on the 3rd fold training data at iteration 10 . The words in italics in the multilingual features represent equivalent translations in English and Romanian . Table 1 : Training set statistics . Out-Of-Vocabulary ( OOV ) rate is regarding the development sets . Figure 2 : Example training run of a pruned 1st -order model on German showing the fraction of pruned gold se- quences ( = sentences ) during training for training ( train ) and development sets ( dev ) . Table 2 : POS tagging experiments with pruned and unpruned CRFs with different orders n. For every language the training time in minutes ( TT ) and the POS accuracy ( ACC ) are given . * indicates models significantly better than CRF ( first line ) . Table 3 : Accuracies for models with and without oracle pruning . * indicates models significantly worse than the oracle model . Table 7 : Test results for POS+MORPH tagging . Best baseline results are underlined and the overall best results bold . * indicates a significant difference between the best baseline and a PCRF model . Table 6 : Development results for POS+MORPH tagging . Given are training times in minutes ( TT ) and accuracies ( ACC ) . Best baseline results are underlined and the overall best results bold . * indicates a significant difference between the best baseline and a PCRF model . Table 5 : Test results for POS tagging . Best baseline results are underlined and the overall best results bold . * indicates a significant difference between the best baseline and a PCRF model . Table 4 : Development results for POS tagging . Given are training times in minutes ( TT ) and accuracies ( ACC ) . Best baseline results are underlined and the overall best results bold . * indicates a significant difference ( positive or negative ) between the best baseline and a PCRF model . Table 1 : Statistics on the Italian EVALITA 2009 and English CoNLL 2003 corpora . Figure 1 : Three kinds of tree kernels . Figure 2 : Semantic structure of the first sequence Table 2 : Global features in the entity kernel for reranking . These features are anchored for each entity instance and adapted to entity categories . For example , the entity string ( first feature ) of the entity “ United Nations ” with entity type “ ORG ” is “ ORG United Nations ” . Table 3 : Reranking results of the three tagging kernels on the Italian and English testset . Table 1 : Regular expression notation in foma . Figure 1 : Illustration of a worsening filter for morpheme boundaries . Figure 2 : OT grammar for devoicing compiled into an FST . Figure 3 : Violation permutation transducer . Figure 4 : Devoicing transducer compiled through a rule . Figure 5 : Example outputs of matching implementation of Finnish OT . Figure 6 : An non-regular OT approximation . Table 2 : Illustrative tableau for a simple constraint sys- tem not capturable as a regular relation . Figure 1 : Baseline results for human word lists . Data : 700 positive and 700 negative reviews . Figure 2 : Results for baseline using introspection and simple statistics of the data ( including test data ) . Figure 3 : Average three-fold cross-validation accuracies , in percent . Boldface : best performance for a given setting ( row ) . Recall that our baseline results ranged from 50 % to 69 % . Figure 1 : Event descriptions spread across two sentences Table 1 : Counts of matches between MUC and Soderland data . Table 2 : Matches between MUC and Soderland data at field level Table 2 : BLEU scores on the News-Commentary development test data Table 1 : BLEU scores on the Europarl development test data Table 1 : Characteristics of the parallel corpus used for experiments . Figure 1 : The role of the standard Basque ( Batua ) ana- lyzer in filtering out unwanted output candidates created by the induced rule set produced by method 1 . Table 2 : Values obtained for Precision , Recall and F- scores with method 1 by changing the minimum fre- quency of the correspondences to construct rules for foma . The rest of the options are the same in all three experiments : only one rule is applied within a word . Table 3 : Values obtained for Precision , Recall and F- score with method 1 by changing the threshold frequency of the correspondences and applying a post-filter . Table 5 : Experiments with the ILP method using a thresh- old of 1–4 ( times a word-pair is seen ) to trigger rule learn- ing . The figures in parentheses are the same results with the added postprocessing unigram filter that , given sev- eral output candidates of the standard dialect , chooses the most frequent one . Table 4 : Method 1 . Exp1 : frequency 2 ; 2 rules applied ; in parallel ; without contextual conditioning . Exp2 : fre- quency 1 ; 1 rule applied ; with contextual conditioning . Exp3 : frequency 2 ; 2 rules applied ; in parallel ; with con- textual conditioning . Figure 2 : Tradeoffs of precision and recall values in the experiments with method 1 using various different pa- rameters . When the unigram filter is applied the precision is much better , but the recall drops . Table 6 : The best results ( per F1 -score of the two meth- ods ) . The parameters of method 1 included using only those string transformations that occur at least 2 times in the training data , and limiting rule application to a maxi- mum of 2 times within a word , and including a unigram post-filter . Rules were contextually conditioned . For method 2 , all the examples ( threshold 1 ) in the training data were used as positive and negative evidence , with- out a unigram filter . Figure 1 . MT system combination . Each 1-best outputs are aligned to create as many Confusion Networks which are connected together to form a lattice . This lattice is then decoded with a token-pass decoder using a Language Model to produce 1-best and/or n-best hypotheses . Figure 2 . Incremental alignment with TERp resulting in a confusion network . Table 1 . Results of system combination on Dev7 ( development ) corpus and Test09 , the oﬃcial test corpus of IWSLT ’ 09 evaluation campaign . Figure 1 . Corpus Excerpt with Dialogue Act Annotation Figure 2 . Automatically detected posture points ( H = headDepth , M = midTorsoDepth , L = lowerTorsoDepth ) Figure 1 : each line of the 6-best translations and BLEU scores with 1-best translation selected by the current param- eter α Figure 2 : This shows the shapes of BLEU and 1-slack SVM objective function for one parameter . These lines were calculated by 800 development sentences randomly selected from dev06 for development data when the hyperparameter Q is fixed 1000.0 . Table 2 : BLEU scores on the test08 and news08 test data obtained by models trained by MERT and SVM . Figure 3 : Tuninig test for hyperparameter Q of structural SVM ( fixed λ=1.0 ) by increasing it . Table 3 : The average improvements of BLEU scores on the test08 and news08 ( out-of-domain ) when we trained the paramenters using only 400 development sentences with MERT and SVM-based algorithms four times . Table 4 : BLEU scores of two open test sets obtained when training by MERT , S-slack-SVM and 1-slack-SVM using four development sets containing 400 sentences randomly se- lecting from WMT-08 dev2006 . Figure 4 : BLEU scores as a function of development data size . Fig . 1 . The polarity classification ( positive and negative ) based on product aspect framework Table 1 . Result for microblog classification Fig . 2 . Cell phone experiment result ( 17 aspects ) Table 2 Comparison of the news and reports corpora . Table 1 Words with the highest association scores , in decreasing order , for the word “ cigarette ” , as extracted automatically . Fig . 1 . Integration of confidence measures – recall/precision curves ( figures in the legend correspond to resp . δ1 and δ2 ) . Fig . 2 . Integration of paradigmatic relations – recall/precision curves . Table 5 Integration of semantic relations – news corpus – best F1-measures . Fig . 4 . Integration of confidence measures and interpolation – recall/precision curves . Fig . 3 . Interpolation – recall/precision curves . Table 8 Best F1-measure values for all possible combination . Figure 1 : Schematic of our proposed method Table 1 : English MWEs and their components with their translation in Persian . Direct matches between the trans- lation of a MWE and its components are shown in bold ; partial matches are underlined . Table 2 : The 10 best languages for R EDDY using LCS . Table 3 : The 10 best languages for the verb component of BANNARD using LCS . Table 4 : The 10 best languages for the particle compo- nent of BANNARD using LCS . Table 8 : Results for the classification task . S TRING S IM MEAN is our method using Mean for f1 Table 7 : Correlation after combining Reddy et al. ’ s method and our method with Mean for f1 ( S TRING S IM MEAN ) . The correlation using Reddy et al. ’ s method is 0.714 . Figure 1 : Tuple extraction from a sentence pair . Table 2 : Translation results in terms of BLEU score and translation edit rate ( TER ) estimated on newstest2010 with the NIST scoring script . Table 1 : English-French translation results in terms of BLEU score and TER estimated on newstest2010 with the NIST script . All means that the translation model is trained on news-commentary , Europarl , and the whole GigaWord . The rows upper quartile and median corre- spond to the use of a filtered version of the GigaWord . Figure 2 : Architecture of the Structured Output Layer Neural Network language model . Table 3 : Translation results from English to French and English to German measured on newstest2010 using a 100-best rescoring with SOUL LMs of different orders . Figure 1 : CCG derivation and unresolved semantics for the sentence “ I saw nothing suspicious ” Figure 2 : DRS for the sentence “ I saw nothing suspi- cious ” Table 2 : Results of the second run ( with postprocessing ) Table 1 : Results of the first run ( without postprocessing ) Table 3 : Results of negated event/property detection on gold standard cue and scope annotation Figure 1 : The System architecture1 Figure 2 : Learning curves using different sam- pling strategies . Table 1 : Performance comparsion of the rule- based robust semantic parser , the reversed two- stage classification system and our SLU systems ( TER : Topic Error Rate ; SER : Slot Error Rate ; Table 3 : Performance comparison of two SLU systems through weakly supervised and super- vised training on the three test sets ( TER : Topic Error Rate ; SER : Slot Error Rate ) Figure 3 : Learning curves of bootstrapping meth- ods for semantic classification on TS1 . Table 2 : Most frequent phrase dependencies with at least 2 words in one of the phrases ( dependencies in which one phrase is entirely punctuation are not shown ) . $ indicates the root of the tree . Table 1 : Key notation . Table 3 : Most probable child phrases for the parent phrase “ made up ” for each direction , sorted by the con- ditional probability of the child phrase given the parent phrase and direction . Figure 1 : String-to-tree configurations ; each is associated with a feature that counts its occurrences in a derivation . Table 5 : Urdu-English Results ( % BLEU ) . Table 4 : Chinese-English Results ( % BLEU ) . Figure 2 : ( a ) Moses translation output along with γ , φ , and a . An English gloss is shown above the Chinese sentence and above the gloss is shown the dependency parse from the Stanford parser . ( b ) QPDG system output with additional structure τφ . ( c ) reference translations . Table 7 : Average feature values across best translations of sentences in the MT03 tuning set , both before MERT ( column 2 ) and after ( column 3 ) . “ Same ” versions of tree- to-tree configuration features are shown ; the rarer “ swap ” features showed a similar trend . Table 6 : Results when using unsupervised dependency parsers . Cells contain averaged % BLEU on the three test sets and % BLEU on tuning data ( MT03 ) in parentheses . Table 1 : Examples of SMT errors due to MWEs . Figure 1 : Framework for MWE acquisition from corpora Table 2 : MWE acquisition applied to lexicography Table 3 : Evaluation of translation of phrasal verbs in test set . Figure 1 : Network after pair-wise TER alignment . Figure 2 : Network after incremental TER alignment . Table 2 : Results on the Arabic GALE Phase 2 evaluation set with one reference translation . Table 1 : Results on the Arabic GALE Phase 2 system combination tuning set with four reference translations . Table 3 : NIST BLEU scores on the German-English ( de- en ) and French-English ( fr-en ) Europarl test2008 set . Figure 1 : Proposed discourse structures for Ex . 4 : ( a ) In terms of informational relations ; ( b ) in terms of inten- tional relations Figure 1 : Stages of the proposed method . Figure 2 : Graph of words for the target word paper . Numbers inside vertices correspond to their degree . Figure 3 : Running example of graph creation Figure 4 : Two dendrograms for the graph in Figure 3 . Table 1 : Sense-tagged corpus for the example in Figure 3 Figure 5 : ( A ) current configuration for internal node Dk and its associated subtrees ( B ) first alternative configuration , ( C ) second alternative configuration . Note that swapping st1 , st2 in ( A ) results in an equivalent tree . Hence , this configuration is excluded . Table 2 : Parameter values used in the evaluation . Figure 6 : Performance analysis of HRGs , CWU , CWW & HAC for different parameter combinations ( Table 2 ) . ( A ) All combinations of p1 , p2 and p3 = 0.05 . ( B ) All combinations of p1 , p2 and p3 = 0.09 . Figure 7 : Performance of HRGs and HAC for different parameter combinations ( Table 2 ) . All combinations of p1 , p2 and p3 ≥ 0.13 . Table 3 : HRGs against recent methods & baselines . Figure 1 : The Buckwalter Arabic Morphological Analyzer ’ s lookup process exemplified for the word lilkitAbi . Figure 2 : Skeleton of basic lexicon transducer in LEXC generated from BAMA lexicons . Table 1 : Features used by the polyglot ranking system . 0 Figure 1 : The computation of DKL ( Pv ( e i ) kPe0i ) using a toy corpus , for e = looking forward to . Note that the sec- Table 2 : Particles and prepositions allowed in phrasal verbs gathered from Wiktionary . Table 3 : Our boosted ranker combining monolingual and bilingual features ( bottom ) compared to three base- lines ( top ) gives comparable performance to the human- curated upper bound . Figure 2 : The solid line shows recall-at-1220 when com- bining the k best-performing bilingual statistics and three monolingual statistics . The dotted line shows the indi- vidual performance of the kth best-performing bilingual statistic , when applied in isolation to rank candidates . Table 4 : An ablation of monolingual statistics shows that they are useful in addition to the 50 bilingual statistics combined , and no single statistic provides maximal per- formance . Table 5 : The highest ranked phrasal verb candidates from our full system that do not appear in either Wiktionary set . Candidates are presented in decreasing rank ; “ pat on ” is the second highest ranked candidate . Figure 1 : Overlaid bilingual embeddings : English words are plotted in yellow boxes , and Chinese words in green ; reference translations to English are provided in boxes with green borders directly below the original word . Table 3 : Vector Matching Alignment AER ( lower is bet- ter ) Table 1 : Results on Chinese Semantic Similarity Table 2 : Results on Named Entity Recognition Table 4 : NIST08 Chinese-English translation BLEU Table 2 : Composition Gold Standards Table 3 : Sample of Gold Standard entries Table 4 : Results tested against gs-so Table 7 : Diff results tested against gs-swaco Table 5 : Results tested against gs-swaco-subjective Table 6 : LL results tested against gs-swaco Table 8 : Detailed DIFF results Figure 1 : Glue Semantics proof for ( 80 ) , Swedish Directed Motion Construction Figure 2 : Glue Semantics proof for ( 83 ) , English Way Construction ( means interpretation ) Figure 3 : Glue Semantics proof for ( 83 ) , English Way Construction ( manner interpretation ) Figure 4 : Glue Semantics proof for ( 86 ) , English Way Construction ( means interpretation ) Figure 1 : Bilingual training size vs. BLEU score ( mid- dle line , left axis ) and phrase table composition ( top line , right axis ) on Arabic Development Set . The baseline BLEU score ( bottom line ) is included for comparison . Figure 1 : Graphical representation of our model . Hyper- parameters , the stickiness factor , and the frame and event initial and transition distributions are not shown for clar- ity . Figure 2 : A partial frame learned by P RO F INDER from the MUC-4 data set , with the most probable emissions for each event and slot . Labels are assigned by the authors for readability . Table 1 : Results on MUC-4 entity extraction . C & J 2011 +granularity refers to their experiment in which they mapped one of their templates to five learned clusters rather than one . Table 2 : Results on TAC 2010 entity extraction with N - best mapping for N = 1 and N = 5 . Intermediate values of N produce intermediate results , and are not shown for brevity . Figure 1 : Syntagmatic vs. paradigmatic axes for words in a simple sentence ( Chandler , 2007 ) . Table 1 : Summary of results in terms of the MTO and VM scores . Standard errors are given in parentheses when available . Starred entries have been reported in the review paper ( Christodoulopoulos et al. , 2010 ) . Distributional models use only the identity of the target word and its context . The models on the right incorporate orthographic and morphological features . Figure 2 : MTO is not sensitive to the number of partitions used to discretize the substitute vector space within our experimental range . Figure 3 : MTO falls sharply for less than 10 S-CODE dimensions , but more than 25 do not help . Figure 4 : MTO is fairly stable as long as the Z̃ constant 5.4 Morphological and orthographic features is within an order of magnitude of the real Z value . Figure 5 : MTO is not sensitive to the number of random substitutes sampled per word token . Figure 6 : Hinton diagram comparing most frequent tags and clusters . Table 1 : AER results . IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM . We choose t = 1 , 5 , and 30 for the fertility HMM . Figure 1 : AER comparison ( en→cn ) Figure 3 : Training time comparison . The training time for each model is calculated from scratch . For example , the training time of IBM Model 4 includes the training time of IBM Model 1 , the HMM , and IBM Model 3 . Figure 2 : AER comparison ( cn →en ) Figure 1 . Segmentation algorithm . Table 1 . Known topic changes found in 90 generated texts using a block size of six . Figure 2 . Position of news story boundaries in a CNN news summary in relation to troughs found by the algorithm . Table 2 . CNN news summaries segmented using different block sizes . Figure 1 : Example candidate space of dimensionality 2 . Note : I = { 1 , 2 } , J ( 1 ) = J ( 2 ) = { 1 , 2 , 3 } . We also show a local scoring function hw ( i , j ) ( where w = [ −2 , 1 ] ) and a local gold scoring function g ( i , j ) . Figure 3 : Result of synthetic data learning experiment for MERT and PRO , with and without added noise . As the dimensionality increases MERT is unable to learn the original weights but PRO still performs adequately . Table 1 : Machine translation performance for the experiments listed in this paper . Scores are case-sensitive IBM B LEU . For every choice of system , language pair , and feature set , PRO performs comparably with the other methods . Table 2 : Data sizes for the experiments reported in this paper ( English words shown ) . Table 3 : Summary of features used in experiments in this paper . Figure 5 : Comparison of MERT , PRO , and MIRA on tuning Urdu-English SBMT systems , and test results at every iteration . PRO performs comparably to MERT and MIRA . Figure 6 : Tune and test curves of five repetitions of the same Urdu-English PBMT baseline feature experiment . PRO is more stable than MERT . Table 1 : State classification by minimum input consumed for the Finnish dictionary Table 3 : The sizes of error models as automata Table 2 : The sizes of dictionaries as automata Table 5 : Effect of language and error models to quality ( recall , proportion of suggestion sets containing a cor- rectly suggested word ) Table 4 : Effect of language and error models to speed ( time in seconds per 10,000 word forms ) Table 6 : Effect of text type on error models to speed ( in seconds per 10,000 word-forms ) Table 1 : Experimental results using different smoothing methods . Table 2 : Experimental results using different phrase ta- bles . OutBp : the out-of-domain phrase table . AdapBp : the adapted phrase table . Figure 1 : Effect of in-domain monolingual corpus size on translation quality . Table 1 : CORE 12 with inflectional features , predicted input . Top : Adding all nine features to CORE 12 . Second part : Adding each feature separately , comparing difference from CORE 12 . Third part : Greedily adding best features from second part . Table 2 : Feature prediction accuracy and set sizes . * = The set includes a `` N/A '' value . Table 3 : Lexical features . Top part : Adding each feature separately ; difference from CORE 12 ( predicted ) . Bottom part : Greedily adding best features from previous part . Table 6 : Functional features : gender , number , rationality . Table 4 : Inflectional+lexical features together . Table 5 : Extended inflectional features . Table 7 : Results on unseen test set for models which performed best on dev set – predicted input . Table 1 : Comparison of SWSD systems Table 2 : S/O classifier with and without SWSD . Table 3 : N/P classifier with and without SWSD Table 4 : S/O classifier with learned SWSD integration Table 5 : N/P classifier with learned SWSD integration Table 6 : Polarity classifier with and without SWSD . Table 1 : Results obtained by applying different types of features in isolation to the Baseline system . Table 2 : Results obtained by adding different types of features incrementally to the Baseline system . Table 3 : Examples errors introduced by YAGO and FrameNet . Table 1 Contingency table for the children of canine in the subject position of run . Table 2 Contingency table for the children of liquid in the object position of drink . Table 3 Example levels of generalization for different values of α . Table 4 Extent of generalization for different values of α and sample sizes . Table 5 Results for the pseudo-disambiguation task . Table 6 Results for the pseudo-disambiguation task with one-fifth training data . Table 7 Disambiguation results for G2 and X2 . Table 2 . Features used in baseline system Table 3 . Accuracy of 5-fold cross-validation with sta- tistics-based semantic features Table 5 . Accuracy of 5-fold cross-validation with self- extracted semantic features based on different levels of syntactic/semantic relations Table 4 . Accuracy of 5-fold cross-validation with self- extracted semantic features Figure 1 : Example of a long jump alignment grid . All possible deletion , insertion , identity and substitution op- erations are depicted . Only long jump edges from the best path are drawn . Table 2 : Corpus statistics of the MATR MT06 corpus that was used for experimental evaluation of the proposed measures . Table 1 : Example of word-dependent substitution costs . Table 3 : Pearson ’ s r and Kendall ’ s τ ( absolute ) between adequacy and automatic evaluation measures on different levels of the MATR MT06 data . Fig . 1 Examples of TERp alignment output . In each example , R , H and H denote the reference , the original hypothesis and the hypothesis after shifting respectively . Shifted words are bolded and other edits are in [ brackets ] . Number of edits shown : TERp ( TER ) Fig . 2 Metric correlations with adequacy on the MetricsMATR 2008 development set . Correlations are significantly different if the center point of one correlation does not lie within the confidence interval of the other correlation Table 1 TERp edit costs optimized for adequacy Fig . 3 Pearson correlation of TERp with selective features Table 1 : Numbers of expressions of all the differ- ent types from the DISCO and Reddy datasets . Table 2 : All the parameters of Measures for de- termining semantic compositionality described in Section 3 used in our experiments . Figure 1 : All the parameters of WSMs described in Section 2 used in all our experiments . Semicolon denotes OR . All the examined combinations of parameters are implied from reading the diagram from left to right . Table 5 : Parameters of WSMs ( Section 2 ) which , combined with particular Measures , achieved the highest average correlation in TrValD . Table 6 : Parameters of Measures ( Section 3 ) which , combined with particular WSMs , achieved the highest average correlation in TrValD . Figure 1 : Translation of PCC sample commentary Figure 2 : Screenshot of Annis Linguistic Database Figure 1 – Selection and weighting of words from the collocation network Figure 2 – Automaton for topic shift detection Table 3 – Pk for C99 corpus Table 2 – Pk for Le Monde corpus Table 1 – Precision/recall for Le Monde corpus Table 4 – Error rates for Le Monde corpus Figure 1 : POS Tagging of Unknown Word using Contextual and Lexical features in a Sequential Model . The input for capitalized classifier has 2 values and therefore 2 ways to create confusion k sets . There are at most  & F   + !  different in- puts for the suffix classifier ( 26 character + 10 digits + 5 other symbols ) , therefore suffix may k emit up to   & R   +R confusion sets . Table 2 : POS tagging of unknown words using contextual and lexical Features ( accuracy in per- cent ) .  is based only on contextual features , T  is based on contextual and lexical features . SM ( _ # § ) 2 denotes that § follows in the sequential model . 2 Table 1 : POS tagging of unknown words using contextual features ( accuracy in percent ) .  is a classifier that uses only contextual features ,  + baseline is the same classifier with the addition of the baseline feature ( “ NNP ” or “ NN ” ) . Table 4 : Processing time for POS tagging of known words using contextual features ( In CPU seconds ) . Train : training time over + sentences . Brill ’ s learner was interrupted after 12 days of train- ing ( default threshold was used ) . Test : average number of seconds to evaluate a single sentence . All runs were done on the same machine . Table 3 : POS Tagging of known words using con- textual features ( accuracy in percent ) . one-vs-all denotes training where example ` serves as positive example to the true tag and as negative example to all the other tags . SM| ¨R© denotes training where 2 example ` serves as positive example to the true tag Figure 2 : Plate diagram depicting the joint model . Hyper- parameters have been omitted for clarity . The L-shaped plate contains the tokens , while the square plates contain the morphological analyses . The t are latent tags , zi is an assignment to a morphological analysis lk = ( sk , fk ) , and wi is the observed word . T is the number of distinct tags , and Kt the number of tables used by tag type t . Figure 1 : Plate diagram depicting the morphology model ( adapted from Goldwater et al . ( 2006 ) ) . Hyperparameters have been omitted for clarity . The left-hand plate depicts the base distribution P0 ; note that the morphological anal- yses lk are generated deterministically as ( tk , sk , fk ) . The observed words wi are also deterministic given zi = k and lk , since wi = sk ⊕ fk . Figure 3 : The posterior distribution of our joint model . Because the sequence of words w is deterministic given analyses l and assignments to analyses ( tables ) z , the joint posterior over all variables P ( w , t , l , z|αt , a , b , αs , α f ) is equal to P ( t , l , z|αt , a , b , αs , α f ) when lzi = wi for all i , and 0 otherwise . We give equations for the non-zero case . ns refer to token counts , ms to table counts . We add two dummy tokens at the start , end , and between sentences to pad the context history . Table 1 : Example sentences in the synthetic languages . Words in Category 1 are made of characters a-d , Cate- gory 2 e-h , Category 3 m-p , Category 4 r-u . Suffixes in Language B are separated with periods ( . ) for illustrative purposes only . Figure 4 : Log probability of the sampler state over 1000 iterations on Languages A and B . Table 3 : Spanish Ornat corpus results . Standard devia- tions are in parentheses ; ∗ denotes a significant difference from the M ORTAG model . Table 2 : English Eve corpus results . Standard deviations are in parentheses ; ∗ denotes a significant difference from the M ORTAG model . Table 1 : Parse Feature Example for the sentence : “ GM says the addition of OnStar , which includes a system that automatically notifies an OnStar operator if the vehicle is involved in a collision , complements the Vue ’ s top five-star safety rating for the driver and front passenger in both front- and side-impact crash tests . ” Table 2 : Training Data Sizes for Common ESL Confused Words Table 4 : Spelling correction accuracy ( % ) , impact of combining word co-occurrence CLASSIFIER : Logistic Regression trained on 1G words of news text , tested on 9-months NYT data . COMBINED SYSTEM : CLASSIFER plus system based on first-order word co-occurrence . & : Relative increase or decrease in error rate compared to CLASSIFIER # : As in Bergsma et al . ( 2009 ; 2010 ) , no morphological variants of the words are used in evaluation Table 3 : Spelling correction precision ( % ) , impact of adding parse features SVM trained on 1G words of news text , tested on 9-months of NYT data . * : Improvement of ( NG+ ) LEX+PAR vs. ( NG+ ) LEX is statistically significant . α : Improvement of NG+LEX+PAR vs. NG is statistically significant . & : Relative increase or decrease of error rate compared to ” NG+LEX ” # : As in Bergsma et al . ( 2009 ; 2010 ) no morphological variants of the words are used in evaluation Figure 1 : The semantic representations of a word W , its inverse W inv and its negation ¬W . The domain part of the representation remains un- changed , while the value part will partially be in- verted ( inverse ) , or inverted and scaled ( negation ) with 0 < µ < 1 . The ( separate ) functional repre- sentation also remains unchanged . Figure 2 : A partially scaled and inverted identity matrix Jµ . Such a matrix can be used to trans- form a vector storing a domain and value repre- sentation into one containing the same domain but a partially inverted value , such as W and ¬W de- scribed in Figure 1 . Figure 3 : The parse tree for This car is not blue , highlighting the limited scope of the negation . Figure 1 : ( a ) RM and large margin solution comparison and ( b ) the spread of the projections given by each . RM and large margin solutions are shown with a darker dotted line and a darker solid line , respectively . Figure 2 : RM update with margin and bounding con- straints . The diagonal dotted line depicts cost–margin equi- librium . The vertical gray dotted line depicts the bound B . White arrows indicate updates triggered by constraint viola- tions . Squares are data points in the k-best list not selected for update in this round . Table 2 : Active sparse feature templates Table 4 : Performance on Ar-En with basic ( left ) and sparse ( right ) feature sets on MT05 and MT08 . Table 5 : RM gain over other optimizers averaged over all test sets . Table 3 : Performance on Zh-En with basic ( left ) and sparse ( right ) feature sets on MT03 and MT05 . Table 1 : Case restoration performance using an MD-trie , English . Table 4 : Improvement in f-score through restoring case . Table 3 : Accuracy on seen and unseen tokens . Table 2 : Recognition performance . Table 5 : Final results for English and German , develop- ment and test sets . Table 1 : The regular expressions available in Foma from highest to lower precedence . Horizontal lines separate precedence classes . Table 2 : A relative comparison of running a se- lection of regular expressions and scripts against other finite-state toolkits . The first and second en- tries are short regular expressions that exhibit ex- ponential behavior . The second results in a FSM with 221 states and 222 arcs . The others are scripts that can be run on both Xerox/PARC and Foma . The file lexicon.lex is a LEXC format English dic- tionary with 38418 entries . North Sami is a large lexicon ( lexc file ) for the North Sami language available from http : //divvun.no . Table 1 : Coreference Definition Differences for MUC and ACE . ( GPE refers to geo-political entities . ) Table 2 : Dataset characteristics including the number of documents , annotated CEs , coreference chains , annotated CEs per chain ( average ) , and number of documents in the train/test split . We use st to indicate a standard train/test split . Table 3 : Impact of Three Subtasks on Coreference Resolution Performance . A score marked with a * indicates that a 0.5 threshold was used because threshold selection from the training data resulted in an extreme version of the system , i.e . one that places all CEs into a single coreference chain . Table 5 : Correlations of resolution class scores with respect to the average . Table 4 : Frequencies and scores for each resolution class . Table 6 : Predicted ( P ) vs Observed ( O ) scores . Table 1 Notation used in this article . Figure 1 Examples of word alignment patterns in German–English that require the increased expressive power of synchronous tree adjoining grammar . Figure 2 Examples of dependency trees with word alignment . Arrows are drawn from children to parents . A child word is a modifier of its parent . Each word has exactly one parent and $ is a special “ wall ” symbol that serves as the parent of all root words in the tree ( i.e. , those with no other parent ) . Figure 3 Example of a sentence pair containing a frequently-observed “ sibling ” relationship in German–English data : in the the→us dependency , the aligned German words are siblings in the source dependency tree . This occurs due to differences in treebank and head rule conventions between the two data sets . The German parser produces flat PPs with little internal structure , so when the dependency tree is generated , each word in the PP attaches to the P , the head of the phrase . Table 2 Key definitions for our model . Figure 5 Example output of our model for Chinese→English translation . The word-segmented Chinese sentence and dependency tree are inputs . Our model ’ s outputs include the English translation , phrase segmentations for each sentence ( a box surrounds each phrase ) , a one-to-one alignment between the English and Chinese phrases , and a projective dependency tree on the English phrases . Note that the Chinese dependency tree is on words whereas the English dependency tree is on phrases . Table 4 Most frequent phrase dependencies in DE→EN data , shown with their counts and attachment directions . Child phrases point to their parents . To focus on interesting phrase dependencies , we only show those in which one phrase has at least two tokens and neither phrase is entirely punctuation . The words forming the longest lexical dependency in each extracted phrase dependency are shown in bold ; these are used for back-off features . Table 3 Top 60 most frequent root phrases in DE→EN data with at least two words , shown with their counts . Shown in bold are the actual root words in the lexical dependency trees from which these phrases were extracted ; these are extracted along with the phrases and used for back-off features . Table 5 Most frequent Brown cluster phrase dependencies extracted from DE→EN data , shown with their counts . As in Table 4 , we only show those in which one phrase has at least two tokens and neither phrase is entirely punctuation . Each cluster is shown as a set of words large enough to cover 95 % of the token counts in the cluster , up to a maximum of four words . It is characteristic of Brown clustering that very frequent tokens ( e.g. , function words ) often receive their own clusters . Figure 6 Examples of illustrative sentence pairs and frequently extracted rules that model verb movement between German and English . An ellipsis indicates that there must be material between the two phrases for the rule to apply . ( a ) Example of movement of the finite verb to the end of a dependent clause . ( b ) Example of movement of an infinitive to the end of an independent clause following a modal verb ( möchte , ‘ would like ’ ) . Discussion of the features used to score these string-to-tree rules is given in Section 5.2 . Figure 7 String-to-tree configurations ; each is associated with a feature that counts its occurrences in a derivation . Figure 8 Quasi-synchronous tree-to-tree configurations from Smith and Eisner ( 2006 ) . There are additional configurations involving NULL alignments and an “ other ” category for those that do not fit into any of the named categories . Figure 10 Lattice dependency parsing using an arc-factored dependency model . Lone indices like p and i denote nodes in the lattice , and an ordered pair like ( i , j ) denotes the lattice edge from node i to node j . S TART is the single start node in the lattice and F INAL is a set of final nodes . We use edgeScore ( i , j ) to denote the model score of crossing lattice edge ( i , j ) , which only includes the phrase-based features h 0 . We use arcScore ( ( i , j ) , ( l , m ) ) to denote the score of building the dependency arc from lattice edge ( i , j ) to its parent ( l , m ) ; arcScore only includes the QPD features h 00 . Table 7 % BLEU on tune and test sets for ZH→EN translation , showing the contribution of feature sets in our QPD model . Both QPD models are significantly better than the best Moses numbers on test sets 1 and 2 , but not on test set 3 . The full QPD model is significantly better than the version with only T GT T REE features on test set 1 but statistically indistinguishable on the other two test sets . Hiero is significantly better than the full QPD model on test set 2 but not on the other two . Table 6 % BLEU on tune and test sets for DE→EN translation , comparing the baselines to our QPD model with target syntactic features ( T GT T REE ) and then also with source syntax ( + T REE T O T REE ) . Here , merely using the additional round of tuning with the SSVM reranker improves the BLEU score to 19.9 , which is statistically indistinguishable from the two QPD feature sets . Differences between Hiero and the three 19.9 numbers are at the border of statistical significance ; the first two are statistically indistinguishable from Hiero but the third is different at p = 0.04 . Table 8 % BLEU on tune and test sets for UR→EN translation , using our unsupervised Urdu parser to incorporate source syntactic features . The two QPD rows are statistically indistinguishable on both test sets . Both are significantly better than all Moses results , but Hiero is significantly better than all others . Table 9 % BLEU on tune and test sets for EN→MG translation , using a supervised English parser and an unsupervised Malagasy parser . The 15.6 BLEU reached by the full QPD model is statistically significantly better than all other results on the test set . All other test set numbers are statistically indistinguishable . Table 10 % BLEU on tune and test sets when comparing parsers for ZH→EN translation . QPD uses all features , including T GT T REE and T REE T O T REE . The table first pairs supervised English parsing with supervised , unsupervised , and random Chinese parsing , then pairs unsupervised English parsing with supervised and unsupervised Chinese parsing . † = significantly better than sup/sup , ∗ = significantly worse than sup/sup . Table 11 Feature ablation experiments for UR→EN translation with string-to-tree features , showing the drop in BLEU when separately removing word ( W ORD ) , cluster ( C LUST ) , and configuration ( C FG ) feature sets . ∗ = significantly worse than T GT T REE . Removing word features causes no significant difference . Removing cluster features results in a significant difference on both test sets , and removing configuration features results in a significant difference on test 2 only . Table 12 Results of human evaluation performed via Amazon Mechanical Turk . The percentages represent the portion of sentences for which one system had more preference judgments than the other system . If a sentence had an equal number of judgments for the two systems , it was counted in the final row ( “ neither preferred ” ) . Table 14 % BLEU on tune and test sets for UR→EN translation , comparing several settings for maximum dependency lengths in the decoder ( ωx is for the source side and ωy is for the target side ) . The upper table shows Moses BLEU scores for comparison . The lower table compares two max dependency length settings during tuning , and several for decoding on the test sets , showing both BLEU scores and average decoding times per sentence . See text for discussion . Table 2 : Four ambiguous words , their senses and frequency Table 4 : Mutual information between feature subset and class label with f req based feature ranking . Table 3 : Mutual information between feature subset and class label with χ2 based feature ranking . Figure 2 : Average accuracy over three procedures in Figure 1 as a function of context window size ( horizontal axis ) for 4 datasets . Figure 1 : Results for three procedures over 4 datases . The horizontal axis corresponds to the context window size . Solid line represents the result of F SGM M + binary , dashed line denotes the result of CGDSV D + idf , and dotted line is the result of CGDterm + idf . Square marker denotes χ2 based feature ranking , while cross marker denotes f req based feature ranking . Table 5 : Average accuracy of three procedures with various settings over 4 datasets . Table 6 : Automatically determined mixture component num- Figure 1 : Discourse tree for two sentences in RST-DT . Each of the sentences contains three EDUs . The second sentence has a well-formed discourse tree , but the first sentence does not have one . Figure 3 : Discourse parsing framework . Figure 2 : Distributions of six most frequent relations in intra-sentential and multi-sentential parsing scenarios . Figure 4 : A chain-structured DCRF as our intra- sentential parsing model . Figure 5 : Our parsing model applied to the sequences at different levels of a sentence-level DT . ( a ) Only possible se- quence at the first level , ( b ) Three possible sequences at the second level , ( c ) Three possible sequences at the third level . Figure 6 : A CRF as a multi-sentential parsing model . Table 1 : Features used in our parsing models . Figure 8 : Extracting sub-trees for S2 . Figure 7 : Two possible DTs for three sentences . Figure 9 : Confusion matrix for relation labels on the RST-DT test set . Y-axis represents true and X-axis repre- sents predicted relations . The relations are Topic-Change ( T-C ) , Topic-Comment ( T-CM ) , Textual Organization ( T- O ) , Manner-Means ( M-M ) , Comparison ( CMP ) , Evaluation ( EV ) , Summary ( SU ) , Condition ( CND ) , Enablement ( EN ) , Cause ( CA ) , Temporal ( TE ) , Explanation ( EX ) , Background ( BA ) , Contrast ( CO ) , Joint ( JO ) , Same-Unit ( S-U ) , Attribu- tion ( AT ) and Elaboration ( EL ) . Table 2 : Parsing results of different models using manual ( gold ) segmentation . Performances significantly superior to HILDA ( with p < 7.1e-05 ) are denoted by * . Significant differences between TSP 1-1 and TSP SW ( with p < 0.01 ) are denoted by † . Table 1 : RST Spanish Treebank statistics Table 2 : Rhetorical relations in RST Spanish Treebank Figure 1 : Example of the non-relation Same-Unit Figure 1 : Topic transfer in bilingual LSA model . Figure 2 : Dirichlet-Tree prior of depth two . Table 1 : Parallel topics extracted by the bLSA model . Top words on the Chinese side are translated into English for illustration purpose . Table 2 : English word perplexity ( PPL ) on the RT04 test set using a unigram LM . Figure 3 : Comparison of training log likelihood of English LSA models bootstrapped from a Chinese LSA and from a flat monolingual English LSA . Figure 4 : Word perplexity with different β using manual reference or ASR hypotheses on CCTV . Figure 5 : BLEU score for those 25 % utterances which resulted in different translations after bLSA adaptation ( manual transcriptions ) Table 4 : Translation performance of baseline and bLSA-Adapted Chinese-English SMT systems on manual transcriptions and 1-best ASR hypotheses Table 1 : Relation weights ( Method 2 ) Figure 1 : Graph of Word Senses Table 2 : Results of SVM and Mincuts with different settings of feature Table 6 : Accuracy with different sizes of unlabeled data ( random selection ) Table 3 : Accuracy for Different Part-Of-Speech Figure 2 : Learning curve with different sizes of labeled data Table 4 : Accuracy with different sizes of labeled data Table 5 : Accuracy with different sizes of unlabeled data from WordNet relation Figure 1 : The clausal and topological field structure of a German sentence . Notice that the subordinate clause receives its own topology . Table 1 : a ) An example of a document from TüBa-D/Z , b ) an abbreviated entity grid representation of it , and c ) the feature vector representation of the abbreviated entity grid for transitions of length two . Mentions of the entity Frauen are underlined . nom : nominative , acc : accusative , oth : dative , oblique , and other arguments Table 2 : Accuracy ( % ) of the permutation de- tection experiment with various entity represen- tations using manual and automatic annotations of topological fields and grammatical roles . The baseline without any additional annotation is un- derlined . Two-tailed sign tests were calculated for each result against the best performing model in each column ( 1 : p = 0.101 ; 2 : p = 0.053 ; + : statis- tically significant , p < 0.05 ; ++ : very statistically significant , p < 0.01 ) . Table 4 : Accuracy ( % ) of permutation detection experiment with various entity representations us- ing manual and automatic annotations of topolog- ical fields and grammatical roles on subset of cor- pus used by Filippova and Strube ( 2007a ) . Table 5 : Results of adding coherence features into a natural language generation system . VF Acc % is the accuracy of selecting the first constituent in main clauses . Acc % is the percentage of per- fectly ordered clauses , tau is Kendall ’ s τ on the constituent ordering . The test set contains 2246 clauses , of which 1662 are main clauses . Figure 1 : Sample pairs of similar caseframes by relation type , and the similarity score assigned to them by our distributional model . Table 1 : A sentence decomposed into its depen- dency edges , and the caseframes derived from those edges that we consider ( in black ) . Figure 2 : Average sentence cover size : the average number of sentences needed to generate the case- frames in a summary sentence ( Study 1 ) . Model summaries are shown in darker bars . Peer system numbers that we focus on are in bold . Table 2 : The average number of source text sen- tences needed to cover a summary sentence . The model average is statistically significantly differ- ent from all the other conditions p < 10−7 ( Study 1 ) . Table 3 : Signature caseframe densities for differ- ent sets of summarizers , for the initial and update guided summarization tasks ( Study 2 ) . ∗ : p < 0.005 . Figure 3 : Density of signature caseframes ( Study 2 ) . Figure 4 : Examples of signature caseframes found in Study 2 . Table 4 : Density of signature caseframes after merging to various threshold for the initial ( Init . ) and update ( Up . ) summarization tasks ( Study 2 ) . Table 6 : The effect on caseframe coverage of adding in-domain and out-of-domain documents . The difference between adding in-domain and out- of-domain text is significant p < 10−3 ( Study 3 ) . Table 5 : Coverage of caseframes in summaries with respect to the source text . The model aver- age is statistically significantly different from all the other conditions p < 10−8 ( Study 3 ) . Figure 5 : Coverage of summary text caseframes in source text ( Study 3 ) . Fig . 1 . ( a ) An example in which an English sentence is parsed into a tree structure with 12 PCFG rules ; ( b ) an instance in which a Chinese sentence ( both Chinese characters and Chinese Pinyin are provided , and note that we will use Chinese Pinyin throughout the paper ) is converted into an English tree using 6 STSG rules . The symbol to the upper right of a node indicates that this node is constructed using rule . Fig . 3 . Lexicalized training example ( POS of target headword is not explicitly given ) . Fig . 4 . Examples of rules used during decoding . Fig . 5 . Two real translation examples , Figure 1 : Example MERT values along one coordi- nate , first unregularized . When regularized with ` 2 , the piecewise constant function becomes piecewise quadratic . When using ` 0 , the function remains piecewise constant with a point discontinuity at 0 . Table 1 : Datasets for the two experimental conditions . Figure 3 : Comparison of rate of convergence between coordinate ascent and our expected BLEU direction finder ( D = 500 ) . Noisy refers to the noisy experimental setting . Table 2 : BLEU scores for GBM features . Model parameters were optimized on the Tune set . For PRO and regularized MERT , we optimized with different hyperparameters ( regularization weight , etc . ) , and retained for each experimental condition the model that worked best on Dev . The table shows the performance of these retained models . Table 3 : BLEU scores for SparseHRM features . Notes in Table 2 also apply here . Figure 1 : A tree showing head information Table 1 : Perplexity results for two previous grammar-based language models Figure 2 : A noun-phrase with sub-structure Table 2 : Perplexity results for the immediate- bihead model Table 3 : Perplexity results for the immediate- trihead model Table 4 : Precision/recall for sentences in which trigram/grammar models performed best Figure 1 : An example of the label consistency problem . Here we would like our model to encourage entities Albert Einstein and Einstein to get the same label , so as to improve the chance that both are labeled PERSON . Table 2 : Table showing the number of ( token sequence , token subsequence ) pairs where the token sequence is assigned a certain entity label , and the token subsequence is assigned a certain entity label . We show these counts both within documents , as well as over the whole corpus . Rows correspond to sequences , and columns to subsequences . These statistics are from the CoNLL 2003 English training set . Table 1 : Table showing the number of pairs of different occurrences of the same token sequence , where one occurrence is given a certain label and the other occurrence is given a certain label . We show these counts both within documents , as well as over the whole corpus . As we would expect , most pairs of the same entity sequence are labeled the same ( i.e . the diagonal has most of the density ) at both the document and corpus levels . These statistics are from the CoNLL 2003 English training set . Table 3 : Table showing improvements obtained with our additional features , over the baseline CRF . We also compare our performance against ( Bunescu and Mooney , 2004 ) and ( Finkel et al. , 2005 ) and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF . Figure 1 : Equivalent Left LM State Computation . Table 4 : BLEU scores after discriminative hypergraph- reranking . Only the language model ( LM ) or the transla- tion model ( TM ) or both ( LM+TM ) may be discrimina- tively trained to prefer the oracle-best hypotheses . Table 3 : Baseline and oracle-best 4-gram BLEU scores with 4 references for NIST Chinese-English MT datasets . Table 2 : Speed of oracle extraction from hypergraphs . The basic dynamic program ( Sec . 2.1 ) improves signifi- cantly by collapsing equivalent oracle states ( Sec . 2.2 ) . Table 1 : Accuracy in Lexical Sample Tasks Table 2 : System Pairwise Agreement Table 2 : Performance using FBIS training corpus ( top ) and NIST corpus ( bottom ) . Improvements are significant at the p < 0.05 level , except where indicated ( ns ) . Table 1 : Filters to improve the dictionary precision . Un- less otherwise noted , the filter was applied if either men- tion in the relation satisfied the condition . Table 3 : Rules of the baseline system . Table 2 : Coreference relations in our dictionary . Table 4 : Dataset statistics : development ( dev ) and test . Table 6 : Performance on the test set . Scores are on gold mentions . Stars indicate a statistically significant difference with respect to the baseline . Table 5 : Incremental results for the four sieves using our dictionary on the development set . Baseline is the Stanford system without the WordNet sieves . Scores are on gold mentions . Figure 1 ( a ) A standard PTB parse of Example ( 1a ) . ( b ) The MWE part of speech functions syntactically like the ordinary nominal category , as shown by this paraphrase . ( c ) We incorporate the presence of the MWE into the syntactic analysis by flattening the tree dominating part of speech and introducing a new non-terminal label multiword noun ( MWN ) for the resulting span . The new representation classifies an MWE according to a global syntactic type and assigns a POS to each of the internal tokens . It makes no commitment to the internal syntactic structure of the MWE , however . Table 1 Semi-fixed MWEs in French and English . The French adverb à terme ( ‘ in the end ’ ) can be modified by a small set of adjectives , and in turn some of these adjectives can be modified by an adverb such as très ( ‘ very ’ ) . Similar restrictions appear in English . Table 2 French grammar development . Incremental effects on grammar size and labeled F1 for each of the manual grammar features ( development set , sentences ≤ 40 words ) . The baseline is a parent-annotated grammar . The features tradeoff between maximizing two objectives : overall parsing F1 and MWE F1 . Table 3 Unknown word model features for Arabic and French . Table 4 DP-TSG notation . For consistency , we largely follow the notation of Liang , Jordan , and Klein ( 2010 ) . Figure 2 Example of two conflicting sites of the same type in a training tree . Define the type of a def site t ( z , s ) = ( Δns:0 , Δns:1 ) . Sites ( 1 ) and ( 2 ) have the same type because t ( z , s1 ) = t ( z , s2 ) . The two sites conflict , however , because the probabilities of setting bs1 and bs2 both depend on counts for the tree fragment rooted at NP . Consequently , sites ( 1 ) and ( 2 ) are not exchangeable : The probabilities of their assignments depend on the order in which they are sampled . Table 5 Gross corpus statistics for the pre-processed corpora used to train and evaluate our models . We compare to the WSJ section of the PTB : train ( Sections 02–21 ) ; dev . ( Section 22 ) ; test ( Section 23 ) . Due to its flat annotation style , the FTB sentences have fewer constituents per sentence . In the ATB , morphological variation accounts for the high proportion of word types to sentences . Table 6 Frequency distribution of the MWE types in the ATB and FTB training sets . Table 9 French standard parsing experiments ( test set , sentences ≤ 40 words ) . FactLex uses basic POS tags predicted by the parser and morphological analyses from Morfette . FactLex* uses gold morphological analyses . Table 8 Arabic standard parsing experiments ( test set , sentences ≤ 40 words ) . SplitPCFG is the same grammar used in the Stanford parser , but without the dependency model . FactLex uses basic POS tags predicted by the parser and morphological analyses from MADA . FactLex* uses gold morphological analyses . Berkeley and DP-TSG results are the average of three independent runs . Table 11 French MWE identification per category and overall results ( test set , sentences ≤ 40 words ) . MWI and MWCL do not occur in the test set . Table 10 Arabic MWE identification per category and overall results ( test set , sentences ≤ 40 words ) . Table 12 MWE identification F1 of the parsing models vs. the mwetoolkit baseline ( test set , sentences ≤ 40 words ) . FactLex∗ uses gold morphological analyses at test time . Table 14 Sample of human-interpretable French TSG rules . Table 13 Sample of human-interpretable Arabic TSG rules . Recursive rules like MWA→A MWA result from memoryless binarization of n-ary rules . This pre-processing step not only increases parsing accuracy , but also allows the generation of previously unseen MWEs of a given type . Figure 1 : An underspecified discourse structure and its five configurations Figure 2 : A wRTG modelling the interdependency constraint for Fig . 1 Figure 5 : A filter RTG corresponding to Ex . 2 C2 Figure 3 : An underspecified d Figure 4 : A RTG integrating the attachment constraint for Contrast from Ex . 2 into Fig . 3 Table 1 : Texts used for the evaluation Table 2 : Normalization accuracy after training on n tokens and evaluating on 1,000 tokens ( average of 10 random training and evaluation sets ) , compared to the “ baseline ” score of the full text without any normalization Table 3 : Tagging accuracy on the gold-standard normalizations ( OrigP = original punctuation , ModP = modern punctuation , NoP = no punctu- ation ) Table 4 : Tagging accuracy on the combined TIGER/Tüba corpus , using 10-fold CV , evaluated with and without capitalization , punctuation , and sentence boundaries ( SB ) Table 5 : POS tagging accuracy on texts without punctuation and capitalization , for tagging on the original data , the gold-standard normalization , and automatic normalizations using the first n tokens as training data Figure 1 : Growth of the Wiktionary over the last three years , showing total number of entries for all languages and for the 9 languages we consider ( left axis ) . We also show the corresponding increase in average accuracy ( right axis ) achieved by our model across the 9 languages ( see details below ) . Figure 2 : Type-level ( top ) and token-level ( bottom ) cov- erage for the nine languages in three versions of the Wik- tionary . Table 1 : Examples of constructing Universal POS tag sets from the Wiktionary . Figure 3 : Word type coverage by normalized frequency : words are grouped by word count / highest word count ratio : low [ 0 , 0.01 ) , medium [ 0.01 , 0.1 ) , high [ 0.1 , 1 ] . Figure 4 : PTB vs. Wiktionary type coverage across sec- tions of the Brown corpus . Figure 5 : The Wiktionary vs. tree bank tag sets . Around 90 % of the Wiktionary tag sets are identical or subsume tree bank tag sets . See text for details . Figure 7 : Tag errors broken down by the word type clas- sified into the six classes : oov , identical , superset , subset , overlap , disjoint ( see text for detail ) . The largest source of error across languages are out-of-vocabulary ( oov ) word types , followed by tag set mismatch types : subset , over- lap , disjoint . Figure 6 : Model accuracy across the Brown cor- pus sections . ST : Stanford tagger , Wik : Wiktionary- tag-set-trained SHMM-ME , PTBD : PTB-tag-set-trained SHMM-ME , PTB : Supervised SHMM-ME . Wik outper- forms PTB and PTBD overall . Table 2 : Accuracy for Unsupervised , Bilingual , Wiktionary and Supervised models . Avg . is the average of all lan- guages except English . Unsupervised models are trained without dictionary and use an oracle to map tags to clusters . Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments . The Projection model uses a dictionary build directly from the part-of-speech projection . The D & P model extends the Projection model dictionary by using Label Propagation . Supervised models are trained using tree bank information with SHMM-ME : Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary and All TBD uses tree bank tag sets for all words . 50 , 100 and All Sent . models are trained in a supervised manner using increasing numbers of training sentences . Table 1 : Experimental results over the 120 evalu- ation sentences . Alignment error rates in both di- rections are provided here . Table 1 : Results on the Arabic Treebank ( ATB ) data set : We compare our models against Poon et al . ( 2009 ) ( PCT09 ) and the Morfessor system ( Morfessor-CAT ) . For our full model ( +T OKEN -S EG ) and its simplifica- tions ( BASIC , +POS , +T OKEN -POS ) , we perform five random restarts and show the mean scores . The sample standard deviations are shown in brackets . The last col- umn shows results of a paired t-test against the preceding model : ++ ( significant at 1 % ) , + ( significant at 5 % ) , ∼ ( not significant ) , - ( test not applicable ) . Table 3 : Segmentation performance on words that have the same final suffix as their preceding words . The F1 scores are computed based on all boundaries within the words , but the accuracies are obtained using only the final suffixes . Table 2 : Segmentation performance on words that begin with prefix “ Al ” ( determiner ) and end with suffix “ At ” ( plural noun suffix ) . The mean F1 scores are computed using all boundaries of words in this set . For each word , we also determine if both affixes are recovered while ig- noring any other boundaries between them . The other two columns report this accuracy at both the type-level and the token-level . Table 1 : Number of synset relations Table 2 : TS of party # n # 1 ( first 10 out of 12,890 total words ) Table 3 : First ten words with weigths and number of senses in WN of the Topic Signature for airport # n # 1 obtained from BNC using InfoMap Table 4 : Minimum distances from airport # n # 1 Table 5 : Sense disambiguated TS for airport # n # 1 obtained from BNC using InfoMap and SSI-Dijkstra Table 6 : Size and percentage of overlapping relations between KnowNet versions and WN+XWN Table 7 : Percentage of overlapping relations between KnowNet versions Table 8 : P , R and F1 fine-grained results for the resources evaluated at Senseval-3 , English Lexical Sample Task Table 3 : Results of the French to English system ( WMT-2012 ) . The marked system ( * ) corresponds to the system submitted for manual evaluation . ( cs : case-sensitive , ci : case-insensitive ) Table 2 : Rules for morphological simplification . Table 1 : Text normalization for FR-EN . Table 4 : Processing steps for the input sentence dire warnings from pentagon over potential defence cuts . Table 5 : Results for French inflection prediction on the WMT-2012 test set . The marked system ( * ) corresponds to the system submitted for manual evaluation . Table 7 : Russian to English machine translation system evaluated on WMT-2012 and WMT-2013 . Human evaluation in WMT13 is performed on the system trained using the original corpus with TA- GIZA++ for alignment ( marked with * ) . Table 6 : Rules for simplifying the morphological complexity for RU . Table 8 : Results on WMT-2013 ( blindtest ) Table 1 : Russian morphological disambiguation . Table 2 : Evaluation of the Russian n-gram model . Figure 1 : The trigram version of our language model rep- resented as a graphical model . G1w is the unigram model of §2.2 . Table 3 : Evaluation of the Turkish n-gram model . Table 4 : Evaluation of Turkish predictive text input . Figure 3 : A complex Turkish-English word alignment ( alignment points in gray : EM/PY-U ( V ) ; black : PY- U ( S ) ) . Table 5 : Word alignment experiments on English-Turkish ( en-tr ) and English-Czech ( en-cs ) data . Figure 2 : Our alignment model , represented as a graphi- cal model . Table 1 : Key notation . Feature factorings are elaborated in Tab . 2 . Table 2 : Factoring of global feature collections g into f . xji denotes hxi , . . . xj i in sequence x = hx1 , . . .i . Figure 1 : Decoding as lattice parsing , with the highest-scoring translation denoted by black lattice arcs ( others are grayed out ) and thicker blue arcs forming a dependency tree over them . Table 3 : Eq . 9 : Log-likelihood . Eq . 10 : Pseudolikelihood . In both cases we maximize w.r.t . θ. Eqs . 11–13 : Recursive DP equations for summing over t and a . Figure 2 : Comparison of size of k-best list for cube decoding with various feature sets . Table 4 : Feature set comparison ( BLEU ) . Table 5 : QG configuration comparison . The name of each configuration , following Smith and Eisner ( 2006 ) , refers to the relationship between a ( τt ( j ) ) and a ( j ) in τs . Figure 1 : Derivation with λ-DRSs , including β-conversion , for “ A record date ” . Com- binatory rules are indicated by solid lines , semantic rules by dotted lines . Figure 2 : CCG derivation as generated by the C & C tools Figure 3 : Boxer output for Shared Task Text 2 Table 1 : Total corpus sizes ( in sentences ) and number of ( S ) ure and ( P ) ossible alignment links in their respective evaluation sets . Table 2 : Results from the empirical evaluation , including the Bayesian model without PoS tags ( Base- line ) , the alternating alignment-annotation algorithm ( AAA ) , the corresponding method but with super- vised PoS taggers for both languages ( Supervised ) , and comparable previous results on the same data . The number of alignment links |A| , of which |A ∩ S| are considered ( S ) ure , and |A ∩ P | ( P ) ossible , are reported . For convenience , precision ( P ) , recall ( R ) , F1 score ( F ) and Alignment Error Rate ( AER ) are also given . Table 1 : Kendall ’ s ( τ ) correlation over WMT 2013 ( all- en ) , for the full dataset and also the subset of the data containing a noun compound in both the reference and the MT output Table 2 : Pearson ’ s ( r ) correlation results over the WMT all-en dataset , and the subset of the dataset that contains noun compounds Table 4 : Two examples from the all-en dataset . Each example shows a reference translation , and the outputs of two machine translation systems . In each case , the output of MT system 1 is annotated as the better translation . Table 1 : Definitions for the top four senses of “ law ” according to WordNet Table 2 : Example confounders for “ festival ” and “ laws ” and their similarities Figure 1 : Pseudo-word discrimination performance Figure 2 : The error frequency distributions for confusing the correct sense with another sense of the given similarity when using a 5-word co-occurrence window as context . Dashed lines indicate the null models . Table 3 : Unsupervised and Supervised scores on the SemEval-2010 WSI Task for each feature and clustering models , with reference scores for the top performing systems for each evaluation shown below . Figure 1 : A typed narrative chain . The four top arguments are given . The ordering O is not shown . Figure 3 : Graphical view of an unordered schema automatically built starting from the verb ‘ arrest ’ . A β value that encouraged splitting was used . Figure 2 : Merging typed chains into a single unordered Narrative Schema . Figure 4 : Graphical view of an unordered schema automatically built from the verb ‘ convict ’ . Each node shape is a chain in the schema . Figure 5 : Six of the top 20 scored Narrative Schemas . Events and arguments in italics were marked misaligned by FrameNet definitions . * indicates verbs not in FrameNet . - indicates verb senses not in FameNet . Figure 6 : Results with varying sizes of training data . Table 1 : Results of the filtering experiments Table 2 : Comparison of Moses and KIT phrase extraction systems Table 3 : Analysis of context length Table 4 : Translation results for German-English Table 5 : Translation results for English-German Table 7 : Translation results for French-English Table 6 : Translation results for English-French Table 1 : Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag . Across all languages , high performance can be attained by selecting a single tag per word type . Figure 1 : Graphical depiction of our model and summary of latent variables and parameters . The type-level tag assignments T generate features associated with word types W . The tag assignments constrain the HMM emission parameters θ . The tokens w are generated by token-level tags t from an HMM parameterized by the lexicon structure . The hyperparameters α and β represent the concentration parameters of the token- and type-level components of the model respectively . They are set to fixed constants . Figure 2 : Graph of the one-to-one accuracy of our full model ( +FEATS ) under the best hyperparameter setting by iteration ( see Section 5 ) . Performance typically stabi- lizes across languages after only a few number of itera- tions . Table 3 : Multi-lingual Results : We report token-level one-to-one and many-to-one accuracy on a variety of languages under several experimental settings ( Section 5 ) . For each language and setting , we report one-to-one ( 1-1 ) and many- to-one ( m-1 ) accuracies . For each cell , the first row corresponds to the result using the best hyperparameter choice , where best is defined by the 1-1 metric . The second row represents the performance of the median hyperparameter setting . Model components cascade , so the row corresponding to +FEATS also includes the PRIOR component ( see Section 3 ) . Table 2 : Statistics for various corpora utilized in exper- iments . See Section 5 . The English data comes from the WSJ portion of the Penn Treebank and the other lan- guages from the training set of the CoNLL-X multilin- gual dependency parsing shared task . Table 5 : Type-level English POS Tag Ranking : We list the top 5 and bottom 5 POS tags in the lexicon and the predictions of our models under the best hyperparameter setting . Table 6 : Type-level Results : Each cell report the type- level accuracy computed against the most frequent tag of each word type . The state-to-tag mapping is obtained from the best hyperparameter setting for 1-1 mapping shown in Table 3 . Table 4 : Comparison of our method ( FEATS ) to state-of-the-art methods . Feature-based HMM Model ( Berg- Kirkpatrick et al. , 2010 ) : The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm ; Posterior regulariation model ( Graça et al. , 2009 ) : The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint . Table 2 : Results for morphological processing , German→English Table 1 : Results for morphological processing , English→German Table 5 : Number of affected words by OOV- preprocessing Table 7 : Results for OOV-processing and MBR , German→English . Table 6 : Results for OOV-processing and MBR , English→German . Figure 1 : Example of the effects of OOV processing for German→English Figure 1 : Question tracking interface to a summa- rization system . Figure 2 : LexRank example : sentence similarity graph with a cosine threshold of 0.15 . Table 1 : Corpus of complex news stories . Table 5 : Development testing evaluation . Table 6 : Average scores by cluster : baseline versus LR [ 0.20,0.95 ] . Table 2 : Training phase : effect of similarity thresh- old ( a ) on Ave. MRR and TRDR . Table 3 : Training phase : effect of question bias ( d ) on Ave. MRR and TRDR . Table 4 : Training phase : systems outperforming the baseline in terms of TRDR score . Table 8 : Top ranked sentences using baseline system on the question “ What caused the Kursk to sink ? ” . Table 7 : Testing phase : baseline vs. LR [ 0.20,0.95 ] . Table 9 : Top ranked sentences using the LR [ 0.20,0.95 ] system on the question “ What caused the Kursk to sink ? ” Figure 1 : Visualisation examples . Top : named en- tity recognition , middle : dependency syntax , bot- tom : verb frames . Figure 2 : Screenshot of the main BRAT user-interface , showing a connection being made between the annotations for “ moving ” and “ Citibank ” . Figure 3 : Incomplete T RANSFER event indicated to the annotator Figure 4 : The BRAT search dialog Table 1 : Total annotation time , portion spent se- lecting annotation type , and absolute improve- ment for rapid mode . Figure 5 : Example annotation from the BioNLP Shared Task 2011 Epigenetics and Post-translational Modifications event extraction task . Table 1 : The total costs for the three MTurk subtasks in- volved with the creation of our Dialectal Arabic-English parallel corpus . Figure 1 : One possible breakdown of spoken Arabic into dialect groups : Maghrebi , Egyptian , Levantine , Gulf and Iraqi . Habash ( 2010 ) gives a breakdown along mostly the same lines . We used this map as an illustration for annotators in our dialect classification task ( Section 3.1 ) , with Arabic names for the dialects instead of English . Table 2 : Statistics about the training/tuning/test datasets used in our experiments . The token counts are calculated before MADA segmentation . Table 3 : Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal Arabic web text . The morphological segmentation uniformly improves translation quality , but the improvements are more dramatic for MSA than for Dialectal Arabic when comparing similarly-sized training corpora . Table 4 : A comparison of translation quality of Egyptian , Levantine , and MSA web text , using various training corpora . The highest BLEU scores are achieved using the full set of dialectal data ( which combines Levantine and Egyptian ) , since the Egyptian alone is sparse . For Levantine , adding Egyptian has no effect . In both cases , adding MSA to the dialectal data results in marginally worse translations . Figure 3 : Examples of ambiguous words that are trans- lated incorrectly by the MSA-English system , but cor- rectly by the Dialectal Arabic-English system . Figure 2 : Examples of improvement in MT output when training on our Dialectal Arabic-English parallel corpus instead of an MSA-English parallel corpus . Table 5 : The most frequent OOV ’ s ( with counts ≥ 10 ) of the dialectal test sets against the MSA training data . Figure 4 : Learning curves showing the effects of increas- ing the size of dialectal training data , when combined with the 150M-word MSA parallel corpus , and when used alone . Adding the MSA training data is only use- ful when the dialectal data is scarce ( 200k words ) . Table 6 : Results on a truly independent test set , consisting of data harvested from Egyptian Facebook pages that are entirely distinct from the our dialectal training set . The improvements over the MSA baseline are still considerable : +2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set . Table 7 : A comparison of the effectiveness of performing Levantine-to-MSA mapping before translating into English , versus translating directly from Levantine into English . The mapping from Levantine to MSA was done manually , so it is an optimistic estimate of what might be done automatically . Although initially helpful to the MSA baseline system , the usefulness of pivoting through MSA drops as more dialectal data is added , eventually hurting performance . Figure 1 : Three representations of NP modifications : ( a ) the original treebank representation ; ( b ) Selective left-corner representation ; and ( c ) a flat structure that is unambiguously equivalent to ( b ) Table 1 : Conditioning features for the probabilistic CFG used in the reported empirical trials Table 3 : Parser performance on Brown ; E , baselines . Note that the Gildea results are for sentences ≤ 40 words in length . Table 4 : Parser performance on WSJ ; 23 , baselines . Note that the Gildea results are for sentences ≤ 40 words in length . All others include all sentences . Table 6 : Parser performance on WSJ ; 23 , baselines Table 5 : Parser performance on Brown ; E , supervised adaptation Table 8 : Parser performance on WSJ ; 23 , unsupervised adaptation . For all trials , the base training is Brown ; T , the held out is Brown ; H plus the parser output for WSJ ; 24 , and the mixing parameter τA is 0.20e c ( A ) . Table 7 : Parser performance on WSJ ; 23 , supervised adaptation . All models use Brown ; T , H as the out-of-domain treebank . Baseline models are built from the fractions of WSJ ; 2-21 , with no out-of-domain treebank . Table 2 : The MEDLINE semantic categories . Table 1 : Filtered 5-gram dataset statistics . Figure 1 : Performance relationship between WMEB and BASILISK on Sgold UNION Table 3 : Variation in precision with random gold seed sets Table 4 : Bagging with 50 gold seed sets Table 5 : Bagging with 50 unsupervised seed sets Figure 2 : Semantic drift in CELL ( n=20 , m=20 ) Table 7 : Final accuracy with drift detection Table 6 : Semantic drift detection results Figure 1 . PoCoS : Core Scheme , Extended Scheme and language-specific instantiations Table 1 : A summary of the parsing and evaluation sce- narios . X depicts gold information , – depicts unknown information , to be predicted by the system . Table 2 : Overview of participating languages and treebank properties . ’ Sents ’ = number of sentences , ’ Tokens ’ = number of raw surface forms . ’ Lex . size ’ and ’ Avg . Length ’ are computed in terms of tagged terminals . ‘ NT ’ = non- terminals in constituency treebanks , ‘ Dep Labels ’ = dependency labels on the arcs of dependency treebanks . – A more comprehensive table is available at http : //www.spmrl.org/spmrl2013-sharedtask.html/ # Prop . Figure 1 : File formats . Trees ( a ) and ( b ) are aligned constituency and dependency trees for a mockup English example . Boxed labels are shared across the treebanks . Figure ( c ) shows an ambiguous lattice . The red part represents the yield of the gold tree . For brevity , we use empty feature columns , but of course lattice arcs may carry any morphological features , in the FEATS CoNLL format . Table 3 : Dependency parsing : LAS scores for full and 5k training sets and for gold and predicted input . Results in bold show the best results per language and setting . Table 4 : Dependency Parsing : MWE results Table 5 : Constituent Parsing : ParsEval F-scores for full and 5k training sets and for gold and predicted input . Results in bold show the best results per language and setting . Table 6 : Constituent Parsing : Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input . Table 8 : Realistic Scenario : Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario . Top upper part refers to constituency results , the lower part refers to dependency results . Table 7 : Realistic Scenario : Tedeval Labeled Accuracy and Exact Match for the Raw scenario . The upper part refers to constituency results , the lower part refers to dependency results Figure 2 : The correlation between treebank size , label set size , and LAS scores . x : treebank size / # labels ; y : LAS ( % ) treebank Correlation between treebank size ( # Non sizenumber terminal ) , / # labelsof sentences ( # sent ) and mean F1 Figure 4 : The correlation between the non terminals per sentence ratio and Leaf Accuracy ( macro ) scores . x : # non terminal/ # sentence ; y : Acc . ( % ) Table 11 : Cross Framework Evaluation : Unlabeled TedEval on generalized gold trees in gold scenario , trained on 5k sentences and tested on 5k terminals . Table 10 : Cross-Language Evaluation : Unlabeled TedEval Results in gold input scenario , On a 5k-sentences set set and a 5k-terminals test set . The upper part refers to constituency parsing and the lower part refers to dependency parsing . For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics . Table 9 : Labeled and Unlabeled TedEval Results for raw Scenarios , Trained on 5k sentences and tested on 5k terminals . The upper part refers to constituency parsing and the lower part refers to dependency parsing . Table 1 : The parts of taxonomic names Figure 1 : The Classification Process Table 4 : Comparison to Related Approaches Figure 1 : An excerpt from the text , with core- ferring noun phrases annotated . English trans- lation in italics . Table 1 : Mention Detection Results Table 2 : Features and functions used in clustering algorithm Table 3 : Coreference Resolution Performance Table 4 : Contribution of individual features to overall performance . Table 1 . Examples of the ACE Relation Types Figure 1 . The RCM structure Figure 2 . System Pipeline ( Test Procedure ) Table 5 . Chinese system performance with system mentions and system relations Table 4 . Performance of English system with system mentions and system relations Table 2 . Performance of English system with perfect mentions and perfect relations Table 3 . Performance of Chinese system with perfect mentions and perfect relations Table 1 . A brief description of the tested parsers . Note that the Tune data is not the data used to train the individual parsers . Higher numbers in the right column reflect just the fact that the Test part is slightly easier to parse . Table 2 : Comparison of various groups of parsers . All percentages refer to the share of the total words in test data , attached correctly . The “ single parser ” part shows shares of the data where a single parser is the only one to know how to parse them . The sizes of the shares should correlate with the uniqueness of the individual parsers ’ strategies and with their contributions to the overall success . The “ at least ” rows give clues about what can be got by majority voting ( if the number represents over 50 % of parsers compared ) or by hypothetical oracle selection ( if the number represents 50 % of the parsers or less , an oracle would generally be needed to point to the parsers that know the correct attachment ) . Table 3 : Results of voting experiments . Table 4 : Voting under hand-invented schemes . Table 5 : Contexts where ec is better than mc+dz . J^ are coordination conjunctions , # is the root , V* are verbs , Nn are nouns in case n , R* are preposi- tions , Z* are punctuation marks , An are adjectives . Figure 1 . The decision tree for ec+mc+zž , learned by C5 . Besides pairwise agreement be- tween the parsers , only morphological case and negativeness matter . Table 6 : Context-sensitive voting . Contexts trained on the Tune data set , accuracy figures apply to the Test data set . Context-free results are given for the sake of comparison . Table 7 : Unbalanced vs. balanced combining . All runs ignored the context . Evaluated on the Test data set . Table 1 : Arabic Verbal Inflection Table 2 : Arabic Nominal Inflection - Broken Plural Table 3 : Adjective Full Inflection Table 5 : Verb Stem Alternation Table 4 : Noun Stem Alternation Table 6 : Derivation by Means of Adding a Suffix Table 9 : Arabic Clitics - Example 1 Table 8 : Syncretism Example 2 Table 7 : Syncretism Example 1 Table 12 : Arabic Pronoun Dropping Table 11 : Compounding in Russian Table 10 : Arabic Clitics - Example 2 Table 15 : Challenge in Elliptical Constructions Table 14 : Arabic Equational Sentences Table 13 : Zero-Copula in Russian Table 16 : Arabic Order-Free Structure Table 17 : Six Accepted Word Orders in Russian Table 18 : Arabic idafa Construct Table 19 : Phonetic Stress in Russian Table 20 : Phonetic Stress in Russian : Fake Homograph Table 21 : Arabic Vocalization Problem Table 23 : A snippet from Russian and Czech tag comparison Table 24 : Splitting Compounds in Russian Table 25 : Arabic Tokenization Schemes Table 26 : Arabic Tokenization Schemes Table 27 : Performance comparison between original and universal tagsets Table 28 : Arabic POS Studies with Different Tagsets Table 29 : Different Arabic Transliterations of `` Los Angeles '' Figure 2 : Shallow parsing : chunking ( Extracted from : http : //kontext.fraunhofer.de ) Figure 3 : An example CCG parse obtained from [ 60 ] Figure 4 : Relation phrase compliance with semantic/lexical constraints [ 32 ] Figure 5 : Dependency parsing and clause constituents Figure 6 : Relation extraction rules used by Gamallo et.al . [ 36 ] Figure 7 : A comparison between QA semantic parsing approaches [ 12 ] Figure 8 : a ) Parsing of input sentence [ 78 ] Table 4 : Combined systems ( Basque ) in cross- validation , best recall in bold . Only vector ( f ) was used for combination . Table 3 : Single systems ( Basque ) in cross- validation , sorted by recall . Table 1 : Single systems ( English ) in cross- validation , sorted by recall . Table 2 : Combined systems ( English ) in cross- validation , best recall in bold . Table 5 : Oﬃcial results for the English and Basque lexical tasks ( recall ) . Figure 1 : Annotated RST Tree for example ( 4 ) . Figure 1 : Properties of the training and test sets used in the shared task . The training data is the Europarl cor- pus , from which also the in-domain test set is taken . There is twice as much language modelling data , since training data for the machine translation system is filtered against sentences of length larger than 40 words . Out-of-domain test data is from the Project Syndicate web site , a compendium of political commentary . Figure 2 : Participants in the shared task . Not all groups participated in all translation directions . Figure 3 : Annotation tool for manual judgement of adequacy and fluency of the system output . Translations from 5 randomly selected systems for a randomly selected sentence is presented . No additional information beyond the instructions on this page are given to the judges . The tool tracks and reports annotation speed . Figure 5 : Evaluation scores for in-domain and out- of-domain test sets , averaged over all systems Figure 4 : Number and ratio of statistically signifi- cant distinction between system performance . Au- tomatic scores are computed on a larger tested than manual scores ( 3064 sentences vs. 300–400 sen- tences ) . Figure 6 : Average scores for different language pairs . Manual scoring is done by different judges , resulting in a not very meaningful comparison . Figure 7 : Evaluation of translation to English on in-domain test data Figure 8 : Evaluation of translation from English on in-domain test data Figure 9 : Evaluation of translation to English on out-of-domain test data Figure 10 : Evaluation of translation from English on out-of-domain test data Figure 11 : Correlation between manual and automatic scores for French-English Figure 12 : Correlation between manual and automatic scores for Spanish-English Figure 13 : Correlation between manual and automatic scores for German-English Figure 14 : Correlation between manual and automatic scores for English-French Figure 15 : Correlation between manual and automatic scores for English-Spanish Figure 16 : Correlation between manual and automatic scores for English-German Figure 1 : Our probabilistic model : a question x is mapped to a latent logical form z , which is then evaluated with respect to a world w ( database of facts ) , producing an answer y . We represent logical forms z as labeled trees , induced automatically from ( x , y ) pairs . Table 1 : Possible relations appearing on the edges of a DCS tree . Here , j , j 0 ∈ { 1 , 2 , . . . } and i ∈ { 1 , 2 , . . . } ∗ . Figure 2 : ( a ) An example of a DCS tree ( written in both the mathematical and graphical notation ) . Each node is labeled with a predicate , and each edge is labeled with a relation . ( b ) A DCS tree z with only join relations en- codes a constraint satisfaction problem . ( c ) The denota- tion of z is the set of consistent values for the root node . Figure 3 : Examples of DCS trees that use the aggregate relation ( Σ ) to ( a ) compute the cardinality of a set and ( b ) take the average over a set . Figure 4 : Example DCS trees for utterances in which syntactic and semantic scope diverge . These trees reflect the syntactic structure , which facilitates parsing , but importantly , these trees also precisely encode the correct semantic scope . The main mechanism is using a mark relation ( E , Q , or C ) low in the tree paired with an execute relation ( Xi ) higher up at the desired semantic point . Figure 5 : Example of the denotation for a DCS tree with a compare relation C. This denotation has two columns , one for each active node—the root node state and the marked node size . Table 3 : Accuracy ( recall ) of systems on the two bench- marks . The systems are divided into three groups . Group 1 uses 10-fold cross-validation ; groups 2 and 3 use the in- dependent test set . Groups 1 and 2 measure accuracy of logical form ; group 3 measures accuracy of the answer ; but there is very small difference between the two as seen from the Kwiatkowski et al . ( 2010 ) numbers . Our best system improves substantially over past work , despite us- ing no logical forms as training data . Table 2 : Results on G EO with 250 training and 250 test examples . Our results are averaged over 10 random 250+250 splits taken from our 600 training examples . Of the three systems that do not use logical forms , our two systems yield significant improvements . Our better sys- tem even outperforms the system that uses logical forms . Figure 1 : Graphical model for PLTM . Figure 2 : EuroParl topics ( T=400 ) Figure 4 : Smoothed histograms of the Jensen-Shannon Figure 3 : Smoothed histograms of the probability of the Figure 5 : Topics sorted by number of words assigned . Table 4 : Topics are meaningful within languages but di- Figure 6 : Are the single most probable words for a given Figure 7 : Percent of query language documents for which Figure 8 : Squares represent the proportion of tokens in each language assigned to a topic . The left topic , world ski km won , Figure 9 : Wikipedia topics ( T=400 ) . Figure 1 : Structure of Cascaded Linear Model . |R| denotes the scale of the feature space of the core perceptron . Table 1 : Feature templates and instances . Suppose we are considering the third character ” U ” in ” e U /¡ ” . Figure 2 : Feature space growing curve . The horizontal scope X [ i : j ] denotes the introduction of different tem- plates . X [ 0:5 ] : Cn ( n = −2..2 ) ; X [ 5:9 ] : Cn Cn+1 ( n = −2..1 ) ; X [ 9:10 ] : C−1 C1 ; X [ 10:15 ] : C0 Cn ( n = −2..2 ) ; X [ 15:19 ] : C0 Cn Cn+1 ( n = −2..1 ) ; X [ 19:20 ] : C0 C−1 C1 ; X [ 20:21 ] : W0 ; X [ 21:22 ] : W−1 W0 . W0 de- notes the current considering word , while W−1 denotes the word in front of W0 . All the data are collected from the training procedure on MSR corpus of SIGHAN bake- off 2 . Figure 3 : Averaged perceptron learning curves with Non- lexical-target and Lexical-target feature templates . Table 2 : F-measure on SIGHAN bakeoff 2 . SIGHAN best : best scores SIGHAN reported on the four corpus , cited from Zhang and Clark ( 2007 ) . Table 4 : Contribution of each feture . ALL : all features , PER : perceptron model , WLM : word language model , PLM : POS language model , GPR : generating model , LPR : labelling model , LEN : word count penalty . Table 3 : F-measure on segmentation and Joint S & T of perceptrons . POS- : perceptron trained without POS , POS+ : perceptron trained with POS . Table 1 : Various features used for computing edge weights between foreign trigram types . Figure 1 : An excerpt from the graph for Italian . Three of the Italian vertices are connected to an automatically la- beled English vertex . Label propagation is used to propa- gate these tags inwards and results in tag distributions for the middle word of each Italian trigram . Table 2 : Part-of-speech tagging accuracies for various baselines and oracles , as well as our approach . “ Avg ” denotes macro-average across the eight languages . Figure 2 : Tags produced by the different models along with the reference set of tags for a part of a sentence from the Italian test set . Italicized tags denote incorrect labels . Table 3 : Size of the vocabularies for the “ No LP ” and “ With LP ” models for which we can impose constraints . Figure 1 : A hypothetical semantic space for horse and run Figure 2 : Distribution of elicited ratings for High and Low similarity items Table 1 : Example Stimuli with High and Low similarity landmarks Table 2 : Model means for High and Low similarity items and correlation coefficients with human judgments ( * : p < 0.05 , ** : p < 0.01 ) Figure 3 : Distribution of predicted similarities for the vector multiplication model on High and Low similarity items Figure 1 : Lookup of “ is one of ” in a reverse trie . Children of each node are sorted by vocabulary identifier so order is consistent but not alphabetical : “ is ” always appears be- fore “ are ” . Nodes are stored in column-major order . For example , nodes corresponding to these n-grams appear in this order : “ are one ” , “ < s > Australia ” , “ is one of ” , “ are one of ” , “ < s > Australia is ” , and “ Australia is one ” . Figure 2 : Speed in lookups per microsecond by data structure and number of 64-bit entries . Performance dips as each data structure outgrows the processor ’ s 12 MB L2 cache . Among hash tables , indicated by shapes , probing is initially slower but converges to 43 % faster than un- ordered or hash set . Interpolation search has a more ex- pensive pivot function but does less reads and iterations , so it is initially slower than binary search and set , but be- comes faster above 4096 entries . Table 1 : Single-threaded speed and memory use on the perplexity task . The P ROBING model is fastest by a sub- stantial margin but generally uses more memory . T RIE is faster than competing packages and uses less memory than non-lossy competitors . The timing basis for Queries/ms in- cludes kernel and user time but excludes loading time ; we also subtracted time to run a program that just reads the query file . Peak virtual memory is reported ; final resident memory is similar except for BerkeleyLM . We tried both aggressive reading and lazy memory mapping where appli- cable , but results were much the same . Table 3 : Multi-threaded time and memory consumption of Moses translating 3003 sentences on eight cores . Our code supports lazy memory mapping ( -L ) and prefault- ing ( -P ) with MAP POPULATE , the default . IRST is not threadsafe . Time for Moses itself to load , including load- ing the language model and phrase table , is included . Along with locking and background kernel operations such as prefaulting , this explains why wall time is not one-eighth that of the single-threaded case . Table 2 : Single-threaded time and memory consumption of Moses translating 3003 sentences . Where applicable , models were loaded with lazy memory mapping ( -L ) , prefaulting ( -P ) , and normal reading ( -R ) ; results differ by at most than 0.6 minute . Table 4 : CPU time , memory usage , and uncased BLEU ( Papineni et al. , 2002 ) score for single-threaded Moses translating the same test set . We ran each lossy model twice : once with specially-tuned weights and once with weights tuned using an exact model . The difference in BLEU was minor and we report the better result . Figure 1 : The first sense heuristic compared with the SENSEVAL -2 English all-words task results Table 2 : Evaluating predominant sense information on SENSEVAL -2 all-words data . Table 3 : Domain specific results Figure 2 : Distribution of domain labels of predom- inant senses for 38 polysemous words ranked using the SPORTS and FINANCE corpus . Figure 1 : The Lattice for the Hebrew Phrase bclm hneim Table 2 : Segmentation , Parsing and Tagging Results us- ing the Setup of ( Cohen and Smith , 2007 ) ( sentence length ≤ 40 ) . The Models ’ are Ordered by Performance . Table 1 : Segmentation , tagging and parsing results on the Standard dev/train Split , for all Sentences Figure 1 : Dependency graph for Czech sentence from the Prague Dependency Treebank1 Figure 2 : Projectivized dependency graph for Czech sentence Table 1 : Encoding schemes ( d = dependent , h = syntactic head , p = path ; n = number of dependency types ) Table 2 : Features used in predicting the next parser action Table 3 : Non-projective sentences and arcs in PDT and DDT ( NonP = non-projective ) Table 4 : Percentage of non-projective arcs recovered correctly ( number of labels in parentheses ) Table 6 : Precision , recall and F-measure for non-projective arcs Table 5 : Parsing accuracy ( AS = attachment score , EM = exact match ; U = unlabeled , L = labeled ) Table 1 : Dependency accuracy on 13 languages . Unlabeled ( UA ) and Labeled Accuracy ( LA ) . Table 2 : Error analysis of parser components av- eraged over Arabic , Bulgarian , Danish , Dutch , Japanese , Portuguese , Slovene , Spanish , Swedish and Turkish . N/P : Allow non-projective/Force pro- jective , S/A : Sequential labeling/Atomic labeling , M/B : Include morphology features/No morphology features . 