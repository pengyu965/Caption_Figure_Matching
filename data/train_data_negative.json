[
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1106/parts/0-Figure-c3.png",
        "Caption": "Figure 4: Definition of the operations used to transform the structure of the underspecified logical form l0 to match the ontology O. The function type(c) calculates a constant c\u2019s type. The function freev(lf ) returns the set of variables that are free in lf (not bound by a lambda term or quantifier). The function subexps(lf ) generates the set of all subexpressions of the lambda calculus expression lf . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-1065/parts/0-Figure-c3.png",
        "Caption": "Figure 4: Chunk-based translation model. The words in bold are head words.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1109/parts/0-Table-c3.png",
        "Caption": "Figure 1 Relation between number of classes and alternations. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4917/parts/0-Table-c7.png",
        "Caption": "Figure 1: An excerpt from the graph for Italian. Three of the Italian vertices are connected to an automatically la- beled English vertex. Label propagation is used to propa- gate these tags inwards and results in tag distributions for the middle word of each Italian trigram. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1141/parts/0-Figure-c1.png",
        "Caption": "Table 4: A subjective pronoun resolution example",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-2606/parts/0-Table-c3.png",
        "Caption": "Table 4: Accuracies (%) for Coarse and Fine-Grained PSD, Using MALT and Heuristics. Sorted by preposition. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P04-1017/parts/0-Table-c4.png",
        "Caption": "Table 3: Comparing our method with the state-of-the-art CWS systems.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1012/parts/0-Table-c6.png",
        "Caption": "Figure 3: Smoothed precision curves over the five corpora.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-4009/parts/0-Table-c4.png",
        "Caption": "Table 3: Reranking results of the three tagging   kernels on the Italian and English testset. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1006/parts/0-Table-c2.png",
        "Caption": "Figure 2. F1-measure with \uf062 in [0,1]",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1126/parts/0-Table-c2.png",
        "Caption": "Table 5. Accuracy of 5-fold cross-validation with self-  extracted semantic features based on different levels             of syntactic/semantic relations ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c13.png",
        "Caption": "Table 1 Distribution of antecedent NP types in the other-anaphora data set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1155/parts/0-Table-c4.png",
        "Caption": "Table 1: Automatically generated training set examples.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2210/parts/0-Table-c1.png",
        "Caption": "Figure 1: The density of the F1 -scores with the three approaches. The prior used is a symmetric Dirichlet with \u03b1 = 0.1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-1017/parts/0-Figure-c2.png",
        "Caption": "Table 2: Word segmentation on IWSLT data sets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S12-1023/parts/0-Table-c4.png",
        "Caption": "Figure 2: Automatic evaluation with 50% of Freebase relation data held out and 50% used in training on the 102 largest relations we use. Precision for three different feature sets (lexical features, syntactic features, and both) is reported at recall levels from 10 to 100,000. At the 100,000 recall level, we classify most of the instances into three relations: 60% as location-contains, 13% as person-place-of-birth, and 10% as person-nationality. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1016/parts/0-Table-c9.png",
        "Caption": "Table 5 Some words extracted from the large corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C08-1002/parts/0-Figure-c1.png",
        "Caption": "Table 4: Examples of high-weight features for several relations. Key: SYN = syntactic feature; LEX = lexical feature; x = reversed; NE# = named entity tag of entity. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P08-2062/parts/0-Figure-c2.png",
        "Caption": "Figure 1: All the parameters of WSMs described in Section 2 used in all our experiments. Semicolon denotes OR. All the examined combinations of parameters are implied from reading the diagram from left to right. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PTP_n09/parts/0-Table-c2.png",
        "Caption": "Table 1: POS/morphological feature accuracies on the development sets.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c8.png",
        "Caption": "Table 2: Results for different predictor configura- tions. Numbers give % reductions in keystrokes. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1079/parts/0-Table-c2.png",
        "Caption": "Figure 1. Linking FrameNet frames and VerbNet classes ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N03-1010/parts/0-Figure-c3.png",
        "Caption": "Table 5: Classification results with decision tree on joined feature set (Method 5) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S0885/parts/0-Figure-c3.png",
        "Caption": "Figure 6: The 10-bipartite clique.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1083/parts/0-Figure-c4.png",
        "Caption": "Table 2: MRR of baseline and reinforced matrices",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D11-1076/parts/0-Figure-c5.png",
        "Caption": "Table 1: Test verbs and their monosemous/polysemic gold standard senses",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-2207/parts/0-Table-c5.png",
        "Caption": "Table 1 Related research integrating context into word-based SMT (WB-SMT) models",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-1065/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Architecture of Name-aware Machine Translation System.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1634/parts/0-Figure-c5.png",
        "Caption": "Figure 1: POS Tagging of Unknown Word using Contextual and Lexical features in a Sequential Model. The input for capitalized classifier has 2 values and therefore 2 ways to create confusion                                      k\u0097 sets. There are at most \u0089\u0095\u0094 &F\u0096 \u0081 \u0081 +!\u0098 different in- puts for the suffix classifier (26 character + 10 digits + 5 other symbols), therefore suffix may                      k\u0097 emit up to \u0089 \u0094 &R\u0096 \u0081 \u0081 +R\u0098 confusion sets. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1053/parts/0-Table-c4.png",
        "Caption": "Table 2: Word segmentation on IWSLT data sets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Figure-c10.png",
        "Caption": "Figure 1: Illustration of entity-relationship graphs",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-2123/parts/0-Figure-c1.png",
        "Caption": "Table 2 Number of tweets mentioning party leaders. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P10-1124/parts/0-Table-c2.png",
        "Caption": "Table 4 Standards and corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2101/parts/0-Figure-c6.png",
        "Caption": "Table 2: Discourse-new prediction results by Bean and Riloff ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J06-2001/parts/0-Table-c6.png",
        "Caption": "Table 1: Accuracy of maximum entropy system using different subsets of features for S ENSEVAL -2 verbs.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P05-1004/parts/0-Table-c1.png",
        "Caption": "Table 1: An example of NE and non-NE",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Figure-c6.png",
        "Caption": "Table 2: Results of the baseline model: best guess",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Figure-c11.png",
        "Caption": "Table 1: Snapshot of the supersense-annotated data. The 7 article titles (translated) in each domain, with total counts of sentences, tokens, and supersense mentions. Overall, there are 2,219 sentences with 65,452 tokens and 23,239 mentions (1.3 tokens/mention on average). Counts exclude sentences marked as problematic and mentions marked ?. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W13-2501/parts/0-Table-c1.png",
        "Caption": "Table 7. Weight of co-occurring words",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1005/parts/0-Figure-c2.png",
        "Caption": "Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2\u201323) and the ATB (p1\u20133). English parsing evaluations usually report results on sentences up to length 40. Arabic sentences of up to length 63 would need to be evaluated to account for the same fraction of the data. We propose a limit of 70 words for Arabic parsing evaluations. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Figure-c5.png",
        "Caption": "Table 3: Results of the experiment.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1127/parts/0-Figure-c6.png",
        "Caption": "Table 5: Type-level English POS Tag Ranking: We list the top 5 and bottom 5 POS tags in the lexicon and the predictions of our models under the best hyperparameter setting. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1125/parts/0-Table-c3.png",
        "Caption": "Table 3: Feature templates used for the chunk s := ws ws+1 ... we where ws and we represent the words at the beginning and ending of the target chunk respectively. pi is the part of speech tag of wi and sci is the shallow parse result of wi . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c26.png",
        "Caption": "Figure 1: Finite-state cascades for five natural language problems.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D10-1100/parts/0-Table-c5.png",
        "Caption": "Fig. 1. System architecture overview",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1126/parts/0-Figure-c5.png",
        "Caption": "Table 1: Test verbs and their monosemous/polysemic gold standard senses",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W02-1020/parts/0-Table-c1.png",
        "Caption": "Figure 9: Word-aligned DRG for the sentence \u201cMichelle thinks that Obama smokes.\u201d ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4011/parts/0-Figure-c3.png",
        "Caption": "Table 3: Clustering evaluation for the experiment with Named Entities ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-1025/parts/0-Table-c2.png",
        "Caption": "Table 1: Scores for CityU corpus",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-4009/parts/0-Table-c6.png",
        "Caption": "Figure 1: Example of a prediction for English to French translation. s is the source sentence, h is the part of its translation that has already been typed, x\u2217 is what the translator wants to type, and x is the prediction. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1104/parts/0-Table-c2.png",
        "Caption": "Table 1: Training, tuning, and test conditions",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1117/parts/0-Table-c5.png",
        "Caption": "Table 3: Impact of removing individual measures and us- ing a tuned weighting scheme (all numbers in %); results that significantly differ from Full are marked with aster- isks (* p<0.05; ** p<0.01). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4904/parts/0-Table-c6.png",
        "Caption": "Figure 3: Boundary information is added to states to cal- culate the bracket scores in the face of word segmentation errors. Left: the original parse tree, Right: the converted parse tree. The numbers in the brackets are the indices of the character boundaries based on word segmentation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P101167/parts/0-Table-c5.png",
        "Caption": "Figure 12 Interdigitation FSRA \u2013 general. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P08-1090/parts/0-Figure-c1.png",
        "Caption": "Figure 2: LexRank example: sentence similarity graph with a cosine threshold of 0.15. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Table-c1.png",
        "Caption": "Figure 6: The 10-bipartite clique.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-3004/parts/0-Table-c3.png",
        "Caption": "Table 1 Closed test, in percentages (%)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1161/parts/0-Figure-c3.png",
        "Caption": "Figure 1: A tree showing head information",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-2302/parts/0-Table-c2.png",
        "Caption": "Figure 3: Discovered metaphorical associations",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-2040/parts/0-Table-c6.png",
        "Caption": "Table 3. GETARUNS pronouns collapsed at structural level",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N06-1027/parts/0-Table-c6.png",
        "Caption": "Table 7: Non-anaphoric DNP examples",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3812/parts/0-Figure-c9.png",
        "Caption": "Table 1: Examples of DTs and their ICD-codes.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1155/parts/0-Table-c3.png",
        "Caption": "Figure 1: Example of word alignment",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D11-1095/parts/0-Table-c4.png",
        "Caption": "Figure 1: Rank trajectories of 4 LDA inferred topics, with incremental topic inference. The x-axis indicates the utterance number. The y-axis indicates a topic\u2019s rank at each utterance. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P04-1017/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Histogram of token movement size ver- sus its occurrences performed by the model Neu- big on the source english data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6213/parts/0-Figure-c2.png",
        "Caption": "Table 2: Overall performance of the 3 systems",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D10-1042/parts/0-Table-c4.png",
        "Caption": "Table 6 Results for the pseudo-disambiguation task with one-fifth training data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1044/parts/0-Figure-c2.png",
        "Caption": "Table 1: Characteristics of the parallel corpus used for experiments. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E12-1020/parts/0-Figure-c1.png",
        "Caption": "Table 10 The comparison of overall accuracies of various joint segmentor and POS-taggers by 10-fold cross validation using CTB. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3909/parts/0-Table-c8.png",
        "Caption": "Table 4: Lexicon-based phrase labeling",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1143/parts/0-Figure-c3.png",
        "Caption": "Table 3: Accuracy for Different Part-Of-Speech",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3604/parts/0-Table-c3.png",
        "Caption": "Table 1: Conditioning features for the probabilistic CFG used in the reported empirical trials ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c24.png",
        "Caption": "Figure 8 Algorithm min-jumps to compute the minimum number of needed jumps D(cJ1 , j) to complete the translation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1056/parts/0-Figure-c2.png",
        "Caption": "Table 6: Rule types in SSTb and HeiST",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-3209/parts/0-Figure-c3.png",
        "Caption": "Table 2: Evaluation of the manual annotation improvement - summarization ratio: 30%.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C08-1002/parts/0-Table-c1.png",
        "Caption": "Table 6 Comparing clustering initializations on D1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1016/parts/0-Table-c11.png",
        "Caption": "Figure 1. An example discussion thread",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1004/parts/0-Table-c5.png",
        "Caption": "Table 5 Comparative performance on MSRPC. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0711/parts/0-Table-c13.png",
        "Caption": "Table 7: Simple parser vs full parser \u2013 syntactic quality. Trained on first 5,000 sentences of the training set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J09-3004/parts/0-Table-c4.png",
        "Caption": "Figure 1 A layered POS tag representation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P00-1022/parts/0-Figure-c3.png",
        "Caption": "Table 6. Thread Classification Results",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1114/parts/0-Figure-c1.png",
        "Caption": "Table 1: Basic features used in the maximum entropy model.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N06-1027/parts/0-Table-c4.png",
        "Caption": "Table 5: The coreference systems that achieved the highest F-measure scores for each test set and scorer combination. The average rank of the candidate partitions produced by each system for the corresponding test set is also shown. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmert_n09/parts/0-Table-c2.png",
        "Caption": "Figure 4: Smoothed recall curves over the five corpora.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J10-3003/parts/0-Figure-c6.png",
        "Caption": "Table 4: Performance on Ar-En with basic (left) and sparse (right) feature sets on MT05 and MT08.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P01-1005/parts/0-Table-c4.png",
        "Caption": "Table 3: Translation results for English-French",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09_qwn/parts/0-Table-c4.png",
        "Caption": "Table 3: Examples of correct (above) and incorrect (below) alignments ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-1068/parts/0-Table-c1.png",
        "Caption": "Figure 1: Tuple extraction from a sentence pair.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1046/parts/0-Figure-c1.png",
        "Caption": "Table 2 The similarity score features used to represent pairs of templates. The columns specify the corpus over which the similarity score was computed, the template representation, the similarity measure employed, and the feature representation (as described in Section 4.1). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-0705/parts/0-Table-c3.png",
        "Caption": "Table 1: Comparison of average per-document ter- comTER with invWER on the EVAL07 GALE Newswire (\u201cNW\u201d) and Weblogs (\u201cWB\u201d) data sets. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1727/parts/0-Table-c1.png",
        "Caption": "Table 5: Performance (UAS/LAS) of the reranker on the development sets. Baseline denotes our baseline. Ranked-dflt and Ranked denote the default and optimized ranker feature sets, respectively. Oracle denotes the oracle scores. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1117/parts/0-Table-c4.png",
        "Caption": "Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in bold show the best results per language and setting. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2211/parts/0-Table-c10.png",
        "Caption": "Figure 5: Topics sorted by number of words assigned.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P04-1017/parts/0-Table-c1.png",
        "Caption": "Table 2: Sources of Dictionaries",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Table-c3.png",
        "Caption": "Figure 3: Comparison of paraphrase generators. Top: the MOSES baseline; middle and bold: the \u201ctrue-score\u201d MCPG; down: the \u201ctranslator\u201d MCPG. The use of \u201ctrue-score\u201d improves the MCPG per- formances. MCPG reaches MOSES performance level. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1115/parts/0-Table-c9.png",
        "Caption": "Figure 1: Caseframe Network Examples",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P01-1027/parts/0-Table-c3.png",
        "Caption": "Figure 5: Multiple Analyses for suis",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J02-2003/parts/0-Table-c6.png",
        "Caption": "Table 3: Translation Candidates for \u8e81\u9b31\u75c5 (manic- depression) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-2003/parts/0-Table-c8.png",
        "Caption": "Table 5: Results of the fill-in-the-blank exercise",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-1004/parts/0-Table-c8.png",
        "Caption": "Figure 1: Example of a long jump alignment grid. All possible deletion, insertion, identity and substitution op- erations are depicted. Only long jump edges from the best path are drawn. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J06-1004/parts/0-Figure-c8.png",
        "Caption": "Table 1: Relation types for ACE 05 corpus",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1109/parts/0-Figure-c4.png",
        "Caption": "Table 1: Results obtained by applying different types of features in isolation to the Baseline system.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-1068/parts/0-Figure-c3.png",
        "Caption": "Table 2: Results on G EO with 250 training and 250 test examples. Our results are averaged over 10 random 250+250 splits taken from our 600 training examples. Of the three systems that do not use logical forms, our two systems yield significant improvements. Our better sys- tem even outperforms the system that uses logical forms. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1138/parts/0-Figure-c5.png",
        "Caption": "Table 4: Gross statistics for several different treebanks. Test set OOV rate is computed using the following splits: ATB (Chiang et al., 2006); CTB6 (Huang and Harper, 2009); Ne- gra (Dubey and Keller, 2003); English, sections 2-21 (train) and section 23 (test). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1044/parts/0-Table-c4.png",
        "Caption": "Figure 7 A scenario where ILP-Global makes a mistake, but ILP-Local is correct. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09prod/parts/0-Figure-c1.png",
        "Caption": "Table 2: Statistics of training, development and test data for NIST task. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1016/parts/0-Table-c5.png",
        "Caption": "Table 1: normalized Mutual Information values for three graphs and different iterations in %. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09_csl2013/parts/0-Figure-c5.png",
        "Caption": "Table 1: Results on Penn (English) Treebank, Wall Street Journal, sentences with 100 words or fewer.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1048/parts/0-Figure-c1.png",
        "Caption": "Table 1: Success rate of anaphora resolution",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2017/parts/0-Figure-c3.png",
        "Caption": "Table 2: Results \u2014 Evaluation A.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-2206/parts/0-Table-c2.png",
        "Caption": "Table 3. Accuracy of the proposed system on a set which contains misspelled queries that the users had reformulated ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1040/parts/0-Table-c2.png",
        "Caption": "Figure 1: A NE detection window",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-1045/parts/0-Table-c5.png",
        "Caption": "Table 1: Performance results on the SemEval-2010 WSI Task, with rank shown in parentheses. Refer- ence scores of the best submitted systems are shown in the bottom. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1084/parts/0-Table-c5.png",
        "Caption": "Figure 2: During training, a classified instance (in this case for the confusible pair {then , than }) are generated from a sentence. During testing, a similar instance is generated. The classifier decides what the corresponding class, and hence, which word should be the focus word. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Figure-c13.png",
        "Caption": "Table 2 Domain/style distribution in the MSR test corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-3236/parts/0-Table-c1.png",
        "Caption": "Figure 1: Dependency structure of text. Tree skeleton in bold ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1115/parts/0-Table-c2.png",
        "Caption": "Figure 2: Learning curve with different sizes of labeled data ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PS15684_w09/parts/0-Figure-c5.png",
        "Caption": "Table 3: Effectiveness of combining the two scor- ing methods. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-2007/parts/0-Table-c2.png",
        "Caption": "Table 4: Performance of different FS machines in terms of the percentage of unclassified entries. All the classified entries were correctly classified, yielding, as a result, a precision of 100%. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-2302/parts/0-Table-c1.png",
        "Caption": "Table 1. Test corpora details",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1143/parts/0-Figure-c2.png",
        "Caption": "Table 3: Comparison of performance across the five PPI corpora",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E06-1030/parts/0-Table-c3.png",
        "Caption": "Table 2: Best results: For English, name lists are used. For German, part-of-speech tags are used ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N07-1024/parts/0-Table-c3.png",
        "Caption": "Fig. 4. Examples of rules used during decoding.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2204/parts/0-Figure-c1.png",
        "Caption": "Figure 8: Contribution of feature sets (material).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1109/parts/0-Figure-c3.png",
        "Caption": "Table 5: New training and testing procedures",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PS15684_w09/parts/0-Table-c1.png",
        "Caption": "Table 5: N/P classifier with learned SWSD integration",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N06-1027/parts/0-Table-c1.png",
        "Caption": "Table 3: Statistics of the Verbmobil test corpus for German-to-English translation. Unknowns are word forms not contained in the training corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-3114/parts/0-Figure-c7.png",
        "Caption": "Table 3. Average precisions over the 10 corpora of different window size (3 seeds)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1084/parts/0-Figure-c5.png",
        "Caption": "Table 3: Experimental results for the two methods on the five corpora. PRE denotes precision, REC denotes recall, and F1 denotes F1-Measure. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P59105ca/parts/0-Figure-c8.png",
        "Caption": "Table 2: Filters applied to candidate pair (H, S)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P87-94/parts/0-Table-c2.png",
        "Caption": "Table 2: Training Data Sizes for Common ESL Confused Words ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1115/parts/0-Figure-c3.png",
        "Caption": "Table 28: Arabic POS Studies with Different Tagsets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1131/parts/0-Table-c1.png",
        "Caption": "Table 4: The BLEU score of self-trained cascaded trans- lation model under five initial training sets. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-0213/parts/0-Figure-c1.png",
        "Caption": "Table 2: Evaluation results for ReWoS variants and baselines.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P101121_p07/parts/0-Table-c1.png",
        "Caption": "Table 4: Example gender/number probability (%)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-3114/parts/0-Figure-c10.png",
        "Caption": "Table 2: Replication of the experiment with a corpus of non-native speakers (CEDEL2, Lozano, 2009)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Figure-c5.png",
        "Caption": "Table 5: Average min/max hypothesis length pro- ducible by each method (h = 1 for CF). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N03-1027/parts/0-Figure-c1.png",
        "Caption": "Figure 5 NP agreement violations that were caught by the agreement filter system. (a) Noun-compound case that was correctly handled. (b) Case involving conjunction that was correctly handled. (c) A case where fixing the agreement violation introduces a PP-attachment mistake. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-2123/parts/0-Table-c3.png",
        "Caption": "Table 14 %BLEU on tune and test sets for UR\u2192EN translation, comparing several settings for maximum dependency lengths in the decoder (\u03c9x is for the source side and \u03c9y is for the target side). The upper table shows Moses BLEU scores for comparison. The lower table compares two max dependency length settings during tuning, and several for decoding on the test sets, showing both BLEU scores and average decoding times per sentence. See text for discussion. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0206/parts/0-Figure-c3.png",
        "Caption": "Table 3: Examples for disagreement between the two judges.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-1115/parts/0-Table-c4.png",
        "Caption": "Table 1: The F1-Measure value is shown for every kernel on each ACE-2005 main relation type. For every relation type the best result is shown in bold font. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1081/parts/0-Figure-c4.png",
        "Caption": "Table 4 \u2013 Error rates for Le Monde corpus",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-3114/parts/0-Figure-c9.png",
        "Caption": "Figure 1: Illustration of Pareto Frontier. Ten hypotheses are plotted by their scores in two metrics. Hypotheses indicated by a circle (o) are pareto-optimal, while those indicated by a plus (+) are not. The line shows the convex hull, which attains only a subset of pareto-optimal points. The triangle (4) is a point that is weakly pareto-optimal but not pareto-optimal. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1070/parts/0-Table-c5.png",
        "Caption": "Figure 3: Using different amounts of annotated training",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1101/parts/0-Table-c4.png",
        "Caption": "Table 7. MRRs of the phonetic transliteration",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Figure-c2.png",
        "Caption": "Table 5 Most frequent sense analysis for all polysemous lemmas in the Senseval-2 and -3 test data, broken down by their frequencies of occurrence in SemCor (adverb data is only from Senseval-2). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1059/parts/0-Figure-c4.png",
        "Caption": "Table 4: Examples of high-weight features for several relations. Key: SYN = syntactic feature; LEX = lexical feature; x = reversed; NE# = named entity tag of entity. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D12-1003/parts/0-Table-c3.png",
        "Caption": "Table 11 The comparative error rates of the pseudo-disambiguation task for the three examined similarity measures, with and without applying the bootstrapped weighting for each of them. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N10-1068/parts/0-Figure-c4.png",
        "Caption": "Table 1: Performance results on the SemEval-2010 WSI Task, with rank shown in parentheses. Refer- ence scores of the best submitted systems are shown in the bottom. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1025/parts/0-Table-c5.png",
        "Caption": "Table 3: Translation Candidates for \u8e81\u9b31\u75c5 (manic- depression) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1063/parts/0-Table-c2.png",
        "Caption": "Table 3: Senses found by our algorithm from first order cooccurrences (LM-1 and LAT-1)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1060/parts/0-Table-c5.png",
        "Caption": "Table 10 The superiority of our joint model on three different domains indicated by type-insensitive (type-sensitive) performance (those signi\ufb01cant entries are marked in comparison with baseline). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P05-1020/parts/0-Table-c3.png",
        "Caption": "Table 4: Comparison to SVM.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Figure-c10.png",
        "Caption": "Figure 3: Examining the learned hidden representation for SRL. In this example the syntactic dependency arcs derived from gold standard syntactic annotations (left) are entirely disjoint from the correct predicate/arguments pairs (shown in the heatmaps by the squares outlined in black), and the observed syntax model fails to recover any of the correct predictions. In contrast, the hidden model structure (right) learns a representation that closely parallels the desired end task predictions, helping it recover three of the four correct SRL predictions (shaded arcs: red corresponds to a correct prediction, with true labels GA, KARA, etc.), and providing some evidence towards the fourth. The dependency tree corresponding to the hidden structure is derived by edge-factored decoding: dependency variables whose beliefs > 0.5 are classified as true (though some arcs not relevant to the SRL predictions are omitted for clarity). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-0213/parts/0-Figure-c2.png",
        "Caption": "Table 1: A protein domain-referring phrase example",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P08-1090/parts/0-Figure-c3.png",
        "Caption": "Figure 2. Growing Algorithm for Language              Model Pruning ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C16-1060/parts/0-Table-c2.png",
        "Caption": "Table 4: Final results on 25 corpora in 20 languages, with the number of induced classes equal to the number of gold standard tags in all cases. k-means and SVD2 models could not produce a clustering in the Czech CoNLL corpus due its size. Best published results are from \u2217 Christodoulopoulos et al. (2010), \u2020 Berg-Kirkpatrick et al. (2010) and \u2021 Lee et al. (2010). The latter two papers do not report VM scores. No best published results are shown for the MULTEXT languages; Christodoulopoulos et al. (2010) report results based on 45 tags suggesting that clark performs best on these corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1149/parts/0-Table-c4.png",
        "Caption": "Figure 4: Learning Curve on RIBES: comparing single- objective optimization and PMO. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1125/parts/0-Table-c3.png",
        "Caption": "Table 2: Results on the Arabic GALE Phase 2 evaluation set with one reference translation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PSMPT_n09/parts/0-Table-c3.png",
        "Caption": "Figure 2: Precision of acquired relations (causality). L and S denote lenient and strict evaluation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1004/parts/0-Table-c4.png",
        "Caption": "Figure 2: Bayesian network for probabilistic rea- soning of locations (variable \u201cfrom desc\u201d), which incorporates ASR N-best information in the vari- able\u201cfrom desc nbest\u201d and dialogue history in- formation in the remaining random variables. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2101/parts/0-Figure-c1.png",
        "Caption": "Figure 4: Distribution of the lexical items",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C02-1033/parts/0-Table-c1.png",
        "Caption": "Table 4: NMI of unconstrained HGFC when trees for T2 and T3 are inferred automatically. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1757/parts/0-Figure-c1.png",
        "Caption": "Table 4: Method 1. Exp1: frequency 2; 2 rules applied; in parallel; without contextual conditioning. Exp2: fre- quency 1; 1 rule applied; with contextual conditioning. Exp3: frequency 2; 2 rules applied; in parallel; with con- textual conditioning. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D07-1018/parts/0-Table-c1.png",
        "Caption": "Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in bold show the best results per language and setting. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1058/parts/0-Figure-c1.png",
        "Caption": "Table 3: Evaluation of the Turkish n-gram model.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PS15684_w09/parts/0-Figure-c2.png",
        "Caption": "Table 3: Accuracy on seen and unseen tokens.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1149/parts/0-Table-c1.png",
        "Caption": "Table 1: Mixed-case TER and BLEU, and lower-case METEOR scores on Arabic NIST MT03+MT04. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1127/parts/0-Figure-c1.png",
        "Caption": "Table 2: MRRs of the frequency correlation meth- ods. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N10-1068/parts/0-Figure-c5.png",
        "Caption": "Table 11: POS tagging error patterns. # means the error number of the corresponding pattern made by the pipeline tagging model. \u2193 and \u2191 mean the error number reduced or increased by the joint model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1048/parts/0-Figure-c4.png",
        "Caption": "Table 1: Single systems (English) in cross- validation, sorted by recall. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W08-0335/parts/0-Table-c1.png",
        "Caption": "Table 2: All the parameters of Measures for de- termining semantic compositionality described in Section 3 used in our experiments. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W03-0432/parts/0-Table-c5.png",
        "Caption": "Table 2: BLEU scores for different configuration of factored translation models. The big prefix de- notes experiments with the larger context for n- gram translation models. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1089/parts/0-Table-c4.png",
        "Caption": "Figure 3 Training with scarce resources. \u201cRestructuring,\u201d \u201clearn phrases,\u201d and \u201cannotation\u201d all require morpho-syntactic analysis of the transformed sentences. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Figure-c8.png",
        "Caption": "Figure 3: Example of feature forest representation of linear chain CRFs. Feature functions are as- signed to \u201cand\u201d nodes. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-3208/parts/0-Figure-c1.png",
        "Caption": "Table 2: Evaluation of the Russian n-gram model.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-0118/parts/0-Table-c3.png",
        "Caption": "Table 4: Average precision of discovered senses      for English in relation with WordNet ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1080/parts/0-Table-c1.png",
        "Caption": "Table 6 Experiments on the threshold\u2013precision relationship of the large corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1143/parts/0-Table-c3.png",
        "Caption": "Table 9: Impact of the improved morphology on the qual- ity of the dependency parser for Czech and German. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-1309/parts/0-Table-c1.png",
        "Caption": "Figure 5 Example of segmentation of German sentence and its English translation into alignment templates. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H05-1083/parts/0-Figure-c2.png",
        "Caption": "Figure 8: The actual output of our parser trained with a fully annotated treebank. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1141/parts/0-Figure-c10.png",
        "Caption": "Table 3: Rules of the baseline system.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-0410/parts/0-Table-c1.png",
        "Caption": "Figure 1: Example MERT values along one coordi- nate, first unregularized. When regularized with `2 , the piecewise constant function becomes piecewise quadratic. When using `0 , the function remains piecewise constant with a point discontinuity at 0. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-1017/parts/0-Table-c7.png",
        "Caption": "Figure 3. Voting Among Classifiers",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C02-1033/parts/0-Figure-c1.png",
        "Caption": "Figure 5: An example word which has very complex structures. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1111/parts/0-Table-c11.png",
        "Caption": "Table 4: Results for the three ACE data sets obtained via the B-CUBED scoring program.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c6.png",
        "Caption": "Table 11 Precision and partial recall of word lengths two to four of the first experiment on IT and AV. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Table-c6.png",
        "Caption": "Table 6: BS on IWSLT 2007 task",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P05-1045/parts/0-Table-c3.png",
        "Caption": "Table 2: Evidence cardinality in the corpora.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1121/parts/0-Figure-c4.png",
        "Caption": "Table 4: Upper bound for combination. The error reduction (ER) rate is a comparison between the F-score produced by the oracle combination sys- tem and the character-based system (see Tab. 1). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Plex_p00/parts/0-Figure-c1.png",
        "Caption": "Table 1: Adapting a parser to a new annotation style. We learn to parse in a \u201ctarget\u201d style (wide column label) given some number (narrow column label) of supervised target-style training sentences. As a font of additional features, all training and test sentences have already been augmented with parses in some \u201csource\u201d style (row label): either gold-standard parses (an oracle experiment) or else the output of a parser trained on 18k source trees (more realistic). If we have 0 training sentences, we simply output the source-style parse. But with 10 or 100 target-style training sentences, each off-diagonal block learns to adapt, mostly closing the gap with the diagonal block in the same column. In the diagonal blocks, source and target styles match, and the QG parser degrades performance when acting as a \u201cstacked\u201d parser. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J06-1004/parts/0-Table-c2.png",
        "Caption": "Table 6: Unlabeled TedEval scores (accuracy/exact match) for the test sets in the predicted segmentation set- ting. Only sentences of length \u2264 70 are evaluated. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1023/parts/0-Table-c2.png",
        "Caption": "Fig. 2. Schematic \ufb02owchart of the work\ufb02ow we followed, regarding the datasets, the training techniques and the operations.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3909/parts/0-Table-c2.png",
        "Caption": "Table 6. The performance on the set of unknown",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1107/parts/0-Figure-c3.png",
        "Caption": "Figure 8 Results of 70 high-frequency two-character CASs. \u2018Voting\u2019 indicates the accuracy of the baseline method that always chooses the more frequent case of a given CAS. \u2018ME\u2019 indicates the accuracy of the maximum-entropy classifier. \u2018VSM\u2019 indicates the accuracy of the method of using VSM for disambiguation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1015/parts/0-Table-c8.png",
        "Caption": "Table 3: Russian to English machine translation system evaluated on tst2012 and tst2013. Human evaluation in WMT13 is performed on the system trained using the original corpus with TA-GIZA++ for alignment (marked with *) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P283_w09/parts/0-Table-c3.png",
        "Caption": "Table 2: Four ambiguous words, their senses and frequency",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/D09-1092/parts/0-Figure-c4.png",
        "Caption": "Figure 1: The Maytag interface",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W12-4006/parts/0-Table-c1.png",
        "Caption": "Figure 3: An example CCG parse obtained from [60]",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2049/parts/0-Figure-c3.png",
        "Caption": "Table 5 Results when the development set is not used to estimate \u03bb and K. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Figure-c6.png",
        "Caption": "Figure 2: Word prediction speed, in terms of the number of classified test examples per second, mea- sured on the three test sets, with increasing training examples. Both axes have a logarithmic scale. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-1045/parts/0-Table-c4.png",
        "Caption": "Table 2: Accuracy scores for the CoNLL 2009 shared task test sets. Rows 1\u20132: Top performing systems in the shared CoNLL Shared Task 2009; Gesmundo et al. (2009) was placed first in the shared task; for Bohnet (2010), we include the updated scores later reported due to some improvements of the parser. Rows 3\u20134: Baseline (k = 1) and best settings for k and \u03b1 on development set. Rows 5\u20136: Wider beam (b1 = 80) and added graph features (G) and cluster features (C). Second beam parameter b2 fixed at 4 in all cases. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-0513/parts/0-Table-c2.png",
        "Caption": "Table 4. Examples of the top-3 candidates in the       transliteration of English \u2013 Chinese ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W03-0432/parts/0-Table-c3.png",
        "Caption": "Table 2. Committee Agreement vs. Accuracy",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P04-1018/parts/0-Table-c2.png",
        "Caption": "Table 5: Effect of supplementing recasing model training data with the test set source. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1016/parts/0-Table-c1.png",
        "Caption": "Figure 4: Sorted frequency of tags for WSJ. The gold standard distribution follows a steep exponential curve while the induced model distributions are more uniform. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W03-0423/parts/0-Table-c2.png",
        "Caption": "Table 1: Summary of the results obtained by our algorithm in comparison to Word 2007",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1018/parts/0-Table-c2.png",
        "Caption": "Figure 5: Coverage of summary text caseframes in source text (Study 3).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0714/parts/0-Table-c1.png",
        "Caption": "Figure 1: A standard logical form derivation using CCG. The NP\u2191 notation means that the subject is type-raised, and taking the verb-phrase as an argument\u2014so is an ab- breviation of S/(S\\NP). This is necessary in part to sup- port a correct semantics for quantifiers. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1141/parts/0-Table-c2.png",
        "Caption": "Table 3: Results for different user simulations. Numbers give % reductions in keystrokes. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3812/parts/0-Figure-c8.png",
        "Caption": "Table 5 Some words extracted from the large corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D10-1042/parts/0-Figure-c1.png",
        "Caption": "Figure 3: Density of signature caseframes (Study 2).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-1006/parts/0-Figure-c1.png",
        "Caption": "Table 1: This table shows the performance achieved by the different systems, shown in accuracy (%). The Number of cases denotes the number of instances in the testset. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1039/parts/0-Table-c1.png",
        "Caption": "Table 3: Adjective Full Inflection",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-3031/parts/0-Table-c5.png",
        "Caption": "Figure 8: MUC-4: Level Distribution of the Five Facts Combined",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-2040/parts/0-Figure-c1.png",
        "Caption": "Table 3: Results of voting experiments.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-2207/parts/0-Table-c7.png",
        "Caption": "Table 3: Disambiguation scores on nine confusable set, attained by confusable experts trained on ex- amples extracted from 1 billion words of text from TRAIN - REUTERS plus TRAIN - NYT, on the three test sets. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J06-2001/parts/0-Table-c4.png",
        "Caption": "Table 4: Comparison of different configurations.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W15-0909/parts/0-Table-c1.png",
        "Caption": "Figure 12: Correlation between manual and automatic scores for Spanish-English",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1147/parts/0-Figure-c3.png",
        "Caption": "Figure 6 Examples of illustrative sentence pairs and frequently extracted rules that model verb movement between German and English. An ellipsis indicates that there must be material between the two phrases for the rule to apply. (a) Example of movement of the finite verb to the end of a dependent clause. (b) Example of movement of an infinitive to the end of an independent clause following a modal verb (mo\u0308chte, \u2018would like\u2019). Discussion of the features used to score these string-to-tree rules is given in Section 5.2. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/D09-1092/parts/0-Figure-c8.png",
        "Caption": "Table 2: Data sizes for the experiments reported in this paper (English words shown). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N13-1093/parts/0-Table-c2.png",
        "Caption": "Table 8 The accuracies of joint segmentation and POS-tagging by 10-fold cross validation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-1068/parts/0-Table-c3.png",
        "Caption": "Figure 5: Six of the top 20 scored Narrative Schemas. Events and arguments in italics were marked misaligned by FrameNet definitions. * indicates verbs not in FrameNet. - indicates verb senses not in FameNet. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1010/parts/0-Figure-c3.png",
        "Caption": "Figure 5: Example problem from the FraCaS suite.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0707/parts/0-Table-c1.png",
        "Caption": "Figure 14 Reduplication for n = 4. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W08-0602/parts/0-Table-c3.png",
        "Caption": "Figure 2: Syntactic frames for VerbNet classes",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P05-1020/parts/0-Table-c4.png",
        "Caption": "Table 3: Number of unique entries in training and    test sets, categorized by semantic attributes ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c20.png",
        "Caption": "Table 1: Examples of translations",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2139/parts/0-Table-c4.png",
        "Caption": "Table 3: Results of the baseline model: best 5 guesses",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-1004/parts/0-Table-c12.png",
        "Caption": "Figure 12: MUC-6: Level Distribution of the Six Facts Combined",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1150/parts/0-Table-c1.png",
        "Caption": "Table 3: SC classification accuracies of different methods for the ACE training set and test set.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1070/parts/0-Table-c7.png",
        "Caption": "Table 3 Accuracy for SO-PMI with different data set sizes, the spin model, the label propagation model, and the random walks model for 10-fold cross-validation and 14 seeds. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C04-1131/parts/0-Table-c8.png",
        "Caption": "Table 3: Comparison of raw input and constrained input.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Table-c7.png",
        "Caption": "Table 1 Feature templates for the word segmentor. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1008/parts/0-Table-c4.png",
        "Caption": "Table 2: Disambiguation scores on nine confusable set, attained by the all-words prediction classifier trained on 30 million examples of TRAIN - REUTERS, and by confusable experts on the same training set. The second column displays the number of exam- ples of each confusable set in the 30-million word training set; the list is ordered on this column. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c6.png",
        "Caption": "Table 15 The training, development, and test data for English dependency parsing. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1121/parts/0-Table-c6.png",
        "Caption": "Table 7: LO cosine sentence configuration scores",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1107/parts/0-Table-c6.png",
        "Caption": "Table 4: Evaluation of Turkish predictive text input.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P11-1060/parts/0-Figure-c4.png",
        "Caption": "Table 3: Counts of the number of times multiple occurrences of a token sequence is labeled as different entity types in the same document. Taken from the CoNLL training set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-1815/parts/0-Table-c1.png",
        "Caption": "Table 2: An example for the \u201cBMES\u201d representa- tion. The sentence is \u201c\u6211\u7231\u5317\u4eac\u5929\u5b89\u95e8\u201d (I love Bei- jing Tian-an-men square), which consists of 4 Chi- nese words: \u201c\u6211\u201d (I), \u201c\u7231\u201d (love), \u201c\u5317\u4eac\u201d (Beijing), and \u201c\u5929\u5b89\u95e8\u201d (Tian-an-men square). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1004/parts/0-Figure-c4.png",
        "Caption": "Table 4: Effect of language and error models to speed (time in seconds per 10,000 word forms) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1061/parts/0-Table-c4.png",
        "Caption": "Table 13 Evaluating predominant sense information for polysemous nouns on the Senseval-2 all-words task data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1081/parts/0-Table-c2.png",
        "Caption": "Figure 4: Devoicing transducer compiled through a rule.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P08-1102/parts/0-Figure-c1.png",
        "Caption": "Table 3: Multi-lingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages under several experimental settings (Section 5). For each language and setting, we report one-to-one (1-1) and many- to-one (m-1) accuracies. For each cell, the first row corresponds to the result using the best hyperparameter choice, where best is defined by the 1-1 metric. The second row represents the performance of the median hyperparameter setting. Model components cascade, so the row corresponding to +FEATS also includes the PRIOR component (see Section 3). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PEAAI_n09/parts/0-Figure-c2.png",
        "Caption": "Figure 2: Semantic structure of the first sequence",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P05-1013/parts/0-Table-c5.png",
        "Caption": "Table 10 The comparison of overall accuracies of various joint segmentor and POS-taggers by 10-fold cross validation using CTB. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1138/parts/0-Figure-c3.png",
        "Caption": "Fig. 3. Interpolation \u2013 recall/precision curves.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-0441/parts/0-Table-c6.png",
        "Caption": "Table 2: Influence of the n-gram model on the perfor- mance of the statistical approach. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00-1025/parts/0-Figure-c8.png",
        "Caption": "Table 1: Sizes of our comparable corpora",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1044/parts/0-Table-c2.png",
        "Caption": "Table 6: Optimized Edit Costs",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P05-1013/parts/0-Table-c2.png",
        "Caption": "Table 1: Example of word-dependent substitution costs.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W10-4128/parts/0-Table-c4.png",
        "Caption": "Table 3: Performance in F1-score over different cluster numbers with intra-stratum sampling on the develop- ment data ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J06-1004/parts/0-Figure-c7.png",
        "Caption": "Figure 1: Example context for WSD S ENSEVAL-2 target word bar (inventory of 21 senses) and extracted features ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmorph_p00/parts/0-Table-c14.png",
        "Caption": "Table 4: Calculated Values of \u03bbi",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Figure-c7.png",
        "Caption": "Figure 2: V-Measure and paired FScore results for different partitionings of the dendrogram. The dashed vertical line indicates SP D ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1010/parts/0-Table-c3.png",
        "Caption": "Table 5: F1 scores of the local CRF and non-local models on the CoNLL 2003 named entity recognition dataset. We also provide the results from Bunescu and Mooney (2004) for comparison. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1013/parts/0-Figure-c4.png",
        "Caption": "Fig. 4. Examples of rules used during decoding.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-5011/parts/0-Figure-c3.png",
        "Caption": "Figure 3. Semantic Role learning curve",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1009/parts/0-Table-c14.png",
        "Caption": "Figure 1: Rule expansion with minimal context (Example 3)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P05-1045/parts/0-Table-c6.png",
        "Caption": "Figure 3: Individual Classifier Properties (cross-validation on SENSEVAL training data)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N10-1068/parts/0-Figure-c1.png",
        "Caption": "Figure 1. The Lattice of the 8 Patterns.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1086/parts/0-Figure-c1.png",
        "Caption": "Figure 3: Example of chunk-based alignment",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P08-1043/parts/0-Table-c2.png",
        "Caption": "Table 1: Examples of non-phonetic translations.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P22324_w09/parts/0-Table-c4.png",
        "Caption": "Table 13 Evaluating predominant sense information for polysemous nouns on the Senseval-2 all-words task data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0301/parts/0-Table-c2.png",
        "Caption": "Table 1: Evaluation of the manual annotation improvement - summarization ratio: 15%.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W10-3708/parts/0-Table-c2.png",
        "Caption": "Table 7: The effect of language and gender in-",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1045/parts/0-Table-c6.png",
        "Caption": "Table 12 NWI results on HK and AS corpora, NWI as post-processor versus unified approach. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D10-1042/parts/0-Table-c3.png",
        "Caption": "Figure 8: Extracting sub-trees for S2 .",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P59105ca/parts/0-Figure-c5.png",
        "Caption": "Table 3: The values of AP, Spearman (\u03c1) and Kendall (\u03c4 ) correlations between the LSA-based and PMI-based model respectively and the Gold data with regards to the expression type. Every zero value in the table corresponds to the theoretically achieved mean value of correlation calculated from the infinite number of correlation values between the ranking of scores assigned by the annotators and the rankings of scores being obtained by a random number genarator. Reddy-WSM stands for the best performing WSM in the DISCO task (Reddy et al., 2011b). StatMix stands for the best performing system based upon association measures (Chakraborty et al., 2011). Only \u03c1-All and \u03c4 -All are available for the models explored by Reddy et al. (2011b) and Chakraborty et al. (2011). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C08-1107/parts/0-Table-c3.png",
        "Caption": "Table 8: Per category performance of the Berkeley parser on sentence lengths \u2264 70 (dev set, gold segmentation). (a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse. We showed in \u00a72 that lexical ambiguity explains the underperformance of these categories. (b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN) and adjectives (e.g., JJ). Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing. (c) Coordination ambiguity is shown in dependency scores by e.g.,  S S S R  and  NP NP NP R .  NP NP PP R  and  NP NP ADJP R  are both iDafa attachment. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2023/parts/0-Figure-c4.png",
        "Caption": "Figure 3: Biography Text Evaluations.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P101167/parts/0-Figure-c4.png",
        "Caption": "Table 7 NEA type-insensitive (type-sensitive) performance with the same English NE recognizer (Mallet system) and different Chinese NE recognizers. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Table-c13.png",
        "Caption": "Table 1 confirms that names participating in re-",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1062/parts/0-Figure-c4.png",
        "Caption": "Figure 4 The effect of varying the number of seeds on accuracy. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N09-1048/parts/0-Table-c4.png",
        "Caption": "Table 3: The sizes of error models as automata",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-2124/parts/0-Figure-c1.png",
        "Caption": "Table 6: Distribution of errors",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1049/parts/0-Table-c6.png",
        "Caption": "Figure 1: Example of a long jump alignment grid. All possible deletion, insertion, identity and substitution op- erations are depicted. Only long jump edges from the best path are drawn. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0711/parts/0-Table-c8.png",
        "Caption": "Table 1 Taxonomy of Chinese words used in developing MSRSeg. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-0812/parts/0-Table-c1.png",
        "Caption": "Figure 5: Example of features for \u201csway\u201d",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-4009/parts/0-Table-c1.png",
        "Caption": "Table 2: Parameter values used in the evaluation.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W12-2303/parts/0-Table-c3.png",
        "Caption": "Table 20 Precision of organization name recognition on the MSR test set, using Viterbi iterative training, initialized by four seed sets with different sizes. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2164/parts/0-Table-c1.png",
        "Caption": "Figure 1: Plate diagram representation of the trigram HMM. The indexes i and j range over the set of tags and k ranges over the set of characters. Hyper-parameters have been omitted from the figure for clarity. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c3.png",
        "Caption": "Table 2: Model means for High and Low similarity items and correlation coefficients with human judgments (*: p < 0.05, **: p < 0.01) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1407/parts/0-Figure-c1.png",
        "Caption": "Figure 2: Plate diagram of the extended model with T kinds of token-level features (f (t) variables) and a single kind of type-level feature (morphology, m). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D07-1012/parts/0-Figure-c1.png",
        "Caption": "Figure 8: Squares represent the proportion of tokens in each language assigned to a topic. The left topic, world ski km won,",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1039/parts/0-Table-c2.png",
        "Caption": "Figure 6: F-measure for the objective and subjective classes for multilingual bootstrapping (versus cross-lingual framework) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1009/parts/0-Table-c1.png",
        "Caption": "Table 10 Experiments with words and parts-of-speech as contextual features",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Figure-c5.png",
        "Caption": "Table 3: HRGs against recent methods & baselines.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W01-0502/parts/0-Table-c1.png",
        "Caption": "Figure 3: Changing a decision in the derivation lattice. All paths generate the observed data. The bold path rep- resents the current sample, and the dotted path represents a sidetrack in which one decision is changed. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2145/parts/0-Table-c6.png",
        "Caption": "Table 4: Slot Value Translation Assessment from Ran- dom Sample of 1000 ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2214/parts/0-Figure-c1.png",
        "Caption": "Figure 3. Comparison of two augmentation strategies over different sampling strategies in selecting the initial seed set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmorph_p00/parts/0-Table-c2.png",
        "Caption": "Table 3: The results for three systems associ- ated with the project for the NP bracketing task, the shared task at CoNLL-99. The baseline re- sults have been obtained by finding NP chunks in the text with an algorithm which selects the most frequent chunk tag associated with each part-of- speech tag. The best results at CoNLL-99 was obtained with a bottom-up memory-based learner. An improved version of that system (MBL) deliv- ered the best project result. The MDL results have been obtained on a different data set and therefore combination of the three systems was not feasible. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W01-0502/parts/0-Table-c4.png",
        "Caption": "Table 1: Perplexity results for two previous grammar-based language models ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-1025/parts/0-Table-c1.png",
        "Caption": "Figure 2: Lexical Caseframe Expectations",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P10-1124/parts/0-Figure-c1.png",
        "Caption": "                   \u00fd Figure 7 (a) A Chinese OAS      . (b) Two sentences in the training set, which contain t whose OASs have been replaced with the single tokens <OAS>. (Li et al. 2003). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D10-1042/parts/0-Figure-c4.png",
        "Caption": "Table 4. Statistics of anaphor and antecedent pairs",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1009/parts/0-Figure-c2.png",
        "Caption": "Table 2: Comparison of dynamic context-sensitive tree span with SPT using our context-sensitive convolution tree kernel on the major relation types of the ACE RDC 2003 (inside the parentheses) and 2004 (outside the parentheses) corpora. 18% of positive instances in the ACE RDC 2003 test data belong to the predicate-linked category. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-3004/parts/0-Table-c12.png",
        "Caption": "Table 2: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with the traditional hierarchical system (\u201cBaseline\u201d) and the topic-specific lexicon translation method (\u201cTopicLex\u201d). \u201cSimSrc\u201d and \u201cSimTgt\u201d denote similarity by source-side and target-side rule-distribution respectively, while \u201cSim+Sen\u201d acti- vates the two similarity and two sensitivity features. \u201cAvg\u201d is the average B LEU score on the two test sets. Scores marked in bold mean significantly (Koehn, 2004) better than Baseline (p < 0.01). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1125/parts/0-Table-c1.png",
        "Caption": "Table 4: Cluster features ordered by importance.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1053/parts/0-Table-c5.png",
        "Caption": "Figure 1: Precision-recall curve for rescoring",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4917/parts/0-Table-c3.png",
        "Caption": "Table 3: Tagging and segmentation results on Estonian Multext-East corpus (Learned seg and Learned tag) com- pared to the semisupervised setting where segmentations are fixed to gold standard (Fixed seg) and tags are fixed to gold standard (Fixed tag). Finally the segmentatation results from Morfessor system for comparison are pre- sented. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c2.png",
        "Caption": "Figure 5: DRS and corresponding DRG (in tuples and in graph format) for \u201cA customer did not pay.\u201d",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1109/parts/0-Table-c2.png",
        "Caption": "Table 3: Reachability of 1000 training sentences: can they be translated with the model? ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2302/parts/0-Table-c4.png",
        "Caption": "Table 5: BLEU scores for the French-to-English translation task measured on nt10 with systems tuned on development sets selected according to their original language (adapted tuning). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1056/parts/0-Figure-c4.png",
        "Caption": "Figure 8 The effect of varying the number of extracted related words on accuracy. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-2611/parts/0-Figure-c1.png",
        "Caption": "Table 6. The performance on the set of unknown",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1080/parts/0-Figure-c3.png",
        "Caption": "Figure 4 A bootstrapping algorithm to extract phrasal paraphrase pairs from monolingual parallel corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1048/parts/0-Figure-c6.png",
        "Caption": "Table 4: Effect of dictionary scale",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2408/parts/0-Figure-c2.png",
        "Caption": "Figure 4: Compilation of a Regular Expression into an fst that Maps between Two Regular Languages ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1010/parts/0-Figure-c4.png",
        "Caption": "Figure 7: Evaluation of translation to English on in-domain test data",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1045/parts/0-Table-c2.png",
        "Caption": "Table 9 Comparing selectional preference slot definitions. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1110/parts/0-Table-c6.png",
        "Caption": "Figure 5. Some subtrees from trees in figure 4",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1110/parts/0-Figure-c1.png",
        "Caption": "Table 2: For each domain the percentage of target domain words (types) that are unseen in the source together with the most frequent OOV words. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c4.png",
        "Caption": "Table 2: Features used by the CRF for the two tasks: named entity recognition (NER) and template filling (TF). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E06-1030/parts/0-Table-c1.png",
        "Caption": "Figure 1. The decision tree for ec+mc+z\u017e, learned by C5. Besides pairwise agreement be- tween the parsers, only morphological case and negativeness matter. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1058/parts/0-Figure-c2.png",
        "Caption": "Table 13 Sample of human-interpretable Arabic TSG rules. Recursive rules like MWA\u2192A MWA result from memoryless binarization of n-ary rules. This pre-processing step not only increases parsing accuracy, but also allows the generation of previously unseen MWEs of a given type. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c6.png",
        "Caption": "Table 2: Top-7 Chinese long-form candidates for the En- glish acronym TAA, according to the LH score. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P22324_w09/parts/0-Table-c5.png",
        "Caption": "Figure 1 Illustration of the paraphrase degree calculation.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1407/parts/0-Figure-c4.png",
        "Caption": "Table 3 \u2013 Pk for C99 corpus",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0311/parts/0-Table-c1.png",
        "Caption": "Figure 1: The Ensemble Semantics framework for information extraction.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1007/parts/0-Figure-c3.png",
        "Caption": "Figure 2. Growing Algorithm for Language              Model Pruning ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1074/parts/0-Table-c4.png",
        "Caption": "Table 2: Results on the MT06 and MT08 test sets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/I05-3013/parts/0-Table-c3.png",
        "Caption": "Table 5: Effect of the introduction of equivalence classes. For the baseline we used the original in- flected word forms. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H01-1062/parts/0-Table-c4.png",
        "Caption": "Figure 5: Example of the denotation for a DCS tree with a compare relation C. This denotation has two columns, one for each active node\u2014the root node state and the marked node size. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1634/parts/0-Figure-c1.png",
        "Caption": "Figure 3: Comparison of rate of convergence between coordinate ascent and our expected BLEU direction finder (D = 500). Noisy refers to the noisy experimental setting. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1089/parts/0-Table-c1.png",
        "Caption": "Table 2: Illustrative tableau for a simple constraint sys- tem not capturable as a regular relation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-2023/parts/0-Table-c2.png",
        "Caption": "Figure 2: Word-to-word alignment.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H05-1083/parts/0-Table-c3.png",
        "Caption": "Table 2: Parsing results of different models using manual (gold) segmentation. Performances significantly superior to HILDA (with p<7.1e-05) are denoted by *. Significant differences between TSP 1-1 and TSP SW (with p<0.01) are denoted by \u2020. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1016/parts/0-Table-c8.png",
        "Caption": "Table 5: Accuracies of single-feature classifiers.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S13-2038/parts/0-Table-c2.png",
        "Caption": "Table 6 Properties of the variations for the corpus-based algorithms for other-anaphora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2605/parts/0-Table-c3.png",
        "Caption": "Figure 1: Topic transfer in bilingual LSA model.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-4015/parts/0-Table-c2.png",
        "Caption": "Table 2: Comparison of Moses and KIT phrase extraction systems ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-3141/parts/0-Table-c1.png",
        "Caption": "Table 5: Results for Positive and Negative Classes.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Table-c2.png",
        "Caption": " Table 1. OBI vs. BI; where the lost of F > 1%,   such as SC-B, is caused by incorrect English segments that will be discussed in the section 4. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1098/parts/0-Table-c3.png",
        "Caption": "Table 20 NE alignment on test data after semi-supervised learning. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-2123/parts/0-Table-c1.png",
        "Caption": "Table 7 %BLEU on tune and test sets for ZH\u2192EN translation, showing the contribution of feature sets in our QPD model. Both QPD models are significantly better than the best Moses numbers on test sets 1 and 2, but not on test set 3. The full QPD model is significantly better than the version with only T GT T REE features on test set 1 but statistically indistinguishable on the other two test sets. Hiero is significantly better than the full QPD model on test set 2 but not on the other two. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S12-1011/parts/0-Table-c4.png",
        "Caption": "Table 6: Overall performance on the evaluation set. L is the upper bound of the length of possible chunks in semi-CRFs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E06-1030/parts/0-Table-c7.png",
        "Caption": "Table 7: Fact vs. Statistical Cross-Doc Features",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W07-0722/parts/0-Table-c1.png",
        "Caption": "Table 1: Statistics about the results of our word sense discovery algorithm",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1087/parts/0-Figure-c6.png",
        "Caption": "Table 8: ROUGE-W in empirical approach",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1067/parts/0-Figure-c1.png",
        "Caption": "Table 1. Categories of spurious relation mentions in fp1 (on a sample of 10% of relation mentions), ranked by the percentage of the examples in each category. In the sample text, red text (also marked with dotted underlines) shows head words of the first arguments and the underlined text shows head words of the second arguments. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1104/parts/0-Table-c5.png",
        "Caption": "Figure 1. Upper triangle of the sentence-similarity matrix.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N07-1024/parts/0-Table-c4.png",
        "Caption": "Table 4 Lexical entailment precision values for top-n similar words by the Bootstrapped LIN and the original LIN method. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S10-1019/parts/0-Table-c2.png",
        "Caption": "Figure 4: POS tagging accuracy using one-at-a- time, character-based POS tagger ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c14.png",
        "Caption": "Figure 3: Precision/Recall/F-score of different models.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D11-1076/parts/0-Figure-c7.png",
        "Caption": "Figure 2: Average Precision and Coherence (\u03ba) for each meta alternation. Correlation: r = 0.743 (p < 0.001) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4904/parts/0-Table-c2.png",
        "Caption": "Figure 2: Architecture of the Structured Output Layer Neural Network language model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-1001/parts/0-Table-c6.png",
        "Caption": "Table 1: good#a#15 SentiWN scores.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1056/parts/0-Figure-c1.png",
        "Caption": "                                       0 Figure 1: The computation of   DKL (Pv(e i)                                             kPe0i ) using a toy corpus, for e = looking forward to. Note that the sec- ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J09-3004/parts/0-Table-c6.png",
        "Caption": "Table 6: Comparison of the existing efforts on ACE RDC task.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P1018/parts/0-Table-c8.png",
        "Caption": "Table 10 The comparison of overall accuracies of various joint segmentor and POS-taggers by 10-fold cross validation using CTB. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P08-2021/parts/0-Table-c1.png",
        "Caption": "Table 1: Examples of zero anaphora",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P1018/parts/0-Table-c2.png",
        "Caption": "Table 5: Results tested against gs-swaco-subjective",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1074/parts/0-Table-c6.png",
        "Caption": "Table 10 Occurrences of error types for the best other-anaphora algorithm algoWebv4 . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1110/parts/0-Figure-c2.png",
        "Caption": "Figure 5: Distribution over number of hits",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1039/parts/0-Figure-c5.png",
        "Caption": "Table 14 Summary of results for unknown-boundary condition. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4916/parts/0-Table-c8.png",
        "Caption": "Table 8 Semantic roles of verbs\u2019 subjects, for the verb classes of Merlo and Stevenson (2001). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1048/parts/0-Table-c1.png",
        "Caption": "Table 4 Experiments on the threshold\u2013partial recall relationship of the small corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2049/parts/0-Table-c1.png",
        "Caption": "Table 5: Estimated precision on human-evaluation experiments of the highest-ranked 100 and 1000 results per relation, using stratified samples. \u2018Average\u2019 gives the mean precision of the 10 relations. Key: Syn = syntactic features only. Lex = lexical features only. We use stratified samples because of the overabundance of location-contains instances among our high-confidence results. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-1037/parts/0-Table-c3.png",
        "Caption": "Table 3: Counts of the number of times multiple occurrences of a token sequence is labeled as different entity types in the same document. Taken from the CoNLL training set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-3012/parts/0-Table-c1.png",
        "Caption": "Table 12 Common values (in percentages) for parse tree path in PropBank data, using gold-standard parses. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S13-1039/parts/0-Table-c2.png",
        "Caption": "Table 2. Overall results Coverage/Accuracy",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1091/parts/0-Figure-c1.png",
        "Caption": "Table 6 Performance of proposed system on MSRVDC Dataset 1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3604/parts/0-Figure-c3.png",
        "Caption": "Figure 8: Extracting sub-trees for S2 .",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1087/parts/0-Figure-c2.png",
        "Caption": "Figure 11: Correlation between manual and automatic scores for French-English",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-1068/parts/0-Figure-c4.png",
        "Caption": "Table 4: Parser performance on WSJ;23, baselines. Note that the Gildea results are for sentences \u2264 40 words in length. All others include all sentences. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1125/parts/0-Table-c2.png",
        "Caption": "Table 4: Development results for POS tagging. Given are training times in minutes (TT) and accuracies (ACC). Best baseline results are underlined and the overall best results bold. * indicates a significant difference (positive or negative) between the best baseline and a PCRF model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P101167/parts/0-Figure-c1.png",
        "Caption": "Figure 1: Accuracy Trends on MicroWnOp Corpus.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Table-c13.png",
        "Caption": "Table 10 Translation results on the Hansards task. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Table-c17.png",
        "Caption": "Table 5: POS annotations of a couplet, i.e., a pair of two verses, in a classical Chinese poem. See     Table 1 for the meaning of the POS tags. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P283_w09/parts/0-Table-c4.png",
        "Caption": "Table 6: Coreference results obtained via the MUC scoring program for the ACE test set.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-2003/parts/0-Table-c2.png",
        "Caption": "Figure 10: Evaluation of translation from English on out-of-domain test data",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1404/parts/0-Figure-c3.png",
        "Caption": "Table 2: Distribution of Error Types",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0206/parts/0-Figure-c4.png",
        "Caption": "Fig. 9. Intrinsic and extrinsic evaluation of alignments in the small data experiments. (a) Alignment dictionary size normalized by the average of source and target vocabulary sizes. (b) Average alignment fertility of aligned singletons. (c) Percentage of unaligned singletons. (d) Number of symmetric alignments normalized by the average of source and target tokens. (e) Percentage of training set vocabulary covered by single-word phrases in the phrase table. (f) Decode-time rate of input words that are in the training vocabulary but without a translation in the phrase table. (g) Phrase table size normalized by the average of source and target tokens. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H01-1052/parts/0-Figure-c1.png",
        "Caption": "Table 1 Taxonomy of Chinese words used in developing MSRSeg. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1107/parts/0-Table-c8.png",
        "Caption": "Table 7: The Effects of Social Features.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2605/parts/0-Figure-c2.png",
        "Caption": "Figure 9: Example word structure annotation. We add an \u2018f\u2019 to the POS tags of words with no further structures. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D10-1034/parts/0-Table-c4.png",
        "Caption": "Table 1: Training and Test Data Statistics",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P05-1004/parts/0-Table-c3.png",
        "Caption": "Table 2: Empirical results for the position-based model, the KL-based models and BAYE S UM, with different inputs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1045/parts/0-Figure-c2.png",
        "Caption": "Table 28: Arabic POS Studies with Different Tagsets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1143/parts/0-Table-c2.png",
        "Caption": "Table 2 The similarity score features used to represent pairs of templates. The columns specify the corpus over which the similarity score was computed, the template representation, the similarity measure employed, and the feature representation (as described in Section 4.1). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1138/parts/0-Figure-c5.png",
        "Caption": "Figure 12 Convergence of results. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N01-1011/parts/0-Table-c2.png",
        "Caption": "Table 2: The NP chunking results for six sys- tems associated with the project. The baseline results have been obtained by selecting the most frequent chunk tag associated with each part-of- speech tag. The best results for this task have been obtained with a combination of seven learn- ers, five of which were operated by project mem- bers. The combination of these five performances is not far off these best results. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1110/parts/0-Table-c3.png",
        "Caption": "Figure 2: Projectivized dependency graph for Czech sentence",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Table-c9.png",
        "Caption": "Figure 6: Tune and test curves of five repetitions of the same Urdu-English PBMT baseline feature experiment. PRO is more stable than MERT. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N15-1071/parts/0-Table-c3.png",
        "Caption": "Table 6 Context model, word classes, class models, and feature functions. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1046/parts/0-Table-c4.png",
        "Caption": "Table 1: Results on the MT02 and MT05 test sets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P04470/parts/0-Table-c6.png",
        "Caption": "Table 9 Evaluation on SemCor, polysemous words only. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4904/parts/0-Table-c3.png",
        "Caption": "Table 1: Association frequencies for target verb.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-2222/parts/0-Figure-c3.png",
        "Caption": "Fig. 1. Integration of confidence measures \u2013 recall/precision curves (figures in the legend correspond to resp. \u03b41 and \u03b42 ).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1059/parts/0-Table-c3.png",
        "Caption": "Figure 2: Correlations between human judge- ments and automatic evaluation metrics for vari- ous edit distances ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1155/parts/0-Table-c1.png",
        "Caption": "Figure 4 Examples of alignment templates obtained in training. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Table-c8.png",
        "Caption": "Figure 4: A RTG integrating the attachment constraint for Contrast from Ex. 2 into Fig. 3",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1053/parts/0-Table-c3.png",
        "Caption": "Table 4. Experimental result of total unknown",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-2123/parts/0-Table-c5.png",
        "Caption": "Table 2: ROUGE-W measures in k-means learning",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1127/parts/0-Figure-c5.png",
        "Caption": "Table 2 Open test, in percentages (%)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0711/parts/0-Table-c6.png",
        "Caption": "Table 7 Most frequent syntactic positions for each semantic role. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/ICDAR99/parts/0-Table-c1.png",
        "Caption": "Fig. 5. Czech English BLEU scores of various al EM(Co), GS(Co), EM(Co)+GS(Co), and VB(Co). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-3236/parts/0-Figure-c6.png",
        "Caption": "Table 5 Comparative precision values for the top 20 similarity lists of the three selected similarity measures, with MI and Bootstrapped feature weighting for each. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-3004/parts/0-Table-c14.png",
        "Caption": "Table 10 Simplified prevalence score, evaluation on SemCor, polysemous words only. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P59105ca/parts/0-Figure-c9.png",
        "Caption": "Figure 1 Illustration of the paraphrase degree calculation.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1138/parts/0-Table-c1.png",
        "Caption": "Table 3: Translation results for French\u2192English",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Figure-c1.png",
        "Caption": "Table 9 Initial NE recognition type-insensitive (type-sensitive) performance across various domains. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1045/parts/0-Table-c5.png",
        "Caption": "Figure 4. All binary trees for NNS VBD JJ NNS         (Investors suffered heavy losses) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1138/parts/0-Figure-c7.png",
        "Caption": "Figure 3: Boxer output for Shared Task Text 2",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C04-1126/parts/0-Table-c2.png",
        "Caption": "Table 1: 14 classes used in Joanis et al. (2008) and their corresponding Levin class numbers ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-3012/parts/0-Table-c2.png",
        "Caption": "Table 2: Topic-specific translation lexicons are learned by a 3-topic BiTAM-1. The third lexicon (Topic-3) prefers to translate the word Korean into ChaoXian (\u008am:North Korean). The co-occurrence (Cooc), IBM-1&4 and HMM only prefer to translate into HanGuo (\u00b8I:South Korean). The two candidate translations may both fade out in the learned translation lexicons. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D12-1003/parts/0-Table-c2.png",
        "Caption": "Figure 4: Sentiment Lexicon Performance",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1009/parts/0-Table-c13.png",
        "Caption": "Table 2: Number of clustering decisions made ac- cording to mention type (rows anaphor, columns antecedent) and percentage of wrong decisions. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c2.png",
        "Caption": "Table 8 The accuracies of joint segmentation and POS-tagging by 10-fold cross validation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-0118/parts/0-Table-c2.png",
        "Caption": "Figure 11 Participle-forming combinations in German. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-2094/parts/0-Table-c1.png",
        "Caption": "Table 5: Effectiveness of score propagation.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-2222/parts/0-Figure-c2.png",
        "Caption": "Figure 3 Algorithm phrase-extract for extracting phrases from a word-aligned sentence pair. Here quasi-consecutive(TP) is a predicate that tests whether the set of words TP is consecutive, with the possible exception of words that are not aligned. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2211/parts/0-Table-c2.png",
        "Caption": "Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing. For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P05-1004/parts/0-Table-c6.png",
        "Caption": "Figure 4 Distribution of domain labels of predominant senses for 38 polysemous nouns ranked using the SPORTS and FINANCE corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1084/parts/0-Table-c6.png",
        "Caption": "Figure 8 FSRA-2 for Arabic nominative definite and indefinite nouns. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0206/parts/0-Table-c5.png",
        "Caption": "Table 7: Word segmentation results.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1086/parts/0-Figure-c4.png",
        "Caption": "Table 3: Coreference Resolution Performance",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-2040/parts/0-Table-c5.png",
        "Caption": "Figure 2: Merging typed chains into a single unordered Narrative Schema.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1008/parts/0-Table-c1.png",
        "Caption": "Figure 1: Rule expansion with minimal context (Example 3)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-1017/parts/0-Table-c4.png",
        "Caption": "Table 3: Signature caseframe densities for differ- ent sets of summarizers, for the initial and update guided summarization tasks (Study 2). \u2217 : p < 0.005. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1059/parts/0-Table-c4.png",
        "Caption": "Table 18: Arabic idafa Construct",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1039/parts/0-Figure-c1.png",
        "Caption": "Table 4: Translation results for English\u2192French",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1068/parts/0-Figure-c4.png",
        "Caption": "Table 4: Language detection accuracies (%) using a 4-gram language model for the letter sequence of          the source name in Latin script. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-0513/parts/0-Figure-c1.png",
        "Caption": "Table 10 Examples of the effect of the hierarchical lexicon. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Figure-c8.png",
        "Caption": "Fig. 5 The semantic graph of an English sentence and the semantic features extracted from it for an SMT phrase ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D12-1074/parts/0-Figure-c1.png",
        "Caption": "Table 2. Accuracy of various instantiations of the system",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W07-1525/parts/0-Figure-c1.png",
        "Caption": "Table 1: Sources of conflict in cross-lingual subjectivity transfer. Definitions and synonyms of the fourth sense of the noun argument, the fourth sense of verb decide, and the first sense of adjective free as provided by the English and Romanian WordNets; for Romanian we also provide the manual translation into English. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c23.png",
        "Caption": "Table 1: Single systems (English) in cross- validation, sorted by recall. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2101/parts/0-Figure-c3.png",
        "Caption": "Table 1: Statistics on the Italian EVALITA 2009       and English CoNLL 2003 corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Figure-c5.png",
        "Caption": "Figure 1: The first set of features in our model. All of them are binary. The final feature set includes two sets: the set here, and a set obtained by its conjunction with the verb\u2019s lemma. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N12-1006/parts/0-Figure-c3.png",
        "Caption": "Table 4: Accuracy for different EM-weighted probability interpolation models for SENSEVAL 2 ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1004/parts/0-Figure-c2.png",
        "Caption": "Table 2: Entity type constraints.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Figure-c7.png",
        "Caption": "Figure 4: Expanding a partial hypothesis via a matching n-gram. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pling_p07/parts/0-Figure-c2.png",
        "Caption": "Table 7: Syncretism Example 1",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2145/parts/0-Table-c2.png",
        "Caption": "Table 4: Sentence error rates of end-to-end evalua- tion (speech recognizer with WER=25%; corpus of 5069 and 4136 dialogue turns for translation Ger- man to English and English to German, respec- tively). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1114/parts/0-Table-c4.png",
        "Caption": "Table 4: Word Alignment Accuracy (F-measure) and Machine Translation Quality for BiTAM Models, comparing with IBM Models, and HMMs with a training scheme of 18 h7 43 on the Treebank data listed in Table 1. For each column, the highlighted alignment (the best one under that model setting) is picked up to further evaluate the translation quality. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P04470/parts/0-Table-c3.png",
        "Caption": "Figure 5: The STTS tags PDAT and ART, their rep- resentation in the Annotation Model and linking with the Reference Model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1063/parts/0-Table-c3.png",
        "Caption": "Table 3: Oracle lower-case BLEU",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1073/parts/0-Table-c1.png",
        "Caption": "Table 14 Comparison scores for HK open and AS open. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Figure-c7.png",
        "Caption": "Table 8: Per category performance of the Berkeley parser on sentence lengths \u2264 70 (dev set, gold segmentation). (a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse. We showed in \u00a72 that lexical ambiguity explains the underperformance of these categories. (b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN) and adjectives (e.g., JJ). Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing. (c) Coordination ambiguity is shown in dependency scores by e.g.,  S S S R  and  NP NP NP R .  NP NP PP R  and  NP NP ADJP R  are both iDafa attachment. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N10-1019/parts/0-Figure-c4.png",
        "Caption": "          A u to m a tic M e tr ic s H u m a n E v a lu a tio n Figure 2: Scores based on Automatic Metrics and Human Evaluation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-2003/parts/0-Table-c5.png",
        "Caption": "Table 1: Labeled precision and recall for the three types of labels. The line labeled \u2018Flat*\u2019 is for unlabeled met- rics of flat words, which is effectively the ordinary word segmentation accuracy. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1007/parts/0-Table-c5.png",
        "Caption": "Figure 4 A comparison between ILP-Global and ILP-Local for two fragments of the test-set concept seizure. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-0611/parts/0-Table-c1.png",
        "Caption": "Table 1: Examples of generated paraphrased head- lines ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c12.png",
        "Caption": "Table 5: Average F1 using different hypothesized type-specific features. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D08-1032/parts/0-Table-c1.png",
        "Caption": "Table 2: Corpus statistics of the MATR MT06 corpus that was used for experimental evaluation of the proposed measures. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Table-c6.png",
        "Caption": "Figure 3: An example CCG parse obtained from [60]",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1009/parts/0-Table-c4.png",
        "Caption": "Table 1: Rules and patterns for the four syntactico-semantic structures. Regular expression notations: \u2018*\u2019 matches the preceding element zero or more times; \u2018+\u2019 matches the preceding element one or more times; \u2018?\u2019 indicates that the preceding element is optional; \u2018|\u2019 indicates or. Abbreviations: Ec (m): coarse-grained entity type of mention m; Ld : labels in dependency path between the headword of two mentions. We use square brackets \u2018[\u2019 and \u2018]\u2019 to denote mention boundaries. The \u2018/\u2019 in the Formulaic row denotes the occurrence of a lexical \u2018/\u2019 in text. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D11-1095/parts/0-Figure-c1.png",
        "Caption": "Table 3: Translation Candidates for \u8e81\u9b31\u75c5 (manic- depression) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pproc2014_n09/parts/0-Table-c1.png",
        "Caption": "Table 2: Classification results with 5-gram and fre- quency threshold 4 (Method 2) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C00-2123/parts/0-Table-c2.png",
        "Caption": "Table 1: WSJ performance comparing previous work to our own model. The columns display the many-to-1 accuracy and the V measure, both averaged over 5 inde- pendent runs. Our model was run with the local sampler (HMM), the type-level sampler (1HMM) and also with the character LM (1HMM-LM). Also shown are results using Dirichlet Process (DP) priors by fixing a = 0. The system abbreviations are CGS10 (Christodoulopoulos et al., 2010), BBDK10 (Berg-Kirkpatrick et al., 2010) and GG07 (Goldwater and Griffiths, 2007). Starred entries denote results reported in CGS10. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S10-1090/parts/0-Table-c2.png",
        "Caption": "Table 1: English MWEs and their components with their translation in Persian. Direct matches between the trans- lation of a MWE and its components are shown in bold; partial matches are underlined. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4917/parts/0-Table-c8.png",
        "Caption": "Table 5: O\ufb03cial results for the English and Basque lexical tasks (recall). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-3004/parts/0-Table-c8.png",
        "Caption": "Table 2: Evidence cardinality in the corpora.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1155/parts/0-Figure-c1.png",
        "Caption": "Figure 1. Consistently formatted term translation                       pairs ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H05-1083/parts/0-Table-c1.png",
        "Caption": "Table 2. Evaluation results for links",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Figure-c3.png",
        "Caption": "Figure 3: Division of Raw Pattern into Combina- tion Pattern Components (Entity-Main-Entity) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0106/parts/0-Table-c3.png",
        "Caption": "Table 3: Effects of combination using the confidence measure. The upper numbers and the lower numbers are of the character-based and the subword-based, respec- tively ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1062/parts/0-Table-c2.png",
        "Caption": "Figure 2: Dirichlet-Tree prior of depth two.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-2123/parts/0-Table-c7.png",
        "Caption": "Figure 2: Performance of Unsupervised Name Mining",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J06-2001/parts/0-Table-c12.png",
        "Caption": "Table 2: Segmentation results by a pure subword-based IOB tagging. The upper numbers are of the character- based and the lower ones, the subword-based. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0206/parts/0-Figure-c2.png",
        "Caption": "Figure 4: Graph for a neo-Davidsonian structure.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-1025/parts/0-Table-c5.png",
        "Caption": "Table 5: The System Performance Based on Com- binations of Surface and Semantic Features. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Table-c11.png",
        "Caption": "Table 10: F-measures for different systems",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1141/parts/0-Figure-c2.png",
        "Caption": "Table 4: Overall accuracy of maximum entropy sys- tem using different subsets of features for People\u2019s Daily News words (manually segmented, part-of- speech-tagged). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/I05-3030/parts/0-Table-c2.png",
        "Caption": "Table 2: Perplexity results for the immediate- bihead model ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D11-1076/parts/0-Figure-c8.png",
        "Caption": "Table 13 Example translations for the translation direction English to German using three different reordering constraints: MON, EG, and S3. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3812/parts/0-Figure-c2.png",
        "Caption": "Table 2: MRR of baseline and reinforced matrices",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S0885/parts/0-Table-c5.png",
        "Caption": "Table 3: Summary of devtest results and shared task test results for submitted systems and LIU baseline with hier- archical reordering. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c10.png",
        "Caption": "Figure 1 Relation between number of classes and alternations. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-1025/parts/0-Figure-c3.png",
        "Caption": "Figure 13 The accuracy/speed tradeoff graph for the phrase-structure parser. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P08-2023/parts/0-Table-c3.png",
        "Caption": "Table 3. Comparison our closed results with the top three in all test sets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2204/parts/0-Table-c1.png",
        "Caption": "Table 1: Performance of our system versus a baseline",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1032/parts/0-Figure-c2.png",
        "Caption": "Figure 4: Example DCS trees for utterances in which syntactic and semantic scope diverge. These trees reflect the syntactic structure, which facilitates parsing, but importantly, these trees also precisely encode the correct semantic scope. The main mechanism is using a mark relation (E, Q, or C) low in the tree paired with an execute relation (Xi ) higher up at the desired semantic point. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2023/parts/0-Figure-c2.png",
        "Caption": "Figure 1. Overview of the WEBRE algorithm (Illustrated with examples sampled from experiment results). The tables and rec- tangles with a database sign show knowledge sources, shaded rectangles show the 2 phases, and the dotted shapes show the sys- tem output, a set of Type A relations and a set of Type B relations. The orange arrows denote resources used in phase 1 and the green arrows show the resources used in phase 2. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PMTS_n09/parts/0-Table-c2.png",
        "Caption": "Figure 4: A narrative chain and its reverse order.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-2015/parts/0-Table-c3.png",
        "Caption": "Table 4: Results for nouns",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1081/parts/0-Figure-c7.png",
        "Caption": "Table 10 Alternatives to training on gold-only feature values. Top: Select MaltParser CORE 12+. . . models re-trained on predicted or gold + predicted feature values. Bottom: Similar models to the top half, with the Easy-First Parser. Statistical significance tested only for CORE 12+. . . models on predicted input: significance of the MaltParser models from the MaltParser CORE 12 baseline model, and significance of the Easy-First Parser models from the Easy-First Parser CORE 12 baseline. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D08-1032/parts/0-Figure-c1.png",
        "Caption": "Figure 2: Precision of acquired relations (causality). L and S denote lenient and strict evaluation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-2036/parts/0-Table-c3.png",
        "Caption": "Figure 2. Consistently formatted sentence trans-                   lation pairs ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P01-1027/parts/0-Table-c7.png",
        "Caption": "Table 3 Top 60 most frequent root phrases in DE\u2192EN data with at least two words, shown with their counts. Shown in bold are the actual root words in the lexical dependency trees from which these phrases were extracted; these are extracted along with the phrases and used for back-off features. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1059/parts/0-Figure-c1.png",
        "Caption": "Table 2. Sources of the training data",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D12-1074/parts/0-Table-c1.png",
        "Caption": "Table 2 Parsing performance with each POS tag set, on gold and predicted input. L AS = labeled attachment accuracy (dependency + relation). U AS = unlabeled attachment accuracy (dependency only). L S = relation label prediction accuracy. L AS diff = difference between labeled attachment accuracy on gold and predicted input. POS acc = POS tag prediction accuracy. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1003/parts/0-Table-c3.png",
        "Caption": "Figure 3: A conceptual gure of the lexicalization",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P04-1036/parts/0-Table-c3.png",
        "Caption": "Figure 1: Dissimilarity of temporal distributions of \u2018WTO\u2019 in English and Chinese corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-1017/parts/0-Table-c3.png",
        "Caption": "Figure 3: Using different amounts of annotated training",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1025/parts/0-Figure-c2.png",
        "Caption": "Table 2: Comparison of Moses and KIT phrase extraction systems ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-3909/parts/0-Table-c4.png",
        "Caption": "Figure 1 The prevalence ranking process for the noun star. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C08-1107/parts/0-Table-c2.png",
        "Caption": "Table 6: Parameters used in our system.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2135/parts/0-Table-c2.png",
        "Caption": "Table 16 Accuracy comparisons between various dependency parsers on English data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-1065/parts/0-Table-c3.png",
        "Caption": "Table 15: Challenge in Elliptical Constructions",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1087/parts/0-Table-c1.png",
        "Caption": "Table 7: ROUGE-2 in empirical approach",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J10-3003/parts/0-Figure-c8.png",
        "Caption": "Figure 4: The constituent Restoring of its constructive and effective role parsed by the three different models (gold segmen- tation). The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals. Like verbs, maSdar takes arguments and assigns case to its objects, whereas it also demonstrates nominal characteristics by, e.g., taking determiners and heading iDafa (Fassi Fehri, 1993). In the ATB, +2  3   asta\u2019adah is tagged 48 times as a noun and 9 times as verbal noun. Consequently, all three parsers prefer the nominal reading. Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify. None of the models attach the attributive adjectives correctly. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-0118/parts/0-Table-c1.png",
        "Caption": "Table 4 Effect of pruning parameter tp and heuristic function on search efficiency for direct-translation model (Np = 50,000). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1007/parts/0-Figure-c1.png",
        "Caption": "Table 3: Most probable child phrases for the parent phrase \u201cmade up\u201d for each direction, sorted by the con- ditional probability of the child phrase given the parent phrase and direction. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N13-1095/parts/0-Table-c2.png",
        "Caption": "Table 5: Individual Performance of KSs for Disasters",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1063/parts/0-Figure-c3.png",
        "Caption": "Table 9: Scale-up to 160K on IWSLT data sets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00-1025/parts/0-Figure-c13.png",
        "Caption": "Table 4: The System Performance Based on Each Single Feature Set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1004/parts/0-Table-c3.png",
        "Caption": "Table 3: Reranking results of the three tagging   kernels on the Italian and English testset. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-1065/parts/0-Table-c7.png",
        "Caption": "Table 4: Comparison with previous work.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Table-c10.png",
        "Caption": "Figure 5: Average rank of correct translation according to average source term frequency ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-3114/parts/0-Figure-c16.png",
        "Caption": "Table 1: BLEU scores on the Europarl development test data ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N07-3010/parts/0-Figure-c2.png",
        "Caption": "Table 4: Ordered List of Increased/Decreased Number of Correctly Tagged Words ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Table-c1.png",
        "Caption": "Table 5 Statistics of corpora for training: Verbmobil and Nespole! Singletons are types occurring only once in training. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1098/parts/0-Table-c4.png",
        "Caption": "Figure 1: An example of alignment for Japanese and English sentences",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1084/parts/0-Table-c2.png",
        "Caption": "Figure 2: Sequence of POS-tagged units used to estimate the bilingual n-gram LM. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-3311/parts/0-Table-c1.png",
        "Caption": "Table 2. Performance of Word Alignment.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1050/parts/0-Figure-c1.png",
        "Caption": "Table 6: Number of evaluated English NEs.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N03-1027/parts/0-Table-c6.png",
        "Caption": "Figure 8: Translated chunked sentence",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1016/parts/0-Table-c6.png",
        "Caption": "Figure 8: Surface composition of embedded structures.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H01-1062/parts/0-Figure-c2.png",
        "Caption": "Figure 3: cumulative distribution of frequency (CDF) of the relative ranking of model-predicted probability of being positive for false negatives in a pool mixed of false negatives and true negatives; and the CDF of the relative ranking of model-predicted probability of being negative for false positives in a pool mixed of false positives and true positives. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3604/parts/0-Figure-c1.png",
        "Caption": "Table 1: Experimental results with individual features, compared against Moses and the moses-chart baseline. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2101/parts/0-Figure-c2.png",
        "Caption": "Table 2. The noun \u201cbox\u201d in Wordnet: each line lists one synset, the set of synonyms, a definition, an optional example sentence, and the supersense label. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1062/parts/0-Figure-c3.png",
        "Caption": "Figure 1: An excerpt from the text, with core- ferring noun phrases annotated. English trans- lation in italics. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P08-1078/parts/0-Table-c1.png",
        "Caption": "Table 7: Vocabulary size of NIST task (40K)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2302/parts/0-Table-c5.png",
        "Caption": "Table 3: Scores for UPUC corpus",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1111/parts/0-Table-c6.png",
        "Caption": "Table 4 DP-TSG notation. For consistency, we largely follow the notation of Liang, Jordan, and Klein (2010). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-2025/parts/0-Table-c1.png",
        "Caption": "Figure 3: Violation permutation transducer.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/D09-1092/parts/0-Figure-c6.png",
        "Caption": "Figure 2: LexRank example: sentence similarity graph with a cosine threshold of 0.15. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1115/parts/0-Table-c4.png",
        "Caption": "Table 1: Key notation.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1161/parts/0-Figure-c6.png",
        "Caption": "Table 2: Pearson\u2019s (r) correlation results over the WMT all-en dataset, and the subset of the dataset that contains noun compounds ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2230/parts/0-Table-c8.png",
        "Caption": "Table 3: Translation Candidates for \u8e81\u9b31\u75c5 (manic- depression) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1005/parts/0-Figure-c1.png",
        "Caption": "Table 1 DP-based algorithm for solving traveling-salesman problems due to Held and Karp. The outermost loop is over the cardinality of subsets of already visited cities. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0311/parts/0-Table-c6.png",
        "Caption": "Table 2: Error analysis of parser components av- eraged over Arabic, Bulgarian, Danish, Dutch, Japanese, Portuguese, Slovene, Spanish, Swedish and Turkish. N/P: Allow non-projective/Force pro- jective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6201/parts/0-Table-c4.png",
        "Caption": "Table 17 Training the Easy-First Parser on gold and predicted tags, accuracy by gold attachment type (selected): subject, object, modification (of a verb or a noun) by a noun, modification (of a verb or a noun) by a preposition, idafa, and overall results (repeated). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1101/parts/0-Table-c1.png",
        "Caption": "Table 12: Performance of Altavista counts and BNC counts for noun countability detection (data from Bald- win and Bond 2003) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1005/parts/0-Table-c5.png",
        "Caption": "Table 5 Comparative precision values for the top 20 similarity lists of the three selected similarity measures, with MI and Bootstrapped feature weighting for each. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1750/parts/0-Table-c3.png",
        "Caption": "Figure 4: Relation between cross entropy and pars-",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1081/parts/0-Figure-c8.png",
        "Caption": "Table 2: Case frame of \u201chaken (dispatch).\u201d",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1121/parts/0-Table-c2.png",
        "Caption": "Table 4: Complexity Analysis of Algorithm 1.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-1014/parts/0-Figure-c1.png",
        "Caption": "Figure 1: Illustration of Pareto Frontier. Ten hypotheses are plotted by their scores in two metrics. Hypotheses indicated by a circle (o) are pareto-optimal, while those indicated by a plus (+) are not. The line shows the convex hull, which attains only a subset of pareto-optimal points. The triangle (4) is a point that is weakly pareto-optimal but not pareto-optimal. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1004/parts/0-Table-c3.png",
        "Caption": "Table 2: Performance of SYSTEM 2 (simplified PDTB tags) when manually counting for improved, equal and degraded translations compared to the BASELINE, in samples from the PDTB section 23 test set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Figure-c4.png",
        "Caption": "Table 2: Accuracy for MBL and SVM classifiers on Talbanken05 nouns in accumulated frequency bins by Parole frequency. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Table-c15.png",
        "Caption": "Table 5: Results tested against gs-swaco-subjective",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-2003/parts/0-Table-c4.png",
        "Caption": "Table 3: Results of the baseline model: best 5 guesses",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1063/parts/0-Figure-c2.png",
        "Caption": "Table 3: Impact of Three Subtasks on Coreference Resolution Performance. A score marked with a * indicates that a 0.5 threshold was used because threshold selection from the training data resulted in an extreme version of the system, i.e. one that places all CEs into a single coreference chain. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-2015/parts/0-Table-c4.png",
        "Caption": "Figure 10 Number of processed arcs for the pseudotranslation task as a function of the input sentence length J (y-axis is given in log scale). The complexity for the four different reordering constraints MON, GE, EG, and S3 is given. The complexity of the S3 constraint is close to J4 . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P04-2007/parts/0-Table-c1.png",
        "Caption": "Table 3: Non-projective sentences and arcs in PDT and DDT (NonP = non-projective)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1727/parts/0-Figure-c1.png",
        "Caption": "Table 3: Segmentation performance on words that have the same final suffix as their preceding words. The F1 scores are computed based on all boundaries within the words, but the accuracies are obtained using only the final suffixes. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c28.png",
        "Caption": "Figure 1: Examples of sentences x, domain-independent underspecified logical forms l0 , fully specified logical forms y, and answers a drawn from the Freebase domain. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H05-1083/parts/0-Table-c4.png",
        "Caption": "Table 3: Impact of Syntactic Features on English Sys- tem After Taking out Distance Features. Numbers are F-measures(%). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2204/parts/0-Figure-c2.png",
        "Caption": "Table 5 Dev-set results when using lattice parsing on top of an external lexicon/analyzer. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1062/parts/0-Figure-c2.png",
        "Caption": "Table 2: accuracy using non-averaged and averaged perceptron.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1125/parts/0-Figure-c6.png",
        "Caption": "Table 2: BLEU scores of English to Russian ma- chine translation system evaluated on tst2012 and tst2013 using baseline GIZA++ alignment and transliteration augmented-GIZA++ alignment and post-processed the output by transliterating OOVs. Human evaluation in WMT13 is performed on TA-GIZA++ tested on tst2013 (marked with *) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W08-0335/parts/0-Table-c6.png",
        "Caption": "Figure 3: Example of the Hybrid Method",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1083/parts/0-Figure-c3.png",
        "Caption": "Table 5: The contribution of MMVC in a rank-based classi- fier combination on S ENSEVAL -1 and S ENSEVAL -2 English as computed by 5-fold cross validation over training data ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P04-1017/parts/0-Figure-c3.png",
        "Caption": "Table 1: Sample of extracted entailment rules.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PS15684_w09/parts/0-Figure-c9.png",
        "Caption": "Table 3: Examples errors introduced by YAGO and FrameNet.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1110/parts/0-Table-c2.png",
        "Caption": "Table 5: Comparison of difference systems on the ACE RDC 2004 corpus over both 7 types (outside the parentheses) and 23 subtypes (inside the parentheses) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/M98-1003/parts/0-Figure-c8.png",
        "Caption": "Figure 1: Example consensus network with votes on word arcs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4916/parts/0-Table-c6.png",
        "Caption": "Table 6: Comparison to onlySL and onlyGraph.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N12-1006/parts/0-Table-c6.png",
        "Caption": "Table 3: Comparison results with TAC 2008 Three Top Ranked Systems (system 1-3 demonstrate top 3 systems in TAC) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-0118/parts/0-Figure-c1.png",
        "Caption": "Table 1: DP algorithm for statistical machine translation.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1068/parts/0-Figure-c3.png",
        "Caption": "Figure 2. The procedure of TBL entity track- ing/coreference model ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W08-0335/parts/0-Table-c7.png",
        "Caption": "Table 4: Examples of the three top candidates in the transliteration of English/Arabic, English/Hindi and English/Chinese. The second column is the rank. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-5011/parts/0-Table-c2.png",
        "Caption": "Table 5: O\ufb03cial results for the English and Basque lexical tasks (recall). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-2122/parts/0-Figure-c1.png",
        "Caption": "Figure 2: Runtimes for sentences of length 10\u201380. The                                          graph shows the average runtimes ( ) of 10 different sample sentences of the respective length with swap op- erations restricted to a maximum swap segment size of 5 and a maximum swap distance of 2. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmorph_p00/parts/0-Table-c10.png",
        "Caption": "Table 7: Non-anaphoric DNP examples",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1027/parts/0-Figure-c1.png",
        "Caption": "Table 3 Precision of existing and proposed approaches. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-3004/parts/0-Table-c9.png",
        "Caption": "Figure 3: Comparison of paraphrase generators. Top: the MOSES baseline; middle and bold: the \u201ctrue-score\u201d MCPG; down: the \u201ctranslator\u201d MCPG. The use of \u201ctrue-score\u201d improves the MCPG per- formances. MCPG reaches MOSES performance level. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-3205/parts/0-Figure-c1.png",
        "Caption": "Table 2: All the parameters of Measures for de- termining semantic compositionality described in Section 3 used in our experiments. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-2049/parts/0-Table-c2.png",
        "Caption": "Table 7: Results of combining the character-category association and rule-based models: best guess ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1012/parts/0-Table-c4.png",
        "Caption": "Table 3: The effect of syntactic features when predicting morphological information. * mark statistically signifi- cantly better models compared to our baseline (sentence- based t-test with \u03b1 = 0.05). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1141/parts/0-Figure-c7.png",
        "Caption": "Table 2: Training Data Sizes for Common ESL Confused Words ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-1025/parts/0-Table-c3.png",
        "Caption": "Figure 2: Mixed Membership MEDLDA",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J09-3004/parts/0-Table-c7.png",
        "Caption": "Table 10: An example antecedent of a nominal in- teraction keyword ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1131/parts/0-Table-c2.png",
        "Caption": "Table 2: Corpus statistics: Verbmobil training. Singletons are types occurring only once in train- ing. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E06-1030/parts/0-Table-c6.png",
        "Caption": "Figure 8 Word sense disambiguation accuracy for \u201cNP1 V to NP2 NP3\u201d frame. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3604/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Improved syntax-based translations due to MIRA-trained weights.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0707/parts/0-Table-c2.png",
        "Caption": "Table 16 Accuracy comparisons between various dependency parsers on English data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-5011/parts/0-Table-c1.png",
        "Caption": "Figure 2: Recall-Precision curves for ranking using: (a) only the prior (baseline); (b) allCP; (c) allCP+pr. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H05-1001/parts/0-Table-c2.png",
        "Caption": "Figure 1: Source, transformed and extracted trees given headline British soldier killed in Afghanistan",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P08-2023/parts/0-Table-c6.png",
        "Caption": "Figure 1: Structure of a term in the original documents",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Q13-1015/parts/0-Table-c3.png",
        "Caption": "Figure 1: Plan to the experiments described in this paper",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-2009/parts/0-Figure-c4.png",
        "Caption": "Figure 1: An example confusion network construc- tion ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Figure-c12.png",
        "Caption": "Table 1: Data examined by the two systems for the ATB",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1110/parts/0-Table-c4.png",
        "Caption": "Table 7 Grammatical contexts used for acquiring the BNC thesaurus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1039/parts/0-Figure-c2.png",
        "Caption": "Table 4 Experiments on the threshold\u2013partial recall relationship of the small corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1125/parts/0-Table-c1.png",
        "Caption": "Figure 3: Individual Classifier Properties (cross-validation on SENSEVAL training data)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2139/parts/0-Table-c2.png",
        "Caption": "Table 4: Cluster features ordered by importance.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Table-c2.png",
        "Caption": "Table 6: Derivation by Means of Adding a Suffix",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1083/parts/0-Table-c3.png",
        "Caption": "Table 3: Mixed-case TER and BLEU, and lower-case METEOR scores on Chinese NIST MT03+MT04. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P08-2003/parts/0-Figure-c2.png",
        "Caption": "Table 4: Density of signature caseframes after merging to various threshold for the initial (Init.) and update (Up.) summarization tasks (Study 2). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-1098/parts/0-Figure-c2.png",
        "Caption": "Figure 8: The actual output of our parser trained with a fully annotated treebank. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W12-1011/parts/0-Table-c2.png",
        "Caption": "Table 1: Recall (R), Precision (P) and Mean Average Pre- cision (MAP) when only matching template hypotheses directly. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1143/parts/0-Figure-c6.png",
        "Caption": "Table 8 Model accuracy using equal distribution of verb frequencies for the estimation of P(c). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6211/parts/0-Figure-c1.png",
        "Caption": "Figure 4: Average results of Reweighting among all 7 speakers when the amount of speaker specific data is 0, 500, 2000 ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H01-1062/parts/0-Figure-c5.png",
        "Caption": "Table 1: Phrase pairs extracted from a document pair               with an economic topic ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1023/parts/0-Table-c4.png",
        "Caption": "Table 2: Dataset characteristics including the number of documents, annotated CEs, coreference chains, annotated CEs per chain (average), and number of documents in the train/test split. We use st to indicate a standard train/test split. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1086/parts/0-Table-c3.png",
        "Caption": "Table 12 Experiments combining dependency relations, words and part-of-speech",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N03-1010/parts/0-Table-c1.png",
        "Caption": "Figure 7: Percent of query language documents for which",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1117/parts/0-Table-c2.png",
        "Caption": "Table 5 Evaluation of Correction and Inference Mechanisms",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W13-2512/parts/0-Figure-c1.png",
        "Caption": "Table 7: Precision of rules with non-neutral parent label (ID: daughters and parent have identical labels)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-1115/parts/0-Table-c3.png",
        "Caption": "Table 1: Meta-evaluation results at document level",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1059/parts/0-Figure-c3.png",
        "Caption": "Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where poor sense prediction hindered absolute performance. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pproc2014_n09/parts/0-Table-c7.png",
        "Caption": "Table 2: POS tagging of unknown words using contextual and lexical Features (accuracy in per- cent).   \u008d is based only on contextual features,  T\u008d \u008e is based on contextual and lexical features. SM(    _ #\u00a7 )                                                  2 denotes that   \u00a7 follows   in the sequential model.                           2 ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D08-1032/parts/0-Table-c4.png",
        "Caption": "Figure 1: BiTAM models for Bilingual document- and sentence-pairs. A node in the graph represents a random variable, and a hexagon denotes a parameter. Un-shaded nodes are hidden variables. All the plates represent replicates. The outmost plate (M -plate) represents M bilingual document-pairs, while the inner N -plate represents the N repeated choice of topics for each sentence-pairs in the document; the inner J-plate represents J word-pairs within each sentence-pair. (a) BiTAM-1 samples one topic (denoted by z) per sentence-pair; (b) BiTAM-2 utilizes the sentence-level topics for both the translation model (i.e., p(f |e, z)) and the monolingual word distribution (i.e., p(e|z)); (c) BiTAM-3 samples one topic per word-pair. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1098/parts/0-Figure-c2.png",
        "Caption": "Table 2: Comparison of three statistical translation approaches (test on text input: 251 sentences = 2197 words + 430 punctuation marks). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Figure-c4.png",
        "Caption": "Table 8: Comparison with other systems",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Figure-c1.png",
        "Caption": "Table 2: Semantic features.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PbaneaCSL/parts/0-Figure-c5.png",
        "Caption": "Figure 1: Workflow for NIL knowledge engineering component. NILE refers to NIL expression, which is identified and annotated by human annotator. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1141/parts/0-Table-c4.png",
        "Caption": "Table 6: Accuracy with different sizes of unlabeled data (random selection) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J06-1004/parts/0-Figure-c11.png",
        "Caption": "Table 1: Word segmentation on NIST data sets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1110/parts/0-Table-c1.png",
        "Caption": "Table 4: Results for feature ablation experiments.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D11-1095/parts/0-Table-c1.png",
        "Caption": "Table 2: Feature prediction accuracy and set sizes. * = The set includes a \"N/A\" value. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2145/parts/0-Table-c3.png",
        "Caption": "Figure 1 Architecture of the statistical translation approach based on Bayes\u2019 decision rule. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H05-1001/parts/0-Table-c4.png",
        "Caption": "Table 7 Disambiguation results for G2 and X2 . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1016/parts/0-Figure-c2.png",
        "Caption": "Figure 2: ANC pronoun resolution accuracy for varying SVM-thresholds. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1009/parts/0-Table-c4.png",
        "Caption": "Figure 4: Translated fragments, according to the lexicon.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E06-1030/parts/0-Table-c5.png",
        "Caption": "Table 4: NIST08 Chinese-English translation BLEU",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1015/parts/0-Table-c10.png",
        "Caption": "Figure 3: Comparison of rate of convergence between coordinate ascent and our expected BLEU direction finder (D = 500). Noisy refers to the noisy experimental setting. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D13-1031/parts/0-Table-c1.png",
        "Caption": "Figure 3: Network of relations. Edges indicate that the relations have a non-empty support inter- section, and edge labels show the size of the inter- section. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1109/parts/0-Table-c1.png",
        "Caption": "Table 2: Syntactic features. h and ld mark features from the head and the left-most daughter, dir is a binary fea- ture marking the direction of the head with respect to the current token. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-3004/parts/0-Table-c16.png",
        "Caption": "Figure 1: Examples of the semantic role features",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pstat_p00/parts/0-Figure-c2.png",
        "Caption": "Table 7 Comparing clustering initializations on D2. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2708/parts/0-Figure-c2.png",
        "Caption": "Figure 5: Results for choosing the correct ordered chain. (\u2265 10) means there were at least 10 pairs of ordered events in the chain. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1033/parts/0-Table-c4.png",
        "Caption": "Table 2 Accuracy(%) of \u2018obscure\u2019 name recognition",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P04470/parts/0-Table-c2.png",
        "Caption": "Figure 1: An example sequence representation. The subgraph on the left represents a bigram feature. The subgraph on the right represents a unigram feature that states the entity type of arg 2 . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c16.png",
        "Caption": "Table 10: Overall transliteration performance",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1107/parts/0-Table-c10.png",
        "Caption": "Figure 13 The accuracy/speed tradeoff graph for the phrase-structure parser. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1059/parts/0-Table-c2.png",
        "Caption": "Figure 2: Feature space growing curve. The horizontal scope X[i:j] denotes the introduction of different tem- plates. X[0:5]: Cn (n = \u22122..2); X[5:9]: Cn Cn+1 (n = \u22122..1); X[9:10]: C\u22121 C1 ; X[10:15]: C0 Cn (n = \u22122..2); X[15:19]: C0 Cn Cn+1 (n = \u22122..1); X[19:20]: C0 C\u22121 C1 ; X[20:21]: W0 ; X[21:22]: W\u22121 W0 . W0 de- notes the current considering word, while W\u22121 denotes the word in front of W0 . All the data are collected from the training procedure on MSR corpus of SIGHAN bake- off 2. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P283_w09/parts/0-Figure-c2.png",
        "Caption": "Figure 2: Framework overview.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3812/parts/0-Figure-c7.png",
        "Caption": "Figure 4: PTB vs. Wiktionary type coverage across sec- tions of the Brown corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1018/parts/0-Table-c3.png",
        "Caption": "Table 3: Analysis of context length",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W08-0602/parts/0-Table-c4.png",
        "Caption": "Table 1: Size of co-occurrence databases",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2164/parts/0-Figure-c6.png",
        "Caption": "Table 5: Gender detection accuracies (%) using a 4-gram language model for the letter sequence of          the source name in Latin script. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c23.png",
        "Caption": "Table 9: ROUGE-SU in empirical approach",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1143/parts/0-Table-c4.png",
        "Caption": "Figure 1: Graphical model for PLTM.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4916/parts/0-Figure-c1.png",
        "Caption": "Table 7. System Performance Comparison.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-1014/parts/0-Table-c2.png",
        "Caption": "Figure 4: Improvements in F-measure on MUC-7 plotted against amount of selected unlabeled data used ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D07-1018/parts/0-Table-c3.png",
        "Caption": "Figure 1: Different tree span categories with SPT (dotted circle) and an ex- ample of the dynamic context-sensitive tree span (solid circle) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1009/parts/0-Table-c6.png",
        "Caption": "Figure 1: Different tree span categories with SPT (dotted circle) and an ex- ample of the dynamic context-sensitive tree span (solid circle) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D07-1076/parts/0-Table-c5.png",
        "Caption": "Table 3: Size of the vocabularies for the \u201cNo LP\u201d and \u201cWith LP\u201d models for which we can impose constraints. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1013/parts/0-Figure-c5.png",
        "Caption": "Table 1: Number of entries in 3 corpora",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P05-1004/parts/0-Table-c7.png",
        "Caption": "Figure 1. F1-measures with \uf061 in [0 3]",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PbaneaCSL/parts/0-Figure-c6.png",
        "Caption": "Figure 1: Three representations of NP modifications: (a) the original treebank representation; (b) Selective left-corner representation; and (c) a flat structure that is unambiguously equivalent to (b) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1058/parts/0-Table-c1.png",
        "Caption": "Figure 3: Dependency parse of a sentence that contains indirect speech (see Sentence 2). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-2206/parts/0-Figure-c3.png",
        "Caption": "Table 5: Results on the NIST MATR 2008 test set for several variations of paraphrase usage.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-1001/parts/0-Figure-c2.png",
        "Caption": "Table 5: Ablation study of the web (w), query- log (q) and table (t) features (bold letters indicate whole feature families). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1063/parts/0-Table-c4.png",
        "Caption": "Figure 3: Boundary information is added to states to cal- culate the bracket scores in the face of word segmentation errors. Left: the original parse tree, Right: the converted parse tree. The numbers in the brackets are the indices of the character boundaries based on word segmentation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Figure-c1.png",
        "Caption": "Table 6. Impact of Confidence Measures",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1086/parts/0-Figure-c3.png",
        "Caption": "Table 1: Part of a sample headline cluster, with sub-clusters ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P10-2049/parts/0-Table-c3.png",
        "Caption": "Figure 2: Results for baseline using introspection and simple statistics of the data (including test data).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-2025/parts/0-Figure-c1.png",
        "Caption": "Table 6 NER type-insensitive (type-sensitive) performance of different Chinese NE recognizers. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1012/parts/0-Table-c1.png",
        "Caption": "Table 4: Cross-domain B3 and MUC results for Reconcile and Sieve with lexical features. Gray cells represent results that are not significantly different from the best results in the column at the 0.05 p-level. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PbaneaCSL/parts/0-Figure-c2.png",
        "Caption": "Table 2: NIL expression forms based on POS attribute.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N12-1045/parts/0-Table-c2.png",
        "Caption": "Table 4: The System Performance Based on Each Single Feature Set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-1624/parts/0-Table-c3.png",
        "Caption": "Table 3: Summary of devtest results and shared task test results for submitted systems and LIU baseline with hier- archical reordering. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4917/parts/0-Table-c1.png",
        "Caption": "Table 5: Search Success Rate (1 million hypothe- ses) [%]. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Psem_p07/parts/0-Figure-c1.png",
        "Caption": "Table 4: Micro-averaged (across the 5 folds) RE results using gold mentions.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1001/parts/0-Table-c2.png",
        "Caption": "Table 1: Relationship types and their argument type con- straints. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c1.png",
        "Caption": "Table 21: Arabic Vocalization Problem",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Table-c3.png",
        "Caption": "Table 3: BasicRE gives the performance of our basic RE system on predicting fine-grained relations, obtained by performing 5-fold cross validation on only the news wire corpus of ACE-2004. Each sub- sequent row +Hier, +Hier+relEntC, +Coref, +Wiki, and +Cluster gives the individual contribution from using each knowledge. The bottom row +ALL gives the performance improvements from adding +Hier+relEntC+Coref+Wiki+Cluster. \u223c indicates no change in score. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1005/parts/0-Table-c3.png",
        "Caption": "Table 4: Translation results for German-English",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Figure-c3.png",
        "Caption": "Table 5: precision decrease when omitting non-",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1004/parts/0-Table-c5.png",
        "Caption": "Figure 2: Skeleton of basic lexicon transducer in LEXC generated from BAMA lexicons. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1008/parts/0-Table-c15.png",
        "Caption": "Figure 3. Comparison of two augmentation strategies over different sampling strategies in selecting the initial seed set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4917/parts/0-Table-c9.png",
        "Caption": "Table 3: BasicRE gives the performance of our basic RE system on predicting fine-grained relations, obtained by performing 5-fold cross validation on only the news wire corpus of ACE-2004. Each sub- sequent row +Hier, +Hier+relEntC, +Coref, +Wiki, and +Cluster gives the individual contribution from using each knowledge. The bottom row +ALL gives the performance improvements from adding +Hier+relEntC+Coref+Wiki+Cluster. \u223c indicates no change in score. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J09-3004/parts/0-Figure-c1.png",
        "Caption": "Table 2: Examples of unigram and bigram features extracted from Figure 1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2101/parts/0-Figure-c7.png",
        "Caption": "Table 3: Baseline and oracle-best 4-gram BLEU scores with 4 references for NIST Chinese-English MT datasets. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N09-1057/parts/0-Table-c1.png",
        "Caption": "Figure 2: Example of a word lattice",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1025/parts/0-Table-c4.png",
        "Caption": "Table 1: Ten relation instances extracted by our system that did not appear in Freebase.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1099/parts/0-Table-c5.png",
        "Caption": "Figure 1: How the IBM models model the translation process. This is a hypothetical example and not taken from any actual training or decoding logs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P04470/parts/0-Table-c4.png",
        "Caption": "Table 2: Comparison of our system with the best-reported systems on MUC-6 and MUC-7",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1140/parts/0-Table-c2.png",
        "Caption": "Figure 5: Average rank of correct translation according to average source term frequency ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1091/parts/0-Table-c2.png",
        "Caption": "Figure 3: Training time comparison. The training time for each model is calculated from scratch. For example, the training time of IBM Model 4 includes the training time of IBM Model 1, the HMM, and IBM Model 3. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1133/parts/0-Table-c1.png",
        "Caption": "Figure 7 The adjusted frequencies of character sequences. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-2207/parts/0-Table-c1.png",
        "Caption": "Table 7 Some of the possible Spanish translations of the English phrase make with their memory-based con- text-dependent translation probabilities (rightmost column) compared against context-independent transla- tion probabilities of the baseline system ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1126/parts/0-Figure-c7.png",
        "Caption": "Table 4: Results of AR for Opinion Targets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S13-1039/parts/0-Table-c3.png",
        "Caption": "Table 2: Disambiguation scores on nine confusable set, attained by the all-words prediction classifier trained on 30 million examples of TRAIN - REUTERS, and by confusable experts on the same training set. The second column displays the number of exam- ples of each confusable set in the 30-million word training set; the list is ordered on this column. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1009/parts/0-Figure-c2.png",
        "Caption": "Table 8 Statistics for the test sets for German to English translation: Verbmobil Eval-2000 (Test and Develop) and Nespole! ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4917/parts/0-Table-c6.png",
        "Caption": "Table 3. System performance on the is-a relation on the CHEM dataset. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-2031/parts/0-Table-c2.png",
        "Caption": "Figure 6: MUC-7: Level Distribution of Each of the Facts",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-1004/parts/0-Table-c13.png",
        "Caption": "Table 3: Best observed performance of RF, SVM and GIZA++ and Levenshtein Distance",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1143/parts/0-Figure-c4.png",
        "Caption": "Figure 1: Rank trajectories of 4 LDA inferred topics, with incremental topic inference. The x-axis indicates the utterance number. The y-axis indicates a topic\u2019s rank at each utterance. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-0304/parts/0-Table-c5.png",
        "Caption": "Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PEAAI_n09/parts/0-Table-c3.png",
        "Caption": "Figure 1 Architecture of the statistical translation approach based on Bayes\u2019 decision rule. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-1604/parts/0-Table-c1.png",
        "Caption": " Table 3. Performance of Chinese system with perfect mentions and perfect relations ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P01-1017/parts/0-Table-c2.png",
        "Caption": "Table 1: The results of different systems for coreference resolution",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N13-1118/parts/0-Figure-c1.png",
        "Caption": "Fig. 9. Intrinsic and extrinsic evaluation of alignments in the small data experiments. (a) Alignment dictionary size normalized by the average of source and target vocabulary sizes. (b) Average alignment fertility of aligned singletons. (c) Percentage of unaligned singletons. (d) Number of symmetric alignments normalized by the average of source and target tokens. (e) Percentage of training set vocabulary covered by single-word phrases in the phrase table. (f) Decode-time rate of input words that are in the training vocabulary but without a translation in the phrase table. (g) Phrase table size normalized by the average of source and target tokens. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-2123/parts/0-Table-c6.png",
        "Caption": "Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep (\u2021 : p < 0.01). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D07-1012/parts/0-Table-c3.png",
        "Caption": "Table 2. Committee Agreement vs. Accuracy",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-1004/parts/0-Figure-c2.png",
        "Caption": "Table 2: Experimental results with combined features.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P05-1021/parts/0-Table-c4.png",
        "Caption": "Table 2. Input sentences.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1083/parts/0-Figure-c1.png",
        "Caption": "Table 1: a) An example of a document from Tu\u0308Ba-D/Z, b) an abbreviated entity grid representation of it, and c) the feature vector representation of the abbreviated entity grid for transitions of length two. Mentions of the entity Frauen are underlined. nom: nominative, acc: accusative, oth: dative, oblique, and other arguments ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1008/parts/0-Table-c17.png",
        "Caption": "Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing. For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1067/parts/0-Table-c2.png",
        "Caption": "Table 3: Evaluation of the three anaphoric resolvers discussed by Ng and Cardie. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1049/parts/0-Table-c4.png",
        "Caption": "Figure 4: MTO is fairly stable as long as the Z\u0303 constant 5.4 Morphological and orthographic features is within an order of magnitude of the real Z value. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-2060/parts/0-Figure-c1.png",
        "Caption": "Table 6 Results for Single Document System",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Figure-c5.png",
        "Caption": "Table 6: Parameters of Measures (Section 3) which, combined with particular WSMs, achieved the highest average correlation in TrValD. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D08-1032/parts/0-Table-c3.png",
        "Caption": "Table 6. Precision at top 100",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2017/parts/0-Figure-c1.png",
        "Caption": "Figure 3: Examples for the effect of the combined lexica.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6202/parts/0-Figure-c5.png",
        "Caption": "Figure 5: Frequencies of patterns in the evaluation data (causation). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S12-1023/parts/0-Table-c3.png",
        "Caption": "Table 4: Comparison of our method (FEATS) to state-of-the-art methods. Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (Grac\u0327a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1138/parts/0-Table-c2.png",
        "Caption": "Table 24 Accuracies of our phrase-structure parser on CTB5 using gold-standard and automatically assigned POS-tags. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N15-1071/parts/0-Table-c1.png",
        "Caption": "Table 7 Models with re-engineered DET and PERSON inflectional features. Statistical significance tested only on predicted input, against the CORE 12 baseline. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1407/parts/0-Figure-c3.png",
        "Caption": "Table 2: B3 results for baselines and lexicalized feature sets across four domains.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-0703/parts/0-Figure-c7.png",
        "Caption": "Table 10 Occurrences of error types for the best other-anaphora algorithm algoWebv4 . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N07-1024/parts/0-Table-c5.png",
        "Caption": "Table 1: Tagging results for different languages. For each language we report median one-to-one (1-1), many-to-one (m-1) and V-measure (V-m) together with standard deviation from five runs where median is taken over V-measure. Types is the number of word types in each corpus, True is the number of gold tags and Induced reports the median number of tags induced by the model together with standard deviation. Best Pub. lists the best published results so far (also 1-1, m-1 and V-m) in (Christodoulopoulos et al., 2011)\u2217 , (Blunsom and Cohn, 2011)? and (Lee et al., 2010)\u2020 . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1083/parts/0-Table-c4.png",
        "Caption": "Table 5: Comparison of difference systems on the ACE RDC 2004 corpus over both 7 types (outside the parentheses) and 23 subtypes (inside the parentheses) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-2040/parts/0-Figure-c2.png",
        "Caption": "Table 6: Results using different parsers",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J06-1004/parts/0-Figure-c3.png",
        "Caption": "Figure 3: Simulation comparing the expected table count (solid lines) versus the approximation under Eq. 3 (dashed lines) for various values of a. This data was gen- erated from a single PYP with b = 1, P0 (i) = 14 and n = 100 customers which all share the same tag. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1045/parts/0-Table-c4.png",
        "Caption": "Table 4: Dataset statistics: development (dev) and test.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0711/parts/0-Table-c12.png",
        "Caption": "Figure 1: Ungrammatical Arabic output of Google Trans- late for the English input The car goes quickly. The subject should agree with the verb in both gender and number, but the verb has masculine inflection. For clarity, the Arabic tokens are arranged left-to-right. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2087/parts/0-Table-c2.png",
        "Caption": "Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcls and the best published result (? Berg-Kirkpatrick et al. (2010) and \u2020 Lee et al. (2010)). This data was taken from the CoNLL-X shared task training sets, resulting in listed corpus sizes. Fine PoS tags were used for evaluation except for items marked with c , which used the coarse tags. For each language the systems were trained to produce the same number of tags as the gold standard. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Table-c12.png",
        "Caption": "Table 1: Sizes of our comparable corpora",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2101/parts/0-Figure-c8.png",
        "Caption": "Table 3: Evaluation against the monosemous (Pred.) and pol- ysemous (Multiple) gold standards. The figures in parentheses are results of evaluation on randomly polysemous data + sig- nificance of the actual figure. Results were obtained with fine- grained SCFs (including prepositions). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1016/parts/0-Table-c1.png",
        "Caption": "Figure 5 NP agreement violations that were caught by the agreement filter system. (a) Noun-compound case that was correctly handled. (b) Case involving conjunction that was correctly handled. (c) A case where fixing the agreement violation introduces a PP-attachment mistake. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C04-1131/parts/0-Table-c4.png",
        "Caption": "Table 1. A brief description of the tested parsers. Note that the Tune data is not the data used to train the individual parsers. Higher numbers in the right column reflect just the fact that the Test part is slightly easier to parse. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Figure-c5.png",
        "Caption": "Figure 3 Speed/accuracy tradeoff of the segmentor. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N12-1006/parts/0-Table-c2.png",
        "Caption": "Table 4: Ordered List of Increased/Decreased Number of Correctly Tagged Words ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1089/parts/0-Table-c5.png",
        "Caption": "Table 1: Examples of zero anaphora",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-3004/parts/0-Table-c15.png",
        "Caption": "Table 1: Word distribution in the extended Cilin",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-3004/parts/0-Table-c13.png",
        "Caption": "Table 3: The effect of syntactic features when predicting morphological information. * mark statistically signifi- cantly better models compared to our baseline (sentence- based t-test with \u03b1 = 0.05). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1404/parts/0-Figure-c1.png",
        "Caption": "Table 5: Parser performance on Brown;E, supervised adaptation",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1404/parts/0-Figure-c4.png",
        "Caption": "Figure 1: Tradeoff between Margin Threshold and name recognition performance ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-3114/parts/0-Figure-c1.png",
        "Caption": "Table 5 Gross corpus statistics for the pre-processed corpora used to train and evaluate our models. We compare to the WSJ section of the PTB: train (Sections 02\u201321); dev. (Section 22); test (Section 23). Due to its flat annotation style, the FTB sentences have fewer constituents per sentence. In the ATB, morphological variation accounts for the high proportion of word types to sentences. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-3014/parts/0-Table-c3.png",
        "Caption": "Figure 7 An example showing the generalization of the word lattice (a) into a slotted lattice (b). The word lattice is produced by aligning seven sentences. Nodes having in-degrees > 1 occur in more than one sentence. Nodes with thick incoming edges occur in all sentences. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-3208/parts/0-Table-c2.png",
        "Caption": "Table 1: This table shows the performance achieved by the different systems, shown in accuracy (%). The Number of cases denotes the number of instances in the testset. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S12-1023/parts/0-Table-c1.png",
        "Caption": "Table 2: Comparison of Moses and KIT phrase extraction systems ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-1004/parts/0-Table-c7.png",
        "Caption": "Table 12 NWI results on HK and AS corpora, NWI as post-processor versus unified approach. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W10-4135/parts/0-Table-c4.png",
        "Caption": "Figure 2: PubMed Results. The curve represents the Pareto Frontier of all results collected after multiple runs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N13-1095/parts/0-Table-c1.png",
        "Caption": "Table 5: BS on IWSLT 2006 task",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3812/parts/0-Figure-c3.png",
        "Caption": "Table 3 Words in the MSR gold test set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J09-3004/parts/0-Table-c11.png",
        "Caption": "Table 3: The results for three systems associ- ated with the project for the NP bracketing task, the shared task at CoNLL-99. The baseline re- sults have been obtained by finding NP chunks in the text with an algorithm which selects the most frequent chunk tag associated with each part-of- speech tag. The best results at CoNLL-99 was obtained with a bottom-up memory-based learner. An improved version of that system (MBL) deliv- ered the best project result. The MDL results have been obtained on a different data set and therefore combination of the three systems was not feasible. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3909/parts/0-Table-c6.png",
        "Caption": "Table 2. OOV term translation examples.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-0910/parts/0-Figure-c2.png",
        "Caption": "Table 2: Example non-coreferent paths: Italicized entities do not generally corefer",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1114/parts/0-Figure-c3.png",
        "Caption": "Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM B\uf76c\uf765\uf775 scores. \u2217 or \u2217\u2217 = significantly better than MERT baseline (p < 0.05 or 0.01, respectively). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1040/parts/0-Table-c1.png",
        "Caption": "Table 2: Sizes of the extracted datasets.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E12-1054/parts/0-Table-c1.png",
        "Caption": "Figure 4 4-tape representation for the Hebrew word htpqdut. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1038/parts/0-Figure-c3.png",
        "Caption": "Table 1: Datasets for the two experimental conditions.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E06-1030/parts/0-Table-c2.png",
        "Caption": "Table 4. Statistics of anaphor and antecedent pairs",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-2094/parts/0-Table-c2.png",
        "Caption": "Table 3: Baseline and oracle-best 4-gram BLEU scores with 4 references for NIST Chinese-English MT datasets. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1059/parts/0-Table-c5.png",
        "Caption": "Table 4: Performance comparison with the literature for candidate selection for MT ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1155/parts/0-Table-c2.png",
        "Caption": "Table 7: Feature blending of translation models",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P05-1004/parts/0-Table-c2.png",
        "Caption": "Table 5. Chinese system performance with   system mentions and system relations ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-3008/parts/0-Table-c1.png",
        "Caption": "Figure 3: Example derivation for the query \u2018how many people visit the public library of new york annu- ally.\u2019 Underspecified constants are labelled with the words from the query that they are associated with for readability. Constants from O, written in typeset, are introduced in step (c). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N10-1068/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Dynamic-Expansion Tree Span Scheme",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1050/parts/0-Figure-c5.png",
        "Caption": "Table 4: Different Context Window Size Setting",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-1065/parts/0-Table-c8.png",
        "Caption": "Table 2: Results for the submitted runs",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1086/parts/0-Figure-c1.png",
        "Caption": "Table 3 The top 10 ranked features for country produced by MI, the weighting function employed in the LIN method. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-0621/parts/0-Table-c2.png",
        "Caption": "Figure 1: The Classification Process",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P101167/parts/0-Table-c4.png",
        "Caption": "Figure 6: F-measure for the objective and subjective classes for multilingual bootstrapping (versus cross-lingual framework) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-1017/parts/0-Table-c2.png",
        "Caption": "Table 4: Example gender/number probability (%)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1045/parts/0-Figure-c1.png",
        "Caption": "Figure 1: The NLM-WSD test set and some of its sub- sets. Note that the test set used by (Joshi et al., 2005) comprises the set union of the terms used by (Liu et al., 2004) and (Leroy and Rindflesch, 2005) while the \u201ccom- mon subset\u201d is formed from their intersection. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1056/parts/0-Table-c3.png",
        "Caption": "Figure 2: Contribution of employing the dynamic         cache on different test documents ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1086/parts/0-Table-c2.png",
        "Caption": "Table 2: micro-average F1 and AUC for the algorithms.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2135/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Ungrammatical Arabic output of Google Trans- late for the English input The car goes quickly. The subject should agree with the verb in both gender and number, but the verb has masculine inflection. For clarity, the Arabic tokens are arranged left-to-right. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-3238/parts/0-Figure-c2.png",
        "Caption": "Figure 2: Some examples for MEDLINE tagset: Number of lex. entries per tag and sample words. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1025/parts/0-Table-c3.png",
        "Caption": "Table 3: Recall on different types of empty categories. YX = (Yang and Xue, 2010), Ours = split 6\u00d7. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1016/parts/0-Table-c2.png",
        "Caption": "Table 7. Impacts of the mined semantic lexicons and the use of PubMed",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PE2006_p00/parts/0-Figure-c8.png",
        "Caption": "Table 3: Comparison of raw input and constrained input.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Table-c17.png",
        "Caption": "Table 7: Simple parser vs full parser \u2013 syntactic quality. Trained on first 5,000 sentences of the training set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2139/parts/0-Table-c2.png",
        "Caption": "Table 1: Relationship types and their argument type con- straints. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/D09-1092/parts/0-Figure-c2.png",
        "Caption": "Table 4: Comparison to Related Approaches",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1017/parts/0-Table-c5.png",
        "Caption": "Table 17 WSD using predominant senses, training, and testing on all domain combinations (automatically classified corpora). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1005/parts/0-Figure-c2.png",
        "Caption": "Figure 6: Matched translations over |Ve | and |Vc |",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1046/parts/0-Table-c6.png",
        "Caption": "Figure 2: Results for baseline using introspection and simple statistics of the data (including test data).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Psem_p07/parts/0-Table-c1.png",
        "Caption": "Table 8 Results of all distributional similarity measures when tuning K over the development set. We encode the description of the measures presented in Table 2 in the following manner\u2014 h = health-care corpus; R = RCV1 corpus; b = binary templates; u = unary templates; L = Lin similarity measure; B = BInc similarity measure; pCt = pair of CUI tuples representation; pC = pair of CUIs representation; Ct = CUI tuple representation; C = CUI representation; Lin & Pantel = similarity lists learned by Lin and Pantel. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H01-1001/parts/0-Table-c3.png",
        "Caption": "Table 2: Sources of Dictionaries",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c13.png",
        "Caption": "Figure 1: Sample dataset (constructed by hand): Finnish verbs, with inflection for person and number.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1114/parts/0-Table-c2.png",
        "Caption": "Table 1: Context Clustering with Spectral-based Clustering technique. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2101/parts/0-Figure-c10.png",
        "Caption": "Table 2: Best results: For English, name lists are used. For German, part-of-speech tags are used ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-3402/parts/0-Table-c1.png",
        "Caption": "Table 5 Error analysis of confidence measure with and without EIV tag",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmert_n09/parts/0-Figure-c3.png",
        "Caption": "Table 2: MRR of baseline and reinforced matrices",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1016/parts/0-Table-c1.png",
        "Caption": "Table 6: Precision for each phrase type (Ev.Ling).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-2063/parts/0-Table-c1.png",
        "Caption": "Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a sequence of L bilingual units (tuples) u1 , ..., uL . Each tuple ui contains a source and a target phrase: si and ti . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/D09-1092/parts/0-Table-c4.png",
        "Caption": "Table 2: Normalization accuracy after training on n tokens and evaluating on 1,000 tokens (average of 10 random training and evaluation sets), compared to the \u201cbaseline\u201d score of the full text without any normalization ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1066/parts/0-Table-c4.png",
        "Caption": "Figure 1: Plate diagram of our model.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P08-1090/parts/0-Figure-c5.png",
        "Caption": "Figure 17: After Applying Compile-Replace to the Lower Side",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/I05-3013/parts/0-Table-c1.png",
        "Caption": "Table 4: Accuracy with different sizes of labeled data",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N07-1024/parts/0-Table-c6.png",
        "Caption": "Table 7. Features with Set Values",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1017/parts/0-Figure-c2.png",
        "Caption": "Figure 4: Overview of annotation environment",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0311/parts/0-Table-c4.png",
        "Caption": "Figure 6: From DRS to DRG: labelling.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P08-1090/parts/0-Figure-c4.png",
        "Caption": "Figure 5: Distribution over number of hits",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E12-1020/parts/0-Figure-c3.png",
        "Caption": "Table 3: The 10 best languages for the verb component of BANNARD using LCS. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C04-1126/parts/0-Table-c1.png",
        "Caption": "Figure 4: A toy instance of lattice construction",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1009/parts/0-Table-c2.png",
        "Caption": "Table 25: Arabic Tokenization Schemes",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J06-2001/parts/0-Table-c10.png",
        "Caption": "Figure 3: MTO falls sharply for less than 10 S-CODE dimensions, but more than 25 do not help. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-2007/parts/0-Figure-c1.png",
        "Caption": "Table 1: Gibbs sampling algorithm for IBM Model 1 (im- plemented in the accompanying software). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W12-1011/parts/0-Table-c3.png",
        "Caption": "Figure 1: One possible breakdown of spoken Arabic into dialect groups: Maghrebi, Egyptian, Levantine, Gulf and Iraqi. Habash (2010) gives a breakdown along mostly the same lines. We used this map as an illustration for annotators in our dialect classification task (Section 3.1), with Arabic names for the dialects instead of English. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N09-1007/parts/0-Figure-c1.png",
        "Caption": "Table 16 Accuracy comparisons between various dependency parsers on English data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1073/parts/0-Figure-c2.png",
        "Caption": "Table 1: Success rate of anaphora resolution",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2023/parts/0-Table-c1.png",
        "Caption": "Figure 2: Word prediction from a partial parse",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-3008/parts/0-Table-c6.png",
        "Caption": "Table 4: Results on the standard 14 CSSC data sets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S13-1039/parts/0-Figure-c1.png",
        "Caption": "Table 22 Accuracies of various phrase-structure parsers on CTB2 with gold-standard POS-tags. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0711/parts/0-Table-c16.png",
        "Caption": "Figure 4: POS tagging accuracy using one-at-a- time, character-based POS tagger ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0815/parts/0-Table-c4.png",
        "Caption": "Figure 1                                CATiB Annotation example. &  ( -  )  23 1+                                                        ,    4   !       $%                                                                          ./0   t\u03c2 ml HfydAt AlkA                                                                        Al\u00f0kyAt fy AlmdArs AlHkwmy  (\u2018The writer\u2019s smart      granddaughters work for public schools\u2019). The words in the tree are presented in the Arabic reading direction (from right to left). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1015/parts/0-Table-c2.png",
        "Caption": "Table 4: Final results on 25 corpora in 20 languages, with the number of induced classes equal to the number of gold standard tags in all cases. k-means and SVD2 models could not produce a clustering in the Czech CoNLL corpus due its size. Best published results are from \u2217 Christodoulopoulos et al. (2010), \u2020 Berg-Kirkpatrick et al. (2010) and \u2021 Lee et al. (2010). The latter two papers do not report VM scores. No best published results are shown for the MULTEXT languages; Christodoulopoulos et al. (2010) report results based on 45 tags suggesting that clark performs best on these corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pbulletin/parts/0-Table-c1.png",
        "Caption": "Table 1: Context Clustering with Spectral-based Clustering technique. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1086/parts/0-Table-c1.png",
        "Caption": "Figure 2: Glue Semantics proof for (83), English Way Construction (means interpretation)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-1014/parts/0-Table-c7.png",
        "Caption": "Table 6 Top 30 features of town by bootstrapped weighting based on LIN, WJ, and COS as initial similarities. The three sets of words are almost identical, with relatively minor ranking differences. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-1604/parts/0-Figure-c2.png",
        "Caption": "Table 6. Size of the test data",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1119/parts/0-Table-c1.png",
        "Caption": "Table 8 BNC results for other-anaphora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1016/parts/0-Table-c10.png",
        "Caption": "Table 1: Corpus statistics in Sighan Bakeoff 2005",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmorph_p00/parts/0-Table-c16.png",
        "Caption": "Table 1: Results for development and test set for the two languages by ME1 ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c8.png",
        "Caption": "Figure 2: performances over eight Variational EM itera- tions of BiTAM-1 using both the \u201cNull\u201d word and the laplace smoothing; IBM-1 is shown over eight EM iterations for comparison. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1086/parts/0-Figure-c3.png",
        "Caption": "Table 6. Examples of transformation rules of",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Plex_p00/parts/0-Figure-c4.png",
        "Caption": "Figure 4: The BRAT search dialog",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1016/parts/0-Table-c7.png",
        "Caption": "Table 1: Results on the MT02 and MT05 test sets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D10-1034/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Outline of word segmentation process",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0711/parts/0-Table-c7.png",
        "Caption": "Figure 2: performances over eight Variational EM itera- tions of BiTAM-1 using both the \u201cNull\u201d word and the laplace smoothing; IBM-1 is shown over eight EM iterations for comparison. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c18.png",
        "Caption": "Table 1: Ten highest-scoring matches for the Xin- hua corpus for 8/13/01. The final column is the \u2212log P estimate for the transliteration. Starred entries are incorrect. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-0621/parts/0-Table-c3.png",
        "Caption": "Table 4 Most frequent phrase dependencies in DE\u2192EN data, shown with their counts and attachment directions. Child phrases point to their parents. To focus on interesting phrase dependencies, we only show those in which one phrase has at least two tokens and neither phrase is entirely punctuation. The words forming the longest lexical dependency in each extracted phrase dependency are shown in bold; these are used for back-off features. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1044/parts/0-Table-c3.png",
        "Caption": "Table 4: Sizes of bilingual dictionaries induced by differ- ent alignment methods. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1027/parts/0-Table-c3.png",
        "Caption": "Table 7 Comparing clustering initializations on D2. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P08-1102/parts/0-Figure-c3.png",
        "Caption": "Figure 3. Semantic Role learning curve",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1025/parts/0-Table-c4.png",
        "Caption": "Figure 4: Salient features for fire and the violence cluster",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1016/parts/0-Table-c12.png",
        "Caption": "Table 3: Number of unique entries in training and    test sets, categorized by semantic attributes ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6211/parts/0-Table-c2.png",
        "Caption": "Table 2: BLEU scores for GBM features. Model parameters were optimized on the Tune set. For PRO and regularized MERT, we optimized with different hyperparameters (regularization weight, etc.), and retained for each experimental condition the model that worked best on Dev. The table shows the performance of these retained models. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1016/parts/0-Table-c13.png",
        "Caption": "Table 1: Performance of the statistical approach using a trigram model based on Google Web1T. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1630/parts/0-Table-c2.png",
        "Caption": "Figure 4: Glue Semantics proof for (86), English Way Construction (means interpretation)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1140/parts/0-Table-c5.png",
        "Caption": "Table 3 The results of setting 2 (Punctuation and other encoding information are not used; the maximum length is 10). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J06-2001/parts/0-Figure-c1.png",
        "Caption": "  Figure 5. Results for webpage snippet number. 7.3 Experiment on Multiple Feature Fusion To verify the effectiveness for multiple feature fusion, the test on the feature combination for OOV term translation is implemented. As shown in Table 1, the highest accuracy (the percentage of the correct translations in all the extracted translations) of 83.1367% can be ac- ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P08-2021/parts/0-Table-c2.png",
        "Caption": "Table 1: Linguistic levels as feature sets.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N10-1019/parts/0-Table-c1.png",
        "Caption": "Table 11 NWI results on PK and CTB corpora, NWI as post-processor versus unified approach. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1005/parts/0-Table-c4.png",
        "Caption": "Table 6: Language Model Results.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1114/parts/0-Table-c3.png",
        "Caption": "Figure 5: Multiple Bayesian learning runs (using anneal- ing with temperature decreasing from 2 to 0.08) for POS tagging. Each point represents one run; the y-axis is tag- ging accuracy and the x-axis is the \u2212 log P(derivation) of the final sample. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1061/parts/0-Figure-c1.png",
        "Caption": "Figure 4: LLDA Fmeausres for 3 feature conditions",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-1007/parts/0-Table-c1.png",
        "Caption": "Table 5 Accuracy for three classes on a general purpose list of 2,000 words. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pbulletin/parts/0-Table-c2.png",
        "Caption": "Table 3: Evaluation results of the methods.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1005/parts/0-Table-c1.png",
        "Caption": "Table 7: Evaluation of the GUITAR system without DN detection off raw text ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H01-1062/parts/0-Figure-c3.png",
        "Caption": "Table 3: Evaluation against the monosemous (Pred.) and pol- ysemous (Multiple) gold standards. The figures in parentheses are results of evaluation on randomly polysemous data + sig- nificance of the actual figure. Results were obtained with fine- grained SCFs (including prepositions). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-2036/parts/0-Table-c2.png",
        "Caption": "Figure 2: Comparing F-measure, precision, and recall of different voting schemes for Chinese relation extraction. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1025/parts/0-Table-c1.png",
        "Caption": "Table 3. Results on three query categories.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1098/parts/0-Figure-c3.png",
        "Caption": "Table 1: List of keywords used in WordNet search for generating WN CLASS features. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P1018/parts/0-Table-c5.png",
        "Caption": "Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1060/parts/0-Table-c2.png",
        "Caption": "Table 13 Sample of human-interpretable Arabic TSG rules. Recursive rules like MWA\u2192A MWA result from memoryless binarization of n-ary rules. This pre-processing step not only increases parsing accuracy, but also allows the generation of previously unseen MWEs of a given type. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c18.png",
        "Caption": "Table 11: POS tagging error patterns. # means the error number of the corresponding pattern made by the pipeline tagging model. \u2193 and \u2191 mean the error number reduced or increased by the joint model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C08-1088/parts/0-Figure-c2.png",
        "Caption": "Figure 2. An example of a hierarchical cluster tree.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Figure-c4.png",
        "Caption": "Table 1: Corpus of complex news stories.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Figure-c6.png",
        "Caption": "Figure 5: Evaluation scores for in-domain and out- of-domain test sets, averaged over all systems ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-3008/parts/0-Figure-c1.png",
        "Caption": "Table 6: Coreference results obtained via the MUC scoring program for the ACE test set.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-1070/parts/0-Table-c6.png",
        "Caption": "Figure 6: An non-regular OT approximation.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1033/parts/0-Table-c6.png",
        "Caption": "Table 11 Precision and partial recall of word lengths two to four of the first experiment on IT and AV. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-3114/parts/0-Figure-c12.png",
        "Caption": "Figure 1: A simplified version in Foma source code of the regular expressions and transducers used to bypass several sources of distortion within the DTs in order to parse variations of unseen input DTs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P08-1028/parts/0-Table-c2.png",
        "Caption": "Figure 1: Example of morphological analyses.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2087/parts/0-Table-c4.png",
        "Caption": "Table 17 Results on large-scale Dutch-to-English translation",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1125/parts/0-Figure-c2.png",
        "Caption": "Fig. 5. Two real translation examples,",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-2002/parts/0-Table-c1.png",
        "Caption": "Table 2 Number of tweets mentioning party leaders. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c6.png",
        "Caption": "Figure 2: The performances of the transliteration models and their comparison on EMatch. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P01-1027/parts/0-Table-c5.png",
        "Caption": "Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k sentences and tested on 5k terminals. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3909/parts/0-Table-c7.png",
        "Caption": "Table 4. The differences of F-measure and ROOV between near-by steps of our CWS. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PS15684_w09/parts/0-Table-c2.png",
        "Caption": "Figure 3: Structure of the out-of-vocabulary word \u623d\u4282 \u483d\u543c \u2018English People\u2019. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1138/parts/0-Figure-c6.png",
        "Caption": "Figure 3: The middle node gets the grey or the black class. Small numbers denote edge weights. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-3014/parts/0-Table-c2.png",
        "Caption": "Table 1 DP-based algorithm for solving traveling-salesman problems due to Held and Karp. The outermost loop is over the cardinality of subsets of already visited cities. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1001/parts/0-Figure-c6.png",
        "Caption": "Table 4: Effect of two-level lexicon combination. For the baseline we used the conventional one-level full form lexicon. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-0410/parts/0-Table-c2.png",
        "Caption": "Fig. 6. F-measure for the objective and subjective classes for multilingual bootstrapping (versus cross-lingual framework).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1001/parts/0-Figure-c4.png",
        "Caption": "Figure 2: Tags produced by the different models along with the reference set of tags for a part of a sentence from the Italian test set. Italicized tags denote incorrect labels. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P11-1060/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Stages of the proposed method.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-3008/parts/0-Table-c7.png",
        "Caption": "Figure 1. An example discussion thread",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PSMPT_n09/parts/0-Table-c1.png",
        "Caption": "Table 2: Weights learned for discount features. Nega- tive weights indicate bonuses; positive weights indicate penalties. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D07-1018/parts/0-Table-c5.png",
        "Caption": "Table 2: Pronouns as Opinion Targets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1761/parts/0-Table-c2.png",
        "Caption": "Table 2: Evaluation of topic identification",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Table-c7.png",
        "Caption": "Table 17 Type-insensitive improvement for Chinese/English NER. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-1045/parts/0-Table-c7.png",
        "Caption": "Table 1: Total annotation time, portion spent se- lecting annotation type, and absolute improve- ment for rapid mode. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P00-1022/parts/0-Figure-c2.png",
        "Caption": "Figure 2: Plate diagram of the extended model with T kinds of token-level features (f (t) variables) and a single kind of type-level feature (morphology, m). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P05-1021/parts/0-Table-c2.png",
        "Caption": "Figure 5: Dendrogram of the participants cluster based on their feedback profile ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-3236/parts/0-Figure-c2.png",
        "Caption": "Table 5: The fraction of verb pairs clustered together, as a function of the number of shared senses (results of the NN algo- rithm) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2135/parts/0-Figure-c1.png",
        "Caption": "Figure 7: Effect of Training Corpus Size (2)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1039/parts/0-Figure-c4.png",
        "Caption": "Table 3: Overall accuracy of maximum entropy sys- tem using different subsets of features for People\u2019s Daily News words (automatically segmented, part- of-speech-tagged, parsed). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Table-c8.png",
        "Caption": "Table 3: Categories of multi-character words that  are considered \u2018strings with internal structures\u2019   (see Section 4.2). Each category is illustrated    with an example from our corpus. Both the   individual characters and the compound they              form receive a POS tag. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2164/parts/0-Table-c2.png",
        "Caption": "Table 6: Functional features: gender, number, rationality.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Table-c13.png",
        "Caption": "Table 3: The 10 most important features and their respective category and values for the English word \u201cwhich\u201d.          f ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P11-1060/parts/0-Figure-c5.png",
        "Caption": "Table 22 Cross-system comparison results. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1023/parts/0-Table-c4.png",
        "Caption": "Table 24 Accuracies of our phrase-structure parser on CTB5 using gold-standard and automatically assigned POS-tags. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E12-1054/parts/0-Table-c3.png",
        "Caption": "Table 1: Training set characteristics",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1109/parts/0-Figure-c5.png",
        "Caption": "Table 1: ROUGE F-scores for different systems",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2211/parts/0-Table-c1.png",
        "Caption": "Table 1: Syntactic Seeding Heuristics",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1159/parts/0-Table-c1.png",
        "Caption": "Table 3: Rhetorical pattern of C-Exclamation",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1634/parts/0-Table-c2.png",
        "Caption": "Table 3: Features for \u2018Astronomer Edwin Hubble was born in Marshfield, Missouri\u2019.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1407/parts/0-Table-c3.png",
        "Caption": "Figure 3: Parser projection with target trees. Using the true or 1-best parse trees in the source language is equivalent to having twice as much data in the target language. Note that the penalty for using automatic alignments instead of gold alignments is negligible; in fact, using Source text alone is often higher than +Gold alignments. Using gold source trees, however, significantly outperforms using 1-best source trees. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c1.png",
        "Caption": "Table 3: Accuracy results for binary decisions.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D10-1042/parts/0-Figure-c2.png",
        "Caption": "Table 7. Features with Set Values",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c23.png",
        "Caption": "Table 4: Rhetorical pattern of C-Colon",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Figure-c1.png",
        "Caption": "Table 6: POS tagging with deterministic constraints. The maximum in each column is bold. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c12.png",
        "Caption": "Table 16 Distribution of the NE type errors (MERT-W). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1010/parts/0-Table-c5.png",
        "Caption": "Figure 3 Accuracy for words with high confidence measure. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-1001/parts/0-Table-c1.png",
        "Caption": "Table 1: Examples for correct templates that were learned by TEASE for input templates. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W13-2512/parts/0-Table-c1.png",
        "Caption": "Table 5: Effect of supplementing recasing model training data with the test set source. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/I05-3013/parts/0-Figure-c6.png",
        "Caption": "Figure 2: Example context for the spelling confusion set {piece,peace} and extracted features ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/D09-1092/parts/0-Figure-c5.png",
        "Caption": "Figure 1. Segmentation algorithm.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P01-1027/parts/0-Table-c4.png",
        "Caption": "Table 2: Results on wide-coverage Question Answer- ing task. CCG-Distributional ranks question/answer pairs by confidence\u2014@250 means we evaluate the top 250 of these. It is not possible to give a recall figure, as the total number of correct answers in the corpus is unknown. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-1014/parts/0-Figure-c6.png",
        "Caption": "Figure 8 An example Chinese dependency tree. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-1017/parts/0-Table-c5.png",
        "Caption": "Table 1: Notation and signatures for our framework.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1667/parts/0-Table-c4.png",
        "Caption": "Table 4: Causes of Error for FPs",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S12-1011/parts/0-Table-c1.png",
        "Caption": "Figure 2 Examples of dependency trees with word alignment. Arrows are drawn from children to parents. A child word is a modifier of its parent. Each word has exactly one parent and $ is a special \u201cwall\u201d symbol that serves as the parent of all root words in the tree (i.e., those with no other parent). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1005/parts/0-Table-c3.png",
        "Caption": "Table 3: Multi-lingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages under several experimental settings (Section 5). For each language and setting, we report one-to-one (1-1) and many- to-one (m-1) accuracies. For each cell, the first row corresponds to the result using the best hyperparameter choice, where best is defined by the 1-1 metric. The second row represents the performance of the median hyperparameter setting. Model components cascade, so the row corresponding to +FEATS also includes the PRIOR component (see Section 3). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1074/parts/0-Table-c2.png",
        "Caption": "Table 6: Part-of-speech annotations of the three-    character strings \u7d30\u67f3\u71df xi liu ying \u2018Little Willow military camp\u2019 and \u65b0\u8c50\u5e02 xin feng shi  \u2018Xinfeng city\u2019. Both are \u2018strings with internal structures\u2019, with nested structures that perfectly   match at all three levels. They are the noun phrases that end both verses in the couplet \u5ffd\u904e               \u65b0\u8c50\u5e02, \u9084\u6b78\u7d30\u67f3\u71df. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J09-3004/parts/0-Figure-c2.png",
        "Caption": "Table 2: An example of words and their bit string representations obtained in this paper. Words in bold are head words that appeared in Table 1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P01-1005/parts/0-Table-c3.png",
        "Caption": "Table 1: Adapting a parser to a new annotation style. We learn to parse in a \u201ctarget\u201d style (wide column label) given some number (narrow column label) of supervised target-style training sentences. As a font of additional features, all training and test sentences have already been augmented with parses in some \u201csource\u201d style (row label): either gold-standard parses (an oracle experiment) or else the output of a parser trained on 18k source trees (more realistic). If we have 0 training sentences, we simply output the source-style parse. But with 10 or 100 target-style training sentences, each off-diagonal block learns to adapt, mostly closing the gap with the diagonal block in the same column. In the diagonal blocks, source and target styles match, and the QG parser degrades performance when acting as a \u201cstacked\u201d parser. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1016/parts/0-Table-c5.png",
        "Caption": "Table 3: Evaluation against the monosemous (Pred.) and pol- ysemous (Multiple) gold standards. The figures in parentheses are results of evaluation on randomly polysemous data + sig- nificance of the actual figure. Results were obtained with fine- grained SCFs (including prepositions). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J06-2001/parts/0-Table-c8.png",
        "Caption": "Figure 2: Example of a word lattice",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1114/parts/0-Table-c5.png",
        "Caption": "Table 5 Training and test conditions for the German-to-English Verbmobil corpus (*number of words without punctuation). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1008/parts/0-Table-c7.png",
        "Caption": "Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Figure-c8.png",
        "Caption": "Table 6 NER type-insensitive (type-sensitive) performance of different Chinese NE recognizers. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Figure-c1.png",
        "Caption": "Table 3 k-means experiment baseline and upper bound. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-3004/parts/0-Table-c17.png",
        "Caption": "Table 12 SemCor results for Nouns using jcn. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2052/parts/0-Table-c1.png",
        "Caption": "Figure 2: Example of a MUC-4 template",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-5011/parts/0-Figure-c1.png",
        "Caption": "Table 6 Comparison between three different decoders for word segmentation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1048/parts/0-Figure-c9.png",
        "Caption": "Table 2: Association overlap for target verbs.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-1101/parts/0-Figure-c4.png",
        "Caption": "Table 26: Arabic Tokenization Schemes",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1004/parts/0-Table-c12.png",
        "Caption": "Table 2 shows the result of varying the number of samplers and iterations for all",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1004/parts/0-Table-c9.png",
        "Caption": "Table 5: Contribution of features",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-2007/parts/0-Table-c3.png",
        "Caption": "Table 5: Urdu-English Results (% BLEU).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1107/parts/0-Table-c5.png",
        "Caption": "Table 4: Tagging accuracy on the combined TIGER/Tu\u0308ba corpus, using 10-fold CV, evaluated with and without capitalization, punctuation, and sentence boundaries (SB) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c1.png",
        "Caption": "Table 3: Senses found by our algorithm from first order cooccurrences (LM-1 and LAT-1)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-3004/parts/0-Table-c11.png",
        "Caption": "Table 5: Development testing evaluation.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P04-1017/parts/0-Table-c3.png",
        "Caption": "Table 7 Results per concept for the ILP-Global. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1009/parts/0-Table-c5.png",
        "Caption": "Table 5: Parameters of WSMs (Section 2) which, combined with particular Measures, achieved the highest average correlation in TrValD. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D11-1076/parts/0-Table-c1.png",
        "Caption": "Table 1: Comparison of average per-document ter- comTER with invWER on the EVAL07 GALE Newswire (\u201cNW\u201d) and Weblogs (\u201cWB\u201d) data sets. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2147/parts/0-Figure-c1.png",
        "Caption": "Table 4 Confusion matrix among subtypes of ArgM, defined in Table 1. Entries are fraction of all ArgM labels. Entries are a fraction of all ArgM labels; true zeros are omitted, while other entries are rounded to zero. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P02-1061/parts/0-Figure-c3.png",
        "Caption": "Table 7 Some of the possible Spanish translations of the English phrase make with their memory-based con- text-dependent translation probabilities (rightmost column) compared against context-independent transla- tion probabilities of the baseline system ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1045/parts/0-Figure-c2.png",
        "Caption": "Table 3: Precision (P%) and usage proportion",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1046/parts/0-Table-c2.png",
        "Caption": "Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non terminal/ #sentence ; y: Acc.(%) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0714/parts/0-Table-c2.png",
        "Caption": "Figure 4: Log probability of the sampler state over 1000 iterations on Languages A and B. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E09-2008/parts/0-Table-c2.png",
        "Caption": "Table 2: Speed of oracle extraction from hypergraphs. The basic dynamic program (Sec. 2.1) improves signifi- cantly by collapsing equivalent oracle states (Sec. 2.2). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1016/parts/0-Figure-c1.png",
        "Caption": "Table 1: Encoding schemes (d = dependent, h = syntactic head, p = path; n = number of dependency types)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1750/parts/0-Table-c1.png",
        "Caption": "Table 3: Eq. 9: Log-likelihood. Eq. 10: Pseudolikelihood. In both cases we maximize w.r.t. \u03b8. Eqs. 11\u201313: Recursive DP equations for summing over t and a. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Table-c11.png",
        "Caption": "Figure 1: Example of Lattice Used in the Markov Model-Based Method",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N03-1027/parts/0-Table-c1.png",
        "Caption": "Fig. 2 Example of CCG supertags. CCG supertags are combined under the operations of forward and backward applications into a parse tree ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-1004/parts/0-Table-c9.png",
        "Caption": "Fig. 6. Algorithm for fuzzy divisive clustering based on nouns.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6202/parts/0-Table-c1.png",
        "Caption": "Figure 8: The actual output of our parser trained with a fully annotated treebank. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3604/parts/0-Table-c2.png",
        "Caption": "Figure 3: A conceptual gure of the lexicalization",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/I05-3013/parts/0-Figure-c1.png",
        "Caption": "Figure 6: Are the single most probable words for a given",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W01-0502/parts/0-Figure-c1.png",
        "Caption": "Table 18 Test accuracies of various dependency parsers on CTB5 data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1081/parts/0-Figure-c5.png",
        "Caption": "Table 4: The fraction of verb pairs clustered together, as a function of the number of different senses between pair mem- bers (results of the NN algorithm) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-1115/parts/0-Figure-c1.png",
        "Caption": "Figure 4: Chunk-based translation model. The words in bold are head words.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Figure-c2.png",
        "Caption": "Figure 2: FDG Analyser\u2019s output example",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Figure-c8.png",
        "Caption": "Table 2: Pseudo-disambiguation: Percentage of correct choices made. L-bound denotes the Web1T lower bound on the (a1 , n) bigram, size the number of decisions made. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P04470/parts/0-Table-c1.png",
        "Caption": "Figure 1: Two fragments of a hierarchy over word class distributions ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2017/parts/0-Figure-c2.png",
        "Caption": "Table 5 NEA type-insensitive (type-sensitive) performance with the same Chinese NE recognizer (Wu\u2019s system) and different English NE recognizers. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Figure-c2.png",
        "Caption": "Table 4: Evaluation of the GUITAR improvement - summarization ratio: 30%.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1121/parts/0-Figure-c1.png",
        "Caption": "Figure 1 Architecture of the statistical translation approach based on Bayes\u2019 decision rule. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1115/parts/0-Table-c5.png",
        "Caption": "Table 7 The official vocabularies in Verbmobil. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1062/parts/0-Figure-c2.png",
        "Caption": "Table 8. EDT and mention detection results.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0711/parts/0-Table-c5.png",
        "Caption": "Table 2. Thread length distribution.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P101167/parts/0-Table-c3.png",
        "Caption": "Table 4 Patterns and instantiations for other-anaphora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Figure-c2.png",
        "Caption": "Table 2. The scored results of our CWS in the MSR_C track (OOV is 0.034) for 3rd bakeoff. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Table-c5.png",
        "Caption": "Table 2: Examples and number of them in Semcor, for sense approach and for class approach ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2145/parts/0-Table-c7.png",
        "Caption": "Figure 7 Word sense disambiguation accuracy for \u201cNP1 V NP2 NP3\u201d frame. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1667/parts/0-Table-c1.png",
        "Caption": "Table 15 Most frequent SFC labels for all senses of polysemous words in WordNet, by part of speech. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1113/parts/0-Table-c3.png",
        "Caption": "Table 2: Sources of Dictionaries",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Table-c4.png",
        "Caption": "Table 8 Model accuracy using equal distribution of verb frequencies for the estimation of P(c). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4011/parts/0-Figure-c1.png",
        "Caption": "Table 1. Unigram, bigram and trigram counts of               the ligature corpus ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PbaneaCSL/parts/0-Figure-c4.png",
        "Caption": "Table 1. Result for microblog classification",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1086/parts/0-Figure-c6.png",
        "Caption": "Table 4. Chinese Name Tagger",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1084/parts/0-Figure-c4.png",
        "Caption": "Table 5: F1 scores of the local CRF and non-local models on the CoNLL 2003 named entity recognition dataset. We also provide the results from Bunescu and Mooney (2004) for comparison. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-2060/parts/0-Table-c3.png",
        "Caption": "Table 10 Simplified prevalence score, evaluation on SemCor, polysemous words only. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-4015/parts/0-Table-c1.png",
        "Caption": "Table 1: Coreference Definition Differences for MUC and ACE. (GPE refers to geo-political entities.) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-1004/parts/0-Table-c3.png",
        "Caption": "Table 9 French standard parsing experiments (test set, sentences \u2264 40 words). FactLex uses basic POS tags predicted by the parser and morphological analyses from Morfette. FactLex* uses gold morphological analyses. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1070/parts/0-Table-c2.png",
        "Caption": "Table 3: Eq. 9: Log-likelihood. Eq. 10: Pseudolikelihood. In both cases we maximize w.r.t. \u03b8. Eqs. 11\u201313: Recursive DP equations for summing over t and a. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S12-1040/parts/0-Table-c3.png",
        "Caption": "Table 3: Gender classification performance (%)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1106/parts/0-Table-c6.png",
        "Caption": "Table 1: Context Clustering with Spectral-based Clustering technique. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Table-c9.png",
        "Caption": "Figure 12: MUC-6: Level Distribution of the Six Facts Combined",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1113/parts/0-Figure-c2.png",
        "Caption": "Table 1: Confidence scores for diese in ex. (1)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1045/parts/0-Table-c3.png",
        "Caption": "Figure 10 Lattice dependency parsing using an arc-factored dependency model. Lone indices like p and i denote nodes in the lattice, and an ordered pair like (i, j) denotes the lattice edge from node i to node j. S TART is the single start node in the lattice and F INAL is a set of final nodes. We use edgeScore(i, j) to denote the model score of crossing lattice edge (i, j), which only includes the phrase-based features h 0 . We use arcScore((i, j), (l, m)) to denote the score of building the dependency arc from lattice edge (i, j) to its parent (l, m); arcScore only includes the QPD features h 00 . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c9.png",
        "Caption": "Figure 1 ESA and input/output data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W05-0612/parts/0-Table-c2.png",
        "Caption": "Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and the score threshold \u03b1. Beam parameters fixed at b1 = 40, b2 = 4. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1161/parts/0-Figure-c5.png",
        "Caption": "Figure 2: The framework of our system. We first enumerate all possible candidate states, and then filter out low probability states by using a light-weight classifier, and represent them by using feature forest. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-1068/parts/0-Figure-c5.png",
        "Caption": "Table 7: Rhetorical pattern of C-Dash",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PE2006_p00/parts/0-Figure-c14.png",
        "Caption": "Table 5: precision decrease when omitting non-",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6209/parts/0-Table-c1.png",
        "Caption": "Figure 4: Compilation of a Regular Expression into an fst that Maps between Two Regular Languages ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H01-1001/parts/0-Figure-c3.png",
        "Caption": "Table 12 MWE identification F1 of the parsing models vs. the mwetoolkit baseline (test set, sentences \u2264 40 words). FactLex\u2217 uses gold morphological analyses at test time. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmorph_p00/parts/0-Table-c11.png",
        "Caption": "Table 6: Derivation by Means of Adding a Suffix",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P04-1018/parts/0-Table-c3.png",
        "Caption": "Figure 5: Frequencies of patterns in the evaluation data (causation). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1070/parts/0-Table-c4.png",
        "Caption": "Figure 8: Rate of obtaining two clusters for mix- tures of SW-graphs dependent on merge rate r. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1005/parts/0-Table-c5.png",
        "Caption": "                            C2 Figure 3: An underspecified d ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09_csl2013/parts/0-Figure-c3.png",
        "Caption": "Table 5: Systems whose F-measures are not signif- icantly different from Alice-ME at the 0.10 signifi- cance level with 0.99 confidence ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Table-c10.png",
        "Caption": "Table 2: Examples of features and associated costs. Pseudofeatures are shown in boldface. Exceptional denotes a situation such as the semivowel [j] substituting for the affricate [dZ]. Substitutions between these two sounds actually occur frequently in second-language error data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H05-1001/parts/0-Figure-c1.png",
        "Caption": "Table 1: Association frequencies for target verb.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E12-1020/parts/0-Table-c1.png",
        "Caption": "Table 3: Features for \u2018Astronomer Edwin Hubble was born in Marshfield, Missouri\u2019.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Figure-c5.png",
        "Caption": "Table 2 Evaluation on Structure Features",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2101/parts/0-Table-c4.png",
        "Caption": "Table 2: Outputs from each algorithm at different sorted ranks",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1107/parts/0-Table-c2.png",
        "Caption": "Figure 3: Average three-fold cross-validation accuracies, in percent. Boldface: best performance for a given setting (row). Recall that our baseline results ranged from 50% to 69%. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1083/parts/0-Table-c5.png",
        "Caption": "Table 8: Detailed DIFF results",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-1003/parts/0-Figure-c2.png",
        "Caption": "Table 5: Precision for 200 candidates (Ev.Rec).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1062/parts/0-Figure-c1.png",
        "Caption": "Table 1: English MWEs and their components with their translation in Persian. Direct matches between the trans- lation of a MWE and its components are shown in bold; partial matches are underlined. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Table-c6.png",
        "Caption": "Table 1: Accuracies (%) for Word-Extraction Us- Litkowski and Hargraves (2007) selected exam- ing MALT Parser or Heuristics.                  ples based on a search for governors8 , most anno- ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1150/parts/0-Table-c3.png",
        "Caption": "Table 5: Results for the unsupervised baseline and the supervised system trained on three kinds of feature sets ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00-1025/parts/0-Figure-c7.png",
        "Caption": "Table 4: Sample of experimental items for the meta alternation anm-fod. (Abbreviations are listed in Table 2.)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Figure-c8.png",
        "Caption": "Table 1. Speech Act Categories and Kappa values",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/D09-1092/parts/0-Figure-c7.png",
        "Caption": "Figure 8 Extracting consistent bilingual phrasal correspondences from the shown sentence pairs. (i1 , j1 ) \u00d7 (i2 , j2 ) denotes the correspondence   fi1 . . . fj1 , ei2 . . . ej2  . Not all extracted correspondences are shown. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Table-c3.png",
        "Caption": "Table 5: Weights learned for generating syntactic nodes of various types anywhere in the English translation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P01-1017/parts/0-Figure-c1.png",
        "Caption": "Figure 3: A conceptual gure of the lexicalization",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1149/parts/0-Figure-c4.png",
        "Caption": "Table 24: Splitting Compounds in Russian",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P11-1060/parts/0-Figure-c3.png",
        "Caption": "Table 5: Contexts where ec is better than mc+dz. J^ are coordination conjunctions, # is the root, V* are verbs, Nn are nouns in case n, R* are preposi- tions, Z* are punctuation marks, An are adjectives. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1110/parts/0-Figure-c2.png",
        "Caption": "Table 1: The chunking results for the six systems associated with the project (shared task CoNLL- 2000). The baseline results have been obtained by selecting the most frequent chunk tag associ- ated with each part-of-speech tag. The best results at CoNLL-2000 were obtained by Support Vector Machines. A majority vote of the six LCG sys- tems does not perform much worse than this best result. A majority vote of the five best systems outperforms the best result slightly (     error re- duction). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c18.png",
        "Caption": "Table 6 Results for Single Document System",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Figure-c9.png",
        "Caption": "Table 1: Performance of our system versus a baseline",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1147/parts/0-Table-c6.png",
        "Caption": "Table 5: Experiment 3: Results for label unknown sense, NN-based outlier detection, \u03b8 = 1.0. \u03c3: stan- dard deviation ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c19.png",
        "Caption": "Table 5: Effect on %BLEU of varying number of non-terminals ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1115/parts/0-Figure-c2.png",
        "Caption": "Figure 5. Some subtrees from trees in figure 4",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-1017/parts/0-Figure-c3.png",
        "Caption": "Table 4: Impact of the use of sampled texts.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2228/parts/0-Table-c1.png",
        "Caption": "Table 5: Gender detection accuracies (%) using a 4-gram language model for the letter sequence of          the source name in Latin script. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1027/parts/0-Table-c3.png",
        "Caption": "Table 9: A* (E+) Success Rate for 12- and 14-word sentences [%].",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1141/parts/0-Figure-c6.png",
        "Caption": "Figure 3: extracts from the Akkadian project",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Figure-c2.png",
        "Caption": "Table 3: MAP(%), under the \u201850 rules, All\u2019 setup, when adding component match scores to Precision (P) or prior- only MAP baselines, and when ranking with allCP or allCP+pr methods but ignoring that component scores. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1630/parts/0-Figure-c1.png",
        "Caption": "Figure 1: Comparison of a confusion network and a lat- tice. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2167/parts/0-Table-c3.png",
        "Caption": "Table 2: good#a#15 gloss and examples.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09_qwn/parts/0-Figure-c1.png",
        "Caption": "Table 1: Training, tuning, and test conditions",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1011/parts/0-Figure-c2.png",
        "Caption": "Table 3: BLEU scores for pre-ordering experi- ments with a n-code system and the approach pro- posed by (Neubig et al., 2012) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1008/parts/0-Table-c8.png",
        "Caption": "Table 2: BLEU scores on the test08 and news08 test data obtained by models trained by MERT and SVM. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-1007/parts/0-Figure-c2.png",
        "Caption": "Table 4: Performance of different FS machines in terms of the percentage of unclassified entries. All the classified entries were correctly classified, yielding, as a result, a precision of 100%. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PE2006_p00/parts/0-Table-c2.png",
        "Caption": "Figure 1. Removal and reduction of constituents using dependencies",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PTP_n09/parts/0-Figure-c1.png",
        "Caption": "Table 2: Sources of Dictionaries",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W03-0432/parts/0-Table-c4.png",
        "Caption": "Figure 5: Avg. runtime per sentence of FindPareto",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1023/parts/0-Table-c5.png",
        "Caption": "Figure 6: CTB 10-fold CV POS tagging accuracy using an all-at-once approach ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-0441/parts/0-Table-c2.png",
        "Caption": "Table 1: Classification results with XLE starredness, parser exceptions and zero parses (Method 1) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-1914/parts/0-Table-c2.png",
        "Caption": "Figure 5: Dendrogram of the participants cluster based on their feedback profile ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P04-2007/parts/0-Table-c3.png",
        "Caption": "Figure 1. Entity detection and tracking system flow. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N09-1007/parts/0-Table-c1.png",
        "Caption": "Figure 2: Contribution of employing the dynamic         cache on different test documents ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6201/parts/0-Table-c1.png",
        "Caption": "Figure 4: Learning curves showing the effects of increas- ing the size of dialectal training data, when combined with the 150M-word MSA parallel corpus, and when used alone. Adding the MSA training data is only use- ful when the dialectal data is scarce (200k words). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1011/parts/0-Figure-c1.png",
        "Caption": "Table 9 Semantic roles for different frame sets of kick. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P101167/parts/0-Figure-c7.png",
        "Caption": "Figure 1: Example of Lattice Used in the Markov Model-Based Method",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D11-1076/parts/0-Figure-c1.png",
        "Caption": "Figure 4: Using the type model for disambiguation in the derivation of file a suit. Type distributions are shown after the variable declarations. Both suit and the object of file are lexically ambiguous between different types, but after the \u03b2 -reduction only one interpretation is likely. If the verb were wear, a different interpretation would be preferred. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1005/parts/0-Figure-c2.png",
        "Caption": "Figure 1. An example discussion thread",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1073/parts/0-Figure-c1.png",
        "Caption": "Figure 1: A hypothetical semantic space for horse and run ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1119/parts/0-Table-c4.png",
        "Caption": "Table 3: Precision, Recall, and F1-score of Baseline, Engkoo, Google, and Ours over test sets Ti",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D08-1032/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Architecture of the translation approach based on Bayes\u2019 decision rule. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1086/parts/0-Figure-c2.png",
        "Caption": "Table 4: Noun Stem Alternation",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-2002/parts/0-Figure-c2.png",
        "Caption": "Figure 5: Results for choosing the correct ordered chain. (\u2265 10) means there were at least 10 pairs of ordered events in the chain. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P05-1045/parts/0-Table-c2.png",
        "Caption": "Table 8: Synonyms for chain",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Table-c4.png",
        "Caption": "Table 4: Sample of experimental items for the meta alternation anm-fod. (Abbreviations are listed in Table 2.)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D11-1076/parts/0-Figure-c2.png",
        "Caption": "Figure 8 Order in which the English source positions are covered for the English-to-German reordering example given in Figure 7. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-1025/parts/0-Table-c2.png",
        "Caption": "Table 3: Evaluation against the monosemous (Pred.) and pol- ysemous (Multiple) gold standards. The figures in parentheses are results of evaluation on randomly polysemous data + sig- nificance of the actual figure. Results were obtained with fine- grained SCFs (including prepositions). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1015/parts/0-Table-c9.png",
        "Caption": "Table 3: Examples of aggregated instances.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N12-1045/parts/0-Table-c3.png",
        "Caption": "Figure 2. Incremental alignment with TERp resulting in a confusion network.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-2055/parts/0-Figure-c2.png",
        "Caption": "Table 1: Results obtained by applying different types of features in isolation to the Baseline system.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1074/parts/0-Table-c1.png",
        "Caption": "Table 1: The set of types and subtypes of relations used in the 2004 ACE evaluation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-0621/parts/0-Table-c1.png",
        "Caption": "Figure 6: Derivation with soft syntax model",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Table-c8.png",
        "Caption": "Table 3: Corpus statistics for Chinese (Zh) character segmentation and English (En)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1015/parts/0-Table-c5.png",
        "Caption": "Figure 3 Training with scarce resources. \u201cRestructuring,\u201d \u201clearn phrases,\u201d and \u201cannotation\u201d all require morpho-syntactic analysis of the transformed sentences. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-2124/parts/0-Table-c3.png",
        "Caption": "Table 10 Top 20 most similar words for country and their ranks in the similarity list by the Bootstrapped LIN measure. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1004/parts/0-Table-c7.png",
        "Caption": "Table 3: Evaluation against the monosemous (Pred.) and pol- ysemous (Multiple) gold standards. The figures in parentheses are results of evaluation on randomly polysemous data + sig- nificance of the actual figure. Results were obtained with fine- grained SCFs (including prepositions). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-3208/parts/0-Table-c6.png",
        "Caption": "Fig. 1. System architecture overview",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-2060/parts/0-Table-c2.png",
        "Caption": "Figure 2 Regular alignment example for the translation direction German to English. For each German source word there is exactly one English target word on the alignment path. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-2206/parts/0-Table-c5.png",
        "Caption": "Figure 2. Percentage of examples of major syntactic classes.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1110/parts/0-Table-c3.png",
        "Caption": "Figure 4 Distribution of domain labels of predominant senses for 38 polysemous nouns ranked using the SPORTS and FINANCE corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1039/parts/0-Figure-c2.png",
        "Caption": "Table 1. Test corpora details",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1006/parts/0-Table-c5.png",
        "Caption": "Figure 6: Metaphors tagged by the system (in bold)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/I08-2080/parts/0-Figure-c1.png",
        "Caption": "Figure 2: A Portion of the Syntactic Tree.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-1004/parts/0-Table-c1.png",
        "Caption": "Table 3: Translation results from English to French and English to German measured on newstest2010 using a 100-best rescoring with SOUL LMs of different orders. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3812/parts/0-Figure-c6.png",
        "Caption": "Table 2: Performance of the mention detection sys- tem using lexical, syntactic, gazetteer features as well as features obtained by running other named-entity classifiers ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Table-c8.png",
        "Caption": "Table 10: Performance of Two Categories",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1670/parts/0-Table-c5.png",
        "Caption": "Table 3: Segmentation performance on words that have the same final suffix as their preceding words. The F1 scores are computed based on all boundaries within the words, but the accuracies are obtained using only the final suffixes. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D07-1018/parts/0-Table-c6.png",
        "Caption": "Table 5: Gender detection accuracies (%) using a 4-gram language model for the letter sequence of          the source name in Latin script. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1044/parts/0-Table-c1.png",
        "Caption": "Table 2: Example confounders for \u201cfestival\u201d and \u201claws\u201d and their similarities ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-0329/parts/0-Table-c1.png",
        "Caption": "Table 3: Precision, Recall, and F1-score of Baseline, Engkoo, Google, and Ours over test sets Ti",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-3311/parts/0-Table-c2.png",
        "Caption": "Table 4: Gross statistics for several different treebanks. Test set OOV rate is computed using the following splits: ATB (Chiang et al., 2006); CTB6 (Huang and Harper, 2009); Ne- gra (Dubey and Keller, 2003); English, sections 2-21 (train) and section 23 (test). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00-1025/parts/0-Figure-c2.png",
        "Caption": "Table 4: MUC, CEAF, and B3 coreference results using system mentions.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N12-1045/parts/0-Figure-c3.png",
        "Caption": "Table 3: Performance in F1-score over different cluster numbers with intra-stratum sampling on the develop- ment data ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1109/parts/0-Figure-c2.png",
        "Caption": "Figure 4: Average results of Reweighting among all 7 speakers when the amount of speaker specific data is 0, 500, 2000 ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C00-1081/parts/0-Figure-c2.png",
        "Caption": "Table 6: Precision, recall and F-measure for non-projective arcs",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-2003/parts/0-Figure-c2.png",
        "Caption": "Figure 2: The average results among all 7 speakers when train with different combinations of speaker specific data and other speakers\u2019 data are displayed. In both Constant adaptation and Reweighted adaptation models the num- ber of speaker specific data are varied from 200, 500, 1000, 1500 to 2000. In Generic model, only all other speakers\u2019 data are used for training data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Table-c1.png",
        "Caption": "Figure 10: Example of parser error. Tree (a) is correct, and (b) is the wrong result by our parser. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1104/parts/0-Table-c1.png",
        "Caption": "Figure 1: The directional matching relationships between a hypothesis (h), an entailment rule (r) and a text (t) in the Contextual Preferences framework. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-2032/parts/0-Table-c3.png",
        "Caption": "Table 6. Precision at top 100",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P05-1053/parts/0-Table-c1.png",
        "Caption": "Figure 3: Learning curves on the development dataset of the Beijing Univ. corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C04-1131/parts/0-Table-c5.png",
        "Caption": "Table 3: Test accuracy with unsupervised training methods",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S10-1090/parts/0-Table-c4.png",
        "Caption": "Figure 4: Posterior likelihood at each of the first 100 iter- ations, from 4 runs (with different random seeds) on 10% of the Morphochallenge dataset (\u03b1i6=j = 0.001, \u03b1i=j = 100, \u03b2 = 0.1), indicating convergence within the first 15 iterations. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Table-c12.png",
        "Caption": "Table 5: Comparison of accuracy scores across linguistic levels.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2167/parts/0-Table-c4.png",
        "Caption": "Figure 1: Example of a prediction for English to French translation. s is the source sentence, h is the part of its translation that has already been typed, x\u2217 is what the translator wants to type, and x is the prediction. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-1001/parts/0-Table-c5.png",
        "Caption": "Table 2 Sample of subjective features appearing in the top 100 discriminant attributes selected with Information Gain on the 3rd fold training data at iteration 10. The words in italics in the multilingual features represent equivalent translations in English and Romanian. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Table-c12.png",
        "Caption": "Table 7: Results for ensemble classifier.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1408/parts/0-Table-c3.png",
        "Caption": "Figure 1 Split constituents: In this case, a single semantic role label points to multiple nodes in the original treebank tree. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1131/parts/0-Table-c5.png",
        "Caption": "Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k sentences and tested on 5k terminals. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c5.png",
        "Caption": "Figure 1: Example of morphological analyses.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1067/parts/0-Table-c5.png",
        "Caption": "Table 1. Evaluation results within sets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c20.png",
        "Caption": "Figure 5: MTO is not sensitive to the number of random substitutes sampled per word token. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1087/parts/0-Figure-c5.png",
        "Caption": "Figure 5: Contribution of combining the three caches",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1408/parts/0-Table-c6.png",
        "Caption": "Table 1: Summary for graphs and test datasets obtained from each seed pair ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1147/parts/0-Table-c3.png",
        "Caption": "Table 1: Basic features used in the maximum entropy model.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-2611/parts/0-Table-c3.png",
        "Caption": "Figure 4: LLDA Fmeausres for 3 feature conditions",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1111/parts/0-Table-c8.png",
        "Caption": "Figure 2: Word prediction from a partial parse",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-1215/parts/0-Table-c4.png",
        "Caption": "Figure 6: Average scores for different language pairs. Manual scoring is done by different judges, resulting in a not very meaningful comparison. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2139/parts/0-Table-c1.png",
        "Caption": "Figure 1: The System architecture1",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1032/parts/0-Table-c5.png",
        "Caption": "Table 3: Impact of Syntactic Features on English Sys- tem After Taking out Distance Features. Numbers are F-measures(%). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09prod/parts/0-Figure-c2.png",
        "Caption": "Table 8: Results for the joint word segmentation and POS tagging task. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0711/parts/0-Table-c9.png",
        "Caption": "Table 2: Segmentation performance on words that begin with prefix \u201cAl\u201d (determiner) and end with suffix \u201cAt\u201d (plural noun suffix). The mean F1 scores are computed using all boundaries of words in this set. For each word, we also determine if both affixes are recovered while ig- noring any other boundaries between them. The other two columns report this accuracy at both the type-level and the token-level. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c15.png",
        "Caption": "Figure 2 Comparison between the acfr-ratio for MI and Bootstrapped LIN methods, when using varying numbers of common top-ranked features in the words\u2019 feature vectors. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W12-2303/parts/0-Table-c2.png",
        "Caption": "Table 10 NW 21 identification results on PK test set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Figure-c11.png",
        "Caption": "Table 2. Unigram, bigram and trigram counts of                the word corpus ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1023/parts/0-Table-c1.png",
        "Caption": "Table 3: Comparison results with TAC 2008 Three Top Ranked Systems (system 1-3 demonstrate top 3 systems in TAC) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1005/parts/0-Figure-c1.png",
        "Caption": "Table 6: Example Translations for the Verbmobil task.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2211/parts/0-Table-c11.png",
        "Caption": "Table 2 Experiments on the threshold\u2013precision relationship of the small corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1161/parts/0-Figure-c4.png",
        "Caption": "Figure 5: The STTS tags PDAT and ART, their rep- resentation in the Annotation Model and linking with the Reference Model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-3144/parts/0-Table-c4.png",
        "Caption": "Table 6: BS on IWSLT 2007 task",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-2910/parts/0-Table-c1.png",
        "Caption": "Figure 12: MUC-6: Level Distribution of the Six Facts Combined",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1114/parts/0-Table-c1.png",
        "Caption": "Table 4: space distribution of most reliable",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1005/parts/0-Figure-c4.png",
        "Caption": "Table 14. Rules of Referral",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P11-1060/parts/0-Table-c3.png",
        "Caption": "Table 1 Sources of con\ufb02ict in cross-lingual subjectivity transfer. Definitions and synonyms of the fourth sense of the noun argument, the fourth sense of verb decide, and the first sense of adjective free as provided by the English and Romanian WordNets; for Romanian we also provide the manual translation into English. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1068/parts/0-Figure-c1.png",
        "Caption": "Table 1: Features based on the token string",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1159/parts/0-Table-c4.png",
        "Caption": "Figure 4: Contribution of combining the dynamic",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1050/parts/0-Table-c1.png",
        "Caption": "Figure 2: A sample CCG parse.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c19.png",
        "Caption": "Figure 3: Screenshot of ConAno",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c11.png",
        "Caption": "Figure 8 FSRA-2 for Arabic nominative definite and indefinite nouns. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W05-0612/parts/0-Table-c3.png",
        "Caption": "Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a sequence of L bilingual units (tuples) u1 , ..., uL . Each tuple ui contains a source and a target phrase: si and ti . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Table-c7.png",
        "Caption": "Figure 1: An underspecified discourse structure and its five configurations",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P01-1017/parts/0-Table-c1.png",
        "Caption": "Table 2: Arabic Nominal Inflection - Broken Plural",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D12-1074/parts/0-Figure-c3.png",
        "Caption": "Table 15 Effect of adjacent contextual (non-NE) bigrams on the test set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2052/parts/0-Table-c3.png",
        "Caption": "Table 5: Urdu-English Results (% BLEU).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1110/parts/0-Table-c5.png",
        "Caption": "Fig. 8. Distribution of alignment fertilities for source language tokens.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10111/parts/0-Figure-c1.png",
        "Caption": "Table 19: Phonetic Stress in Russian",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1070/parts/0-Table-c3.png",
        "Caption": "Table 6: Corpus characteristics for perplexity quality experiments. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1670/parts/0-Table-c4.png",
        "Caption": "Table 7: Results for OOV-processing and MBR, German\u2192English. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-1101/parts/0-Table-c3.png",
        "Caption": "Figure 8: MUC-4: Level Distribution of the Five Facts Combined",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1110/parts/0-Table-c4.png",
        "Caption": "Figure 4: Using different amounts of annotated training data for the article meta-classifier. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1079/parts/0-Table-c4.png",
        "Caption": "Figure 1: Effect of in-domain monolingual corpus size on translation quality. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S13-1002/parts/0-Table-c1.png",
        "Caption": "Table 6: Type-level Results: Each cell report the type- level accuracy computed against the most frequent tag of each word type. The state-to-tag mapping is obtained from the best hyperparameter setting for 1-1 mapping shown in Table 3. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2302/parts/0-Table-c3.png",
        "Caption": "Figure 1. Upper triangle of the sentence-similarity matrix.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2049/parts/0-Figure-c1.png",
        "Caption": "Table 10 Numeric-type compounds extracted. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/M98-1003/parts/0-Figure-c11.png",
        "Caption": "Figure 3: cumulative distribution of frequency (CDF) of the relative ranking of model-predicted probability of being positive for false negatives in a pool mixed of false negatives and true negatives; and the CDF of the relative ranking of model-predicted probability of being negative for false positives in a pool mixed of false positives and true positives. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-0304/parts/0-Table-c2.png",
        "Caption": "Table 2: Results on the Arabic GALE Phase 2 evaluation set with one reference translation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PS15684_w09/parts/0-Table-c4.png",
        "Caption": "Fig. 1. Fuzzy hierarchical clustering for Paraphrase Extraction.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c8.png",
        "Caption": "Table 1: Part-of-speech tags of the Penn Chinese   Treebank that are referenced in this paper.      Please see (Xia, 2000) for the full list. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P59105ca/parts/0-Figure-c4.png",
        "Caption": "Table 3: 2 billion word corpus statistics",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1087/parts/0-Figure-c1.png",
        "Caption": "Table 3: N/P classifier with and without SWSD",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W02-1020/parts/0-Figure-c3.png",
        "Caption": "Table 1: Corpus of complex news stories.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1081/parts/0-Figure-c1.png",
        "Caption": "Table 1: Automatically generated training set examples.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1111/parts/0-Table-c2.png",
        "Caption": "Table 6: Unlabeled TedEval scores (accuracy/exact match) for the test sets in the predicted segmentation set- ting. Only sentences of length \u2264 70 are evaluated. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P05-1053/parts/0-Table-c2.png",
        "Caption": "Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form    an. The distinctions in the ATB are linguistically justified, but complicate parsing. Table 8a shows that the best model recovers SBAR at only 71.0% F1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-1815/parts/0-Table-c2.png",
        "Caption": "Table 1: Recall (R), Precision (P) and Mean Average Pre- cision (MAP) when only matching template hypotheses directly. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1159/parts/0-Table-c2.png",
        "Caption": "Figure 1. Bootstrapping for Name Tagging",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1125/parts/0-Figure-c1.png",
        "Caption": "Table 7. System performance on the reaction relation on the CHEM dataset. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N06-2021/parts/0-Table-c1.png",
        "Caption": "Figure 2: An example of alignment units",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Figure-c7.png",
        "Caption": "Figure 3: Propagation: Core items",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W01-0712/parts/0-Table-c1.png",
        "Caption": "Fig. 6 Distances found between phrase boundaries with linked modifier words and with parent words",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1110/parts/0-Table-c2.png",
        "Caption": "Figure 5: Avg. runtime per sentence of FindPareto",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-3008/parts/0-Table-c4.png",
        "Caption": "Figure 4: Sentiment Lexicon Performance",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pstat_p00/parts/0-Figure-c1.png",
        "Caption": "Figure 2: The decision tree (Nwire) for the system using the single semantic relatedness feature ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PE2006_p00/parts/0-Figure-c1.png",
        "Caption": "Figure 1 Na\u0131\u0308ve FSA with duplicated paths. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-2015/parts/0-Table-c1.png",
        "Caption": "Table 1: Corpus of complex news stories.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Table-c6.png",
        "Caption": "Figure 3: Structure of the out-of-vocabulary word \u623d\u4282 \u483d\u543c \u2018English People\u2019. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-2049/parts/0-Table-c3.png",
        "Caption": "Table 15 Most frequent SFC labels for all senses of polysemous words in WordNet, by part of speech. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-2060/parts/0-Table-c1.png",
        "Caption": "Figure 4: Example of Morph-Related Heteroge- neous Information Network ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1159/parts/0-Table-c7.png",
        "Caption": "Table 4: space distribution of most reliable",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2139/parts/0-Table-c4.png",
        "Caption": "Table 1: Experimental results over the 120 evalu- ation sentences. Alignment error rates in both di- rections are provided here. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1115/parts/0-Table-c1.png",
        "Caption": "Table 4: Bagging with 50 gold seed sets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0206/parts/0-Table-c4.png",
        "Caption": "Table 3: Training phase: effect of question bias (d) on Ave. MRR and TRDR. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1147/parts/0-Figure-c2.png",
        "Caption": "Table 4: Results of different feature groups under the TC model for N-pron resolution ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1126/parts/0-Figure-c6.png",
        "Caption": "Table 2: Translation accuracy (BLEU) results.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmorph_p00/parts/0-Table-c7.png",
        "Caption": "Table 4: Results of the gloss classifier.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1001/parts/0-Figure-c2.png",
        "Caption": "Table 6 Properties of the variations for the corpus-based algorithms for other-anaphora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-2012/parts/0-Table-c1.png",
        "Caption": "Table 2: Segmentation, Parsing and Tagging Results us- ing the Setup of (Cohen and Smith, 2007) (sentence length \u2264 40). The Models\u2019 are Ordered by Performance. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1086/parts/0-Figure-c5.png",
        "Caption": "Table 1: Our segmentation results by the dictionary- based approach for the closed test of Bakeoff 2005, very low R-oov rates due to no OOV recognition applied. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J02-2003/parts/0-Table-c5.png",
        "Caption": "Figure 5: DRS and corresponding DRG (in tuples and in graph format) for \u201cA customer did not pay.\u201d",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1033/parts/0-Table-c9.png",
        "Caption": "Figure 2: Example of the Character Tagging Method: Word boundaries are indicated by vertical lines (\u2018|\u2019).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P08-1078/parts/0-Figure-c1.png",
        "Caption": "Table 15 Size of training data set and the adaptation results on AS open. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1033/parts/0-Table-c7.png",
        "Caption": "Table 1: English-French translation results in terms of BLEU score and TER estimated on newstest2010 with the NIST script. All means that the translation model is trained on news-commentary, Europarl, and the whole GigaWord. The rows upper quartile and median corre- spond to the use of a filtered version of the GigaWord. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N12-1006/parts/0-Table-c5.png",
        "Caption": "Table 3: Evaluation results for all combinations of mixture adapted language and translation models: Baseline(bl) scores are italicized, best scores are in bold ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P11-1060/parts/0-Figure-c1.png",
        "Caption": "Table 2: Combined systems (English) in cross- validation, best recall in bold. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2101/parts/0-Figure-c4.png",
        "Caption": "Table 1: Gibbs sampling for Bayesian inference outperforms both EM and Variational Bayesian EM. \u2217 The output of EM alignment was used as the gold standard. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1017/parts/0-Table-c4.png",
        "Caption": "Table 10 Numeric-type compounds extracted. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W01-0712/parts/0-Table-c2.png",
        "Caption": "Table 5: Agreement counts in morphological annotation compared between the baseline system and the oracle system using gold syntax. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Table-c11.png",
        "Caption": "Table 2: The semantic roles of cases beside C-1 verb cluster ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Table-c5.png",
        "Caption": "Figure 1: Overlaid bilingual embeddings: English words are plotted in yellow boxes, and Chinese words in green; reference translations to English are provided in boxes with green borders directly below the original word. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/ICDAR99/parts/0-Figure-c2.png",
        "Caption": "Table 8: Parser performance on WSJ;23, unsupervised adaptation. For all trials, the base training is Brown;T, the held out is Brown;H plus the parser output for WSJ;24, and the mixing parameter \u03c4A is 0.20e c(A). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1048/parts/0-Figure-c5.png",
        "Caption": "                                   3 .  (7     $    %  19: 6       7( Figure 2 Error analysis example. . . . )  82                                 mrt \u00c2yAm \u03c2 l\u00fd A\u030cxtfA\u2019 Alzmyl Almhnd . . . (\u2018Several days have passed since the disappearance of the colleague the engineer . . . \u2019), as parsed by the baseline system using only CORE 12 (left) and as using the best performing model (right). Bad predictions are marked with <<< . . . >>>. The words in the tree are presented in the Arabic reading direction (from right to left). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P08-2023/parts/0-Table-c4.png",
        "Caption": "Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag. Across all languages, high performance can be attained by selecting a single tag per word type. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1159/parts/0-Table-c6.png",
        "Caption": "Figure 6: Difference between our output (a) of parsing the word \u78be\u789c\u6268 \u2018olive oil\u2019 and the output (b) of Luo (2003). In (c) we have a true flat word, namely the loca- tion name \u49e2\u54eb\u819d \u2018Los Angeles\u2019. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Table-c3.png",
        "Caption": "Figure 3 Word sense disambiguation accuracy for \u201cNP1 V NP2 NP3\u201d frame. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1004/parts/0-Table-c1.png",
        "Caption": "Table 4 Candidates for equivalence classes. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-1309/parts/0-Figure-c1.png",
        "Caption": "Table 1: A text segment from MUC-6 data set",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-2036/parts/0-Figure-c1.png",
        "Caption": "Table 3: Evaluation results of the methods.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1149/parts/0-Figure-c5.png",
        "Caption": "Table 3: Impact of Syntactic Features on English Sys- tem After Taking out Distance Features. Numbers are F-measures(%). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1149/parts/0-Table-c3.png",
        "Caption": "Table 8. Subject and Object Agreement Features",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P10-2049/parts/0-Table-c2.png",
        "Caption": "Table 6: Comparison of the existing efforts on ACE RDC task.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1025/parts/0-Table-c1.png",
        "Caption": "Table 1: Part of a sample headline cluster, with sub-clusters ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Table-c10.png",
        "Caption": "Table 5: Effectiveness of Latent Topic Extraction from Multi-Language Corpus",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-3238/parts/0-Figure-c1.png",
        "Caption": "Table 2. Accuracy of various instantiations of the system",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P05-1013/parts/0-Table-c1.png",
        "Caption": "Table 3: Most frequent monosemic words in BG",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Table-c21.png",
        "Caption": "Figure 15 Reduplication \u2013 general case. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1012/parts/0-Table-c7.png",
        "Caption": "Figure 1: Precision and recall for prepositions.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1010/parts/0-Table-c4.png",
        "Caption": "Figure 5: Example annotation from the BioNLP Shared Task 2011 Epigenetics and Post-translational Modifications event extraction task. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-2032/parts/0-Table-c1.png",
        "Caption": "Table 2: Translation results in terms of BLEU score and translation edit rate (TER) estimated on newstest2010 with the NIST scoring script. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P04470/parts/0-Table-c5.png",
        "Caption": "Table 3: Performance of Altavista counts and BNC counts for candidate selection for MT (data from Prescher et al. 2000) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1138/parts/0-Figure-c6.png",
        "Caption": "Table 2: Statistics of three test sets.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P10-2049/parts/0-Table-c4.png",
        "Caption": "Table 4: Comparison to SVM.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P05-1013/parts/0-Table-c4.png",
        "Caption": "Figure 3: Opinion Question Answering System",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-1025/parts/0-Table-c5.png",
        "Caption": "Table 7: Number of translations generated by each method in the final translation output of system COMB: decoder (Orig.), re-decoding (RD), n-gram expansion (NE) and confusion network (CN). \u201cTot.\u201d is the size of the dev/test set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1004/parts/0-Table-c6.png",
        "Caption": "Table 4. Chinese Name Tagger",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N10-1019/parts/0-Figure-c1.png",
        "Caption": "Table 1 The best two performing systems of each type (according to fine-grained recall) in Senseval-2 and -3. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0815/parts/0-Figure-c1.png",
        "Caption": "Figure 1: Caseframe Network Examples",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1003/parts/0-Table-c5.png",
        "Caption": "Figure 6: Average number of Pareto points",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3909/parts/0-Table-c1.png",
        "Caption": "Table 1: Ten relation instances extracted by our system that did not appear in Freebase.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1016/parts/0-Figure-c5.png",
        "Caption": "Figure 1: Example of the non-relation Same-Unit",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Table-c14.png",
        "Caption": "Table 10 Model accuracy using equal distribution of verb frequencies for the estimation of P(c). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Figure-c4.png",
        "Caption": "Figure 4: Illustration of search in statistical trans- lation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-2122/parts/0-Table-c1.png",
        "Caption": "Table 12: Performance of Altavista counts and BNC counts for noun countability detection (data from Bald- win and Bond 2003) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1010/parts/0-Table-c1.png",
        "Caption": "Table 2 Initial type-sensitive Chinese/English NER performance. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W05-0612/parts/0-Figure-c1.png",
        "Caption": "Figure 1: The combined sequence and parse tree representation of the relation instance \u201cleader of a minority government.\u201d The non-essential nodes for \u201ca\u201d and for \u201cminority\u201d are removed based on the algorithm from Qian et al. (2008). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/ICDAR99/parts/0-Table-c2.png",
        "Caption": "Figure 1: Addition method.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c29.png",
        "Caption": "Table 3: Our boosted ranker combining monolingual and bilingual features (bottom) compared to three base- lines (top) gives comparable performance to the human- curated upper bound. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-1017/parts/0-Table-c1.png",
        "Caption": "Figure 3: Individuals for accusative and sin- gular in the TIGER Annotation Model ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Figure-c8.png",
        "Caption": "Figure 2: Average Precision and Coherence (\u03ba) for each meta alternation. Correlation: r = 0.743 (p < 0.001) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C00-2123/parts/0-Table-c1.png",
        "Caption": "Table 2: micro-average F1 and AUC for the algorithms.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-2003/parts/0-Figure-c4.png",
        "Caption": "Table 6: InvR scores ranked by difference, Giga- word to Web Corpus ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/M98-1003/parts/0-Figure-c1.png",
        "Caption": "Table 4: Results for feature ablation experiments.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1009/parts/0-Figure-c1.png",
        "Caption": "Table 17 Results on large-scale Dutch-to-English translation",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-2003/parts/0-Figure-c3.png",
        "Caption": "Figure 1: Example queries for abbreviation \u201cBSA\u201d",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1046/parts/0-Figure-c3.png",
        "Caption": "Table 4: Lexicon-based phrase labeling",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1046/parts/0-Table-c8.png",
        "Caption": "Fig. 8 Average number of target phrase distribution sizes for source phrases for TRIBL and IGTree com- pared to the Moses baseline ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2230/parts/0-Table-c5.png",
        "Caption": "Table 4 Accuracy for adjectives only for the spin model, the bootstrap method, and the random walk model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1050/parts/0-Table-c3.png",
        "Caption": "Figure 2: Word prediction speed, in terms of the number of classified test examples per second, mea- sured on the three test sets, with increasing training examples. Both axes have a logarithmic scale. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J06-1004/parts/0-Figure-c1.png",
        "Caption": "Table 3. Number of candidates for each target                 language. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-2025/parts/0-Table-c2.png",
        "Caption": "Table 3: BLEU scores for pre-ordering experi- ments with a n-code system and the approach pro- posed by (Neubig et al., 2012) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/M98-1003/parts/0-Figure-c9.png",
        "Caption": "Table 1: Performance of the mention detection sys- tem using lexical features only. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Q13-1015/parts/0-Table-c1.png",
        "Caption": "Table 3: Comparing the best F-measure obtained by At-Least-N Voting with Majority Voting, Summing and the single best classifier. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P10-1124/parts/0-Figure-c3.png",
        "Caption": "Table 5: precision decrease when omitting non-",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Table-c20.png",
        "Caption": "Figure 4 Examples of alignment templates obtained in training. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-0611/parts/0-Figure-c1.png",
        "Caption": "Table 17 Example translations for the translation direction French to English using the S3 reordering constraint. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-2207/parts/0-Table-c2.png",
        "Caption": "Figure 3: Graphical view of an unordered schema automatically built starting from the verb \u2018arrest\u2019. A \u03b2 value that encouraged splitting was used. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1107/parts/0-Table-c4.png",
        "Caption": "Table 1: Test verbs and their monosemous/polysemic gold standard senses",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H01-1062/parts/0-Figure-c1.png",
        "Caption": "Table 1: 14 classes used in Joanis et al. (2008) and their corresponding Levin class numbers ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1025/parts/0-Figure-c1.png",
        "Caption": "Figure 2: (a) An example of a DCS tree (written in both the mathematical and graphical notation). Each node is labeled with a predicate, and each edge is labeled with a relation. (b) A DCS tree z with only join relations en- codes a constraint satisfaction problem. (c) The denota- tion of z is the set of consistent values for the root node. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1704/parts/0-Table-c2.png",
        "Caption": "Table 3: The 10 best languages for the verb component of BANNARD using LCS. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-1020/parts/0-Table-c1.png",
        "Caption": "Table 4: A comparison of translation quality of Egyptian, Levantine, and MSA web text, using various training corpora. The highest BLEU scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian), since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the dialectal data results in marginally worse translations. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P04-1017/parts/0-Table-c2.png",
        "Caption": "Table 2: Recognition performance.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P03-1028/parts/0-Table-c4.png",
        "Caption": "Table 4: Gross statistics for several different treebanks. Test set OOV rate is computed using the following splits: ATB (Chiang et al., 2006); CTB6 (Huang and Harper, 2009); Ne- gra (Dubey and Keller, 2003); English, sections 2-21 (train) and section 23 (test). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Figure-c8.png",
        "Caption": "Table 4: Performance of different FS machines in terms of the percentage of unclassified entries. All the classified entries were correctly classified, yielding, as a result, a precision of 100%. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1072/parts/0-Table-c5.png",
        "Caption": "Table 4 DP-TSG notation. For consistency, we largely follow the notation of Liang, Jordan, and Klein (2010). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4904/parts/0-Table-c5.png",
        "Caption": "Table 5: Urdu-English Results (% BLEU).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/I05-3013/parts/0-Figure-c4.png",
        "Caption": "Table 4: Results of Uryupina\u2019s discourse new clas- sifier ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2708/parts/0-Figure-c4.png",
        "Caption": "Figure 2: Layers used in our model.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1005/parts/0-Table-c2.png",
        "Caption": "Table 5: Systems whose F-measures are not signif- icantly different from Alice-ME at the 0.10 signifi- cance level with 0.99 confidence ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Q13-1015/parts/0-Figure-c3.png",
        "Caption": "Figure 4 The effect of varying the number of seeds on accuracy. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1107/parts/0-Table-c7.png",
        "Caption": "Figure 6 Accuracy of foreign word polarity identification. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W05-0709/parts/0-Table-c3.png",
        "Caption": "Table 2: False negative matches on the Riedel (Riedel et al., 2010) and KBP dataset (Surdeanu et al., 2012). All numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and 5% in Riedel and KBP dataset, respectively. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1140/parts/0-Table-c1.png",
        "Caption": "Figure 5: Decoding algorithm using semantic role features. Sema(c1 .role, c2 .role, t) denotes the triggered semantic role features when combining two children states, and ex- amples can be found in Figure 3. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00-1025/parts/0-Figure-c12.png",
        "Caption": "Figure 2: Lexicalize and generalize operators over t1 (part) in Figure 1. Although here only shows the nodes, we also need to change relative edges actually. (1) Applying lexicalize operator on the non-terminal node X0,1 in (a) results a new derivation shown in (b). (2) When visiting bei in (b), the generalize operator changes the derivation into (c). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-2003/parts/0-Table-c3.png",
        "Caption": "Table 2 Domain/style distribution in the MSR test corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/I08-2080/parts/0-Table-c4.png",
        "Caption": "Figure 2: Learning curves of systems with different features",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Table-c7.png",
        "Caption": "Figure 1: Graph of Word Senses",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W08-0335/parts/0-Table-c4.png",
        "Caption": "Fig. 1. An example for grouped entity tuples. Entity tuples in big frame are those suitable for the template X direct Y , whereas entity tuples in small frame are those held the same relation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1109/parts/0-Table-c1.png",
        "Caption": "Table 1: NIL expression forms based on word formation.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C00-1081/parts/0-Figure-c1.png",
        "Caption": "Table 2 French grammar development. Incremental effects on grammar size and labeled F1 for each of the manual grammar features (development set, sentences \u2264 40 words). The baseline is a parent-annotated grammar. The features tradeoff between maximizing two objectives: overall parsing F1 and MWE F1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0815/parts/0-Table-c8.png",
        "Caption": "Table 8 %BLEU on tune and test sets for UR\u2192EN translation, using our unsupervised Urdu parser to incorporate source syntactic features. The two QPD rows are statistically indistinguishable on both test sets. Both are significantly better than all Moses results, but Hiero is significantly better than all others. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-1004/parts/0-Table-c8.png",
        "Caption": "Table 3 Performance measures of the training algorithms. Green indicates the best performance, while red the worst. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-3114/parts/0-Figure-c4.png",
        "Caption": "Figure 1: Addition method.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4047/parts/0-Figure-c3.png",
        "Caption": "Figure 2: A partially scaled and inverted identity matrix J\u00b5 . Such a matrix can be used to trans- form a vector storing a domain and value repre- sentation into one containing the same domain but a partially inverted value, such as W and \u00acW de- scribed in Figure 1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-3306/parts/0-Table-c2.png",
        "Caption": "Fig. 1. Fuzzy hierarchical clustering for Paraphrase Extraction.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P05-1020/parts/0-Table-c1.png",
        "Caption": "Table 2: Translation Performance (%).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P283_w09/parts/0-Figure-c1.png",
        "Caption": "Table 2: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with the traditional hierarchical system (\u201cBaseline\u201d) and the topic-specific lexicon translation method (\u201cTopicLex\u201d). \u201cSimSrc\u201d and \u201cSimTgt\u201d denote similarity by source-side and target-side rule-distribution respectively, while \u201cSim+Sen\u201d acti- vates the two similarity and two sensitivity features. \u201cAvg\u201d is the average B LEU score on the two test sets. Scores marked in bold mean significantly (Koehn, 2004) better than Baseline (p < 0.01). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Table-c9.png",
        "Caption": "Table 2: Additional features designed to improve model of long-range reordering. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2049/parts/0-Table-c2.png",
        "Caption": "Table 2: The MEDLINE semantic categories.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E12-1054/parts/0-Table-c2.png",
        "Caption": "Table 1: Two characteristic topics for the Y slot of \u2018acquire\u2019, along with their topic-biased Lin sim- ilarities scores Lint , compared with the original Lin similarity, for two rules. The relevance of each topic to different arguments of \u2018acquire\u2019 is illus- trated by showing the top 5 words in the argument           y vector vacquire for which the illustrated topic is the most likely one. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P08-1028/parts/0-Figure-c1.png",
        "Caption": "Table 8. System performance on the production relation on the CHEM dataset. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Figure-c2.png",
        "Caption": "Table 4: Example input and best output found",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-1014/parts/0-Figure-c3.png",
        "Caption": "Table 1. Gibbs sampling for word alignment.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0401/parts/0-Table-c2.png",
        "Caption": "Table 8: Preliminary translation results for the Verbmobil Test-147 for different contextual infor- mation and different thresholds using the top-10 translations. The baseline translation results for model 4 are WER=54.80 and PER=43.07. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J06-1004/parts/0-Figure-c15.png",
        "Caption": "Table 4: Check feature templates: G f\u0087&@j         ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-3014/parts/0-Table-c1.png",
        "Caption": "Table 4: Number of recall errors according to mention type (rows anaphor, columns antecedent). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1110/parts/0-Table-c5.png",
        "Caption": "Figure 1: All the parameters of WSMs described in Section 2 used in all our experiments. Semicolon denotes OR. All the examined combinations of parameters are implied from reading the diagram from left to right. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1058/parts/0-Figure-c3.png",
        "Caption": "Table 2: Comparison of our system with the best-reported systems on MUC-6 and MUC-7",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W11-2123/parts/0-Figure-c1.png",
        "Caption": "Table 3: Performance of Altavista counts and BNC counts for candidate selection for MT (data from Prescher et al. 2000) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1048/parts/0-Figure-c1.png",
        "Caption": "Table 7 Generative patterns of ONA, where sij denotes the j-th character of the i-th word of ON (Sun, Zhou and Gao 2003). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PS15684_w09/parts/0-Table-c7.png",
        "Caption": "Table 1: Russian morphological disambiguation.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-2049/parts/0-Figure-c1.png",
        "Caption": "Table 5: Precision for 200 candidates (Ev.Rec).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-2055/parts/0-Table-c1.png",
        "Caption": "Table 6: Semantic drift detection results",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1125/parts/0-Table-c2.png",
        "Caption": "Figure 5. Performance Comparison of Different                Pruning Methods ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N09-1048/parts/0-Figure-c2.png",
        "Caption": "Figure 4: PTB vs. Wiktionary type coverage across sec- tions of the Brown corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P00-1022/parts/0-Figure-c4.png",
        "Caption": "Table 1: Phrase pairs extracted from a document pair               with an economic topic ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C08-1107/parts/0-Table-c1.png",
        "Caption": "Table 6: Language Model Results.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1141/parts/0-Table-c1.png",
        "Caption": "Table 1: Summary of results in terms of the MTO and VM scores. Standard errors are given in parentheses when available. Starred entries have been reported in the review paper (Christodoulopoulos et al., 2010). Distributional models use only the identity of the target word and its context. The models on the right incorporate orthographic and morphological features. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1023/parts/0-Table-c1.png",
        "Caption": "Figure 6: Results with varying sizes of training data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J06-1004/parts/0-Table-c1.png",
        "Caption": "Table 5 NEA type-insensitive (type-sensitive) performance with the same Chinese NE recognizer (Wu\u2019s system) and different English NE recognizers. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-1914/parts/0-Table-c1.png",
        "Caption": "Table 8: Syncretism Example 2",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c20.png",
        "Caption": "Figure 1: CoreMRR scores with different \u03b1 values using score combination. A higher \u03b1 puts more weight on the phonetic model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PTP_n09/parts/0-Table-c4.png",
        "Caption": "Figure 1. Overview of the method",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Figure-c6.png",
        "Caption": "Figure 2: Average accuracy over three procedures in Figure 1 as a function of context window size (horizontal axis) for 4 datasets. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J10-3003/parts/0-Figure-c4.png",
        "Caption": "Table 2: Illustrative tableau for a simple constraint sys- tem not capturable as a regular relation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P04-1017/parts/0-Table-c5.png",
        "Caption": "Figure 4: Salient features for fire and the violence cluster",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N10-1068/parts/0-Figure-c3.png",
        "Caption": "Figure 3: Outcome of clustering procedure",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Figure-c9.png",
        "Caption": "Figure 4: Precision of acquired relations (material). L and S denote lenient and strict evaluation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PS15684_w09/parts/0-Figure-c8.png",
        "Caption": "Figure 3: Macro-accuracy for cross-lingual bootstrapping",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-2037/parts/0-Table-c2.png",
        "Caption": "Table 5 Comparative precision values for the top 20 similarity lists of the three selected similarity measures, with MI and Bootstrapped feature weighting for each. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-1309/parts/0-Table-c2.png",
        "Caption": "Table 1: Combinatorial Search Problems in Decoding",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S13-1039/parts/0-Table-c4.png",
        "Caption": "Table 2: P r (f, a\u0303|e) for IBM Models",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D07-1076/parts/0-Figure-c1.png",
        "Caption": "Table 6: Precision, recall and F-measure for non-projective arcs",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2230/parts/0-Table-c4.png",
        "Caption": "Table 15 Size of training data set and the adaptation results on AS open. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1750/parts/0-Table-c2.png",
        "Caption": "Table 9: Example patterns of nominal interaction keywords ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H05-1083/parts/0-Table-c6.png",
        "Caption": "Table 3 Number of learned splits per NT-category after five split-merge cycles. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D08-1032/parts/0-Table-c2.png",
        "Caption": "Figure 7: Contribution of feature sets (prevention).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-2066/parts/0-Table-c1.png",
        "Caption": "Table 8. System performance on the production relation on the CHEM dataset. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N15-1071/parts/0-Figure-c1.png",
        "Caption": "Table 11: Term variation examples",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D08-1032/parts/0-Table-c8.png",
        "Caption": "Table 7: Translation results for French-English",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1045/parts/0-Figure-c1.png",
        "Caption": "Figure 7 String-to-tree configurations; each is associated with a feature that counts its occurrences in a derivation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P11-1061/parts/0-Figure-c2.png",
        "Caption": "Figure 2: Clustering-based stratified seed sampling",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4916/parts/0-Table-c2.png",
        "Caption": "Table 5: Parameters of WSMs (Section 2) which, combined with particular Measures, achieved the highest average correlation in TrValD. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1408/parts/0-Table-c1.png",
        "Caption": "Fig. 3. Interpolation \u2013 recall/precision curves.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P10-1124/parts/0-Table-c1.png",
        "Caption": "Figure 5: Decoding algorithm using semantic role features. Sema(c1 .role, c2 .role, t) denotes the triggered semantic role features when combining two children states, and ex- amples can be found in Figure 3. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P02-1061/parts/0-Figure-c2.png",
        "Caption": "Table 3: Multi-threaded time and memory consumption of Moses translating 3003 sentences on eight cores. Our code supports lazy memory mapping (-L) and prefault- ing (-P) with MAP POPULATE, the default. IRST is not threadsafe. Time for Moses itself to load, including load- ing the language model and phrase table, is included. Along with locking and background kernel operations such as prefaulting, this explains why wall time is not one-eighth that of the single-threaded case. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-1065/parts/0-Table-c1.png",
        "Caption": "Figure 2. F1-measure with \uf062 in [0,1]",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P08-1102/parts/0-Table-c1.png",
        "Caption": "Figure 2. Position of news story boundaries in a CNN news summary in relation to troughs found by the algorithm. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0106/parts/0-Table-c2.png",
        "Caption": "Table 6: F1 scores of the local CRF and non-local models on the CMU Seminar Announcements dataset. We also provide the results from Sutton and McCallum (2004) for comparison. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-1624/parts/0-Figure-c1.png",
        "Caption": "Figure 1. NPs in a sample from the Catalan training data (left) and the English translation (right). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S10-1090/parts/0-Figure-c1.png",
        "Caption": "Table 2 The top 20 most similar words for country (and their ranks) in the similarity list of LIN, followed by the next four words in the similarity list that were judged as entailing at least in one direction. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1150/parts/0-Table-c2.png",
        "Caption": "Figure 2: Performance of TL-comb and TL-auto as H changes. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P04470/parts/0-Table-c7.png",
        "Caption": "Figure 10 Number of processed arcs for the pseudotranslation task as a function of the input sentence length J (y-axis is given in log scale). The complexity for the four different reordering constraints MON, GE, EG, and S3 is given. The complexity of the S3 constraint is close to J4 . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-1309/parts/0-Table-c3.png",
        "Caption": "Figure 1: The density of the F1 -scores with the three approaches. The prior used is a symmetric Dirichlet with \u03b1 = 0.1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Table-c4.png",
        "Caption": "Table 1: The semantic roles of cases beside C-3 verb cluster ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1161/parts/0-Figure-c9.png",
        "Caption": "Table 1: DP algorithm for statistical machine translation.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D10-1042/parts/0-Table-c2.png",
        "Caption": "Table 7 Results for Mutiple Document System",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-1104/parts/0-Table-c1.png",
        "Caption": "Table 9: Performance of each individual relation type based on 5-fold cross-validation.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1048/parts/0-Figure-c7.png",
        "Caption": "Table 3 Confusion matrix for argument labels, with ArgM labels collapsed into one category. Entries are a fraction of total annotations; true zeros are omitted, while other entries are rounded to zero. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1704/parts/0-Table-c3.png",
        "Caption": "Table 3: Analysis of context length",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2708/parts/0-Figure-c1.png",
        "Caption": "Table 1: Results of a 10-fold cross-validation for various machine learning algorithms. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Figure-c9.png",
        "Caption": "Table 5: The System Performance Based on Com- binations of Surface and Semantic Features. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Table-c5.png",
        "Caption": "Table 13 Evaluating predominant sense information for polysemous nouns on the Senseval-2 all-words task data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Figure-c2.png",
        "Caption": "Table 1: Feature set for the baseline pronoun res- olution system ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-1045/parts/0-Figure-c2.png",
        "Caption": "Figure 2: Glue Semantics proof for (83), English Way Construction (means interpretation)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1761/parts/0-Figure-c8.png",
        "Caption": "Figure 4: PTB vs. Wiktionary type coverage across sec- tions of the Brown corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Figure-c4.png",
        "Caption": "Table 2: Sample of subjective features appearing in the top 100 discriminant attributes selected with Information Gain on the 3rd fold training data at iteration 10. The words in italics in the multilingual features represent equivalent translations in English and Romanian. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-2206/parts/0-Figure-c1.png",
        "Caption": "Table 8: Detailed DIFF results",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c9.png",
        "Caption": "Figure 8: Squares represent the proportion of tokens in each language assigned to a topic. The left topic, world ski km won,",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H05-1083/parts/0-Table-c5.png",
        "Caption": "Figure 4: Definition of the operations used to transform the structure of the underspecified logical form l0 to match the ontology O. The function type(c) calculates a constant c\u2019s type. The function freev(lf ) returns the set of variables that are free in lf (not bound by a lambda term or quantifier). The function subexps(lf ) generates the set of all subexpressions of the lambda calculus expression lf . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P08-1090/parts/0-Figure-c7.png",
        "Caption": "Table 3: Description of feature sets. \u2217 Glob only uses the same set of similarity measures when combined with other semantic features. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1027/parts/0-Table-c2.png",
        "Caption": "Figure 3: Using different amounts of annotated training",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Table-c5.png",
        "Caption": "Table 3: Comparison to previous work on the 7 re- lations of ACE 2004. K: kernel-based; F: feature- based; yes/no: models argument order explicitly. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-2611/parts/0-Table-c1.png",
        "Caption": "Figure 6: Metaphors tagged by the system (in bold)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1074/parts/0-Table-c2.png",
        "Caption": "Table 2: Results on Penn (Chinese) Treebank.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W03-0432/parts/0-Table-c1.png",
        "Caption": "Table 1: Results of segmentation of entry titles (F-score (precision/recall)).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1004/parts/0-Figure-c1.png",
        "Caption": "Table 3: Statistics of anaphoric expressions",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P08-1102/parts/0-Figure-c2.png",
        "Caption": "Figure 3: Smoothed precision curves over the five corpora.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-3114/parts/0-Figure-c13.png",
        "Caption": "Table 4: Coverage/precision with various rule collections",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1025/parts/0-Table-c6.png",
        "Caption": "Figure 2. Example patterns in student discussion                    threads ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1727/parts/0-Table-c2.png",
        "Caption": "Table 3: F1 scores and speed (in sentences per sec.) of SegTagDep on CTB-5c-1 w.r.t. the beam size. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C02-1033/parts/0-Figure-c2.png",
        "Caption": "Table 2: Evidence cardinality in the corpora.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c14.png",
        "Caption": "Figure 1: Our probabilistic model: a question x is mapped to a latent logical form z, which is then evaluated with respect to a world w (database of facts), producing an answer y. We represent logical forms z as labeled trees, induced automatically from (x, y) pairs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0301/parts/0-Table-c1.png",
        "Caption": "Figure 2: Selected morphosyntactic categories in the OLiA Reference Model ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09_csl2013/parts/0-Table-c2.png",
        "Caption": "Fig. 6 Distances found between phrase boundaries with linked modifier words and with parent words",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1140/parts/0-Table-c4.png",
        "Caption": "Table 1: Linguistic levels as feature sets.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J09-3004/parts/0-Table-c8.png",
        "Caption": "Table 3: Translation results for English-French",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1016/parts/0-Table-c5.png",
        "Caption": "Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1010/parts/0-Table-c2.png",
        "Caption": "Table 5: Results of the fill-in-the-blank exercise",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-2009/parts/0-Figure-c2.png",
        "Caption": "Figure 6: F-measure for the objective and subjective classes for multilingual bootstrapping (versus cross-lingual framework) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3812/parts/0-Table-c3.png",
        "Caption": "Table 11: Term variation examples",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-3306/parts/0-Table-c1.png",
        "Caption": "Figure 2: Example of a so-called semi-formal text, where one can see that here more time points are available, and that those can be complemen- tary to the time points to be extracted from formal texts. So, already at this level, a unification or merging of extracted time points is necessary. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D13-1031/parts/0-Table-c4.png",
        "Caption": "Table 5 Training and test conditions for the German-to-English Verbmobil corpus (*number of words without punctuation). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1133/parts/0-Figure-c2.png",
        "Caption": "Table 2. The scored results of our CWS in the MSR_C track (OOV is 0.034) for 3rd bakeoff. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1080/parts/0-Table-c2.png",
        "Caption": "Figure 3: Objective values for the different mappings used in our experiments for four languages. Note that the goal of the optimization procedure is to minimize the objective value. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c7.png",
        "Caption": "Table 1: Sample of extracted entailment rules.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1104/parts/0-Table-c4.png",
        "Caption": "Table 3: Translation results for English-French",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Table-c7.png",
        "Caption": "Table 3: Comparison of the three decoders by the ratio each decoder produced search errors. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1138/parts/0-Table-c4.png",
        "Caption": "Table 4: Test corpora statistics.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-3008/parts/0-Table-c3.png",
        "Caption": "Figure 11 An example Chinese lexicalized phrase-structure parse tree. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-1006/parts/0-Table-c1.png",
        "Caption": "Fig. 3. Interpolation \u2013 recall/precision curves.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1141/parts/0-Figure-c8.png",
        "Caption": "Table 2: Chinese character usage in 3 corpora. The   numbers in brackets indicate the percentage of  characters that are shared by at least 2 corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0301/parts/0-Table-c3.png",
        "Caption": "Table 1: BLEU scores of English to Russian ma- chine translation system evaluated on tst2012 us- ing baseline GIZA++ alignment and translitera- tion augmented-GIZA++. OOV-TI presents the score of the system trained using TA-GIZA++ af- ter transliterating OOVs ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0206/parts/0-Table-c6.png",
        "Caption": "Table 3: Corpus statistics for Chinese (Zh) character segmentation and English (En)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1059/parts/0-Table-c2.png",
        "Caption": "Figure 2 Disambiguation of conventional dictionaries. \u201cLearn phrases,\u201d \u201canalyze,\u201d and \u201cannotation\u201d require morpho-syntactic analysis of the transformed sentences. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1062/parts/0-Figure-c6.png",
        "Caption": "Figure 6 Accuracy of foreign word polarity identification. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W03-0432/parts/0-Table-c2.png",
        "Caption": "Figure 4: Chunk-based translation model. The words in bold are head words.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1059/parts/0-Table-c3.png",
        "Caption": "Table 7 LIN (MI) weighting: The top 10 common features for country\u2013state and country\u2013party, along with their corresponding ranks in each of the two feature vectors. The features are sorted by the sum of their feature weights with both words. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W10-3212/parts/0-Table-c3.png",
        "Caption": "Table 6: Overall performance on the evaluation set. L is the upper bound of the length of possible chunks in semi-CRFs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1115/parts/0-Table-c3.png",
        "Caption": "Table 5: RM gain over other optimizers averaged over all test sets. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1115/parts/0-Figure-c1.png",
        "Caption": "Figure 8: Squares represent the proportion of tokens in each language assigned to a topic. The left topic, world ski km won,",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-1624/parts/0-Figure-c3.png",
        "Caption": "Figure 4: POS tagging accuracy using one-at-a- time, character-based POS tagger ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P04-1018/parts/0-Figure-c2.png",
        "Caption": "Table 2: The NP chunking results for six sys- tems associated with the project. The baseline results have been obtained by selecting the most frequent chunk tag associated with each part-of- speech tag. The best results for this task have been obtained with a combination of seven learn- ers, five of which were operated by project mem- bers. The combination of these five performances is not far off these best results. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1086/parts/0-Figure-c2.png",
        "Caption": "Table 10 Arabic MWE identification per category and overall results (test set, sentences \u2264 40 words). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Figure-c6.png",
        "Caption": "Figure 2: Clustering-based stratified seed sampling",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J02-2003/parts/0-Table-c3.png",
        "Caption": "Table 1: Regular expression notation in foma.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P05-1013/parts/0-Figure-c2.png",
        "Caption": "Table 2: good#a#15 gloss and examples.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N07-1024/parts/0-Table-c7.png",
        "Caption": "Table 5: Search Success Rate (1 million hypothe- ses) [%]. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Table-c13.png",
        "Caption": "Figure 2: Learning curve with different sizes of labeled data ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1044/parts/0-Table-c5.png",
        "Caption": "Figure 1 Relation between number of classes and alternations. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1140/parts/0-Figure-c3.png",
        "Caption": "Figure 1: Example MERT values along one coordi- nate, first unregularized. When regularized with `2 , the piecewise constant function becomes piecewise quadratic. When using `0 , the function remains piecewise constant with a point discontinuity at 0. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P03-1028/parts/0-Table-c1.png",
        "Caption": "Table 8 Models with functional features: GENDER, NUMBER, rationality (RAT). F N * = functional feature(s) based on Alkuhlani and Habash (2011); GN = GENDER + NUMBER ; GNR = GENDER + NUMBER + RAT . Statistical significance tested only for CORE 12+. . . models on predicted input, against the CORE 12 baseline. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1062/parts/0-Table-c4.png",
        "Caption": "Table 6 Conventional dictionary used to complement the training corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2230/parts/0-Table-c2.png",
        "Caption": "Figure 1: Accuracy Trends on MicroWnOp Corpus.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1016/parts/0-Figure-c2.png",
        "Caption": "Figure 2: BLEU difference of 1000 bootstrap sam- ples. 95% confidence interval is [.15, .90] The proposed approach therefore seems to be a stable method. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1008/parts/0-Figure-c1.png",
        "Caption": "Table 3: Comparison figures on subsets of the Stanford Sentiment Treebank ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1114/parts/0-Figure-c2.png",
        "Caption": "Figure 3: An example showing the combination of the se- mantic role sequences of the states. Above/middle is the state information before/after applying the TTS template, and bot- tom is the used TTS template and the triggered SRFs during the combination. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D07-1076/parts/0-Table-c1.png",
        "Caption": "Table 14 Training and test conditions for the Hansards task (*number of words without punctuation). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-1045/parts/0-Figure-c3.png",
        "Caption": "Figure 1: Dependency structure of text. Tree skeleton in bold ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-3004/parts/0-Table-c2.png",
        "Caption": "Table 2: Weights learned for discount features. Nega- tive weights indicate bonuses; positive weights indicate penalties. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1059/parts/0-Table-c1.png",
        "Caption": "Table 1: Monolingual and Crosslingual Baseline Slot Filling Pipelines ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1016/parts/0-Table-c2.png",
        "Caption": "Table 11: POS tagging error patterns. # means the error number of the corresponding pattern made by the pipeline tagging model. \u2193 and \u2191 mean the error number reduced or increased by the joint model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1012/parts/0-Table-c5.png",
        "Caption": "Table 2: English Eve corpus results. Standard deviations are in parentheses; \u2217 denotes a significant difference from the M ORTAG model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3909/parts/0-Table-c5.png",
        "Caption": "Figure 2: Distribution of generated paraphrases per Lev- enshtein distance ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Figure-c1.png",
        "Caption": "Table 3: Tagging accuracy on the gold-standard normalizations (OrigP = original punctuation, ModP = modern punctuation, NoP = no punctu- ation) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1111/parts/0-Table-c3.png",
        "Caption": "Table 1. Incremental Improvement from          Self-training (English) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1634/parts/0-Figure-c3.png",
        "Caption": "Table 1: Part of a sample headline cluster, with sub-clusters ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c7.png",
        "Caption": "Table 2: Results for different predictor configura- tions. Numbers give % reductions in keystrokes. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P03-1028/parts/0-Figure-c2.png",
        "Caption": "Table 6: Syntactic features for featurama (Czech). * mark statistically significantly better models compared to feat- urama (sentence-based t-test with \u03b1 = 0.05). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Figure-c7.png",
        "Caption": "Table 6 Comparing clustering initializations on D1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1045/parts/0-Table-c3.png",
        "Caption": "Table 4: Seeds with the Highest Weight",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1072/parts/0-Table-c2.png",
        "Caption": "Table 6: Rules for simplifying the morphological complexity for RU. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1087/parts/0-Figure-c3.png",
        "Caption": "Table 1: Size of Seed Lexicons",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P59105ca/parts/0-Figure-c6.png",
        "Caption": "Table 7: Diff results tested against gs-swaco",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4916/parts/0-Table-c5.png",
        "Caption": "Figure 4: A chain-structured DCRF as our intra- sentential parsing model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1111/parts/0-Table-c1.png",
        "Caption": "Table 5: Correlations of resolution class scores with respect to the average. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J09-3004/parts/0-Table-c10.png",
        "Caption": "Table 1: Accuracy and error reduction (ER) results (in percents) for our model and the MF baseline. Error reduction is computed as M ODEL\u2212M                                     100\u2212M F                                              F                                                . Results are given for the WSJ and GENIA corpora test sets. The top table is for a model receiving gold standard parses of the test data. The bottom is for a model using (Charniak and Johnson, 2005) state-of-the-art parses of the test data. In the main scenario (left), instances were always mapped to VN classes, while in the OIP one (right) it was possible (during both training and test) to map instances as not belonging to any existing class. For the latter, no results are displayed for polysemous verbs, since each verb can be mapped both to \u2018other\u2019 and to at least one class. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C08-1088/parts/0-Table-c3.png",
        "Caption": "Table 2: Training phase: effect of similarity thresh- old (a) on Ave. MRR and TRDR. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Figure-c6.png",
        "Caption": "Table 2: Data sizes for the experiments reported in this paper (English words shown). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D13-1031/parts/0-Table-c9.png",
        "Caption": "Figure 4: Using the type model for disambiguation in the derivation of file a suit. Type distributions are shown after the variable declarations. Both suit and the object of file are lexically ambiguous between different types, but after the \u03b2 -reduction only one interpretation is likely. If the verb were wear, a different interpretation would be preferred. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1004/parts/0-Figure-c3.png",
        "Caption": "Figure 1: Organisation of the hierarchical graph of concepts",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-4223/parts/0-Table-c1.png",
        "Caption": "Table 3 Feature templates used for CRF in our experiments",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1011/parts/0-Figure-c2.png",
        "Caption": "Table 3: Adjective Full Inflection",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Table-c4.png",
        "Caption": "Figure 1: Example of word alignment",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Psem_p07/parts/0-Figure-c3.png",
        "Caption": "Figure 3: learning curves of the averaged and non- averaged perceptron algorithms ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3604/parts/0-Table-c1.png",
        "Caption": "Table 1 Closed test, in percentages (%)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1408/parts/0-Table-c4.png",
        "Caption": "Table 1 Space comparison between FSAs and FSRAs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W05-1518/parts/0-Table-c4.png",
        "Caption": "Table 5: Bagging with 50 unsupervised seed sets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-1068/parts/0-Figure-c1.png",
        "Caption": "Table 7 Effect of pruning parameter Np and heuristic function on error rate for direct-translation model (tp = 10\u221212 ). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W12-2303/parts/0-Figure-c2.png",
        "Caption": "Figure 2: BLEU difference of 1000 bootstrap sam- ples. 95% confidence interval is [.15, .90] The proposed approach therefore seems to be a stable method. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-1014/parts/0-Table-c5.png",
        "Caption": "Figure 5: Six of the top 20 scored Narrative Schemas. Events and arguments in italics were marked misaligned by FrameNet definitions. * indicates verbs not in FrameNet. - indicates verb senses not in FameNet. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1003/parts/0-Table-c1.png",
        "Caption": "Table 1 Notation used in this article. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N13-1118/parts/0-Figure-c2.png",
        "Caption": "Figure 2. The pseudo code of Algorithm 1.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1079/parts/0-Table-c1.png",
        "Caption": "Table 2: False negative matches on the Riedel (Riedel et al., 2010) and KBP dataset (Surdeanu et al., 2012). All numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and 5% in Riedel and KBP dataset, respectively. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1115/parts/0-Table-c1.png",
        "Caption": "Table 8: ROUGE-W in empirical approach",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P11-1060/parts/0-Table-c2.png",
        "Caption": "Table 9. MRRs of the phonetic transliteration",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Table-c11.png",
        "Caption": "Table 6: Parameters of Measures (Section 3) which, combined with particular WSMs, achieved the highest average correlation in TrValD. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1016/parts/0-Table-c7.png",
        "Caption": "Table 1: Supersense evaluation results. Values are the percentage of correctly assigned supersenses. k indicates the number of nearest neighbours considered. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N10-1019/parts/0-Figure-c3.png",
        "Caption": "Table 1: IBM Model 3",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PTASL_n09/parts/0-Figure-c3.png",
        "Caption": "Table 2: Combination results (using SVMacc)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W08-0602/parts/0-Table-c5.png",
        "Caption": "Figure 1: Syntactic tree kernel (STK).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c21.png",
        "Caption": "Figure 1: Proposed discourse structures for Ex. 4: (a) In terms of informational relations; (b) in terms of inten- tional relations ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N07-1024/parts/0-Table-c9.png",
        "Caption": "Table 5 NEA type-insensitive (type-sensitive) performance with the same Chinese NE recognizer (Wu\u2019s system) and different English NE recognizers. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c19.png",
        "Caption": "Fig. 6. German English BLEU scores of various al EM(Co), GS(Co), EM(Co)+GS(Co), and VB(Co). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W12-1011/parts/0-Table-c6.png",
        "Caption": "Figure 6: Creation of a Lexical Transducer",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1121/parts/0-Table-c3.png",
        "Caption": "Table 4: Performance of different relation types and major subtypes in the test data",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1056/parts/0-Table-c4.png",
        "Caption": "Figure 1: Illustration of Pareto Frontier. Ten hypotheses are plotted by their scores in two metrics. Hypotheses indicated by a circle (o) are pareto-optimal, while those indicated by a plus (+) are not. The line shows the convex hull, which attains only a subset of pareto-optimal points. The triangle (4) is a point that is weakly pareto-optimal but not pareto-optimal. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Table-c12.png",
        "Caption": "Figure 1: Addition method.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1063/parts/0-Table-c11.png",
        "Caption": "Table 4: NIST08 Chinese-English translation BLEU",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1062/parts/0-Table-c3.png",
        "Caption": "Table 2: Distribution of SCs in the ACE corpus.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2139/parts/0-Figure-c1.png",
        "Caption": "Figure 2: Duration (in seconds) of each lexical type ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N12-1006/parts/0-Table-c1.png",
        "Caption": "Figure 4: Learning Curve on RIBES: comparing single- objective optimization and PMO. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1113/parts/0-Figure-c1.png",
        "Caption": "Figure 2. Extracted NE pair instances and context",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2204/parts/0-Table-c5.png",
        "Caption": "Table 3: Baseline performance and n-best oracle scores (UAS/LAS) on the development sets. mate\u2019 uses the prepro- cessing provided by the organizers, the other parsers use the preprocessing described in Section 2. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N06-1027/parts/0-Figure-c1.png",
        "Caption": "Table 1: Statistics of training, development and test data for IWSLT task. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-1004/parts/0-Table-c3.png",
        "Caption": "Table 1: Case restoration performance using an MD-trie, English. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H05-1001/parts/0-Table-c1.png",
        "Caption": "Table 1: Properties of abbreviations corpus retrieved from Medline ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-3031/parts/0-Figure-c1.png",
        "Caption": "Table 1: Examples of zero anaphora",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D10-1042/parts/0-Figure-c5.png",
        "Caption": "Table 5: An example where syntactic features help to link the PRO mention \u00d1\u00eb (hm) with its antecedent, the NAM                   ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S0885/parts/0-Figure-c1.png",
        "Caption": "Table 3 Descriptive statistics for WordNet hyp/syn relations for other-anaphora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-0329/parts/0-Table-c3.png",
        "Caption": "Table 1 Semi-fixed MWEs in French and English. The French adverb \u00e0 terme (\u2018in the end\u2019) can be modified by a small set of adjectives, and in turn some of these adjectives can be modified by an adverb such as tr\u00e8s (\u2018very\u2019). Similar restrictions appear in English. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P01-1005/parts/0-Figure-c3.png",
        "Caption": "Table 1: An example of English, Chinese and French terms consisting of the same morphemes",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E12-1054/parts/0-Table-c5.png",
        "Caption": "Table 6 Performance of proposed system on MSRVDC Dataset 1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6202/parts/0-Figure-c6.png",
        "Caption": "Figure 6: Metaphors tagged by the system (in bold)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-1020/parts/0-Table-c5.png",
        "Caption": "Figure 2: A tiered graphic representing the three different SRL model configurations. The baseline system is described in the bottom (c & d), the separate panels highlighting the independent predictions of this model: sense labels are assigned in an entirely separate process from argument prediction. Pruning in the model takes place primarily in this tier, since we observe true predicates we only instantiate over these indices. The middle tier (b.) illustrates the syntactic representation layer, and the connective factors between syntax and SRL. In the observed syntax model the Link variables are clamped to their correct values, with no need for a factor to coordinate them to form a valid tree. Finally, the hidden model comprises all layers, including a combinatorial syntactic constraint (a.) over syntactic variables. In this scenario all labels in (b.) are hidden at both training and test time. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1059/parts/0-Table-c6.png",
        "Caption": "Figure 9: Example error cases, with associated frequencies, illustrating system output and gold standard references. 5% of the cases were miscellaneous or otherwise difficult to categorize. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c10.png",
        "Caption": "Figure 7 FSRA for the pattern hit a e . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Table-c6.png",
        "Caption": "Table 5: Segmentation performance presented in previous work and of our combination model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1070/parts/0-Table-c1.png",
        "Caption": "Figure 3: Example derivation for the query \u2018how many people visit the public library of new york annu- ally.\u2019 Underspecified constants are labelled with the words from the query that they are associated with for readability. Constants from O, written in typeset, are introduced in step (c). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-0703/parts/0-Figure-c4.png",
        "Caption": "Figure 4: Overview of annotation environment",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1082/parts/0-Table-c3.png",
        "Caption": "Figure 1: A tree showing head information",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1104/parts/0-Figure-c2.png",
        "Caption": "Table 2: Results of our alignment quality experiments. All timing and accuracy figures use means from five independently initial- ized runs. Note that lower is better for AER, higher is better for F0.5 . All experiments are run on a system with two Intel Xeon E5645 CPUs running at 2.4 GHz, in total 12 physical (24 virtual) cores. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Figure-c7.png",
        "Caption": "Figure 2: Segmentation and tagging of the Arabic token A\u00eeE\u00f1J. J\u00baJ\u0083\u00f0 \u2018and they will write it\u2019. This token has four seg- ments with conflicting grammatical features. For example, the number feature is singular for the pronominal object and plural for the verb. Our model segments the raw to- ken, tags each segment with a morpho-syntactic class (e.g., \u201cPron+Fem+Sg\u201d), and then scores the class sequences. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-1025/parts/0-Figure-c1.png",
        "Caption": "Figure 2: Comparing F-measure, precision, and recall of different voting schemes for Chinese relation extraction. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6211/parts/0-Table-c3.png",
        "Caption": "Table 5: Effect of the introduction of equivalence classes. For the baseline we used the original in- flected word forms. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1408/parts/0-Table-c9.png",
        "Caption": "Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2\u201323) and the ATB (p1\u20133). English parsing evaluations usually report results on sentences up to length 40. Arabic sentences of up to length 63 would need to be evaluated to account for the same fraction of the data. We propose a limit of 70 words for Arabic parsing evaluations. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Figure-c3.png",
        "Caption": "Figure 3 Algorithm phrase-extract for extracting phrases from a word-aligned sentence pair. Here quasi-consecutive(TP) is a predicate that tests whether the set of words TP is consecutive, with the possible exception of words that are not aligned. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1056/parts/0-Table-c1.png",
        "Caption": "Figure 9: Example word structure annotation. We add an \u2018f\u2019 to the POS tags of words with no further structures. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1634/parts/0-Table-c4.png",
        "Caption": "Table 7 The official vocabularies in Verbmobil. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1005/parts/0-Table-c2.png",
        "Caption": "Figure 1 Percentage of correct entailments within the top 40 candidate pairs of each of the methods, LIN and Bootstrapped LIN (denoted as LINB in the \ufb01gure), when using varying numbers of top-ranked features in the feature vector. The value of \u201cAll\u201d corresponds to the full size of vectors and is typically in the range of 300\u2013400 features. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1115/parts/0-Table-c3.png",
        "Caption": "Fig. 6. Algorithm for fuzzy divisive clustering based on nouns.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1098/parts/0-Figure-c1.png",
        "Caption": "Table 2: Results of UniGraph, BiGraph, and Bi- Graph*. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P01-1027/parts/0-Table-c2.png",
        "Caption": "Figure 2. Learning Curves for Confusable Disambiguation",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P22324_w09/parts/0-Table-c3.png",
        "Caption": "Table 1: Snapshot of the supersense-annotated data. The 7 article titles (translated) in each domain, with total counts of sentences, tokens, and supersense mentions. Overall, there are 2,219 sentences with 65,452 tokens and 23,239 mentions (1.3 tokens/mention on average). Counts exclude sentences marked as problematic and mentions marked ?. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E09-2008/parts/0-Table-c1.png",
        "Caption": "Figure 2: Framework overview.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmert_n09/parts/0-Figure-c1.png",
        "Caption": "Table 3: Segmentation performance on words that have the same final suffix as their preceding words. The F1 scores are computed based on all boundaries within the words, but the accuracies are obtained using only the final suffixes. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-0513/parts/0-Table-c4.png",
        "Caption": "Table 2: Clustering performance on the predominant senses, with and without prepositions. The last entry presents the per- formance of random clustering with K = 25, which yielded the best results among the three values K=25, 35 and 42. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1089/parts/0-Table-c2.png",
        "Caption": "Figure 2: Bayesian network: \u03b1 and \u03b2 are vectors of hy- perparameters, and \u03b8 i (for i \u2208 {1, . . . , nc }) and \u03c6 are distributions. u is a vector of underlying forms, generated from \u03c6, and si (for i \u2208 nu ) is a set of observed surface forms generated from the hidden variable ui according to \u03b8i ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P08-1102/parts/0-Table-c4.png",
        "Caption": "Figure 1: CTB 10-fold CV word segmentation F- measure for our word segmenter ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-1098/parts/0-Table-c1.png",
        "Caption": "Table 5: Results for verbs",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W10-4138/parts/0-Table-c2.png",
        "Caption": "Table 6: Recall and precision of the patterns.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1761/parts/0-Table-c6.png",
        "Caption": "Figure 1: An example confusion network construc- tion ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2211/parts/0-Table-c8.png",
        "Caption": "Figure 1: The relationship extraction system.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1115/parts/0-Table-c6.png",
        "Caption": "Table 7. Precision at top 200",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-1003/parts/0-Figure-c3.png",
        "Caption": "Table 1. Results of system combination on Dev7 (development) corpus and Test09,             the o\ufb03cial test corpus of IWSLT\u201909 evaluation campaign. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D07-1018/parts/0-Table-c2.png",
        "Caption": "Table 2: Values obtained for Precision, Recall and F- scores with method 1 by changing the minimum fre- quency of the correspondences to construct rules for foma. The rest of the options are the same in all three experiments: only one rule is applied within a word. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1147/parts/0-Table-c1.png",
        "Caption": "Table 12 Translation results for the translation direction English to German on the TEST-331 test set. The results are given in terms of computing time, WER, and PER for three different reordering constraints: MON, EG, and S3. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1161/parts/0-Figure-c2.png",
        "Caption": "Table 8: Syncretism Example 2",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2210/parts/0-Table-c3.png",
        "Caption": "Fig. 1 Examples of TERp alignment output. In each example, R, H and H  denote the reference, the original hypothesis and the hypothesis after shifting respectively. Shifted words are bolded and other edits are in [brackets]. Number of edits shown: TERp (TER) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-1020/parts/0-Table-c2.png",
        "Caption": "Table 3: F-measure after successive addition of each global feature group ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-0813/parts/0-Table-c5.png",
        "Caption": "Table 2 Estimation of model parameters. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2708/parts/0-Figure-c3.png",
        "Caption": "Table 3: Meta-evaluation results at document and system level for submitted metrics",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1138/parts/0-Table-c3.png",
        "Caption": "Figure 2: Average sentence cover size: the average number of sentences needed to generate the case- frames in a summary sentence (Study 1). Model summaries are shown in darker bars. Peer system numbers that we focus on are in bold. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1025/parts/0-Table-c6.png",
        "Caption": "Table 4: Comparison to Related Approaches",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2101/parts/0-Table-c5.png",
        "Caption": "Table 3: ROUGE-SU in k-means learning",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1127/parts/0-Table-c1.png",
        "Caption": "Table 1: A protein domain-referring phrase example",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W13-2512/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Topic transfer in bilingual LSA model.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-3144/parts/0-Table-c2.png",
        "Caption": "Table 1: Summary of results in terms of the MTO and VM scores. Standard errors are given in parentheses when available. Starred entries have been reported in the review paper (Christodoulopoulos et al., 2010). Distributional models use only the identity of the target word and its context. The models on the right incorporate orthographic and morphological features. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N12-1006/parts/0-Table-c4.png",
        "Caption": "Table 7. Results from Direct Thread Classification",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Table-c16.png",
        "Caption": "Table 5: Final results on CTB-5j",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1630/parts/0-Table-c5.png",
        "Caption": "Table 10 Occurrences of error types for the best other-anaphora algorithm algoWebv4 . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1138/parts/0-Figure-c2.png",
        "Caption": "Table 5: Details of the unlabeled data.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2167/parts/0-Table-c2.png",
        "Caption": "Figure 1: Examples of sentences x, domain-independent underspecified logical forms l0 , fully specified logical forms y, and answers a drawn from the Freebase domain. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H01-1062/parts/0-Table-c3.png",
        "Caption": "Figure 1: The density of the F1 -scores with the three approaches. The prior used is a symmetric Dirichlet with \u03b1 = 0.1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-1004/parts/0-Table-c4.png",
        "Caption": "Table 1: Statistics about the results of our word sense discovery algorithm",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1009/parts/0-Table-c5.png",
        "Caption": "Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-1624/parts/0-Figure-c2.png",
        "Caption": "Table 1: Phrase pairs extracted from a document pair               with an economic topic ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-2031/parts/0-Table-c1.png",
        "Caption": "Figure 2: (a) Moses translation output along with \u03b3, \u03c6, and a. An English gloss is shown above the Chinese sentence and above the gloss is shown the dependency parse from the Stanford parser. (b) QPDG system output with additional structure \u03c4\u03c6 . (c) reference translations. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1004/parts/0-Table-c8.png",
        "Caption": "Table 5: Resolution accuracy (%)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-2611/parts/0-Figure-c3.png",
        "Caption": "Table 6: The System Performance of Integrating Cross Source and Cross Genre Information. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C04-1131/parts/0-Table-c2.png",
        "Caption": "Table 8: Final UAS/LAS scores for dependencies on the test sets for the predicted setting. Other denotes the highest scoring other participant in the Shared Task. ST Baseline denotes the MaltParser baseline provided by the Shared Task organizers. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N03-1010/parts/0-Figure-c5.png",
        "Caption": "Figure 1: System Architecture.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Table-c14.png",
        "Caption": "Figure 3: Performance of baseline and joint models w.r.t. the average processing time (in sec.) per sen- tence. Each point corresponds to the beam size of 4, 8, 16, 32, (64). The beam size of 16 is used for SegTag in SegTag+Dep and SegTag+TagDep. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1138/parts/0-Figure-c2.png",
        "Caption": "Table 5: Results of Multiple Trials and Compari- son to Simulated Annealing ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1007/parts/0-Table-c1.png",
        "Caption": "Table 5: Meta alternations and their average precision values for the task. The random baseline performs at 0.313 while the frequency baseline ranges from 0.255 to 0.369 with a mean of 0.291. Alternations for which the model outperforms the frequency baseline are in boldface (mean AP: 0.399, standard deviation: 0.119). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D13-1031/parts/0-Table-c5.png",
        "Caption": "Table 9 Experiments on the threshold\u2013partial recall relationship of the large corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PTP_n09/parts/0-Figure-c4.png",
        "Caption": "Figure 1 Architecture of the statistical translation approach based on Bayes\u2019 decision rule. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H01-1062/parts/0-Figure-c4.png",
        "Caption": "Table 11: Term variation examples",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1046/parts/0-Figure-c2.png",
        "Caption": "Table 23 Comparisons against other segmenters: In Column 1, SXX indicates participating sites in the 1st SIGHAN International Chinese Word Segmentation Bakeoff, and CRFs indicates the word segmenter reported in (Peng et al. 2004). In Columns 2 to 5, entries contain the F-measure of each segmenter on different open runs, with the best performance in bold. Column Site-Avg is the average F-measure over the data sets on which a segmenter reported results of open runs, where a bolded entry indicates the segmenter outperforms MSRSeg. Column Our-Avg is the average F-measure of MSRSeg over the same data sets, where a bolded entry indicates that MSRSeg outperforms the other segmenter. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1408/parts/0-Table-c8.png",
        "Caption": "Figure 4: Selected morphological features in the OLiA Reference Model ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P08-2023/parts/0-Table-c1.png",
        "Caption": "Table 3: Human Assessment of Errors",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1004/parts/0-Table-c11.png",
        "Caption": "Figure 1: Average Precision, Recall and F1 at dif- ferent top K rule cutoff points. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Overview of the system.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1011/parts/0-Table-c2.png",
        "Caption": "Table 9 Evaluation on SemCor, polysemous words only. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1025/parts/0-Table-c2.png",
        "Caption": "Table 6: Precision for each phrase type (Ev.Ling).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D07-1076/parts/0-Table-c2.png",
        "Caption": "Table 2: English Eve corpus results. Standard deviations are in parentheses; \u2217 denotes a significant difference from the M ORTAG model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C04-1194/parts/0-Table-c1.png",
        "Caption": "Table 1: Sizes of our comparable corpora",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Figure-c3.png",
        "Caption": "Figure 2: Learning curve with different sizes of labeled data ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-3236/parts/0-Figure-c3.png",
        "Caption": "Table 1: List of keywords used in WordNet search for generating WN CLASS features. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c27.png",
        "Caption": "Table 2: Results of SVM and Mincuts with different settings of feature",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-2003/parts/0-Table-c2.png",
        "Caption": "Table 11 Comparison between a ME framework and the derived model on the same test set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1109/parts/0-Table-c2.png",
        "Caption": "Table 4 Results when tuning for performance over the development set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1056/parts/0-Table-c1.png",
        "Caption": "Table 6 Results for Single Document System",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W10-3708/parts/0-Table-c1.png",
        "Caption": "Figure 3: Boxer output for Shared Task Text 2",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0106/parts/0-Table-c1.png",
        "Caption": "Table 3: Summary of LINGUA performance",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-1020/parts/0-Figure-c1.png",
        "Caption": "Figure 8: Evaluation of translation from English on in-domain test data",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1046/parts/0-Table-c3.png",
        "Caption": "Table 9 Test-set results of the best-performing models. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c3.png",
        "Caption": "Table 13 Example translations for the translation direction English to German using three different reordering constraints: MON, EG, and S3. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1083/parts/0-Table-c3.png",
        "Caption": "Figure 14 Reduplication for n = 4. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W13-2501/parts/0-Table-c3.png",
        "Caption": "Table 11 Feature ablation experiments for UR\u2192EN translation with string-to-tree features, showing the drop in BLEU when separately removing word (W ORD), cluster (C LUST), and configuration (C FG) feature sets. \u2217 = significantly worse than T GT T REE. Removing word features causes no significant difference. Removing cluster features results in a significant difference on both test sets, and removing configuration features results in a significant difference on test 2 only. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1121/parts/0-Figure-c2.png",
        "Caption": "Figure 6: MUC-7: Level Distribution of Each of the Facts",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1016/parts/0-Table-c1.png",
        "Caption": "Table 3: Results of POS Guessing of Unknown Words",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P04-1080/parts/0-Figure-c1.png",
        "Caption": "Figure 3: Changing a decision in the derivation lattice. All paths generate the observed data. The bold path rep- resents the current sample, and the dotted path represents a sidetrack in which one decision is changed. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1008/parts/0-Table-c10.png",
        "Caption": "Table 5 Error analysis of confidence measure with and without EIV tag",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pbulletin/parts/0-Table-c2.png",
        "Caption": "Table 2: Training Data Sizes for Common ESL Confused Words ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-0441/parts/0-Table-c3.png",
        "Caption": "Figure 3: Performance of TL-NE, BL and BL-A as the number of seed instances S of the target type increases. (H = 500. \u03bbT\u00b5 was set to 104 and 102 ). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PS15684_w09/parts/0-Table-c6.png",
        "Caption": "Table 1. A brief description of the tested parsers. Note that the Tune data is not the data used to train the individual parsers. Higher numbers in the right column reflect just the fact that the Test part is slightly easier to parse. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J02-2003/parts/0-Table-c7.png",
        "Caption": "Figure 3: Word type coverage by normalized frequency: words are grouped by word count / highest word count ratio: low [0, 0.01), medium [0.01, 0.1), high [0.1, 1]. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1067/parts/0-Table-c6.png",
        "Caption": "Table 4: Details of the PKU data",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0707/parts/0-Table-c7.png",
        "Caption": "Table 1. Categories of spurious relation mentions in fp1 (on a sample of 10% of relation mentions), ranked by the percentage of the examples in each category. In the sample text, red text (also marked with dotted underlines) shows head words of the first arguments and the underlined text shows head words of the second arguments. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W15-0909/parts/0-Table-c2.png",
        "Caption": "Figure 6: Are the single most probable words for a given",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D12-1074/parts/0-Table-c2.png",
        "Caption": "Table 11 Large-scale clustering on D1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2302/parts/0-Table-c2.png",
        "Caption": "Figure 2: Learning curve of BLC20 on SE3",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2214/parts/0-Table-c3.png",
        "Caption": "Figure 5: Results for choosing the correct ordered chain. (\u2265 10) means there were at least 10 pairs of ordered events in the chain. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1016/parts/0-Table-c4.png",
        "Caption": "Table 1: Summary of the previous work on coreference resolution that employs the learning algorithms, the clustering algorithms, the feature sets, and the training instance creation methods discussed in Section 3.1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-1101/parts/0-Figure-c5.png",
        "Caption": "Table 2. OOV term translation examples.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PbaneaCSL/parts/0-Figure-c3.png",
        "Caption": "Table 7 Results per concept for the ILP-Global. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Table-c2.png",
        "Caption": "Figure 1: Distribution of Class Labels in the WSJ Section of the Penn TreeBank. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1133/parts/0-Table-c5.png",
        "Caption": "Table 2: Most frequent phrase dependencies with at least 2 words in one of the phrases (dependencies in which one phrase is entirely punctuation are not shown). $ indicates the root of the tree. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1066/parts/0-Figure-c5.png",
        "Caption": "Table 4: Comparison to Related Approaches",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1670/parts/0-Table-c1.png",
        "Caption": "Table 2: A sample of manually annotated expressions from Disco-En-Gold with their numerical scores (Ns) and coarse scores (Cs). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D08-1032/parts/0-Table-c7.png",
        "Caption": "Table 3: Results of negated event/property detection on gold standard cue and scope annotation ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1033/parts/0-Table-c5.png",
        "Caption": "Figure 3: Example of chunk-based alignment",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-1037/parts/0-Table-c1.png",
        "Caption": "Table 1: The animacy data set from Talbanken05; number of noun lemmas (Types) and tokens in each class. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1110/parts/0-Table-c2.png",
        "Caption": "Figure 2 In this example, the path from the predicate ate to the argument NP He can be represented as VBjVPjS,NP, with j indicating upward movement in the parse tree and , downward movement. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1045/parts/0-Table-c5.png",
        "Caption": "Figure 2: Runtimes for sentences of length 10\u201380. The                                          graph shows the average runtimes ( ) of 10 different sample sentences of the respective length with swap op- erations restricted to a maximum swap segment size of 5 and a maximum swap distance of 2. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W12-1011/parts/0-Table-c5.png",
        "Caption": "Figure 6 Word internal structure and class-type transformation templates. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-2207/parts/0-Table-c4.png",
        "Caption": "Figure 16 FSRA* for Arabic nominative definite nouns. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N03-1027/parts/0-Table-c7.png",
        "Caption": "Table 4. Some of the top selected features by Infor-                    mation Gain ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c18.png",
        "Caption": "Table 2: The semantic roles of cases beside C-1 verb cluster ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1149/parts/0-Figure-c3.png",
        "Caption": "Figure 1. MT system combination. Each 1-best outputs are aligned to create as many Confusion Networks which are connected together to form a lattice. This lattice is then decoded with a token-pass decoder using a Language Model to produce 1-best and/or                                 n-best hypotheses. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E12-2021/parts/0-Figure-c1.png",
        "Caption": "Table 2: Recall (R), Precision (P) and Mean Average Pre- cision (MAP) when also using rules for matching. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-1070/parts/0-Table-c5.png",
        "Caption": "                                                                               \u00a9\u00b0          \u00a92\u00c1 Figure 4: WSD example showing the utility of the MVC method. A sense with a high variational coefficient is preferred to the mode     of the MM distribution (the fields corresponding to the true sense are highlighted) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2101/parts/0-Table-c3.png",
        "Caption": "Table 2: Entity type constraints.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2139/parts/0-Table-c5.png",
        "Caption": "Figure 6 The initial frequencies of character sequences. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C04-1126/parts/0-Figure-c1.png",
        "Caption": "Figure 4: Compilation of a Regular Expression into an fst that Maps between Two Regular Languages ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W15-0909/parts/0-Table-c4.png",
        "Caption": "Table 4: The fraction of verb pairs clustered together, as a function of the number of different senses between pair mem- bers (results of the NN algorithm) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Q13-1015/parts/0-Figure-c5.png",
        "Caption": "Table 1: Overview of the tasks investigated in this paper (n: size of n-gram; POS: parts of speech; Ling: linguistic knowledge; Type: type of task) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Figure-c11.png",
        "Caption": "Table 2: Accuracy scores for the CoNLL 2009 shared task test sets. Rows 1\u20132: Top performing systems in the shared CoNLL Shared Task 2009; Gesmundo et al. (2009) was placed first in the shared task; for Bohnet (2010), we include the updated scores later reported due to some improvements of the parser. Rows 3\u20134: Baseline (k = 1) and best settings for k and \u03b1 on development set. Rows 5\u20136: Wider beam (b1 = 80) and added graph features (G) and cluster features (C). Second beam parameter b2 fixed at 4 in all cases. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-1215/parts/0-Table-c3.png",
        "Caption": "Table 3 Descriptive statistics for WordNet hyp/syn relations for other-anaphora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P05-1053/parts/0-Table-c3.png",
        "Caption": "Table 3: Automatic evaluation and sentence Levenshtein scores",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N06-1027/parts/0-Table-c5.png",
        "Caption": "Table 5: Final results on CTB-5j",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1073/parts/0-Figure-c4.png",
        "Caption": "Figure 2. Growing Algorithm for Language              Model Pruning ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1059/parts/0-Figure-c3.png",
        "Caption": "Figure 3 Speed/accuracy tradeoff of the segmentor. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1089/parts/0-Table-c3.png",
        "Caption": "Table 1: A text segment from MUC-6 data set",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-3909/parts/0-Table-c3.png",
        "Caption": "Figure 2 Example of a (symmetrized) word alignment (Verbmobil task). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D11-1076/parts/0-Figure-c3.png",
        "Caption": "Table 5: Equation 1 settings",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1083/parts/0-Figure-c5.png",
        "Caption": "Figure 1: Annotated RST Tree for example (4).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S10-1019/parts/0-Table-c1.png",
        "Caption": "Figure 5: Merging left-to-right and right-to-left hypotheses (ef and eb ) in bidirectional decoding method. Figure 5(a) merge two open hypotheses, while Figure 5(b) merge them with inserted zero fer- tility words. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1063/parts/0-Table-c7.png",
        "Caption": "Figure 1: Improved syntax-based translations due to MIRA-trained weights.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D10-1042/parts/0-Figure-c3.png",
        "Caption": "Figure 14 Reduplication for n = 4. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1099/parts/0-Table-c3.png",
        "Caption": "Table 3 Unknown word model features for Arabic and French. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H01-1062/parts/0-Table-c2.png",
        "Caption": "Figure 2: Results for baseline using introspection and simple statistics of the data (including test data).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c5.png",
        "Caption": "Table 3 Training, development, and test data for word segmentation on CTB5. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PbaneaCSL/parts/0-Table-c1.png",
        "Caption": "Table 1: normalized Mutual Information values for three graphs and different iterations in %. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-0802/parts/0-Figure-c1.png",
        "Caption": "Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1011/parts/0-Figure-c6.png",
        "Caption": "Table 3 The results of setting 2 (Punctuation and other encoding information are not used; the maximum length is 10). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2104/parts/0-Figure-c2.png",
        "Caption": "Figure 5 Recall by Named Entity Class",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1045/parts/0-Table-c1.png",
        "Caption": "Figure 2: FDG Analyser\u2019s output example",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-1604/parts/0-Table-c2.png",
        "Caption": "Table 5: Official BakeOff2005 results. Keys: F - Regular Tagging only, all training data are used P1 - Regular Tagging only, 90% of training data are used P2 - Regular Tagging only, 70% of training data are used S - Regular and Correctional Tagging, Separated Mode I - Regular and Correctional Tagging, Integrated Mode ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2104/parts/0-Table-c3.png",
        "Caption": "Figure 1: Example of Lattice Used in the Markov Model-Based Method",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-4223/parts/0-Table-c3.png",
        "Caption": "Table 1: TreeTagger and RFTagger outputs. Starred word forms are modified during preprocessing.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P11-1061/parts/0-Table-c3.png",
        "Caption": "Figure 2: Example of a MUC-4 template",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4011/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Discourse tree for two sentences in RST-DT. Each of the sentences contains three EDUs. The second sentence has a well-formed discourse tree, but the first sentence does not have one. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D10-1034/parts/0-Table-c3.png",
        "Caption": "Table 2: F-measure on SIGHAN bakeoff 2. SIGHAN best: best scores SIGHAN reported on the four corpus, cited from Zhang and Clark (2007). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-3236/parts/0-Figure-c1.png",
        "Caption": "Figure 2: Sample Minipar parse and extracted gram- matical function features ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2211/parts/0-Table-c3.png",
        "Caption": "Figure 6: SMT performance results",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00-1025/parts/0-Figure-c10.png",
        "Caption": "Table 2: Synthetic Data Set from Xinhua News",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1038/parts/0-Table-c4.png",
        "Caption": "Table 3. Performance comparison on the ACE 2003/2003 data over both 5 major types (the numbers outside parentheses) and 24 subtypes (the numbers in parentheses) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P59105ca/parts/0-Figure-c7.png",
        "Caption": "Figure 2: The response of the rhyme search engine.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J06-1004/parts/0-Figure-c14.png",
        "Caption": "Figure 1: Examples of parallel phrases used in word alignment. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P02-1061/parts/0-Table-c1.png",
        "Caption": "Fig. 4. Simpli\ufb01ed Lesk algorithm [21].",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-2040/parts/0-Table-c8.png",
        "Caption": "Figure 2: This shows the shapes of BLEU and 1-slack SVM objective function for one parameter. These lines were calculated by 800 development sentences randomly selected from dev06 for development data when the hyperparameter Q is fixed 1000.0. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S12-1011/parts/0-Table-c2.png",
        "Caption": "          A u to m a tic M e tr ic s H u m a n E v a lu a tio n Figure 2: Scores based on Automatic Metrics and Human Evaluation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-2009/parts/0-Figure-c3.png",
        "Caption": "Table 1. Experimental data sets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-3004/parts/0-Table-c1.png",
        "Caption": "Table 4: F-measures for every kernel in (Khayyamian et al., 2009) and MEDLDA",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Figure-c2.png",
        "Caption": "Table 1: Filtering out objective phrases",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P05-1020/parts/0-Table-c5.png",
        "Caption": "Figure 2: Unary rule normalization. Nonterminal-yield unary chains are collapsed to single unary rules. Identity unary rules are added to spans that have no unary rule. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pstat_p00/parts/0-Figure-c3.png",
        "Caption": "Table 3: Rules of the baseline system.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c14.png",
        "Caption": "Figure 4: Improvement in (predicted mention) RE.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Table-c5.png",
        "Caption": "Table 5: Development testing evaluation.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1068/parts/0-Figure-c5.png",
        "Caption": "Table 18: Arabic idafa Construct",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Figure-c10.png",
        "Caption": "Table 10: F-measures for different systems",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H01-1001/parts/0-Table-c4.png",
        "Caption": "Table 2 shows the result of varying the number of samplers and iterations for all",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Figure-c3.png",
        "Caption": "Table 7: Diff results tested against gs-swaco",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1046/parts/0-Table-c11.png",
        "Caption": "Table 4: Confusion matrix for the MBL-classifier with a general feature space on the >10 data set on Talbanken05 nouns. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Table-c15.png",
        "Caption": "Table 5: O\ufb03cial results for the English and Basque lexical tasks (recall). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1121/parts/0-Table-c1.png",
        "Caption": "Figure 2. Incremental alignment with TERp resulting in a confusion network.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c17.png",
        "Caption": "Table 1: WMT10 system combination tuning/testing data ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-1001/parts/0-Table-c3.png",
        "Caption": "Figure 1: Aymara: utamankapxasamachiwa = \u201dit appears that they are in your house\u201d",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N09-1048/parts/0-Table-c2.png",
        "Caption": "Figure 1: Graphical model representing M L SLDA. Shaded nodes represent observations, plates denote repli- cation, and lines show probabilistic dependencies. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W03-0423/parts/0-Table-c1.png",
        "Caption": "Table 9 Initial NE recognition type-insensitive (type-sensitive) performance across various domains. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-2206/parts/0-Table-c4.png",
        "Caption": "Figure 4: Chunk-based translation model. The words in bold are head words.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-3311/parts/0-Figure-c1.png",
        "Caption": "Figure 2: Accuracy by sentence length for Method 5 measured on separate grammatical and ungrammat- ical data: Gr = Grammatical, AG = Agreement, RW = Real-Word, EW = Extra Word, MW = Missing Word ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1038/parts/0-Figure-c2.png",
        "Caption": "Table 1: Context Clustering with Spectral-based Clustering technique. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W13-2501/parts/0-Table-c4.png",
        "Caption": "Table 7. System performance on the reaction relation on the CHEM dataset. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1067/parts/0-Table-c1.png",
        "Caption": "Table 3: Effectiveness of post-processing rules",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D13-1031/parts/0-Table-c8.png",
        "Caption": "Table 3: Examples of correct and incorrect paraphrases extracted by our supervised method with their rank.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-3004/parts/0-Table-c6.png",
        "Caption": "Figure 6: Smoothed quality curves for PM method over the five corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-3008/parts/0-Figure-c2.png",
        "Caption": "Table 2: A second example of disagreement in segmentation guidelines",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1049/parts/0-Table-c5.png",
        "Caption": "Table 2: BLEU scores on the News-Commentary development test data ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0106/parts/0-Figure-c1.png",
        "Caption": "Figure 1: Dependency tree for the sentence \u201cPROT1 contains a sequence motif binds to PROT2.\u201d ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N07-1024/parts/0-Table-c11.png",
        "Caption": "Table 1: The parts of taxonomic names",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P05-1045/parts/0-Figure-c1.png",
        "Caption": "Figure 1 Architecture of the statistical translation approach based on Bayes\u2019 decision rule. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1016/parts/0-Table-c2.png",
        "Caption": "Figure 8 Quasi-synchronous tree-to-tree configurations from Smith and Eisner (2006). There are additional configurations involving NULL alignments and an \u201cother\u201d category for those that do not fit into any of the named categories. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pbulletin/parts/0-Table-c3.png",
        "Caption": "Figure 1: An example of the label consistency problem. Here we would like our model to encourage entities Albert Einstein and Einstein to get the same label, so as to improve the chance that both are labeled PERSON. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00-1025/parts/0-Figure-c6.png",
        "Caption": "Table 3: Most probable child phrases for the parent phrase \u201cmade up\u201d for each direction, sorted by the con- ditional probability of the child phrase given the parent phrase and direction. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1150/parts/0-Figure-c4.png",
        "Caption": "Figure 2: OT grammar for devoicing compiled into an FST. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1067/parts/0-Figure-c2.png",
        "Caption": "Figure 4: Training instances obtained from Verb- Net (upper) and VerbNet+SemLink (lower) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S10-1019/parts/0-Table-c3.png",
        "Caption": "Figure 7: Two possible DTs for three sentences.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P01-1017/parts/0-Table-c3.png",
        "Caption": "Table 3: Classifier combination accuracy over 5 base classifiers: NB, BR, TBL, DL, MMVC. Best perform- ing methods are shown in bold. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2211/parts/0-Table-c6.png",
        "Caption": "Figure 3: Example of Learned Name Pairs with Gloss Translations in Parentheses",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1032/parts/0-Table-c4.png",
        "Caption": "Figure 1: Examples of the semantic role features",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-2066/parts/0-Table-c2.png",
        "Caption": "Table 1: Evaluation of context-sensitive convolution tree kernels using SPT on the major relation types of the ACE RDC 2003 (inside the parentheses) and 2004 (outside the parentheses) corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1007/parts/0-Table-c8.png",
        "Caption": "Table 1: Feature space describing each candidate instance (S indicates the set of seeds for a given class).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-0813/parts/0-Table-c3.png",
        "Caption": "Figure 6: Multiple Bayesian learning runs (using averag- ing) for POS tagging. Each point represents one run; the y-axis is tagging accuracy and the x-axis is the average \u2212 log P(derivation) over all samples after burn-in. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1059/parts/0-Table-c3.png",
        "Caption": "Figure 10: MUC-5: Level Distribution of the Five Facts Combined",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1004/parts/0-Figure-c4.png",
        "Caption": "Figure 2: Performance of TL-comb and TL-auto as H changes. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1016/parts/0-Figure-c2.png",
        "Caption": "Table 18 Precision of person name recognition on the MSR test set, using Viterbi iterative training, initialized by four seed sets with different sizes. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1059/parts/0-Table-c4.png",
        "Caption": "Table 5: BLEU scores for the French-to-English translation task measured on nt10 with systems tuned on development sets selected according to their original language (adapted tuning). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N01-1011/parts/0-Figure-c1.png",
        "Caption": "Figure 1: Workflow for NIL knowledge engineering component. NILE refers to NIL expression, which is identified and annotated by human annotator. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6211/parts/0-Table-c4.png",
        "Caption": "Table 3: ROUGE-SU in k-means learning",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-1215/parts/0-Table-c7.png",
        "Caption": "Figure 4 The effect of varying the number of seeds on accuracy. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-2003/parts/0-Figure-c1.png",
        "Caption": "Table 4: Classification results with decision tree on vectors of frequency of rarest n-grams (Method 4) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c15.png",
        "Caption": "Table 13 Accuracy of semantic-role prediction for unknown boundaries (the system must identify the correct constituents as arguments and give them the correct roles). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N07-1015/parts/0-Figure-c3.png",
        "Caption": "Table 2 \u2013 Pk for Le Monde corpus",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c7.png",
        "Caption": "Figure 1: AER comparison (en\u2192cn)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/I08-2080/parts/0-Table-c3.png",
        "Caption": "Figure 2: An example packed forest representing hy- potheses in Figure 1(a). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-1098/parts/0-Table-c4.png",
        "Caption": "Table 13 Descriptive statistics for WordNet hyp/syn relations on the coreference data set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H01-1001/parts/0-Figure-c2.png",
        "Caption": "Table 4: Improvement in f-score through restoring case.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-2932/parts/0-Table-c1.png",
        "Caption": "Table 6 NER type-insensitive (type-sensitive) performance of different Chinese NE recognizers. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C02-1025/parts/0-Table-c5.png",
        "Caption": "Table 1: Decoder performance on the June 2002 TIDES MT evluation test set with multiple searches from randomized starting points (MSD=2, MSSS=5). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-2009/parts/0-Figure-c1.png",
        "Caption": "Table 2: accuracy using non-averaged and averaged perceptron.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2211/parts/0-Table-c5.png",
        "Caption": "Table 1. The F-score over the bakeoff-2 data.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J09-3004/parts/0-Table-c2.png",
        "Caption": "Figure 5: Decoding algorithm using semantic role features. Sema(c1 .role, c2 .role, t) denotes the triggered semantic role features when combining two children states, and ex- amples can be found in Figure 3. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Table-c10.png",
        "Caption": "Table 3 The results of setting 2 (Punctuation and other encoding information are not used; the maximum length is 10). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1073/parts/0-Figure-c3.png",
        "Caption": "Table 3: Feature templates for parsing, where X can be word, first and last character of word, first and last character bigram of word, POS tag. Xl+a /Xr\u2212a denotes the first/last ath X in the span, while Xl\u2212a /Xr+a denotes the ath X left/right to span. Xm is the first X of right child, and Xm\u22121 is the last X of the left child. len, lenl , lenr denote the length of the span, left child and right child respectively. wl is the length of word. ROOT/LEAF means the template can only generate the features for the root/initial span. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-1616/parts/0-Figure-c2.png",
        "Caption": "Table 16 Accuracy comparisons between various dependency parsers on English data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-2910/parts/0-Table-c2.png",
        "Caption": "Table 4. Accuracy of 5-fold cross-validation with self-              extracted semantic features ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PMTS_n09/parts/0-Table-c1.png",
        "Caption": "Table 5 Training and test conditions for the German-to-English Verbmobil corpus (*number of words without punctuation). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1038/parts/0-Table-c2.png",
        "Caption": "Table 2: Features for SVM Learning of Prediction Model ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09_qwn/parts/0-Table-c6.png",
        "Caption": "Figure 6 Dependencies in the alignment template model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6202/parts/0-Figure-c4.png",
        "Caption": "Table 1: The pool of features for all languages.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c4.png",
        "Caption": "Table 6: precision with and without feature",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J06-2001/parts/0-Table-c1.png",
        "Caption": "Table 21 Experimental results for large-scale English-to-Chinese translation ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1141/parts/0-Figure-c4.png",
        "Caption": "Table 2: Performance of WSD system using various combinations of learning algorithms and features.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N15-1071/parts/0-Figure-c2.png",
        "Caption": "Table 5: O\ufb03cial results for the English and Basque lexical tasks (recall). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1004/parts/0-Table-c3.png",
        "Caption": "Figure 4: PTB vs. Wiktionary type coverage across sec- tions of the Brown corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3325/parts/0-Figure-c1.png",
        "Caption": "Figure 5 Example output of our model for Chinese\u2192English translation. The word-segmented Chinese sentence and dependency tree are inputs. Our model\u2019s outputs include the English translation, phrase segmentations for each sentence (a box surrounds each phrase), a one-to-one alignment between the English and Chinese phrases, and a projective dependency tree on the English phrases. Note that the Chinese dependency tree is on words whereas the English dependency tree is on phrases. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0711/parts/0-Table-c11.png",
        "Caption": "Figure 6: Performance analysis of HRGs, CWU, CWW & HAC for different parameter combinations (Table 2). (A) All combinations of p1 , p2 and p3 = 0.05. (B) All combinations of p1 , p2 and p3 = 0.09. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1040/parts/0-Figure-c1.png",
        "Caption": "Figure 1: An example of the label consistency problem. Here we would like our model to encourage entities Albert Einstein and Einstein to get the same label, so as to improve the chance that both are labeled PERSON. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P01-1027/parts/0-Table-c6.png",
        "Caption": "Table 1: Tagging results for different languages. For each language we report median one-to-one (1-1), many-to-one (m-1) and V-measure (V-m) together with standard deviation from five runs where median is taken over V-measure. Types is the number of word types in each corpus, True is the number of gold tags and Induced reports the median number of tags induced by the model together with standard deviation. Best Pub. lists the best published results so far (also 1-1, m-1 and V-m) in (Christodoulopoulos et al., 2011)\u2217 , (Blunsom and Cohn, 2011)? and (Lee et al., 2010)\u2020 . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PS15684_w09/parts/0-Figure-c4.png",
        "Caption": "Table 26: Arabic Tokenization Schemes",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1110/parts/0-Table-c5.png",
        "Caption": "Figure 2: A multiword expression in HeiST",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P08-1102/parts/0-Table-c2.png",
        "Caption": "Table 1: Association frequencies for target verb.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1067/parts/0-Table-c3.png",
        "Caption": "Figure 6: Derivation with soft syntax model",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Figure-c6.png",
        "Caption": "Figure 4: Devoicing transducer compiled through a rule.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1140/parts/0-Table-c3.png",
        "Caption": "Table 4: BS on NIST task",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-0705/parts/0-Table-c8.png",
        "Caption": "Figure 3 Training with scarce resources. \u201cRestructuring,\u201d \u201clearn phrases,\u201d and \u201cannotation\u201d all require morpho-syntactic analysis of the transformed sentences. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1113/parts/0-Table-c2.png",
        "Caption": "Table 9: A* (E+) Success Rate for 12- and 14-word sentences [%].",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-0705/parts/0-Table-c7.png",
        "Caption": "Table 1: Examples of phrase \u201cmeaningfulness\u201d (Note that the comments are not presented to Turkers).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W02-1020/parts/0-Figure-c1.png",
        "Caption": "Figure 4: Three types of transitivity constraint violations.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-1045/parts/0-Table-c8.png",
        "Caption": "Figure 3: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the second order dataset ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-2036/parts/0-Table-c1.png",
        "Caption": "Table 4: BLEU scores for CWS schemes",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P02-1061/parts/0-Figure-c4.png",
        "Caption": "Figure 1: Graph of Word Senses",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J06-2001/parts/0-Table-c9.png",
        "Caption": "Table 1: Results for 4-fold site-wise cross-validation us- ing the DP corpus ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1045/parts/0-Table-c7.png",
        "Caption": "Figure 7: Two possible DTs for three sentences.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-3114/parts/0-Figure-c8.png",
        "Caption": "Fig. 8 Average number of target phrase distribution sizes for source phrases for TRIBL and IGTree com- pared to the Moses baseline ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N13-1118/parts/0-Figure-c5.png",
        "Caption": "Figure 8 The hierarchical form of a result. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N15-1071/parts/0-Table-c2.png",
        "Caption": "Figure 3: Example of Learned Name Pairs with Gloss Translations in Parentheses",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PSMPT_n09/parts/0-Table-c2.png",
        "Caption": "Figure 3: Examples for the effect of the combined lexica.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1081/parts/0-Figure-c1.png",
        "Caption": "Table 4: The System Performance Based on Each Single Feature Set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-0329/parts/0-Figure-c1.png",
        "Caption": "Table 5: F1s of some individual FN role classifiers and the overall multiclassifier accuracy (454 roles).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1117/parts/0-Figure-c1.png",
        "Caption": "Table 3: Plurality language families across 20 clusters. The columns indicate portion of lan- guages in the plurality family, number of lan- guages, and entropy over families. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-2012/parts/0-Table-c5.png",
        "Caption": "Table 4: Development Sets Results.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1053/parts/0-Table-c7.png",
        "Caption": "Figure 9: Chunk - Length and count of glue rules used decoding test set ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-0705/parts/0-Figure-c1.png",
        "Caption": "Figure 1: GC examples.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P04-1018/parts/0-Table-c4.png",
        "Caption": "Figure 1: The screenshot of our web-based system shows a simple quantitative analysis of the frequency of two terms in news articles over time. While in the 90s the term Friedensmission (peace operation) was predominant a reverse tendency can be observed since 2001 with Auslandseinsatz (foreign intervention) being now frequently used. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1670/parts/0-Table-c3.png",
        "Caption": "Table 5: Results for Positive and Negative Classes.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1727/parts/0-Table-c5.png",
        "Caption": "Table 7. Features with Set Values",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-3238/parts/0-Figure-c3.png",
        "Caption": "Table 19 The combined segmentation, POS-tagging, and dependency parsing F-scores using different pipelined systems. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0711/parts/0-Table-c10.png",
        "Caption": "Figure 3: Morphological Analysis/Generation as a Relation between Analyses and Words ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D07-1012/parts/0-Table-c5.png",
        "Caption": "Table 15 Training the MaltParser on gold tags, accuracy by gold attachment type (selected): subject, object, modification (of a verb or a noun) by a noun, modification (of a verb or a noun) by a preposition, idafa, and overall results (repeated). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1083/parts/0-Figure-c6.png",
        "Caption": "Table 8 Statistics for the test sets for German to English translation: Verbmobil Eval-2000 (Test and Develop) and Nespole! ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0711/parts/0-Table-c2.png",
        "Caption": "Table 10: Overall transliteration performance",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1080/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Outline of word segmentation process",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09_qwn/parts/0-Table-c5.png",
        "Caption": "Table 8 Models with functional features: GENDER, NUMBER, rationality (RAT). F N * = functional feature(s) based on Alkuhlani and Habash (2011); GN = GENDER + NUMBER ; GNR = GENDER + NUMBER + RAT . Statistical significance tested only for CORE 12+. . . models on predicted input, against the CORE 12 baseline. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1074/parts/0-Table-c1.png",
        "Caption": "Table 4: Final results on 25 corpora in 20 languages, with the number of induced classes equal to the number of gold standard tags in all cases. k-means and SVD2 models could not produce a clustering in the Czech CoNLL corpus due its size. Best published results are from \u2217 Christodoulopoulos et al. (2010), \u2020 Berg-Kirkpatrick et al. (2010) and \u2021 Lee et al. (2010). The latter two papers do not report VM scores. No best published results are shown for the MULTEXT languages; Christodoulopoulos et al. (2010) report results based on 45 tags suggesting that clark performs best on these corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1404/parts/0-Figure-c2.png",
        "Caption": "Table 3: Examples of correct and incorrect paraphrases extracted by our supervised method with their rank.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2605/parts/0-Table-c2.png",
        "Caption": "Table 1: Confidence scores for diese in ex. (1)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-2124/parts/0-Figure-c2.png",
        "Caption": "Table 3 Polysemous word types in the Senseval-2 and -3 English all-words tasks test documents with no data in SemCor (0 columns), or with very little data (\u2264 1 and \u2264 5 occurrences). Note that there are no annotations for adverbs in the Senseval-3 documents. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-1006/parts/0-Table-c3.png",
        "Caption": "Table 3: Results for two kinds of headlines",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c13.png",
        "Caption": "Table 5: Number of affected words by OOV- preprocessing ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1009/parts/0-Table-c8.png",
        "Caption": "Figure 1: Framework for MWE acquisition from corpora",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1704/parts/0-Figure-c1.png",
        "Caption": "Figure 4: Precision of acquired relations (material). L and S denote lenient and strict evaluation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PEAAI_n09/parts/0-Figure-c1.png",
        "Caption": "Figure 4: Parameter setup for \u03bb, \u03b8, and \u03b4",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E12-1020/parts/0-Table-c2.png",
        "Caption": "Figure 4: A RTG integrating the attachment constraint for Contrast from Ex. 2 into Fig. 3",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P00-1022/parts/0-Figure-c1.png",
        "Caption": "Table 4: Bagging with 50 gold seed sets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S13-2038/parts/0-Figure-c1.png",
        "Caption": "Figure 8: The actual output of our parser trained with a fully annotated treebank. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1048/parts/0-Table-c2.png",
        "Caption": "Figure 6 The influence of beam-sizes, and the convergence of the perceptron for the joint segmentor and POS -tagger. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1159/parts/0-Table-c5.png",
        "Caption": "Table 1: Results of different systems on the CoNLL\u201912 English data sets.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-2910/parts/0-Table-c3.png",
        "Caption": "Figure 1: A pair of comparable, non-parallel documents",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0707/parts/0-Table-c3.png",
        "Caption": "Table 1: Morph Examples and Motivations.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1089/parts/0-Table-c3.png",
        "Caption": "Table 2: Example confounders for \u201cfestival\u201d and \u201claws\u201d and their similarities ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P11-1060/parts/0-Table-c1.png",
        "Caption": "Table 4. Accuracy of 5-fold cross-validation with self-              extracted semantic features ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-1001/parts/0-Table-c2.png",
        "Caption": "Figure 1. Relation Feature Spaces of the Example Sentence \u201c\u2026\u2026 to stop the merger of an estimated",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N09-1057/parts/0-Table-c2.png",
        "Caption": "Table 4: Accuracy of all slots on the TST3 and TST4 test set ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Table-c10.png",
        "Caption": "Figure 2: Architecture of NILER system.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-1004/parts/0-Figure-c2.png",
        "Caption": "Figure 4. Step-by-step Growing Algorithm",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-1115/parts/0-Table-c5.png",
        "Caption": "Figure 1: The value of the penalized loss based on the number of iterations: DPLVMs vs. CRFs on the MSR data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D11-1076/parts/0-Figure-c6.png",
        "Caption": "Table 4: Translation results for French-English",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Table-c3.png",
        "Caption": "Table 2. Performance of Word Alignment.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W10-4128/parts/0-Table-c3.png",
        "Caption": "Table 3: Mutual information between feature subset and class label with \u03c72 based feature ranking. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N09-1057/parts/0-Figure-c1.png",
        "Caption": "Table 5: Precision on the covered RTE data",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W12-4006/parts/0-Table-c2.png",
        "Caption": "Figure 1: Rule expansion with minimal context (Example 3)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W10-3708/parts/0-Figure-c1.png",
        "Caption": "Figure 5: MTO is not sensitive to the number of random substitutes sampled per word token. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1011/parts/0-Figure-c1.png",
        "Caption": "Figure 1. Corpus Excerpt with Dialogue Act Annotation",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S10-1090/parts/0-Table-c1.png",
        "Caption": "Table 2: A sample of manually annotated expressions from Disco-En-Gold with their numerical scores (Ns) and coarse scores (Cs). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6202/parts/0-Figure-c1.png",
        "Caption": "Table 12 Overview of the results for all baselines for coreference. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-1101/parts/0-Table-c2.png",
        "Caption": "Figure 2: Learning curves on the development dataset of the HK City Univ. corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1068/parts/0-Table-c1.png",
        "Caption": "Figure 1: An Initial Learning Curve for Confusable                   Disambiguation ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1147/parts/0-Figure-c1.png",
        "Caption": "Table 3: The 10 best languages for the verb component of BANNARD using LCS. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D07-1012/parts/0-Figure-c2.png",
        "Caption": "Table 14 Training and test conditions for the Hansards task (*number of words without punctuation). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-4009/parts/0-Table-c3.png",
        "Caption": "Table 4: Average Precision (P) and Yield (Y) at the rule and template levels. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1141/parts/0-Figure-c3.png",
        "Caption": "Table 5. SA classification results",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J10-3003/parts/0-Figure-c1.png",
        "Caption": "Figure 5 Overall architecture of MSRSeg. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2167/parts/0-Table-c6.png",
        "Caption": "Figure 5: Derivation with Hierarchical model",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c12.png",
        "Caption": "Table 4: Filtering results using the naive Bayes classifier. The number of entity candidates for the training set was 4179662, and that of the develop- ment set was 418628. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1039/parts/0-Figure-c3.png",
        "Caption": "Figure 7: An Employment Chain. Dotted lines indicate incorrect before relations. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-2009/parts/0-Figure-c5.png",
        "Caption": "Table 6: English\u2013German results in % BLEU",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1630/parts/0-Table-c3.png",
        "Caption": "Table 2. CNN news summaries segmented using different block sizes. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Figure-c1.png",
        "Caption": "Table 2: The average number of source text sen- tences needed to cover a summary sentence. The model average is statistically significantly differ- ent from all the other conditions p < 10\u22127 (Study 1). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6202/parts/0-Figure-c3.png",
        "Caption": "Table 1: Accuracy in Lexical Sample Tasks",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Table-c6.png",
        "Caption": "Table 6: POS tagging with deterministic constraints. The maximum in each column is bold. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W08-0602/parts/0-Table-c2.png",
        "Caption": "Table 1: Example of topic-to-topic correspondence. The last line shows the correspondence probability. Each col- umn means a topic represented by its top-10 topical word- s. The first column is a target-side topic, while the rest three columns are source-side topics. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1008/parts/0-Table-c5.png",
        "Caption": "Table 4: Percentage of non-projective arcs recovered correctly (number of labels in parentheses)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c3.png",
        "Caption": "Table 4: Rhetorical pattern of C-Colon",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2139/parts/0-Figure-c2.png",
        "Caption": "Figure 3: Size (in words) of reorderings (%) ob- served in training bi-texts. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-0329/parts/0-Table-c2.png",
        "Caption": "Table 3: Performance of the knowledge-based ap- proach using the JiangConrath semantic relatedness measure. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4904/parts/0-Table-c4.png",
        "Caption": "Fig. 1 Examples of ambiguity for the English word play, together with different translations depending on the context ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W10-4135/parts/0-Table-c3.png",
        "Caption": "Table 1: Notation and signatures for our framework.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1125/parts/0-Table-c5.png",
        "Caption": "Table 5 Gross corpus statistics for the pre-processed corpora used to train and evaluate our models. We compare to the WSJ section of the PTB: train (Sections 02\u201321); dev. (Section 22); test (Section 23). Due to its flat annotation style, the FTB sentences have fewer constituents per sentence. In the ATB, morphological variation accounts for the high proportion of word types to sentences. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Figure-c1.png",
        "Caption": "Figure 7: Tag errors broken down by the word type clas- sified into the six classes: oov, identical, superset, subset, overlap, disjoint (see text for detail). The largest source of error across languages are out-of-vocabulary (oov) word types, followed by tag set mismatch types: subset, over- lap, disjoint. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c7.png",
        "Caption": "Table 14 Top four worst-case statistics of features for NE boundary errors. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4917/parts/0-Figure-c2.png",
        "Caption": "Figure 4: Graph for a neo-Davidsonian structure.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c24.png",
        "Caption": "Figure 3: Training time comparison. The training time for each model is calculated from scratch. For example, the training time of IBM Model 4 includes the training time of IBM Model 1, the HMM, and IBM Model 3. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-1215/parts/0-Table-c5.png",
        "Caption": "Figure 2: Probability that a prediction will be ac- cepted versus its gain. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P08-2003/parts/0-Figure-c1.png",
        "Caption": "Figure 4: string insertion operation for right-to-left decoding method. A string e0 was prepended before the partial output string, e, and the first word in e 0 was aligned from f j . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pstat_p00/parts/0-Figure-c4.png",
        "Caption": "Figure 3: Comparing F-measure, precision, and recall of different voting schemes for Arabic relation ex- traction. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-0705/parts/0-Table-c6.png",
        "Caption": "Figure 1: Lattice representation of the sentence bclm hneim. Double-circles denote token boundaries. Lattice arcs correspond to different segments of the token, each lattice path encodes a possible reading of the sentence. Notice how the token bclm have analyses which include segments which are not directly present in the unsegmented form, such as the definite article h (1-3) and the pronominal suffix which is expanded to the sequence fl hm (\u201cof them\u201d, 2-4, 4-5). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P04470/parts/0-Table-c10.png",
        "Caption": "Table 5: Selected entries from the confusion matrix for parts of speech in German with F-scores for the left-hand- side category. ADJ* (ADJD or ADJA) = adjective; ADV = adverb; ART = determiner; APPR = preposition; NE = proper noun; NN = common noun; PRELS = relative pronoun; VVFIN = finite verb; VVINF = non-finite verb; VAFIN = finite auxiliary verb; VAINF = non-finite auxil- iary verb; VVPP = participle; XY = not a word. We use \u03b1* to denote the set of categories with \u03b1 as a prefix. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-2050/parts/0-Table-c1.png",
        "Caption": "Table 4: Coverage/precision with various rule collections",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P101167/parts/0-Figure-c8.png",
        "Caption": "Table 3. Number of candidates for each target                 language. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1011/parts/0-Figure-c5.png",
        "Caption": "Table 2: Performance on Bilingual Lexicon Extraction",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S0885/parts/0-Table-c1.png",
        "Caption": "Table 2. Accuracy of various instantiations of the system",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J09-3004/parts/0-Table-c1.png",
        "Caption": "Table 5: Results on the MUC6 formal test set.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N06-1043/parts/0-Figure-c1.png",
        "Caption": "Table 1. A brief description of the tested parsers. Note that the Tune data is not the data used to train the individual parsers. Higher numbers in the right column reflect just the fact that the Test part is slightly easier to parse. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1083/parts/0-Figure-c7.png",
        "Caption": "Figure 3: The BAD application before entering a verse, showing two possible rhyme patterns.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4917/parts/0-Table-c5.png",
        "Caption": "Figure 2 Examples of dependency trees with word alignment. Arrows are drawn from children to parents. A child word is a modifier of its parent. Each word has exactly one parent and $ is a special \u201cwall\u201d symbol that serves as the parent of all root words in the tree (i.e., those with no other parent). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J02-2003/parts/0-Table-c4.png",
        "Caption": "Table 3: Op. Target - Op. Word Pair Extraction",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-2206/parts/0-Table-c3.png",
        "Caption": "Table 2: The Division of LDC annotated data into training and development test sets. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3325/parts/0-Table-c4.png",
        "Caption": "Figure 2: A noun-phrase with sub-structure",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-2910/parts/0-Table-c4.png",
        "Caption": "Table 3: Disambiguation scores on nine confusable set, attained by confusable experts trained on ex- amples extracted from 1 billion words of text from TRAIN - REUTERS plus TRAIN - NYT, on the three test sets. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D11-1095/parts/0-Table-c2.png",
        "Caption": "Table 1: Properties of the manually aligned corpus",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2167/parts/0-Table-c5.png",
        "Caption": "Fig. 2 Example of CCG supertags. CCG supertags are combined under the operations of forward and backward applications into a parse tree ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W13-2512/parts/0-Table-c3.png",
        "Caption": "Table 3: Accuracy for Different Part-Of-Speech",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-1045/parts/0-Table-c9.png",
        "Caption": "Table 4: Validation Features for Crosslingual Slot Filling",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0401/parts/0-Table-c1.png",
        "Caption": "Figure 1. Overview of the WEBRE algorithm (Illustrated with examples sampled from experiment results). The tables and rec- tangles with a database sign show knowledge sources, shaded rectangles show the 2 phases, and the dotted shapes show the sys- tem output, a set of Type A relations and a set of Type B relations. The orange arrows denote resources used in phase 1 and the green arrows show the resources used in phase 2. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1125/parts/0-Figure-c3.png",
        "Caption": "Table 1: Texts used for the evaluation",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P04-1080/parts/0-Table-c2.png",
        "Caption": "Table 2: the largest clusters from partitioning the second order graph with CW. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1005/parts/0-Table-c6.png",
        "Caption": "Table 3: Official Bakeoff Outcome",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1068/parts/0-Table-c7.png",
        "Caption": "Table 1 Distribution of antecedent NP types in the other-anaphora data set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PMTS_n09/parts/0-Table-c3.png",
        "Caption": "Figure 12: The Malay fst After the Application of Compile-Replace to the Lower-Side Language",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-3004/parts/0-Table-c5.png",
        "Caption": "Figure 2: Rhetorical Function of Exclamation    Mark in Chinese and German corpora ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1127/parts/0-Figure-c4.png",
        "Caption": "Table 1: An example of NE and non-NE",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6202/parts/0-Table-c2.png",
        "Caption": "Figure 6 Distribution of probabilities given by the classi\ufb01er over all node pairs of the test-set graphs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Table-c7.png",
        "Caption": "Table 1 Semi-fixed MWEs in French and English. The French adverb \u00e0 terme (\u2018in the end\u2019) can be modified by a small set of adjectives, and in turn some of these adjectives can be modified by an adverb such as tr\u00e8s (\u2018very\u2019). Similar restrictions appear in English. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1761/parts/0-Table-c1.png",
        "Caption": "Figure 3: Morphological Analysis/Generation as a Relation between Analyses and Words ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1005/parts/0-Table-c4.png",
        "Caption": "Table 3: Reachability of 1000 training sentences: can they be translated with the model? ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1083/parts/0-Table-c1.png",
        "Caption": "Figure 1. Inter-annotator agreement of ACE 2005 relation annotation. Numbers are the distinct relation mentions whose both arguments are in the list of adjudicated entity mentions. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1053/parts/0-Table-c2.png",
        "Caption": "Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep (\u2021 : p < 0.01). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1025/parts/0-Figure-c1.png",
        "Caption": "Figure 3: Boundary information is added to states to cal- culate the bracket scores in the face of word segmentation errors. Left: the original parse tree, Right: the converted parse tree. The numbers in the brackets are the indices of the character boundaries based on word segmentation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J10-3003/parts/0-Figure-c2.png",
        "Caption": "Table 1: IBM Model 3",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1127/parts/0-Figure-c3.png",
        "Caption": "Figure 1: Average decoding time",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1025/parts/0-Table-c5.png",
        "Caption": "Table 2: Frequency of Major Relation SubTypes in the ACE training and devtest corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1062/parts/0-Table-c1.png",
        "Caption": "Table 7 Effect of pruning parameter Np and heuristic function on error rate for direct-translation model (tp = 10\u221212 ). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P08-1078/parts/0-Table-c3.png",
        "Caption": "Figure 9: Wikipedia topics (T=400).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Figure-c5.png",
        "Caption": "Table 1. Accuracy of our system in each period (M = 10) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P08-2023/parts/0-Table-c2.png",
        "Caption": "Table 1: Gibbs sampling for Bayesian inference outperforms both EM and Variational Bayesian EM. \u2217 The output of EM alignment was used as the gold standard. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S12-1040/parts/0-Table-c1.png",
        "Caption": "Table 8 Model accuracy using equal distribution of verb frequencies for the estimation of P(c). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1110/parts/0-Figure-c1.png",
        "Caption": "Table 2 Overview of the results for all baselines for other-anaphora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1008/parts/0-Table-c3.png",
        "Caption": "Figure 3 Back-off lattice with more specific distributions towards the top. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c17.png",
        "Caption": "Figure 6: Derivation with soft syntax model",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/I05-3013/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Stages of the proposed method.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2230/parts/0-Table-c7.png",
        "Caption": "Table 7: Percentage of overlapping relations between KnowNet versions",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1119/parts/0-Table-c3.png",
        "Caption": "Table 3: Results of MET2 under different configurations",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1016/parts/0-Table-c2.png",
        "Caption": "Figure 1: Bilingual training size vs. BLEU score (mid- dle line, left axis) and phrase table composition (top line, right axis) on Arabic Development Set. The baseline BLEU score (bottom line) is included for comparison. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N12-1045/parts/0-Figure-c1.png",
        "Caption": "Table 2: Discourse-new prediction results by Bean and Riloff ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W10-4128/parts/0-Table-c2.png",
        "Caption": "Table 3: Feature counts for Ling and Seed feature sets.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-3031/parts/0-Table-c3.png",
        "Caption": "Figure 3: Improvements in F-measure on MUC-6 plotted against amount of selected unlabeled data used ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Table-c13.png",
        "Caption": "Table 2: Accuracy scores for the CoNLL 2009 shared task test sets. Rows 1\u20132: Top performing systems in the shared CoNLL Shared Task 2009; Gesmundo et al. (2009) was placed first in the shared task; for Bohnet (2010), we include the updated scores later reported due to some improvements of the parser. Rows 3\u20134: Baseline (k = 1) and best settings for k and \u03b1 on development set. Rows 5\u20136: Wider beam (b1 = 80) and added graph features (G) and cluster features (C). Second beam parameter b2 fixed at 4 in all cases. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1040/parts/0-Table-c3.png",
        "Caption": "Figure 2: Layers used in our model.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-1004/parts/0-Table-c10.png",
        "Caption": "Table 5: Performance comparison on the ACE 2004 data over the 7 relation types. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C00-1081/parts/0-Figure-c3.png",
        "Caption": "Table 2 performance each step of our system achieves",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1038/parts/0-Figure-c1.png",
        "Caption": "Table 4. Pairwise precision/recall/F1 of WEBRE and SNE.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1032/parts/0-Table-c6.png",
        "Caption": "Table 7: The effect of language and gender in-",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S12-1023/parts/0-Table-c5.png",
        "Caption": "Table 8 Statistics for the test sets for German to English translation: Verbmobil Eval-2000 (Test and Develop) and Nespole! ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E12-1020/parts/0-Table-c4.png",
        "Caption": "Table 4: The System Performance Based on Each Single Feature Set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-1025/parts/0-Table-c4.png",
        "Caption": "Table 3: Mutual information between feature subset and class label with \u03c72 based feature ranking. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J02-1001/parts/0-Figure-c1.png",
        "Caption": "Figure 4: 10 most used verbs (lemma) in indirect speech. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-2003/parts/0-Table-c4.png",
        "Caption": "Table 5: Type-level English POS Tag Ranking: We list the top 5 and bottom 5 POS tags in the lexicon and the predictions of our models under the best hyperparameter setting. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D08-1032/parts/0-Table-c10.png",
        "Caption": "Table 1: Results of different systems on the CoNLL\u201912 English data sets.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W10-4128/parts/0-Table-c1.png",
        "Caption": "Figure 5: DRS and corresponding DRG (in tuples and in graph format) for \u201cA customer did not pay.\u201d",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c17.png",
        "Caption": "                                   3 .  (7     $    %  19: 6       7( Figure 2 Error analysis example. . . . )  82                                 mrt \u00c2yAm \u03c2 l\u00fd A\u030cxtfA\u2019 Alzmyl Almhnd . . . (\u2018Several days have passed since the disappearance of the colleague the engineer . . . \u2019), as parsed by the baseline system using only CORE 12 (left) and as using the best performing model (right). Bad predictions are marked with <<< . . . >>>. The words in the tree are presented in the Arabic reading direction (from right to left). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2139/parts/0-Table-c5.png",
        "Caption": "Figure 1: Example of a prediction for English to French translation. s is the source sentence, h is the part of its translation that has already been typed, x\u2217 is what the translator wants to type, and x is the prediction. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-3402/parts/0-Table-c3.png",
        "Caption": "Figure 5: Examples of viterbi chunking and chunk alignment for English-to-Japanese translation model. Chunks are bracketed and the words with \u2217 to the left are head words. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P1018/parts/0-Table-c7.png",
        "Caption": "Figure 3: Discourse parsing framework.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D10-1100/parts/0-Table-c2.png",
        "Caption": "Fig. 1. Structure and data preprocessing of the initial dataset and the cleaned one after preprocessing.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1023/parts/0-Figure-c2.png",
        "Caption": "Table 4: Development results for POS tagging. Given are training times in minutes (TT) and accuracies (ACC). Best baseline results are underlined and the overall best results bold. * indicates a significant difference (positive or negative) between the best baseline and a PCRF model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1115/parts/0-Figure-c1.png",
        "Caption": "Table 3. Pseudo-code to extract UW",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Table-c9.png",
        "Caption": "Table 3 Precision of existing and proposed approaches. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1044/parts/0-Table-c7.png",
        "Caption": "Table 12: Results of the corpus-based model on words with different frequency ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1046/parts/0-Table-c10.png",
        "Caption": "Table 2: Categories of multi-character words that      are considered \u2018strings without internal  structures\u2019 (see Section 4.1). Each category is  illustrated with one example from our corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1063/parts/0-Table-c1.png",
        "Caption": "Table 4: Improvement in f-score through restoring case.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1008/parts/0-Table-c6.png",
        "Caption": "Figure 1: Syntagmatic vs. paradigmatic axes for words in a simple sentence (Chandler, 2007). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1045/parts/0-Table-c4.png",
        "Caption": "Table 2 \u2013 Pk for Le Monde corpus",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1670/parts/0-Table-c2.png",
        "Caption": "Table 8: Top ranked sentences using baseline system on the question \u201cWhat caused the Kursk to sink?\u201d. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1066/parts/0-Table-c1.png",
        "Caption": "Fig. 3 Example of LTAG supertags. LTAG supertags are combined under the operations of substitution and adjunction into a parse tree ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2164/parts/0-Figure-c3.png",
        "Caption": "Table 3: Precision statistics for pronouns. Rows are pronoun surfaces, columns number of cluster- ing decisions and percentage of wrong decisions for all and only anaphoric pronouns respectively. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1125/parts/0-Figure-c3.png",
        "Caption": "Figure 4: General Knowledge Sources",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-1616/parts/0-Figure-c1.png",
        "Caption": "Table 5 Some words extracted from the large corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2104/parts/0-Figure-c1.png",
        "Caption": "Table 8. Number of evaluated English Name",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W10-3708/parts/0-Table-c3.png",
        "Caption": "Figure 4: Smoothed histograms of the Jensen-Shannon",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N04-1004/parts/0-Figure-c1.png",
        "Caption": "Fig. 1 GETARUNS AR algorithm",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N09-1007/parts/0-Table-c3.png",
        "Caption": "Table 3: Accuracies for models with and without oracle pruning. * indicates models significantly worse than the oracle model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-4009/parts/0-Table-c5.png",
        "Caption": "Table 5: POS annotations of a couplet, i.e., a pair of two verses, in a classical Chinese poem. See     Table 1 for the meaning of the POS tags. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N06-1027/parts/0-Table-c2.png",
        "Caption": "Figure 5: MEDLDA Fmeausres for 3 feature conditions",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-1006/parts/0-Figure-c2.png",
        "Caption": "Table 2: Statistics of three test sets.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Q13-1015/parts/0-Figure-c4.png",
        "Caption": "Table 8: Translation quality.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1007/parts/0-Figure-c2.png",
        "Caption": "Figure 2: Integrating Brown cluster information",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J06-1004/parts/0-Figure-c16.png",
        "Caption": "Table 1: Manual analysis of suggested corrections on CLC data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-0703/parts/0-Figure-c5.png",
        "Caption": "Table 19 Precision of location name recognition on the MSR test set, using Viterbi iterative training, initialized by four seed sets with different sizes. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Table-c10.png",
        "Caption": "Figure 1: Improved syntax-based translations due to MIRA-trained weights.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2147/parts/0-Table-c3.png",
        "Caption": "Figure 1: The greedy binding problem. (a) The correct binding, (b) the greedy binding, (c) the result.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1062/parts/0-Table-c2.png",
        "Caption": "Table 2: System Pairwise Agreement",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PTP_n09/parts/0-Table-c3.png",
        "Caption": "Table 1: Summary of the previous work on coreference resolution that employs the learning algorithms, the clustering algorithms, the feature sets, and the training instance creation methods discussed in Section 3.1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Table-c18.png",
        "Caption": "Figure 8: Contribution of feature sets (material).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P04-1080/parts/0-Figure-c2.png",
        "Caption": "Table 1: Counts of matches between MUC and Soderland data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N10-1040/parts/0-Figure-c1.png",
        "Caption": "Figure 2: Semantic drift in CELL (n=20, m=20)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S13-1002/parts/0-Figure-c1.png",
        "Caption": "Table 5: System for the task of relation classifica- tion. The two classes are INR and COG, and we evaluate using accuracy (Acc.). The proportion of INR relations in training and test set is 49.7% and 49.63% respectively. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1016/parts/0-Table-c1.png",
        "Caption": "Table 9: Comparison of our approach with using only the Gigaword corpus",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N12-1045/parts/0-Table-c1.png",
        "Caption": "Figure 3: Macro-accuracy for cross-lingual bootstrapping",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N03-1010/parts/0-Figure-c1.png",
        "Caption": "Table 2. MAP of different IR systems with differ- ent segmenters. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1104/parts/0-Figure-c1.png",
        "Caption": "Figure 1: Example of LMR Tagging.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-1025/parts/0-Table-c3.png",
        "Caption": "Fig. 1. Fuzzy hierarchical clustering for Paraphrase Extraction.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1023/parts/0-Table-c3.png",
        "Caption": "Table 2: Particles and prepositions allowed in phrasal verbs gathered from Wiktionary. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J06-1004/parts/0-Figure-c12.png",
        "Caption": "Table 4: Improvement in f-score through restoring case.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2052/parts/0-Table-c2.png",
        "Caption": "Figure 5 Example of segmentation of German sentence and its English translation into alignment templates. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1003/parts/0-Table-c4.png",
        "Caption": "Table 2: Intercoder agreement for activities: The meeting dialogues and Santa Barbara corpus have been annotated by a semi-naive coder and the first author of the paper. The \u03ba-coefficient is determined as in Carletta et al. (1997) and mutual information measures how much one label \u201cinforms\u201d the other (see Sec. 3). For CallHome Spanish 3 dialogues were coded for activities by two coders and the result seems to indicate that the task was easier. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Table-c11.png",
        "Caption": "Table 3: 2 billion word corpus statistics",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1016/parts/0-Table-c8.png",
        "Caption": "Table 4: Variation in performance by feature set. Features sets are abbreviated as in Table 3. For the first seven columns, features were added cumulatively to each other. The next two columns, allgen and notok, are as de- scribed in Table 3. The final two columns give inter annotator agreement and corrected inter annotator agreement, for comparison. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pling_p07/parts/0-Figure-c1.png",
        "Caption": "Figure 2: Overview of the complete processing chain. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1006/parts/0-Figure-c2.png",
        "Caption": "Table 1: De\ufb01nition of NE in IREX.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-3114/parts/0-Figure-c11.png",
        "Caption": "Figure 2: F1 scores (in %) of SegTagDep on CTB- 5c-1 w.r.t. the training epoch (x-axis) and parsing feature weights (in legend). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1027/parts/0-Table-c2.png",
        "Caption": "Figure 2: Learning curves using different sam- pling strategies. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D07-1018/parts/0-Table-c7.png",
        "Caption": "Table 3: Tagging accuracy on the gold-standard normalizations (OrigP = original punctuation, ModP = modern punctuation, NoP = no punctu- ation) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c4.png",
        "Caption": "Table 2: Combination results (using SVMacc)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W11-2123/parts/0-Table-c3.png",
        "Caption": "Figure 16: Correlation between manual and automatic scores for English-German",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1048/parts/0-Figure-c3.png",
        "Caption": "Table 8. Comparisons among different strategies on Medstract",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2211/parts/0-Table-c7.png",
        "Caption": "Table 2: Analysis of the number of relevant documents out of the top 10 and the total number of retrieved documents (up to 100) for a sample of queries. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J06-1004/parts/0-Figure-c4.png",
        "Caption": "Fig. 1 GETARUNS AR algorithm",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1011/parts/0-Table-c1.png",
        "Caption": "Figure 4: A chain-structured DCRF as our intra- sentential parsing model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/M98-1003/parts/0-Figure-c5.png",
        "Caption": "Table 5 NEA type-insensitive (type-sensitive) performance with the same Chinese NE recognizer (Wu\u2019s system) and different English NE recognizers. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-1025/parts/0-Figure-c2.png",
        "Caption": "Table 2: F-score on development data",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1005/parts/0-Table-c1.png",
        "Caption": "Figure 1: Syntactical Variations of \u201cactivate\u201d",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1082/parts/0-Table-c2.png",
        "Caption": "Figure 1: Wrong assignment due to missing sense: from the Hound of the Baskervilles, Ch. 14 ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-5011/parts/0-Figure-c2.png",
        "Caption": "Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing. For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1050/parts/0-Figure-c3.png",
        "Caption": "Figure 1: Example MERT values along one coordi- nate, first unregularized. When regularized with `2 , the piecewise constant function becomes piecewise quadratic. When using `0 , the function remains piecewise constant with a point discontinuity at 0. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C04-1131/parts/0-Table-c7.png",
        "Caption": "Table 6. System performance on the succession relation on the TREC-9 dataset. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1032/parts/0-Table-c1.png",
        "Caption": "Table 3: POS Tagging of known words using con- textual features (accuracy in percent). one-vs-all denotes training where example ` serves as positive example to the true tag and as negative example to all the other tags. SM| \u00a8R\u00a9 denotes training where                            2  example ` serves as positive example to the true tag ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J09-3004/parts/0-Table-c9.png",
        "Caption": "Figure 2: Plate diagram of the extended model with T kinds of token-level features (f (t) variables) and a single kind of type-level feature (morphology, m). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P05-1013/parts/0-Table-c3.png",
        "Caption": "Figure 2: Sequence of POS-tagged units used to estimate the bilingual n-gram LM. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S10-1090/parts/0-Table-c3.png",
        "Caption": "Table 10: An example antecedent of a nominal in- teraction keyword ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-3114/parts/0-Figure-c5.png",
        "Caption": "Table 16. De\ufb01nition of Lingala Verbal Morphology",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D08-1032/parts/0-Table-c6.png",
        "Caption": "Figure 1: Proposed discourse structures for Ex. 4: (a) In terms of informational relations; (b) in terms of inten- tional relations ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D12-1003/parts/0-Table-c4.png",
        "Caption": "Table 5. Impact of Data Selection (Chinese)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2139/parts/0-Figure-c3.png",
        "Caption": "Table 1: Overview of the ACE 2005 data.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D10-1042/parts/0-Table-c1.png",
        "Caption": "Table 2: Sample I(s) Values",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C00-2123/parts/0-Figure-c1.png",
        "Caption": "Table 6: Translation results for English-French",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-1025/parts/0-Figure-c1.png",
        "Caption": "Table 2: Statistics of training, development and test data for NIST task. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-1104/parts/0-Figure-c1.png",
        "Caption": "Table 4: Upper bound for combination. The error reduction (ER) rate is a comparison between the F-score produced by the oracle combination sys- tem and the character-based system (see Tab. 1). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1082/parts/0-Table-c1.png",
        "Caption": "Table 8 Thesaurus coverage of polysemous words (excluding multiwords) in WordNet 1.6. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D07-1018/parts/0-Table-c4.png",
        "Caption": "Table 4: Example translations from the different methods. Boldface indicates correct translations. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-2206/parts/0-Table-c1.png",
        "Caption": "Figure 6: A CRF as a multi-sentential parsing model.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1133/parts/0-Figure-c3.png",
        "Caption": "Figure 4 Examples of alignment templates obtained in training. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1127/parts/0-Figure-c2.png",
        "Caption": "Table 4: Final results on 25 corpora in 20 languages, with the number of induced classes equal to the number of gold standard tags in all cases. k-means and SVD2 models could not produce a clustering in the Czech CoNLL corpus due its size. Best published results are from \u2217 Christodoulopoulos et al. (2010), \u2020 Berg-Kirkpatrick et al. (2010) and \u2021 Lee et al. (2010). The latter two papers do not report VM scores. No best published results are shown for the MULTEXT languages; Christodoulopoulos et al. (2010) report results based on 45 tags suggesting that clark performs best on these corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1058/parts/0-Table-c1.png",
        "Caption": "Table 4: Results of the gloss classifier.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pproc2014_n09/parts/0-Table-c6.png",
        "Caption": "Table 1: Basic Travel Expression Corpus",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6211/parts/0-Table-c1.png",
        "Caption": "Fig. 2. Multilingual bootstrapping.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/M98-1003/parts/0-Figure-c3.png",
        "Caption": "Table 3: Sizes of rule application test set for each learned rule-set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-3031/parts/0-Table-c1.png",
        "Caption": "Table 5: Comparison of accuracy scores across linguistic levels.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1011/parts/0-Figure-c3.png",
        "Caption": "Table 9 Select models trained using the Easy-First Parser. Statistical significance tested only for CORE 12. . . models on predicted input: significance of the Easy-First Parser CORE 12 baseline model against its MaltParser counterpart; and significance of all other CORE 12+. . . models against the Easy-First Parser CORE 12 baseline model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1073/parts/0-Table-c3.png",
        "Caption": "Table 4: Sentence error rates of end-to-end evalua- tion (speech recognizer with WER=25%; corpus of 5069 and 4136 dialogue turns for translation Ger- man to English and English to German, respec- tively). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0711/parts/0-Table-c1.png",
        "Caption": "Table 17 Training the Easy-First Parser on gold and predicted tags, accuracy by gold attachment type (selected): subject, object, modification (of a verb or a noun) by a noun, modification (of a verb or a noun) by a preposition, idafa, and overall results (repeated). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1109/parts/0-Figure-c1.png",
        "Caption": "Table 4. Gold standard length distribution.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2167/parts/0-Table-c7.png",
        "Caption": "Table 2: An example of words and their bit string representations obtained in this paper. Words in bold are head words that appeared in Table 1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pbulletin/parts/0-Table-c4.png",
        "Caption": "Table 11: Effects of Popularity of Morphs",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/D09-1092/parts/0-Figure-c9.png",
        "Caption": "Table 3: Clustering evaluation for the experiment with Named Entities ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c22.png",
        "Caption": "Figure 3: Discovered metaphorical associations",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-0304/parts/0-Table-c4.png",
        "Caption": "Figure 3: A decoding trace using improvement caching and tiling (ICT). The search in the second and later iterations is limited to areas where a change has been applied (marked in bold print) \u2014 note that the number of alignment checked goes down over time. The higher number of alignments checked in the second iteration is due to the insertion of an additional word, which increases the number of possible swap and insertion operations. Decoding without ICT results in the same translation but requires 11 iterations and checks a total of 17701 alignments as opposed to 5 iterations with a total of 4464 alignments with caching. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N13-1118/parts/0-Figure-c3.png",
        "Caption": "Table 14: An example result of BioAR",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P01-1027/parts/0-Table-c9.png",
        "Caption": "Table 1: Size of Seed Lexicons",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P05-1004/parts/0-Table-c5.png",
        "Caption": "Figure 2: Extraction of Raw Pattern",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1408/parts/0-Table-c7.png",
        "Caption": "Table 2: The number of vocabularies in the 10k, 50k and 100k data sets. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-1634/parts/0-Figure-c1.png",
        "Caption": "Figure 3: MUC-7: Level Distribution of Each of the Facts",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Table-c2.png",
        "Caption": "Figure 1: CTB 10-fold CV word segmentation F- measure for our word segmenter ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1107/parts/0-Table-c11.png",
        "Caption": "Table 6. SA strength scores.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4047/parts/0-Figure-c1.png",
        "Caption": "Figure 1: Dissimilarity of temporal distributions of \u2018WTO\u2019 in English and Chinese corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c4.png",
        "Caption": "Table 4. Gold standard length distribution.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P1018/parts/0-Table-c4.png",
        "Caption": "Table 5: Translation results for English-German",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D07-1076/parts/0-Table-c4.png",
        "Caption": "Table 6: Weights learned for word-context features, which fire when English word e is generated aligned to Chinese word f , with Chinese word f\u22121 to the left or f+1 to the right. Glosses for Chinese words are not part of features. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S10-1019/parts/0-Figure-c1.png",
        "Caption": "Figure 2: Type-level (top) and token-level (bottom) cov- erage for the nine languages in three versions of the Wik- tionary. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P08-2003/parts/0-Table-c1.png",
        "Caption": "Figure 3: Example of Learned Name Pairs with Gloss Translations in Parentheses",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1067/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Segmentation recall relative to gold word frequency. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N06-2021/parts/0-Figure-c1.png",
        "Caption": "Table 1: Texts used for the evaluation",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1106/parts/0-Table-c4.png",
        "Caption": "Table 1: Results on Chinese Semantic Similarity",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W13-2512/parts/0-Table-c2.png",
        "Caption": "Table 3: Precision (P%) and usage proportion",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C02-1033/parts/0-Table-c4.png",
        "Caption": "Table 2 shows the result of varying the number of samplers and iterations for all",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PS15684_w09/parts/0-Table-c5.png",
        "Caption": "Table 2: Examples of features and associated costs. Pseudofeatures are shown in boldface. Exceptional denotes a situation such as the semivowel [j] substituting for the affricate [dZ]. Substitutions between these two sounds actually occur frequently in second-language error data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1004/parts/0-Figure-c2.png",
        "Caption": "Table 4. Examples of the top-3 candidates in the       transliteration of English \u2013 Chinese ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Table-c8.png",
        "Caption": "Table 13 Example translations for Chinese\u2013English MT. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-1624/parts/0-Table-c1.png",
        "Caption": "Table 3 Precision of existing and proposed approaches. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1634/parts/0-Figure-c2.png",
        "Caption": "Figure 3: Discovered metaphorical associations",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PS15684_w09/parts/0-Figure-c1.png",
        "Caption": "Figure 2: An example packed forest representing hy- potheses in Figure 1(a). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W02-1020/parts/0-Figure-c2.png",
        "Caption": "Table 3. Accuracy of the proposed system on a set which contains misspelled queries that the users had reformulated ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N12-1006/parts/0-Figure-c4.png",
        "Caption": "Figure 1: Graphical model representing M L SLDA. Shaded nodes represent observations, plates denote repli- cation, and lines show probabilistic dependencies. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c8.png",
        "Caption": "Figure 2: Maximally Accurate Assignment",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-1045/parts/0-Figure-c4.png",
        "Caption": "Table 7 LIN (MI) weighting: The top 10 common features for country\u2013state and country\u2013party, along with their corresponding ranks in each of the two feature vectors. The features are sorted by the sum of their feature weights with both words. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W13-2501/parts/0-Table-c2.png",
        "Caption": "Table 5: the accuracies over the first SIGHAN bake- off data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W11-2123/parts/0-Figure-c2.png",
        "Caption": "Figure 5: Example of similar document pairs.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-3141/parts/0-Figure-c1.png",
        "Caption": "Table 16: Arabic Order-Free Structure",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P05-1004/parts/0-Table-c4.png",
        "Caption": "Table 5 Comparative performance on MSRPC. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-2124/parts/0-Table-c1.png",
        "Caption": "Figure 1: Tuple extraction from a sentence pair.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1143/parts/0-Table-c1.png",
        "Caption": "Figure 1: Tradeoff between Margin Threshold and name recognition performance ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-1616/parts/0-Table-c2.png",
        "Caption": "Table 2: The sizes of dictionaries as automata",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E06-1004/parts/0-Table-c1.png",
        "Caption": "Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario. The upper part refers to constituency results, the lower part refers to dependency results ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-1616/parts/0-Figure-c4.png",
        "Caption": "Table 3: Results by relation",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1006/parts/0-Figure-c1.png",
        "Caption": "Table 7. Impacts of the mined semantic lexicons and the use of PubMed",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W12-4006/parts/0-Figure-c1.png",
        "Caption": "Table 2: Performance of WSD system using various combinations of learning algorithms and features.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N07-1024/parts/0-Table-c2.png",
        "Caption": "Table 2: Sources of Dictionaries",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/I05-3013/parts/0-Figure-c3.png",
        "Caption": "Table 5. Another example of some discovered paraphrases.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2087/parts/0-Figure-c1.png",
        "Caption": "Table 2: MRRs of the frequency correlation meth- ods. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1053/parts/0-Table-c9.png",
        "Caption": "Table 8. Comparisons among different strategies on Medstract",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1141/parts/0-Table-c1.png",
        "Caption": "Figure 1: Example of a term construction rule as a branch in a decision tree.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-0705/parts/0-Table-c9.png",
        "Caption": "Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcls and the best published result (? Berg-Kirkpatrick et al. (2010) and \u2020 Lee et al. (2010)). This data was taken from the CoNLL-X shared task training sets, resulting in listed corpus sizes. Fine PoS tags were used for evaluation except for items marked with c , which used the coarse tags. For each language the systems were trained to produce the same number of tags as the gold standard. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1091/parts/0-Table-c1.png",
        "Caption": "Figure 6. Perplexity Comparison of Different               Pruning Methods ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N12-1045/parts/0-Figure-c2.png",
        "Caption": "Figure 5: Our parsing model applied to the sequences at different levels of a sentence-level DT. (a) Only possible se- quence at the first level, (b) Three possible sequences at the second level, (c) Three possible sequences at the third level. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0206/parts/0-Table-c1.png",
        "Caption": "Table 1: A sentence decomposed into its depen- dency edges, and the caseframes derived from those edges that we consider (in black). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-1004/parts/0-Figure-c1.png",
        "Caption": "Table 1: Examples of non-phonetic translations.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09_qwn/parts/0-Table-c2.png",
        "Caption": "Figure 4: Improvements in F-measure on MUC-7 plotted against amount of selected unlabeled data used ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N15-1071/parts/0-Table-c6.png",
        "Caption": "Table 4: Noun Stem Alternation",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/I05-3030/parts/0-Table-c1.png",
        "Caption": "Table 7 NEA type-insensitive (type-sensitive) performance with the same English NE recognizer (Mallet system) and different Chinese NE recognizers. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N15-1071/parts/0-Table-c4.png",
        "Caption": "Figure 4: Creation of a Lexical Transducer. The .o. operator represents the composition operation.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1113/parts/0-Table-c4.png",
        "Caption": "Figure 3: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the second order dataset ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pproc2014_n09/parts/0-Table-c2.png",
        "Caption": "Figure 9: Example error cases, with associated frequencies, illustrating system output and gold standard references. 5% of the cases were miscellaneous or otherwise difficult to categorize. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1141/parts/0-Figure-c1.png",
        "Caption": "Table 1: The set of types and subtypes of relations used in the 2004 ACE evaluation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1004/parts/0-Table-c2.png",
        "Caption": "Table 6: F1 per coarse relation type (ACE 2005). SYS is the final model, i.e. last row (PET+PET WC+PET LSA) of Table 5. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1630/parts/0-Table-c6.png",
        "Caption": "Figure 3: Simulation comparing the expected table count (solid lines) versus the approximation under Eq. 3 (dashed lines) for various values of a. This data was gen- erated from a single PYP with b = 1, P0 (i) = 14 and n = 100 customers which all share the same tag. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2605/parts/0-Table-c5.png",
        "Caption": "Table 4. Comparison of Unsupervised Learning                   Methods ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1109/parts/0-Figure-c3.png",
        "Caption": "Table 3. Pseudo-code to extract UW",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c9.png",
        "Caption": "Table 1: Some training events for the English word \u201cwhich\u201d. The symbol \u201c \u201d is the placeholder of the English word \u201cwhich\u201d in the English context. In the German part the placeholder (\u201c \u201d) corresponds to the word aligned to \u201cwhich\u201d, in the first example the German word \u201cdie\u201d, the word \u201cdas\u201d in the second and the word \u201cwas\u201d in the third. The considered English and German contexts are separated by the double bar \u201c  p  \u201d.The last number in the rightmost position is the number of occurrences of the event in the whole corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-3004/parts/0-Table-c7.png",
        "Caption": "Table 4: S/O classifier with learned SWSD integration",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-1101/parts/0-Table-c1.png",
        "Caption": "Table 2: Experiment 2: Results for label unknown sense, NN-based outlier detection, \u03b8 = 1.0. \u03c3: stan- dard deviation ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J02-2003/parts/0-Table-c1.png",
        "Caption": "Figure 1: Growth of the Wiktionary over the last three years, showing total number of entries for all languages and for the 9 languages we consider (left axis). We also show the corresponding increase in average accuracy (right axis) achieved by our model across the 9 languages (see details below). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P04-1080/parts/0-Table-c3.png",
        "Caption": "Table 1: Average ratings and Pearson correlation for rules from the personal stories corpus. Lower ratings are better; see Fig. 2. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P05-1053/parts/0-Table-c4.png",
        "Caption": "Table 6 Context model, word classes, class models, and feature functions. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1107/parts/0-Figure-c2.png",
        "Caption": "Figure 3: The BAD application before entering a verse, showing two possible rhyme patterns.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-2032/parts/0-Table-c4.png",
        "Caption": "Figure 4: MTO is fairly stable as long as the Z\u0303 constant 5.4 Morphological and orthographic features is within an order of magnitude of the real Z value. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W05-1518/parts/0-Table-c3.png",
        "Caption": "Figure 2: Instructions for judging of unsharpened factoids.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W12-1011/parts/0-Table-c4.png",
        "Caption": "Figure 2: Tags produced by the different models along with the reference set of tags for a part of a sentence from the Italian test set. Italicized tags denote incorrect labels. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-2037/parts/0-Table-c3.png",
        "Caption": "Table 1: Comparison of word alignment accuracy. The best results are indicated in bold type. The additional data set sizes are (a) 10k, (b) 50k, (c) 100k. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1045/parts/0-Figure-c3.png",
        "Caption": "Table 2: Evaluation of coarse-grained POS tagging on test data ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1115/parts/0-Table-c7.png",
        "Caption": "Fig. 2. Multilingual bootstrapping.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1059/parts/0-Table-c8.png",
        "Caption": "Table 17: Six Accepted Word Orders in Russian",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P101167/parts/0-Figure-c6.png",
        "Caption": "Table 7: Test results for POS+MORPH tagging. Best baseline results are underlined and the overall best results bold. * indicates a significant difference between the best baseline and a PCRF model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1072/parts/0-Table-c3.png",
        "Caption": "Table 2: Results \u2014 Evaluation A.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1143/parts/0-Figure-c5.png",
        "Caption": "Table 4: Parser performance on WSJ;23, baselines. Note that the Gildea results are for sentences \u2264 40 words in length. All others include all sentences. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Table-c2.png",
        "Caption": "Table 7: Results for ensemble classifier.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Figure-c7.png",
        "Caption": "Table 1: Example coreferent paths: Italicized entities generally corefer.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1056/parts/0-Figure-c3.png",
        "Caption": "Figure 3: Usefulness of syntactic information: (black) dash-dotted line \u2013 word boundaries only, (red) dashed line \u2013 POS info, and (blue) solid line \u2013 full parse trees. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2214/parts/0-Table-c2.png",
        "Caption": "Table 6: Development results for POS+MORPH tagging. Given are training times in minutes (TT) and accuracies (ACC). Best baseline results are underlined and the overall best results bold. * indicates a significant difference between the best baseline and a PCRF model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1048/parts/0-Figure-c8.png",
        "Caption": "Figure 2: Two STs composing a STN",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D12-1003/parts/0-Table-c1.png",
        "Caption": "Table 2: Evaluation of the Russian n-gram model.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1016/parts/0-Table-c3.png",
        "Caption": "Table 5 Comparing similarity measures on D1 and D2. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1016/parts/0-Figure-c3.png",
        "Caption": "Figure 2: F-score curves on the MSR, CU, and PKU datasets: ADF learning vs. SGD and LBFGS training methods.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1099/parts/0-Table-c7.png",
        "Caption": "Table 1: SyntSem tagged corpus extract.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N13-1118/parts/0-Figure-c6.png",
        "Caption": "Table 4: LO Dice configuration scores",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c16.png",
        "Caption": "Table 8 Arabic standard parsing experiments (test set, sentences \u2264 40 words). SplitPCFG is the same grammar used in the Stanford parser, but without the dependency model. FactLex uses basic POS tags predicted by the parser and morphological analyses from MADA. FactLex* uses gold morphological analyses. Berkeley and DP-TSG results are the average of three independent runs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W01-0502/parts/0-Table-c3.png",
        "Caption": "Table 5 Comparative performance on MSRPC. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1083/parts/0-Table-c2.png",
        "Caption": "Figure 3. Calculation of \"Importance\"              of Bigrams ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P87-94/parts/0-Table-c1.png",
        "Caption": "Figure 4 The extended generic beam-search algorithm with multiple beams. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W02-1020/parts/0-Table-c3.png",
        "Caption": "Table 5: Results of Uryupina\u2019s uniqueness classifier",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1121/parts/0-Figure-c5.png",
        "Caption": "Table 5: Comparison of the performance of previous methods on ACE RDC task.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0311/parts/0-Table-c5.png",
        "Caption": "Table 3: Experimental results (F-measure).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0119/parts/0-Table-c5.png",
        "Caption": "Table 3 The top 10 ranked features for country produced by MI, the weighting function employed in the LIN method. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/I08-2080/parts/0-Table-c1.png",
        "Caption": "Figure 1: The role of the standard Basque (Batua) ana- lyzer in filtering out unwanted output candidates created by the induced rule set produced by method 1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P04-1080/parts/0-Table-c5.png",
        "Caption": "Figure 2: Segmentation and tagging of the Arabic token A\u00eeE\u00f1J. J\u00baJ\u0083\u00f0 \u2018and they will write it\u2019. This token has four seg- ments with conflicting grammatical features. For example, the number feature is singular for the pronominal object and plural for the verb. Our model segments the raw to- ken, tags each segment with a morpho-syntactic class (e.g., \u201cPron+Fem+Sg\u201d), and then scores the class sequences. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-1007/parts/0-Figure-c1.png",
        "Caption": "Table 4 Comparing distributions on D1 and D2. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P101167/parts/0-Table-c2.png",
        "Caption": "Table 1 Sources of con\ufb02ict in cross-lingual subjectivity transfer. Definitions and synonyms of the fourth sense of the noun argument, the fourth sense of verb decide, and the first sense of adjective free as provided by the English and Romanian WordNets; for Romanian we also provide the manual translation into English. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1068/parts/0-Table-c4.png",
        "Caption": "Figure 2 Word sense disambiguation accuracy for \u201cNP1 V NP2\u201d frame. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1083/parts/0-Table-c2.png",
        "Caption": "Figure 1: A tree showing head information",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0311/parts/0-Table-c3.png",
        "Caption": "Figure 1: CCG derivation and unresolved semantics for the sentence \u201cI saw nothing suspicious\u201d",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P08-1028/parts/0-Figure-c2.png",
        "Caption": "Figure 2 Two different dependency tree paths (a and b) that are considered paraphrastic because the same words ( John and problem) are used to \ufb01ll the corresponding slots (shown co-indexed) in both the paths. The implied meaning of each dependency path is also shown. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PEAAI_n09/parts/0-Table-c2.png",
        "Caption": "Table 4: Cross-domain B3 and MUC results for Reconcile and Sieve with lexical features. Gray cells represent results that are not significantly different from the best results in the column at the 0.05 p-level. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1141/parts/0-Table-c1.png",
        "Caption": "Table 1: Examples of new alternations",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-1101/parts/0-Figure-c2.png",
        "Caption": "Table 1: Word distribution in the extended Cilin",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Table-c16.png",
        "Caption": "Table 2: Comparison with other PPI extraction systems in the AIMed corpus ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1761/parts/0-Figure-c9.png",
        "Caption": "Table 7: Vocabulary size of NIST task (40K)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-1065/parts/0-Table-c5.png",
        "Caption": "Table 2: Number of clustering decisions made ac- cording to mention type (rows anaphor, columns antecedent) and percentage of wrong decisions. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C08-1002/parts/0-Figure-c2.png",
        "Caption": "Figure 3: Acceptance rates for a noun phrase in the course of iteration. All models were with back-off mix- ing (+BM). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1045/parts/0-Table-c2.png",
        "Caption": "Table 2: Features Used for Initial Distribution",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W08-0335/parts/0-Table-c3.png",
        "Caption": "Table 3: B3 results for baselines and lexicalized feature sets on the broad-coverage ACE 2004 data set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J06-2001/parts/0-Table-c13.png",
        "Caption": "Table 2: TS of party#n#1 (first 10 out of 12,890 total words)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-1004/parts/0-Table-c5.png",
        "Caption": "Table 1: Corpus statistics of Chinese side, where Sent., Avg., Lon., and Len. are short for sentence, longest, average, and length respectively. RT RAIN denotes the reachable (given rule table without added rules) subset of T RAIN data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1072/parts/0-Table-c1.png",
        "Caption": "Table 1: 50-document corpora averages",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Figure-c3.png",
        "Caption": "Table 2: Performance using FBIS training corpus (top) and NIST corpus (bottom). Improvements are significant at the p <0.05 level, except where indicated (ns ). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1001/parts/0-Figure-c1.png",
        "Caption": "Figure 6: Opinion PageRank Performance with varying parameter \u00b5 (\u03bb = 0.2) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1063/parts/0-Table-c5.png",
        "Caption": "Figure 3: Semantic Caseframe Expectations",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1015/parts/0-Table-c6.png",
        "Caption": "Table 2: Syntactic dependency scheme used in this work. Labels that aren\u2019t self-explanatory or similar to the labels used by Tratz and Hovy (2011) for English or CATiB for Arabic (Habash and Roth, 2009) are in bold (for completely new relations) or italics (for similarly named but semantically different relations) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/D09-1092/parts/0-Figure-c1.png",
        "Caption": "Figure 13: A Template Network and Two Filler Networks ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N07-1024/parts/0-Table-c1.png",
        "Caption": "Figure 4 A comparison between ILP-Global and ILP-Local for two fragments of the test-set concept seizure. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-0704/parts/0-Table-c1.png",
        "Caption": "Table 6: Listing of all seeds used for KEdis and KEpat , as well as the top-10 entities discovered by ES-all on one of our test folds. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1038/parts/0-Table-c5.png",
        "Caption": "Table 9: Results of the combined model for classify- ing unknown words into major and medium catego- ries: best guess ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06128/parts/0-Figure-c2.png",
        "Caption": "Table 2: Weights learned for discount features. Nega- tive weights indicate bonuses; positive weights indicate penalties. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-2049/parts/0-Table-c1.png",
        "Caption": "Table 2: P r (f, a\u0303|e) for IBM Models",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P04-1080/parts/0-Table-c4.png",
        "Caption": "Table 2: Meaning of diacritics indicating statistical sig- nificance (\u03c72 tests) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-1101/parts/0-Table-c4.png",
        "Caption": "Table 1: normalized Mutual Information values for three graphs and different iterations in %. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1110/parts/0-Table-c4.png",
        "Caption": "Table 9: ROUGE-SU in empirical approach",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-0910/parts/0-Figure-c3.png",
        "Caption": "Figure 4: PTB vs. Wiktionary type coverage across sec- tions of the Brown corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c21.png",
        "Caption": "Table 1: Success rate of anaphora resolution",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P08-1078/parts/0-Table-c2.png",
        "Caption": "Table 9: Comparison of our approach with using only the Gigaword corpus",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-2222/parts/0-Figure-c1.png",
        "Caption": "Table 2: Translation results for English-German",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1104/parts/0-Figure-c1.png",
        "Caption": "Table 6: Polarity classifier with and without SWSD.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1634/parts/0-Figure-c7.png",
        "Caption": "Figure 1. A pruned phrase tokenization lattice. Edges are tokenizations of phrases, e.g. e5 represents tokenizing \u8d28\u7591 \u2018question\u2019 into a word and e7 represents tokenizing \u7591\u4ed6 \u2018doubt him\u2019 into a partial word \u7591 \u2018doubt\u2019 followed by a word \u4ed6 \u2018him\u2019. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Figure-c7.png",
        "Caption": "Fig. 5. (a) Binary merging of clusters. (b) Merging of multiple clusters.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Figure-c8.png",
        "Caption": "Table 3: Summary of features used in experiments in this paper.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C02-1025/parts/0-Table-c2.png",
        "Caption": "Figure 5: Example annotation from the BioNLP Shared Task 2011 Epigenetics and Post-translational Modifications event extraction task. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P08-1078/parts/0-Figure-c2.png",
        "Caption": "Table 10: Performance of Altavista counts and BNC counts for compound interpretation (data from Lauer 1995) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P04-1080/parts/0-Table-c6.png",
        "Caption": "Table 4: Number of features used according to different cut-off threshold. In the second column of the table are shown the number of features used when only the English context is considered. The third column correspond to English, German and Word-Classes contexts. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4011/parts/0-Figure-c5.png",
        "Caption": "Table 2: Experiment 2: Results for label unknown sense, NN-based outlier detection, \u03b8 = 1.0. \u03c3: stan- dard deviation ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-0410/parts/0-Table-c3.png",
        "Caption": "Table 4: Dataset statistics: development (dev) and test.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-1017/parts/0-Figure-c1.png",
        "Caption": "Table 2: Overview of participating languages and treebank properties. \u2019Sents\u2019 = number of sentences, \u2019Tokens\u2019 = number of raw surface forms. \u2019Lex. size\u2019 and \u2019Avg. Length\u2019 are computed in terms of tagged terminals. \u2018NT\u2019 = non- terminals in constituency treebanks, \u2018Dep Labels\u2019 = dependency labels on the arcs of dependency treebanks. \u2013 A more comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1125/parts/0-Figure-c5.png",
        "Caption": "Figure 6: A CRF as a multi-sentential parsing model.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-0705/parts/0-Table-c2.png",
        "Caption": "Table 2: Experimental Results. C5.0 is supervised accuracy; Base         is      on random clusters.  set; Ling is manually selected subset; Seed is seed-verb-selected set. See text for further description. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N03-1010/parts/0-Figure-c2.png",
        "Caption": "Table 1: Conditioning features for the probabilistic CFG used in the reported empirical trials ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1025/parts/0-Table-c2.png",
        "Caption": "Figure 5: Example annotation from the BioNLP Shared Task 2011 Epigenetics and Post-translational Modifications event extraction task. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1017/parts/0-Table-c2.png",
        "Caption": "Table 10 The superiority of our joint model on three different domains indicated by type-insensitive (type-sensitive) performance (those signi\ufb01cant entries are marked in comparison with baseline). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J02-2003/parts/0-Table-c2.png",
        "Caption": "Figure 8: Evaluation of translation from English on in-domain test data",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1133/parts/0-Figure-c1.png",
        "Caption": "Table 8 The comparison between NPYLM and ESA. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1064/parts/0-Table-c1.png",
        "Caption": "Table 6. Size of the test data",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00-1025/parts/0-Figure-c17.png",
        "Caption": "Table 17 Comparison of performance of MSRSeg: The versions that are trained using (semi-)supervised iterative training with different initial training sets (Rows 1 to 8) versus the version that is trained on annotated corpus of 20 million words (Row 9). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00-1025/parts/0-Figure-c1.png",
        "Caption": "Table 9 Bootstrapped weighting: top 10 common features for country\u2013state and country\u2013party along with their corresponding ranks in the two (sorted) feature vectors. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Table-c13.png",
        "Caption": "Figure 1: Example of semantic trees",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-2709/parts/0-Figure-c1.png",
        "Caption": "Figure 1: Graphical model for PLTM.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/I08-2080/parts/0-Table-c2.png",
        "Caption": "Table 6 Baseline + Word Clustering by Relation +           Re-ranking by Coreference +              Re-ranking by Relation ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1062/parts/0-Table-c3.png",
        "Caption": "Figure 1: Learning curves: word-segmentation F- measure and parsing label F-measure vs. percentage of training data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E06-1030/parts/0-Table-c8.png",
        "Caption": "Table 4: Rhetorical pattern of C-Colon",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-1604/parts/0-Figure-c1.png",
        "Caption": "Table 1: Evaluation of coarse-grained POS tagging on test data ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-2302/parts/0-Table-c3.png",
        "Caption": "Table 10 Comparing selectional preference frame definitions. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1147/parts/0-Table-c4.png",
        "Caption": "Table 4: Results tested against gs-so",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-1014/parts/0-Figure-c4.png",
        "Caption": "Table 2: Normalization accuracy after training on n tokens and evaluating on 1,000 tokens (average of 10 random training and evaluation sets), compared to the \u201cbaseline\u201d score of the full text without any normalization ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-1728/parts/0-Figure-c3.png",
        "Caption": "Figure 7: Corpus size vs. KL-divergence",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1141/parts/0-Figure-c5.png",
        "Caption": "Table 1: Results from WSD system applied to various sections of the NLM-WSD data set using a variety of fea- tures and machine learning algorithms. Results from baseline and previously published approaches are included for comparison. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-1215/parts/0-Table-c2.png",
        "Caption": "Table 5: The contribution of MMVC in a rank-based classi- fier combination on S ENSEVAL -1 and S ENSEVAL -2 English as computed by 5-fold cross validation over training data ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W05-0709/parts/0-Table-c1.png",
        "Caption": "Figure 3: Propagation: Core items",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c2.png",
        "Caption": "Table 2: Examples and number of them in Semcor, for sense approach and for class approach ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-0812/parts/0-Table-c2.png",
        "Caption": "Table 5. The performance on the set of unknown",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/M98-1003/parts/0-Figure-c2.png",
        "Caption": "Table 8 FT detection results on the MSR gold test set. The \u2018All\u2019 column shows the results of detecting all 10 types of factoids, as described in Table 1, which amount to 6630 factoids, as shown in Table 3. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E12-2021/parts/0-Figure-c3.png",
        "Caption": "Table 1: Training and Test Data Statistics",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6201/parts/0-Table-c6.png",
        "Caption": "Table 7: Vocabulary size of NIST task (40K)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J06-2001/parts/0-Table-c11.png",
        "Caption": "Figure 2: Precision-recall curve for the algorithms.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3909/parts/0-Table-c4.png",
        "Caption": "Table 10: Performance of Two Categories",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-1025/parts/0-Table-c4.png",
        "Caption": "Figure 2. Position of news story boundaries in a CNN news summary in relation to troughs found by the algorithm. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pling_p07/parts/0-Table-c1.png",
        "Caption": "Table 12: Arabic Pronoun Dropping",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1126/parts/0-Table-c1.png",
        "Caption": "Figure 1: Organisation of the hierarchical graph of concepts",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2228/parts/0-Table-c3.png",
        "Caption": "Figure 1. Different representations of a relation instance in the example sentence \u201c\u2026provide bene-",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1107/parts/0-Table-c9.png",
        "Caption": "Table 3: Values obtained for Precision, Recall and F- score with method 1 by changing the threshold frequency of the correspondences and applying a post-filter. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P22324_w09/parts/0-Figure-c2.png",
        "Caption": "Table 1: Decoder performance on the June 2002 TIDES MT evluation test set with multiple searches from randomized starting points (MSD=2, MSSS=5). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pstat_p00/parts/0-Figure-c5.png",
        "Caption": "Table 1. Nouns and verbs supersense labels, and short description (from the Wordnet documentation).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1009/parts/0-Table-c9.png",
        "Caption": "Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pproc2014_n09/parts/0-Table-c5.png",
        "Caption": "Table 1: Meta-evaluation results at document level",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Q13-1015/parts/0-Figure-c2.png",
        "Caption": "Figure 2: User Interface with Arabic Script Dis- play in Java. Mouse clicks on the virtual keyboard or key presses on the physical keyboard are inter- cepted, converted to Arabic Unicode characters, and stored in a buffer, which has a start and an end but no inherent ordering. The Arabic Canvas Object observes the buffer and contains an Ara- bic Scribe object that renders the string of Uni- code characters right-to-left as connected Arabic glyphs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Figure-c3.png",
        "Caption": "Table 3: Effect of discriminatively learned penalties for OOV words. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1058/parts/0-Table-c2.png",
        "Caption": "Table 5: Evaluation of 100 randomly sampled variation nu- clei types. The samples from each corpus were indepen- dently evaluated. The ATB has a much higher fraction of nuclei per tree, and a higher type-level error rate. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P101121_p07/parts/0-Table-c2.png",
        "Caption": "Table 4: Effect of different sets of reference translations used during tuning. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1016/parts/0-Figure-c4.png",
        "Caption": "Figure 2: Bayesian network for probabilistic rea- soning of locations (variable \u201cfrom desc\u201d), which incorporates ASR N-best information in the vari- able\u201cfrom desc nbest\u201d and dialogue history in- formation in the remaining random variables. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1126/parts/0-Figure-c4.png",
        "Caption": "Figure 1: The Maytag interface",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D07-1012/parts/0-Table-c2.png",
        "Caption": "Table 1: Examples of constructing Universal POS tag sets from the Wiktionary.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P59105ca/parts/0-Figure-c11.png",
        "Caption": "Figure 3: Boxer output for Shared Task Text 2",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1125/parts/0-Figure-c1.png",
        "Caption": "Figure 9 Word sense disambiguation accuracy for \u201cNP1 V for NP2 NP3\u201d frame. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N03-1010/parts/0-Figure-c6.png",
        "Caption": "Figure 6: Average number of Pareto points",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P01-1027/parts/0-Table-c1.png",
        "Caption": "Fig. 3: NEs after agents-based modification",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1081/parts/0-Table-c1.png",
        "Caption": "Table 1: Morph Examples and Motivations.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-1728/parts/0-Table-c2.png",
        "Caption": "Table 2: Feature templates used in R-phase. Ex- ample used is \u201c32 ddd\u201d. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c3.png",
        "Caption": "Table 3 Unknown word model features for Arabic and French. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1007/parts/0-Table-c6.png",
        "Caption": "Table 2: Translation results in terms of BLEU score and translation edit rate (TER) estimated on newstest2010 with the NIST scoring script. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-0704/parts/0-Figure-c2.png",
        "Caption": "Table 1: Examples of templates suggested by DIRT and TEASE as having an entailment relation, in some direction, with the input template \u2018X change Y \u2019. The entailment direction arrows were judged manually and added for readability. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmert_n09/parts/0-Table-c1.png",
        "Caption": "Table 3. Number of candidates for each target                 language. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2164/parts/0-Figure-c2.png",
        "Caption": "Table 6. The performance on the set of unknown",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09_qwn/parts/0-Table-c3.png",
        "Caption": "Table 1: Basic features used in the maximum entropy model.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E06-2012/parts/0-Figure-c1.png",
        "Caption": "Table 2. Overall results Coverage/Accuracy",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1115/parts/0-Figure-c4.png",
        "Caption": "Table 2: Training phase: effect of similarity thresh- old (a) on Ave. MRR and TRDR. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Table-c4.png",
        "Caption": "Table 1: Entries from the English-Slovene sense cluster inventory.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1004/parts/0-Table-c1.png",
        "Caption": "Table 2: Segmentation results by a pure subword-based IOB tagging. The upper numbers are of the character- based and the lower ones, the subword-based. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0711/parts/0-Table-c3.png",
        "Caption": "Fig. 6. German English BLEU scores of various al EM(Co), GS(Co), EM(Co)+GS(Co), and VB(Co). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1015/parts/0-Table-c4.png",
        "Caption": "Figure 2: Examples of context- free and context-sensitive sub- trees related with Figure 1(b). Note: the bold node is the root for a sub-tree. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1083/parts/0-Figure-c2.png",
        "Caption": "Table 2. Precision @ top N (with 3 seeds, and window size w = 3)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P05-1051/parts/0-Table-c5.png",
        "Caption": "Table 8 Comparing feature descriptions. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N12-1006/parts/0-Table-c7.png",
        "Caption": "Table 3. The Riv over the bakeoff-2 data.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1007/parts/0-Table-c9.png",
        "Caption": "          A u to m a tic M e tr ic s H u m a n E v a lu a tio n Figure 2: Scores based on Automatic Metrics and Human Evaluation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Figure-c6.png",
        "Caption": "Table 2. CNN news summaries segmented using different block sizes. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1004/parts/0-Table-c2.png",
        "Caption": "Table 1: Parallel Corpus.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2408/parts/0-Table-c1.png",
        "Caption": "Table 3: Number of unique entries in training and    test sets, categorized by semantic attributes ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P101167/parts/0-Figure-c2.png",
        "Caption": "Table 4: Different Context Window Size Setting",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P05-1020/parts/0-Table-c2.png",
        "Caption": "Table 1: Comparison of SWSD systems",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-1215/parts/0-Table-c1.png",
        "Caption": "Fig. 2. Multilingual bootstrapping.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-3909/parts/0-Table-c2.png",
        "Caption": "Figure 1: EM input for our example sentence. j-values follow each lexical candidate. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W13-2512/parts/0-Figure-c3.png",
        "Caption": "Table 2: Dataset characteristics including the number of documents, annotated CEs, coreference chains, annotated CEs per chain (average), and number of documents in the train/test split. We use st to indicate a standard train/test split. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1070/parts/0-Table-c6.png",
        "Caption": "Table 4: A comparison of translation quality of Egyptian, Levantine, and MSA web text, using various training corpora. The highest BLEU scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian), since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the dialectal data results in marginally worse translations. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2214/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Learning curves of word prediction accu- racies of IGT REE trained on TRAIN - REUTERS, and tested on REUTERS, ALICE, and BROWN. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2052/parts/0-Table-c4.png",
        "Caption": "Figure 1: Dynamic-Expansion Tree Span Scheme",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P87-94/parts/0-Table-c3.png",
        "Caption": "Table 8: P, R and F1 fine-grained results for the resources evaluated at Senseval-3, English Lexical Sample Task ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1121/parts/0-Table-c4.png",
        "Caption": "Table 3: Lexical features for relation extraction.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4917/parts/0-Table-c4.png",
        "Caption": "Table 3: Disambiguation examples (\u2217 : using morpho-syntactic analysis).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C08-1088/parts/0-Figure-c1.png",
        "Caption": "Figure 3: Tagging part of log-likelihood plotted against V-measure ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-3114/parts/0-Figure-c14.png",
        "Caption": "Figure 7: A Path in a Transducer for English",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4916/parts/0-Table-c1.png",
        "Caption": "Table 4: WCN x Baseline",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-1914/parts/0-Figure-c2.png",
        "Caption": "Table 2: Single-threaded time and memory consumption of Moses translating 3003 sentences. Where applicable, models were loaded with lazy memory mapping (-L), prefaulting (-P), and normal reading (-R); results differ by at most than 0.6 minute. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Table-c3.png",
        "Caption": "Table 2: An example for the \u201cBMES\u201d representa- tion. The sentence is \u201c\u6211\u7231\u5317\u4eac\u5929\u5b89\u95e8\u201d (I love Bei- jing Tian-an-men square), which consists of 4 Chi- nese words: \u201c\u6211\u201d (I), \u201c\u7231\u201d (love), \u201c\u5317\u4eac\u201d (Beijing), and \u201c\u5929\u5b89\u95e8\u201d (Tian-an-men square). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1727/parts/0-Table-c7.png",
        "Caption": "Figure 1: Finite-state cascades for five natural language problems.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1015/parts/0-Table-c7.png",
        "Caption": "Table 2: Model means for High and Low similarity items and correlation coefficients with human judgments (*: p < 0.05, **: p < 0.01) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-1068/parts/0-Figure-c2.png",
        "Caption": "Table 2: Translation results in terms of BLEU score and translation edit rate (TER) estimated on newstest2010 with the NIST scoring script. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C08-1088/parts/0-Table-c1.png",
        "Caption": "Table 7: Word segmentation results.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/M98-1003/parts/0-Figure-c12.png",
        "Caption": "Figure 4: Graph for a neo-Davidsonian structure.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1072/parts/0-Figure-c1.png",
        "Caption": "Table 2: Features Used for Initial Distribution",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D10-1100/parts/0-Figure-c1.png",
        "Caption": "Figure 1: Rank trajectories of 4 LDA inferred topics, with incremental topic inference. The x-axis indicates the utterance number. The y-axis indicates a topic\u2019s rank at each utterance. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-1065/parts/0-Figure-c1.png",
        "Caption": "Table 1: Relation Extraction Results. Models using hidden constituency syntax provide significant gains over the syntactically-uniformed baseline model in both languages, but the advantages of the latent syntax were mitigated on the smaller Chinese data set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1126/parts/0-Figure-c3.png",
        "Caption": "Table 1: Results for morphological processing, English\u2192German ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-1616/parts/0-Table-c1.png",
        "Caption": "Table 8. EDT and mention detection results.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/M98-1003/parts/0-Figure-c7.png",
        "Caption": "Table 4: Average precision of discovered senses      for English in relation with WordNet ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C04-1131/parts/0-Table-c3.png",
        "Caption": "Table 5. Error distribution of major types on both the 2003 and 2004 data for the compos- ite kernel by polynomial expansion ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-0705/parts/0-Table-c5.png",
        "Caption": "Figure 3: string insertion operator for left-to-right decoding method. A string e0 was appended after the partial output string, e, and the last word in e 0 was aligned from f j . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2052/parts/0-Table-c5.png",
        "Caption": "Figure 12 Interdigitation FSRA \u2013 general. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W08-0602/parts/0-Table-c1.png",
        "Caption": "Table 1: Combinatorial Search Problems in Decoding",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S0885/parts/0-Table-c2.png",
        "Caption": "Table 2. Features used in baseline system",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Figure-c7.png",
        "Caption": "Table 1: Example of word-dependent substitution costs.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Figure-c3.png",
        "Caption": "Table 6: The effect on caseframe coverage of adding in-domain and out-of-domain documents. The difference between adding in-domain and out- of-domain text is significant p < 10\u22123 (Study 3). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1048/parts/0-Figure-c2.png",
        "Caption": "Table 3: The results for three systems associ- ated with the project for the NP bracketing task, the shared task at CoNLL-99. The baseline re- sults have been obtained by finding NP chunks in the text with an algorithm which selects the most frequent chunk tag associated with each part-of- speech tag. The best results at CoNLL-99 was obtained with a bottom-up memory-based learner. An improved version of that system (MBL) deliv- ered the best project result. The MDL results have been obtained on a different data set and therefore combination of the three systems was not feasible. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Table-c9.png",
        "Caption": "Table 6: LO cosine sentence configuration scores",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-4223/parts/0-Figure-c2.png",
        "Caption": "Figure 3: Learning curves of bootstrapping meth- ods for semantic classification on TS1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2302/parts/0-Table-c1.png",
        "Caption": "Figure 1: Architecture of the dependency ranking system.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1102/parts/0-Figure-c1.png",
        "Caption": "Table 8: Parser performance on WSJ;23, unsupervised adaptation. For all trials, the base training is Brown;T, the held out is Brown;H plus the parser output for WSJ;24, and the mixing parameter \u03c4A is 0.20e c(A). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1117/parts/0-Table-c3.png",
        "Caption": "Table 3: Three different vocabulary sizes used in subword- based tagging. s1 contains all the characters. s2 and s3 contains some common words. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1068/parts/0-Figure-c6.png",
        "Caption": "Figure 1: Workflow for NIL knowledge engineering component. NILE refers to NIL expression, which is identified and annotated by human annotator. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Table-c14.png",
        "Caption": "Figure 1: Examples of parallel phrases used in word alignment. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P01-1005/parts/0-Figure-c2.png",
        "Caption": "Table 3: The sizes of error models as automata",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-2302/parts/0-Figure-c1.png",
        "Caption": "Table 6: Overall performance on the evaluation set. L is the upper bound of the length of possible chunks in semi-CRFs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N15-1071/parts/0-Table-c5.png",
        "Caption": "Table 9:      Top ranked sentences using the LR[0.20,0.95] system on the question \u201cWhat caused the Kursk to sink?\u201d ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P01-1017/parts/0-Figure-c2.png",
        "Caption": "Table 6: Example Translations for the Verbmobil task.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1106/parts/0-Table-c5.png",
        "Caption": "Figure 6 The influence of beam-sizes, and the convergence of the perceptron for the joint segmentor and POS -tagger. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W05-0709/parts/0-Figure-c1.png",
        "Caption": "Table 5: Performance of our proposed method (Spectral- based clustering) compared with other unsupervised methods: ((Hasegawa et al., 2004))\u2019s clustering method and K-means clustering. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-0703/parts/0-Figure-c2.png",
        "Caption": "Figure 2: Some examples for MEDLINE tagset: Number of lex. entries per tag and sample words. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-2066/parts/0-Table-c3.png",
        "Caption": "Table 2: Coverage set hypothesis extensions for the IBM re-ordering.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1630/parts/0-Table-c7.png",
        "Caption": "Figure 1: Three representations of NP modifications: (a) the original treebank representation; (b) Selective left-corner representation; and (c) a flat structure that is unambiguously equivalent to (b) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P08-1028/parts/0-Figure-c3.png",
        "Caption": "Figure 1: Derivation with \u03bb-DRSs, including \u03b2-conversion, for \u201cA record date\u201d. Com- binatory rules are indicated by solid lines, semantic rules by dotted lines. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2101/parts/0-Table-c2.png",
        "Caption": "Table 5: Parser performance on Brown;E, supervised adaptation",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W07-0909/parts/0-Table-c2.png",
        "Caption": "Table 3: Evaluation of the three anaphoric resolvers discussed by Ng and Cardie. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-3238/parts/0-Table-c3.png",
        "Caption": "Figure 1: Tuple extraction from a sentence pair.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1140/parts/0-Figure-c2.png",
        "Caption": "Figure 2. Modified Viterbi search \u2013 stop-word treatment",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1106/parts/0-Table-c3.png",
        "Caption": "Table 1: A summary of the parsing and evaluation sce- narios. X depicts gold information, \u2013 depicts unknown information, to be predicted by the system. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2211/parts/0-Table-c9.png",
        "Caption": "Table 3: Disambiguation examples (\u2217 : using morpho-syntactic analysis).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-2124/parts/0-Table-c5.png",
        "Caption": "Figure 2 Taxonomy of morphologically derived words (MDWs) in MSRSeg. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-2122/parts/0-Table-c1.png",
        "Caption": "Table 1: The pool of features for all languages.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Figure-c4.png",
        "Caption": "                            C2 Figure 3: An underspecified d ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P08-2023/parts/0-Table-c5.png",
        "Caption": "Figure 2: One of the 69 test documents, containing 10 narrative events. The protagonist is President Bush. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P22324_w09/parts/0-Figure-c3.png",
        "Caption": "Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario. Top upper part refers to constituency results, the lower part refers to dependency results. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-2060/parts/0-Figure-c2.png",
        "Caption": "Table 1: AER results. IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM. We choose t = 1, 5, and 30 for the fertility HMM. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-1070/parts/0-Figure-c1.png",
        "Caption": "Figure 1: Illustration of Pareto Frontier. Ten hypotheses are plotted by their scores in two metrics. Hypotheses indicated by a circle (o) are pareto-optimal, while those indicated by a plus (+) are not. The line shows the convex hull, which attains only a subset of pareto-optimal points. The triangle (4) is a point that is weakly pareto-optimal but not pareto-optimal. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1408/parts/0-Table-c2.png",
        "Caption": "Table 10 Simplified prevalence score, evaluation on SemCor, polysemous words only. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1066/parts/0-Figure-c1.png",
        "Caption": "Table 4: Accuracy of all slots on the TST3 and TST4 test set ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c17.png",
        "Caption": "Figure 1 A general architecture for paraphrasing approaches leveraging the distributional similarity hypothesis. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W11-2123/parts/0-Table-c1.png",
        "Caption": "Table 1: Syntactic Seeding Heuristics",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-0611/parts/0-Table-c2.png",
        "Caption": "Figure 2: Diffs in the course of iteration. All models were with back-off mixing (+BM). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-3236/parts/0-Figure-c5.png",
        "Caption": "Table 3: Baseline and oracle-best 4-gram BLEU scores with 4 references for NIST Chinese-English MT datasets. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1012/parts/0-Figure-c2.png",
        "Caption": "Table 4: Accuracy for different EM-weighted probability interpolation models for SENSEVAL 2 ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1066/parts/0-Figure-c3.png",
        "Caption": "Figure 1: a) A related work section extracted from (Wu and Oard, 2008); b) An associated topic hierar- chy tree of a); c) An associated topic tree, annotated with key words/phrases. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N12-1006/parts/0-Figure-c2.png",
        "Caption": "Table 11 Comparison between a ME framework and the derived model on the same test set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00-1025/parts/0-Figure-c11.png",
        "Caption": "Table 1: Most frequent BLC\u201320 semantic classes on WordNet 3.0",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1075/parts/0-Table-c2.png",
        "Caption": "Table 6: Performance on the test set. Scores are on gold mentions. Stars indicate a statistically significant difference with respect to the baseline. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Psem_p07/parts/0-Figure-c2.png",
        "Caption": "Table 4: Combined systems (Basque) in cross- validation, best recall in bold. Only vector(f) was used for combination. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1099/parts/0-Table-c6.png",
        "Caption": "Table 4 NER type-insensitive (type-sensitive) performance of different English NE recognizers. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-2037/parts/0-Table-c1.png",
        "Caption": "Table 7: Unbalanced vs. balanced combining. All runs ignored the context. Evaluated on the Test data set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1033/parts/0-Table-c3.png",
        "Caption": "Table 1: Distribution of activity types: Both databases contain a lot of discussing, informing and story-telling activities however the meeting data contains a lot more planning and advising. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1009/parts/0-Table-c3.png",
        "Caption": "Table 5: Average F1 using different hypothesized type-specific features. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1073/parts/0-Figure-c6.png",
        "Caption": "Figure 1: Four synchronous rules with topic distributions. Each sub-graph shows a rule with its topic distribution, where the X-axis means topic index and the Y-axis means the topic probability. Notably, the rule (b) and rule (c) shares the same source Chinese string, but they have different topic distributions due to the different English translations. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Table-c6.png",
        "Caption": "Table 3 Training, development, and test data for word segmentation on CTB5. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C00-1081/parts/0-Figure-c4.png",
        "Caption": "Table 6: precision with and without feature",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P59105ca/parts/0-Figure-c2.png",
        "Caption": "Figure 15 Reduplication \u2013 general case. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-1001/parts/0-Table-c4.png",
        "Caption": "Figure 4: Number of search iterations (left) and total number of alignments considered (right) during search in depen- dence of input length. The data is taken from the translation of the Chinese testset from the TIDES MT evaluation in June 2002. Translations were performed with a maximum swap distance of 2 and a maximum swap segment size of 5. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N13-1095/parts/0-Figure-c2.png",
        "Caption": "Table 3 Human Evaluation of translation",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PS15684_w09/parts/0-Table-c8.png",
        "Caption": "Table 1: Examples of learned pronoun probabilities.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6212/parts/0-Figure-c1.png",
        "Caption": "                                                                   ings of the 13th International Conference, pages 182\u2013190. Table 6: Accuracy on S ENSEVAL-1 and S ENSEVAL-2 En-                                                                 A. R. Golding and D. Roth. 1999. A winnow-based appro glish test data (only the supervised systems with a coverage of    to context-sensitive spelling correction. Machine Learni at least 97% were used to compute the mean and variance)           34(1-3):107\u2013130. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2101/parts/0-Table-c1.png",
        "Caption": "Figure 6: Performance analysis of HRGs, CWU, CWW & HAC for different parameter combinations (Table 2). (A) All combinations of p1 , p2 and p3 = 0.05. (B) All combinations of p1 , p2 and p3 = 0.09. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H01-1052/parts/0-Table-c2.png",
        "Caption": "Figure 1: A pair of comparable, non-parallel documents",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1050/parts/0-Figure-c4.png",
        "Caption": "Table 5 Effect of pruning parameter tp and heuristic function on error rate for direct-translation model (Np = 50,000). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N06-2021/parts/0-Table-c2.png",
        "Caption": "Figure 1: Illustration of entity-relationship graphs",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-3004/parts/0-Table-c4.png",
        "Caption": "Table 4: Quantitative Evaluation of Common Topic Finding (\u201ccross-collection\u201d log-likelihood) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J09-3004/parts/0-Table-c5.png",
        "Caption": "Table 1: Kendall\u2019s (\u03c4 ) correlation over WMT 2013 (all- en), for the full dataset and also the subset of the data containing a noun compound in both the reference and the MT output ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c5.png",
        "Caption": "Figure 6: F-measure for the objective and subjective classes for multilingual bootstrapping (versus cross-lingual framework) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C04-1131/parts/0-Table-c6.png",
        "Caption": "Table 3: Mutual information between feature subset and class label with \u03c72 based feature ranking. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W02-1020/parts/0-Table-c2.png",
        "Caption": "Table 1 Space comparison between FSAs and FSRAs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c15.png",
        "Caption": "Figure 2. Incremental alignment with TERp resulting in a confusion network.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1060/parts/0-Figure-c1.png",
        "Caption": "Table 4: Development results for POS tagging. Given are training times in minutes (TT) and accuracies (ACC). Best baseline results are underlined and the overall best results bold. * indicates a significant difference (positive or negative) between the best baseline and a PCRF model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c10.png",
        "Caption": "Table 2: Factoring of global feature collections g into f . xji denotes hxi , . . . xj i in sequence x = hx1 , . . .i. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-2036/parts/0-Figure-c2.png",
        "Caption": "Table 1: Features used in our parsing models.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P02-1061/parts/0-Table-c3.png",
        "Caption": "Figure 2: Speed in lookups per microsecond by data structure and number of 64-bit entries. Performance dips as each data structure outgrows the processor\u2019s 12 MB L2 cache. Among hash tables, indicated by shapes, probing is initially slower but converges to 43% faster than un- ordered or hash set. Interpolation search has a more ex- pensive pivot function but does less reads and iterations, so it is initially slower than binary search and set, but be- comes faster above 4096 entries. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E12-1020/parts/0-Figure-c2.png",
        "Caption": "Table 1: A summary of the parsing and evaluation sce- narios. X depicts gold information, \u2013 depicts unknown information, to be predicted by the system. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P05-1051/parts/0-Table-c6.png",
        "Caption": "Table 1: Processed Data Statistics",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Table-c4.png",
        "Caption": "Table 6: Levels all and morph against the Gold Standard.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1149/parts/0-Figure-c2.png",
        "Caption": "Figure 1. Overview of the method",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-1065/parts/0-Table-c6.png",
        "Caption": "Figure 2: Example context for the spelling confusion set {piece,peace} and extracted features ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1068/parts/0-Table-c2.png",
        "Caption": "Table 1 TERp edit costs optimized for adequacy ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-1014/parts/0-Table-c4.png",
        "Caption": "Table 5 The results of setting 4 (Punctuation and other encoding information are used; the maximum length is 30). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1005/parts/0-Figure-c4.png",
        "Caption": "Table 3: Results of the baseline model: best 5 guesses",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1121/parts/0-Table-c5.png",
        "Caption": "Table 5: Selected entries from the confusion matrix for parts of speech in German with F-scores for the left-hand- side category. ADJ* (ADJD or ADJA) = adjective; ADV = adverb; ART = determiner; APPR = preposition; NE = proper noun; NN = common noun; PRELS = relative pronoun; VVFIN = finite verb; VVINF = non-finite verb; VAFIN = finite auxiliary verb; VAINF = non-finite auxil- iary verb; VVPP = participle; XY = not a word. We use \u03b1* to denote the set of categories with \u03b1 as a prefix. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P05-1013/parts/0-Table-c6.png",
        "Caption": "Table 1: Corpus of complex news stories.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W09-2411/parts/0-Figure-c1.png",
        "Caption": "Table 4: The System Performance Based on Each Single Feature Set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PEAAI_n09/parts/0-Figure-c4.png",
        "Caption": "Table 3: F1 scores and speed (in sentences per sec.) of SegTagDep on CTB-5c-1 w.r.t. the beam size. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1149/parts/0-Table-c2.png",
        "Caption": "Table 1: Coreference Definition Differences for MUC and ACE. (GPE refers to geo-political entities.) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1039/parts/0-Table-c2.png",
        "Caption": "Table 4 Relative recall evaluation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2204/parts/0-Table-c2.png",
        "Caption": "Table 4: MUC, CEAF, and B3 coreference results using system mentions.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P04-1018/parts/0-Table-c1.png",
        "Caption": "Table 2: Distribution of dialogue acts in our dataset.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-1101/parts/0-Figure-c3.png",
        "Caption": "Table 6: F1 scores of the local CRF and non-local models on the CMU Seminar Announcements dataset. We also provide the results from Sutton and McCallum (2004) for comparison. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-2037/parts/0-Figure-c2.png",
        "Caption": "Table 2: Resulting sets of entailment rules",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-2003/parts/0-Table-c7.png",
        "Caption": "Figure 2: (a) An example of a DCS tree (written in both the mathematical and graphical notation). Each node is labeled with a predicate, and each edge is labeled with a relation. (b) A DCS tree z with only join relations en- codes a constraint satisfaction problem. (c) The denota- tion of z is the set of consistent values for the root node. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1008/parts/0-Table-c9.png",
        "Caption": "Figure 2 Summariser and VPA Architecture",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1761/parts/0-Figure-c5.png",
        "Caption": "Figure 2: The response of the rhyme search engine.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1704/parts/0-Table-c1.png",
        "Caption": "Figure 4: A toy instance of lattice construction",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1003/parts/0-Table-c2.png",
        "Caption": "Figure 3: extracts from the Akkadian project",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Table-c12.png",
        "Caption": "Figure 1: A translation forest which is the running example throughout this paper. The reference translation is \u201cthe gunman was killed by the police\u201d. (1) Solid hyperedges denote a \u201creference\u201d derivation tree t1 which exactly yields the reference translation. (2) Replacing e3 in t1 with e4 results a competing non-reference derivation t2 , which fails to swap the order of X3,4 . (3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S12-1040/parts/0-Figure-c1.png",
        "Caption": "Fig. 2. Integration of paradigmatic relations \u2013 recall/precision curves.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1058/parts/0-Table-c3.png",
        "Caption": "Figure 1: An example of MOD feature extraction. An oval in the dependency tree denotes a bunsetsu. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PSMPT_n09/parts/0-Figure-c1.png",
        "Caption": "Table 2: Recall (R), Precision (P) and Mean Average Pre- cision (MAP) when also using rules for matching. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/I05-3013/parts/0-Figure-c5.png",
        "Caption": "Figure 5: (A) current configuration for internal node Dk and its associated subtrees (B) first alternative configuration, (C) second alternative configuration. Note that swapping st1, st2 in (A) results in an equivalent tree. Hence, this configuration is excluded. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P01-1027/parts/0-Table-c8.png",
        "Caption": "Table 4: Upper bound for combination. The error reduction (ER) rate is a comparison between the F-score produced by the oracle combination sys- tem and the character-based system (see Tab. 1). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P05-1013/parts/0-Figure-c1.png",
        "Caption": "Fig. 6 Distances found between phrase boundaries with linked modifier words and with parent words",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c10.png",
        "Caption": "Table 3: Results of the French to English system (WMT-2012). The marked system (*) corresponds to the system submitted for manual evaluation. (cs: case-sensitive, ci: case-insensitive) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S13-1039/parts/0-Table-c8.png",
        "Caption": "Figure 6: Opinion PageRank Performance with varying parameter \u00b5 (\u03bb = 0.2) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-1728/parts/0-Figure-c2.png",
        "Caption": "Table 3: F-measure on MUC-6 and MUC-7 test data",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-0513/parts/0-Table-c5.png",
        "Caption": "Table 1: Most frequent BLC\u201320 semantic classes on WordNet 3.0",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1027/parts/0-Table-c1.png",
        "Caption": "Table 3: F1 scores and speed (in sentences per sec.) of SegTagDep on CTB-5c-1 w.r.t. the beam size. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C00-1081/parts/0-Table-c2.png",
        "Caption": "Figure 6: The 10-bipartite clique.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/M98-1003/parts/0-Figure-c10.png",
        "Caption": "Table 3: Plurality language families across 20 clusters. The columns indicate portion of lan- guages in the plurality family, number of lan- guages, and entropy over families. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1059/parts/0-Table-c1.png",
        "Caption": "Figure 13: A Template Network and Two Filler Networks ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1107/parts/0-Table-c1.png",
        "Caption": "Table 1: Corpus statistics in Sighan Bakeoff 2005",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W12-2303/parts/0-Table-c1.png",
        "Caption": "Table 4: Performance of different relation types and major subtypes in the test data",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J06-2001/parts/0-Table-c3.png",
        "Caption": "Table 2: Translation results in lower-case BLEU. CN for confusion network and CF for confusion forest with different vertical (v) and horizontal (h) Markovization order. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2023/parts/0-Figure-c3.png",
        "Caption": "Table 6: Polarity classifier with and without SWSD.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-2606/parts/0-Table-c1.png",
        "Caption": "Table 9: Accuracy of Target Candidate Detection",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E12-2021/parts/0-Table-c1.png",
        "Caption": "Table 2: Distribution of reasons for false positives (incorrect argument extractions) by BInc at K=20. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1141/parts/0-Table-c3.png",
        "Caption": "Figure 4: A narrative chain and its reverse order.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1016/parts/0-Table-c6.png",
        "Caption": "Table 2. Overall results Coverage/Accuracy",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W12-4006/parts/0-Table-c3.png",
        "Caption": "Table 3: Feature templates used for the chunk s := ws ws+1 ... we where ws and we represent the words at the beginning and ending of the target chunk respectively. pi is the part of speech tag of wi and sci is the shallow parse result of wi . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09_csl2013/parts/0-Figure-c4.png",
        "Caption": "Figure 1: EM input for our example sentence. j-values follow each lexical candidate. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-1634/parts/0-Figure-c2.png",
        "Caption": "Table 7: Syncretism Example 1",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Figure-c9.png",
        "Caption": "Figure 3: A complex Turkish-English word alignment (alignment points in gray: EM/PY-U(V ); black: PY- U(S)). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0711/parts/0-Table-c4.png",
        "Caption": "Table 5: Results for Positive and Negative Classes.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-0813/parts/0-Table-c4.png",
        "Caption": "Figure 1: Architecture of Name-aware Machine Translation System.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P04-1018/parts/0-Figure-c1.png",
        "Caption": "Table 5: Show type detection: Using the neural net- work described in Sec. 2 the show type was detected. If there is a number in the word column the word feature is being used. The number indicates how many word/part of speech pairs are in the vocabu- lary additionally to the parts of speech. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P08-1090/parts/0-Figure-c6.png",
        "Caption": "Table 1: Data examined by the two systems for the ATB",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1125/parts/0-Figure-c3.png",
        "Caption": "Table 2: A sample of manually annotated expressions from Disco-En-Gold with their numerical scores (Ns) and coarse scores (Cs). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1107/parts/0-Figure-c1.png",
        "Caption": "Figure 1: Sample dataset (constructed by hand): Finnish verbs, with inflection for person and number.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c11.png",
        "Caption": "Table 6: Search errors [%].",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1140/parts/0-Figure-c1.png",
        "Caption": "Table 9. Comparison with voted cache",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H01-1001/parts/0-Table-c2.png",
        "Caption": "Table 1: Summary of the results obtained by our algorithm in comparison to Word 2007",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3325/parts/0-Table-c1.png",
        "Caption": "Figure 2: A multiword expression in HeiST",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W08-0602/parts/0-Figure-c1.png",
        "Caption": "Table 8: Simple parser vs full parser \u2013 morphological quality. The parsing models were trained on the first 5,000 sentences of the training data, the morphological tagger was trained on the full training set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E12-2021/parts/0-Figure-c5.png",
        "Caption": "Table 2: Illustrative tableau for a simple constraint sys- tem not capturable as a regular relation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2139/parts/0-Table-c3.png",
        "Caption": "Figure 3: Annotation tool for manual judgement of adequacy and fluency of the system output. Translations from 5 randomly selected systems for a randomly selected sentence is presented. No additional information beyond the instructions on this page are given to the judges. The tool tracks and reports annotation speed. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P03-1028/parts/0-Table-c5.png",
        "Caption": "Figure 2. The pseudo code of Algorithm 1.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1121/parts/0-Figure-c3.png",
        "Caption": "Table 3: Effectiveness of post-processing rules",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c21.png",
        "Caption": "Figure 1: Precision and recall for prepositions.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1062/parts/0-Table-c1.png",
        "Caption": "Table 1: Automatically generated training set examples.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P04470/parts/0-Table-c8.png",
        "Caption": "Table 7: # of features.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4916/parts/0-Table-c3.png",
        "Caption": "Figure 4: string insertion operation for right-to-left decoding method. A string e0 was prepended before the partial output string, e, and the first word in e 0 was aligned from f j . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1115/parts/0-Table-c5.png",
        "Caption": "Table 4. Results of 3000 sentences",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1127/parts/0-Table-c2.png",
        "Caption": "Table 2: Parameter values used in the evaluation.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Table-c4.png",
        "Caption": "Figure 2: Distributions of six most frequent relations in intra-sentential and multi-sentential parsing scenarios. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00-1025/parts/0-Figure-c5.png",
        "Caption": "Figure 1: Rule expansion with minimal context (Example 3)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2101/parts/0-Figure-c9.png",
        "Caption": "Figure 2: F1 scores (in %) of SegTagDep on CTB- 5c-1 w.r.t. the training epoch (x-axis) and parsing feature weights (in legend). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Figure-c4.png",
        "Caption": "Table 3: Translation Candidates for \u8e81\u9b31\u75c5 (manic- depression) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2230/parts/0-Table-c1.png",
        "Caption": "Figure 1: WSI and WSD Pipeline",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0707/parts/0-Table-c6.png",
        "Caption": "Table 3: F-measure after successive addition of each global feature group ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1012/parts/0-Table-c2.png",
        "Caption": "Table 1: Sense-tagged corpus for the example in Figure 3",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-3208/parts/0-Table-c5.png",
        "Caption": "Fig. 9 BLEU difference curves of four context-informed models using TRIBL",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1074/parts/0-Table-c3.png",
        "Caption": "Table 2 Feature templates of a typical character-based word segmentor. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c5.png",
        "Caption": "Table 5: System for the task of relation classifica- tion. The two classes are INR and COG, and we evaluate using accuracy (Acc.). The proportion of INR relations in training and test set is 49.7% and 49.63% respectively. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c16.png",
        "Caption": "Table 2: Results of the baseline model: best guess",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N15-1071/parts/0-Table-c7.png",
        "Caption": "Table 9 Select models trained using the Easy-First Parser. Statistical significance tested only for CORE 12. . . models on predicted input: significance of the Easy-First Parser CORE 12 baseline model against its MaltParser counterpart; and significance of all other CORE 12+. . . models against the Easy-First Parser CORE 12 baseline model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Table-c1.png",
        "Caption": "Table 1: Single-threaded speed and memory use on the perplexity task. The P ROBING model is fastest by a sub- stantial margin but generally uses more memory. T RIE is faster than competing packages and uses less memory than non-lossy competitors. The timing basis for Queries/ms in- cludes kernel and user time but excludes loading time; we also subtracted time to run a program that just reads the query file. Peak virtual memory is reported; final resident memory is similar except for BerkeleyLM. We tried both aggressive reading and lazy memory mapping where appli- cable, but results were much the same. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-3012/parts/0-Table-c2.png",
        "Caption": "Table 1: Task characteristics: #sentences in Train/Dev, # of features, and metrics used. Our MT models are trained with standard phrase-based Moses software (Koehn and others, 2007), with IBM M4 alignments, 4gram SRILM, lexical ordering for PubMed and distance ordering for the NIST system. The decoder generates 50-best lists each iteration. We use SVMRank (Joachims, 2006) as opti- mization subroutine for PRO, which efficiently handle all pairwise samples without the need for sampling. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0707/parts/0-Table-c5.png",
        "Caption": "Table 1: Basic Features for CRF-based Segmenter",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2023/parts/0-Table-c3.png",
        "Caption": "Figure 6 Distribution of probabilities given by the classi\ufb01er over all node pairs of the test-set graphs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1667/parts/0-Table-c2.png",
        "Caption": "Table 4: Different Context Window Size Setting",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N10-1019/parts/0-Figure-c2.png",
        "Caption": "Table 7: Russian to English machine translation system evaluated on WMT-2012 and WMT-2013. Human evaluation in WMT13 is performed on the system trained using the original corpus with TA- GIZA++ for alignment (marked with *). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1083/parts/0-Table-c6.png",
        "Caption": "Figure 2: Example of a word lattice",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1007/parts/0-Table-c2.png",
        "Caption": "Figure 7 Summaries Recall and Precision",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S13-1039/parts/0-Table-c7.png",
        "Caption": "Table 3: The 10 best languages for the verb component of BANNARD using LCS. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2605/parts/0-Table-c6.png",
        "Caption": "Table 1: Possible relations appearing on the edges of a DCS tree. Here, j, j 0 \u2208 {1, 2, . . . } and i \u2208 {1, 2, . . . }\u2217 . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-4223/parts/0-Figure-c1.png",
        "Caption": "Table 4: F1s of some individual ILC classifiers and the overall multiclassifier accuracy (180 classes on PB and 133 on FN). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1634/parts/0-Figure-c6.png",
        "Caption": "Table 2 Key definitions for our model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P05-1053/parts/0-Table-c6.png",
        "Caption": "Figure 1: Three kinds of tree kernels.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1040/parts/0-Figure-c2.png",
        "Caption": "Figure 2: A pair of comparable sentences.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Table-c2.png",
        "Caption": "Figure 1: Aligned parsed sentence",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2230/parts/0-Table-c6.png",
        "Caption": "Figure 2: Dependency representation of example (2) from Talbanken05. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1126/parts/0-Figure-c2.png",
        "Caption": "Table 5 Another example of some discovered paraphrases.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1141/parts/0-Table-c3.png",
        "Caption": "Table 3. Rank of correct translation for period Dec 01 \u2013 Dec 15 and Dec 16 \u2013 Dec 31. \u2018Cont. rank\u2019 is the context rank, \u2018Trans. Rank\u2019 is the transliteration rank. \u2018NA\u2019 means the word cannot be transliterated. \u2018insuff\u2019 means the correct translation appears less than 10 times in the English part of the comparable corpus. \u2018comm\u2019 means the correct translation is a word ap- pearing in the dictionary we used or is a stop word. \u2018phrase\u2019 means the correct translation contains multi- ple English words. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1068/parts/0-Table-c5.png",
        "Caption": "Table 1. Incremental Improvement from          Self-training (English) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1159/parts/0-Table-c3.png",
        "Caption": "Table 2 Sample analysis of an English sentence. Input: Do we have to reserve rooms?. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S12-1040/parts/0-Figure-c2.png",
        "Caption": "Table 6. The performance on the set of unknown",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W07-0722/parts/0-Table-c2.png",
        "Caption": "Figure 2: A tiered graphic representing the three different SRL model configurations. The baseline system is described in the bottom (c & d), the separate panels highlighting the independent predictions of this model: sense labels are assigned in an entirely separate process from argument prediction. Pruning in the model takes place primarily in this tier, since we observe true predicates we only instantiate over these indices. The middle tier (b.) illustrates the syntactic representation layer, and the connective factors between syntax and SRL. In the observed syntax model the Link variables are clamped to their correct values, with no need for a factor to coordinate them to form a valid tree. Finally, the hidden model comprises all layers, including a combinatorial syntactic constraint (a.) over syntactic variables. In this scenario all labels in (b.) are hidden at both training and test time. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1131/parts/0-Table-c3.png",
        "Caption": "Table 2: Examples of unigram and bigram features extracted from Figure 1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PS15684_w09/parts/0-Figure-c3.png",
        "Caption": "Table 5 Models with lexical morpho-semantic features. Top: Adding all lexical features together on top of the CORE 12 baseline. Center: Adding each feature separately. Bottom: Greedily adding best features from previous part, on predicted input. Statistical significance tested only on predicted (non-gold) input, against the CORE 12 baseline. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1058/parts/0-Table-c4.png",
        "Caption": "Figure 4: Example of packed representation of semi-CRFs. The states that have the same end po- sition and prev-entity label are packed. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P04-1036/parts/0-Table-c2.png",
        "Caption": "Figure 9: Chunk - Length and count of glue rules used decoding test set ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P08-1028/parts/0-Table-c1.png",
        "Caption": "Figure 9: Example error cases, with associated frequencies, illustrating system output and gold standard references. 5% of the cases were miscellaneous or otherwise difficult to categorize. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P01-1005/parts/0-Figure-c4.png",
        "Caption": "Figure 1: Topic transfer in bilingual LSA model.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D10-1042/parts/0-Figure-c6.png",
        "Caption": "Fig. 3. Macro-accuracy for cross-lingual bootstrapping.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W10-3212/parts/0-Table-c2.png",
        "Caption": "Table 3: Average results for 35 verbs",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1033/parts/0-Figure-c2.png",
        "Caption": "Table 3 Precision of existing and proposed approaches. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P05-1051/parts/0-Table-c1.png",
        "Caption": "Table 1: Baseline performance.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09_qwn/parts/0-Table-c1.png",
        "Caption": "Table 1: Results on the RTE-1 Test Set.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-3008/parts/0-Table-c2.png",
        "Caption": "Table 2: Results for GigaPairs (all numbers in %); re- sults that significantly differ from Full are marked with asterisks (* p<0.05; ** p<0.01). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-1006/parts/0-Table-c2.png",
        "Caption": "Table 3: Results of the mapping algorithm.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-2122/parts/0-Figure-c1.png",
        "Caption": "Table 3: Best observed performance of RF, SVM and GIZA++ and Levenshtein Distance",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W10-4138/parts/0-Table-c1.png",
        "Caption": "Table 5: Translation results for English-German",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1073/parts/0-Figure-c5.png",
        "Caption": "Figure 1: Bell tree representation for three mentions: numbers in [] denote a partial entity. In-focus entities are marked on the solid arrows, and active mentions are marked by *. Solid arrows signify that a mention is linked with an in-focus partial entity while dashed arrows indicate starting of a new entity. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1110/parts/0-Figure-c3.png",
        "Caption": "Table 1: Properties of the manually aligned corpus",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-0910/parts/0-Figure-c1.png",
        "Caption": "Figure 1: An underspecified discourse structure and its five configurations",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PS15684_w09/parts/0-Figure-c6.png",
        "Caption": "Table 5: System for the task of relation classifica- tion. The two classes are INR and COG, and we evaluate using accuracy (Acc.). The proportion of INR relations in training and test set is 49.7% and 49.63% respectively. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-0802/parts/0-Figure-c3.png",
        "Caption": "Table 3: Multi-threaded time and memory consumption of Moses translating 3003 sentences on eight cores. Our code supports lazy memory mapping (-L) and prefault- ing (-P) with MAP POPULATE, the default. IRST is not threadsafe. Time for Moses itself to load, including load- ing the language model and phrase table, is included. Along with locking and background kernel operations such as prefaulting, this explains why wall time is not one-eighth that of the single-threaded case. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1035/parts/0-Figure-c1.png",
        "Caption": "Table 5: Comparison of the performance of previous methods on ACE RDC task.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c11.png",
        "Caption": "Table 8 The comparison between NPYLM and ESA. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PS15684_w09/parts/0-Table-c3.png",
        "Caption": "Table 2: Case frame of \u201chaken (dispatch).\u201d",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c13.png",
        "Caption": "Figure 1: Example of LMR Tagging.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-3209/parts/0-Figure-c1.png",
        "Caption": "Table 3 Number of learned splits per NT-category after five split-merge cycles. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P05-1053/parts/0-Table-c5.png",
        "Caption": "Figure 1: A typed narrative chain. The four top arguments are given. The ordering O is not shown. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C08-1107/parts/0-Figure-c1.png",
        "Caption": "Figure 1: A simplified version in Foma source code of the regular expressions and transducers used to bypass several sources of distortion within the DTs in order to parse variations of unseen input DTs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N07-3010/parts/0-Figure-c1.png",
        "Caption": "Figure 6: Average number of Pareto points",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1038/parts/0-Table-c1.png",
        "Caption": "Figure 1: Example of the effects of OOV processing for German\u2192English",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1115/parts/0-Figure-c2.png",
        "Caption": "Table 3: Lexical variations creating new rules based on DIRT rule X face threat of Y \u2192 X at risk of Y ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C16-1060/parts/0-Table-c1.png",
        "Caption": "Table 2 The similarity score features used to represent pairs of templates. The columns specify the corpus over which the similarity score was computed, the template representation, the similarity measure employed, and the feature representation (as described in Section 4.1). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1015/parts/0-Table-c1.png",
        "Caption": "Table 3: Performance comparison of two SLU systems through weakly supervised and super- vised training on the three test sets (TER: Topic Error Rate; SER: Slot Error Rate) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P22324_w09/parts/0-Table-c2.png",
        "Caption": "Table 5: Evaluating Word Alignment Accuracies and Machine Translation Qualities for BiTAM Models, IBM Models, HMMs, and boosted BiTAMs using all the training data listed in Table. 1. Other experimental conditions are similar to Table. 4. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1003/parts/0-Figure-c2.png",
        "Caption": "Table 6: Selected entries from the confusion matrix for parts of speech in English with F-scores for the left-hand- side category. DT = determiner; IN = preposition or sub- ordinating conjunction; JJ = adjective; JJR = compara- tive adjective; NN = singular or mass noun; NNS = plural noun; POS = possessive clitic; RB = adverb; RBR = com- parative adverb; RP = particle; UH = interjection; VB = base form verb; VBD = past tense verb; VBG = gerund or present participle; VBN = past participle; VBP = present tense verb, not 3rd person singular; VBZ = present tense verb, 3rd person singular. We use \u03b1* to denote the set of categories with \u03b1 as a prefix. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3812/parts/0-Table-c4.png",
        "Caption": "Table 1: Comparison of average per-document ter- comTER with invWER on the EVAL07 GALE Newswire (\u201cNW\u201d) and Weblogs (\u201cWB\u201d) data sets. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S13-1002/parts/0-Table-c2.png",
        "Caption": "Table 2: Contribution of different features over 43          relation subtypes in the test data ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1004/parts/0-Figure-c3.png",
        "Caption": "Figure 4: Smoothed histograms of the Jensen-Shannon",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W07-0909/parts/0-Figure-c1.png",
        "Caption": "Table 4. Some of the top selected features by Infor-                    mation Gain ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pproc2014_n09/parts/0-Table-c4.png",
        "Caption": "Table 2: Results of UniGraph, BiGraph, and Bi- Graph*. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PE2006_p00/parts/0-Figure-c4.png",
        "Caption": "Table 8: Detailed DIFF results",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4917/parts/0-Figure-c4.png",
        "Caption": "Table 8: ROUGE-W in empirical approach",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1060/parts/0-Table-c3.png",
        "Caption": "Table 1: Types of features extracted for edge e from h to n",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N10-1068/parts/0-Table-c1.png",
        "Caption": "Figure 5: A filter RTG corresponding to Ex. 2",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1111/parts/0-Table-c5.png",
        "Caption": "Figure 3: Boxer output for Shared Task Text 2",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1033/parts/0-Table-c8.png",
        "Caption": "Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example. Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological features, in the FEATS CoNLL format. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1125/parts/0-Table-c4.png",
        "Caption": "Table 2: Single-threaded time and memory consumption of Moses translating 3003 sentences. Where applicable, models were loaded with lazy memory mapping (-L), prefaulting (-P), and normal reading (-R); results differ by at most than 0.6 minute. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D08-1032/parts/0-Table-c9.png",
        "Caption": "Figure 1: Dependency graph for Czech sentence from the Prague Dependency Treebank1",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1033/parts/0-Figure-c1.png",
        "Caption": "Table 1: The pool of features for all languages.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C04-1194/parts/0-Table-c3.png",
        "Caption": "Table 6 The results brought by different maximum lengths. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1017/parts/0-Table-c3.png",
        "Caption": "Table 7. System Performance Comparison.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-3031/parts/0-Table-c4.png",
        "Caption": "Figure 1: Example of Lattice Used in the Markov Model-Based Method",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1110/parts/0-Table-c3.png",
        "Caption": "Table 10: Comparison of our approach with the state-of-art systems",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D13-1031/parts/0-Table-c10.png",
        "Caption": "Table 8: Comparison with other systems",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-1017/parts/0-Figure-c4.png",
        "Caption": "Table 10 Occurrences of error types for the best other-anaphora algorithm algoWebv4 . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Table-c7.png",
        "Caption": "Figure 2: Semantic structure of the first sequence",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-1037/parts/0-Figure-c1.png",
        "Caption": "Figure 2: Bidirectional checking of entailment relation (\u2192) of p1 \u2192 p2 and p2 \u2192 p1 . p1 is \u201creduces bone mass\u201d in s1 and p2 is \u201cdecreases the quantity of bone\u201d in s2 . p1 and p2 are exchanged between s1 and s2 to generate corresponding paraphrased sentences s01 and s02 . p1 \u2192 p2 (p2 \u2192 p1 ) is verified if s1 \u2192 s01 (s2 \u2192 s02 ) holds. In this case, both of them hold. English is used for ease of explanation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1067/parts/0-Figure-c3.png",
        "Caption": "Figure 4: Devoicing transducer compiled through a rule.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-2123/parts/0-Figure-c2.png",
        "Caption": "Fig. 2. Multilingual bootstrapping.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1119/parts/0-Table-c2.png",
        "Caption": "Figure 9 Illustration of the IBM-style reordering constraint. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P101121_p07/parts/0-Figure-c1.png",
        "Caption": "Table 1: Phrase pairs extracted from a document pair               with an economic topic ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1009/parts/0-Table-c3.png",
        "Caption": "Table 7: Results for ensemble classifier.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1013/parts/0-Figure-c1.png",
        "Caption": "Table 6: Summary of supersense tagging accuracies",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4002/parts/0-Figure-c1.png",
        "Caption": "Figure 1: Examples of the semantic role features",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N07-1024/parts/0-Table-c12.png",
        "Caption": "Table 3. Pseudo-code to extract UW",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1757/parts/0-Figure-c2.png",
        "Caption": "Table 1. Comparison with other approaches",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1141/parts/0-Figure-c1.png",
        "Caption": "  Table 1. Data sets used for our alignment quality experiments. The total number of sentences in the respective corpora are given along with the number of sentences and    gold-standard (S)ure and (P)ossible alignment links in the corresponding test set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0401/parts/0-Figure-c1.png",
        "Caption": "Table 3 \u2013 Pk for C99 corpus",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W05-1518/parts/0-Table-c7.png",
        "Caption": "Table 3 Resolution of ambiguity on the Verbmobil corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1015/parts/0-Table-c3.png",
        "Caption": "Figure 5 A comparison between ILP-Global and Greedy-Global. Parts A1\u2013A3 depict the incremental progress of Greedy Global for a fragment of the headache graph. Part B depicts the corresponding fragment in ILP-Global. Nodes surrounded by a bold oval shape are strongly connected components. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1111/parts/0-Table-c7.png",
        "Caption": "Table 4 DP-TSG notation. For consistency, we largely follow the notation of Liang, Jordan, and Klein (2010). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-1025/parts/0-Table-c6.png",
        "Caption": "Figure 2: An iterative algorithm for minimizing our ob- jective in Eq. (7). For simplicity we assume that all the weights \u03b1i and \u03bb are equal to one. It can be shown that the objective monotonically decreases in every iteration. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2087/parts/0-Table-c3.png",
        "Caption": "Figure 1: Example MERT values along one coordi- nate, first unregularized. When regularized with `2 , the piecewise constant function becomes piecewise quadratic. When using `0 , the function remains piecewise constant with a point discontinuity at 0. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1007/parts/0-Table-c4.png",
        "Caption": "Figure 7 String-to-tree configurations; each is associated with a feature that counts its occurrences in a derivation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H01-1001/parts/0-Table-c1.png",
        "Caption": "Table 3: Performance comparison of two SLU systems through weakly supervised and super- vised training on the three test sets (TER: Topic Error Rate; SER: Slot Error Rate) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1113/parts/0-Table-c5.png",
        "Caption": "Table 1: Results on Penn (English) Treebank, Wall Street Journal, sentences with 100 words or fewer.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1125/parts/0-Table-c1.png",
        "Caption": "Table 13 Macro-average recall, precision, and F1 on the development set and test set using the parameters that maximize F1 of the learned edges over the development set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0311/parts/0-Table-c2.png",
        "Caption": "Figure 6: Contribution of the static cache on the first            sentence of each test document           (i.e. with empty dynamic cache) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-0304/parts/0-Table-c3.png",
        "Caption": "Figure 4: Learning curve of SuperSense on SE3",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1667/parts/0-Table-c6.png",
        "Caption": "Figure 3: Result of synthetic data learning experiment for MERT and PRO, with and without added noise. As the dimensionality increases MERT is unable to learn the original weights but PRO still performs adequately. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J10-3003/parts/0-Figure-c3.png",
        "Caption": "Figure 2: Inuktitut: Parimunngaujumaniralauqsimanngittunga = \u201cI never said I wanted to go to Paris\u201d ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00-1025/parts/0-Figure-c9.png",
        "Caption": "Table 3: Experiment 2: Results by training set size, \u03b8 = 1.0 ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1133/parts/0-Table-c6.png",
        "Caption": "Figure 1: EM input for our example sentence. j-values follow each lexical candidate. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1023/parts/0-Table-c3.png",
        "Caption": "Figure 1. The Lattice of the 8 Patterns.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P101167/parts/0-Figure-c3.png",
        "Caption": "Figure 1: Parse tree binarization",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2164/parts/0-Figure-c4.png",
        "Caption": "Fig. 3. Algorithm for Fuzzy Agglomerative Clustering based on verbs.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1107/parts/0-Table-c3.png",
        "Caption": "Figure 4: Compilation of a Regular Expression into an fst that Maps between Two Regular Languages ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1006/parts/0-Table-c1.png",
        "Caption": "Figure 1: Example MERT values along one coordi- nate, first unregularized. When regularized with `2 , the piecewise constant function becomes piecewise quadratic. When using `0 , the function remains piecewise constant with a point discontinuity at 0. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1761/parts/0-Table-c3.png",
        "Caption": "Table 7: List of results in Sighan Bakeoff 2005",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-2611/parts/0-Figure-c2.png",
        "Caption": "Table 3: Values obtained for Precision, Recall and F- score with method 1 by changing the threshold frequency of the correspondences and applying a post-filter. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1004/parts/0-Table-c2.png",
        "Caption": "Figure 2: Graphical model of synonym pair gen- erative process ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Table-c12.png",
        "Caption": "Table 10 Occurrences of error types for the best other-anaphora algorithm algoWebv4 . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Plex_p00/parts/0-Figure-c3.png",
        "Caption": "Table 4: Accuracy for different EM-weighted probability interpolation models for SENSEVAL 2 ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1133/parts/0-Table-c3.png",
        "Caption": "Figure 1: Example of a German noun phrase. First and last word agree in number, gender, and case value.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-2124/parts/0-Table-c2.png",
        "Caption": "Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing. For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1059/parts/0-Figure-c1.png",
        "Caption": "Figure 4 The path of Selection. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1138/parts/0-Figure-c4.png",
        "Caption": "Table 4 Performance on Internet data",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00-1025/parts/0-Figure-c3.png",
        "Caption": "Table 2: Chinese character usage in 3 corpora. The   numbers in brackets indicate the percentage of  characters that are shared by at least 2 corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2135/parts/0-Table-c3.png",
        "Caption": "Figure 10: Analysis of NP coordination, in a distributive (left) and a collective interpretation (right).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P11-1061/parts/0-Table-c2.png",
        "Caption": "Table 1: Bilingual training corpus, recognition lex- icon and translation lexicon (PM = punctuation mark). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/M98-1003/parts/0-Figure-c4.png",
        "Caption": "Figure 6: Derivation with soft syntax model",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Figure-c4.png",
        "Caption": "Figure 1: Proposed discourse structures for Ex. 4: (a) In terms of informational relations; (b) in terms of inten- tional relations ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2228/parts/0-Table-c2.png",
        "Caption": "Table 5: POS annotations of a couplet, i.e., a pair of two verses, in a classical Chinese poem. See     Table 1 for the meaning of the POS tags. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6201/parts/0-Table-c5.png",
        "Caption": "Table 1: Key notation.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2052/parts/0-Figure-c1.png",
        "Caption": "Figure 2. A derivation for Mary likes Susan",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1016/parts/0-Table-c3.png",
        "Caption": "Table 13: An annotation example for the necessity of species information ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3812/parts/0-Figure-c5.png",
        "Caption": "Figure 5: Example of similar document pairs.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1113/parts/0-Table-c1.png",
        "Caption": "Figure 1: Proposed method: data flow.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1099/parts/0-Table-c2.png",
        "Caption": "Table 2: Best results: For English, name lists are used. For German, part-of-speech tags are used ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1005/parts/0-Figure-c6.png",
        "Caption": "Table 3: Results of MET2 under different configurations",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P283_w09/parts/0-Table-c5.png",
        "Caption": "Table 2. Results of 1000 sentences",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W11-2123/parts/0-Table-c2.png",
        "Caption": "Table 5: Language-dependent lexical features. A word list can be collected to encode different ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Table-c3.png",
        "Caption": "Table 3: Evaluation of the GUITAR improvement - summarization ratio: 15%.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1004/parts/0-Table-c1.png",
        "Caption": "Figure 4: Learning Curve on RIBES: comparing single- objective optimization and PMO. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N13-1118/parts/0-Figure-c4.png",
        "Caption": "Table 2: Results on the Arabic GALE Phase 2 evaluation set with one reference translation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W11-2123/parts/0-Table-c4.png",
        "Caption": "Table 3: MT06 Dev. Optimization & Test Set Spearman Correlation Results",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Figure-c8.png",
        "Caption": "Fig. 3. Lexicalized training example (POS of target headword is not explicitly given).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-1025/parts/0-Table-c6.png",
        "Caption": "Table 7: Parser performance on WSJ;23, supervised adaptation. All models use Brown;T,H as the out-of-domain treebank. Baseline models are built from the fractions of WSJ;2-21, with no out-of-domain treebank. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1046/parts/0-Table-c7.png",
        "Caption": "Figure 4: Example of Morph-Related Heteroge- neous Information Network ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3812/parts/0-Table-c2.png",
        "Caption": "Table 1. The count of the types of anaphora per corpus.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1761/parts/0-Figure-c1.png",
        "Caption": "Table 2: The semantic roles of cases beside C-1 verb cluster ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6213/parts/0-Figure-c1.png",
        "Caption": "Figure 4 Distribution of domain labels of predominant senses for 38 polysemous nouns ranked using the SPORTS and FINANCE corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1064/parts/0-Figure-c1.png",
        "Caption": "Table 1: Syntactic Seeding Heuristics",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1101/parts/0-Table-c2.png",
        "Caption": "Table 2. The result of pre-processing",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1050/parts/0-Figure-c2.png",
        "Caption": "Table 6 Evaluation of Feature and Their Combinations",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/I05-3013/parts/0-Figure-c7.png",
        "Caption": "Table 2: Topic words extracted from target-side doc-                       uments ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Figure-c3.png",
        "Caption": "Fig. 10 Dutch-to-English Learning curves (left-hand side graphs) and difference curves (right-hand side graphs) comparing the Moses baseline against four context-informed models (PR, OE, POS\u00b12 and Word\u00b12). These curves are plotted with scores obtained using three evaluation metrics: BLEU (top), METEOR (centre) and TER (bottom) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1143/parts/0-Figure-c7.png",
        "Caption": "Figure 1 (a) A Chinese sentence. Slashes indicate word boundaries. (b) An output of our word segmentation system. Square brackets indicate word boundaries. + indicates a morpheme boundary. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Table-c5.png",
        "Caption": "Figure 1: Similarity graph after its sparsification",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S12-1023/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Plate diagram of our model.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4904/parts/0-Table-c1.png",
        "Caption": "Table 4: Sizes of bilingual dictionaries induced by differ- ent alignment methods. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1761/parts/0-Figure-c6.png",
        "Caption": "Figure 3: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the second order dataset ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1757/parts/0-Table-c1.png",
        "Caption": "Figure 5: Macro-accuracy for multilingual bootstrapping (versus cross-lingual framework) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06128/parts/0-Figure-c1.png",
        "Caption": "Table 2: Overall performance of the 3 systems",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N07-1015/parts/0-Figure-c1.png",
        "Caption": "Table 3:     Accuracies (%) for Leave-One- Out (LOO) and Only-One Word-Extraction-Rule Evaluation. none includes all words and serves for comparison. Important words reduce accuracy for LOO, but rank high when used as only rule. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-2123/parts/0-Table-c2.png",
        "Caption": "Figure 1: The directional matching relationships between a hypothesis (h), an entailment rule (r) and a text (t) in the Contextual Preferences framework. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0815/parts/0-Figure-c2.png",
        "Caption": "Table 7 Models with re-engineered DET and PERSON inflectional features. Statistical significance tested only on predicted input, against the CORE 12 baseline. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1086/parts/0-Table-c1.png",
        "Caption": "Fig. 1 Examples of TERp alignment output. In each example, R, H and H  denote the reference, the original hypothesis and the hypothesis after shifting respectively. Shifted words are bolded and other edits are in [brackets]. Number of edits shown: TERp (TER) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1101/parts/0-Table-c3.png",
        "Caption": "Table 4: Results of task#17",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Figure-c2.png",
        "Caption": "Figure 2. F1-measure with \uf062 in [0,1]",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1080/parts/0-Table-c3.png",
        "Caption": "Table 3 Coreference factors for name recognition",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-2123/parts/0-Table-c4.png",
        "Caption": "Table 5: Accuracies of single-feature classifiers.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-1045/parts/0-Table-c1.png",
        "Caption": "Table 3: Segmentation performance on words that have the same final suffix as their preceding words. The F1 scores are computed based on all boundaries within the words, but the accuracies are obtained using only the final suffixes. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W01-0712/parts/0-Table-c3.png",
        "Caption": "Figure 1: Effect of in-domain monolingual corpus size on translation quality. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D12-1094/parts/0-Figure-c1.png",
        "Caption": "Table 3: HRGs against recent methods & baselines.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D11-1076/parts/0-Figure-c4.png",
        "Caption": "Fig. 1. The polarity classification (positive and negative) based on product aspect framework",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P09-1098/parts/0-Table-c1.png",
        "Caption": "Figure 5: Dendrogram of the participants cluster based on their feedback profile ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1104/parts/0-Table-c1.png",
        "Caption": "Figure 3: Precision curves of paraphrase extraction.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2135/parts/0-Table-c1.png",
        "Caption": "Table 1: Experimental results in terms of BLEU scores measured on the newstest2011 and newstest2012. For newstest2012, the scores are provided by the organizers. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D08-1032/parts/0-Table-c5.png",
        "Caption": "Table 5: BLEU scores for the French-to-English translation task measured on nt10 with systems tuned on development sets selected according to their original language (adapted tuning). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W05-1518/parts/0-Table-c1.png",
        "Caption": "Table 3: Comparing our method with the state-of-the-art CWS systems.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0206/parts/0-Figure-c1.png",
        "Caption": "Table 2: F-measure on SIGHAN bakeoff 2. SIGHAN best: best scores SIGHAN reported on the four corpus, cited from Zhang and Clark (2007). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2167/parts/0-Table-c1.png",
        "Caption": "Figure 2: (a) An example of a DCS tree (written in both the mathematical and graphical notation). Each node is labeled with a predicate, and each edge is labeled with a relation. (b) A DCS tree z with only join relations en- codes a constraint satisfaction problem. (c) The denota- tion of z is the set of consistent values for the root node. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N03-1027/parts/0-Table-c4.png",
        "Caption": "Table 4. Some of the top selected features by Infor-                    mation Gain ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2147/parts/0-Table-c1.png",
        "Caption": "Table 4. Examples of the top-3 candidates in the       transliteration of English \u2013 Chinese ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Figure-c9.png",
        "Caption": "Figure 10 Lattice dependency parsing using an arc-factored dependency model. Lone indices like p and i denote nodes in the lattice, and an ordered pair like (i, j) denotes the lattice edge from node i to node j. S TART is the single start node in the lattice and F INAL is a set of final nodes. We use edgeScore(i, j) to denote the model score of crossing lattice edge (i, j), which only includes the phrase-based features h 0 . We use arcScore((i, j), (l, m)) to denote the score of building the dependency arc from lattice edge (i, j) to its parent (l, m); arcScore only includes the QPD features h 00 . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-2032/parts/0-Table-c2.png",
        "Caption": "Table 2: Features Used for Initial Distribution",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S12-1023/parts/0-Table-c2.png",
        "Caption": "Table 2: MWE acquisition applied to lexicography",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1111/parts/0-Table-c4.png",
        "Caption": "Table 6: LO cosine sentence configuration scores",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-0813/parts/0-Table-c1.png",
        "Caption": "Table 2: Results of semantic classification.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1059/parts/0-Table-c7.png",
        "Caption": "Table 2: New Verb Classes",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-1001/parts/0-Figure-c1.png",
        "Caption": "Table 4. Performance with SVM trained on a fraction of adj. It shows 5 fold cross validation results.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1016/parts/0-Table-c4.png",
        "Caption": "Figure 2: A multiword expression in HeiST",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1107/parts/0-Figure-c4.png",
        "Caption": "Figure 1: Illustration of dictionary based segmenta- tion finite state transducer ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N12-1006/parts/0-Table-c3.png",
        "Caption": "Figure 3: Objective values for the different mappings used in our experiments for four languages. Note that the goal of the optimization procedure is to minimize the objective value. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1004/parts/0-Table-c10.png",
        "Caption": "Table 1: Types of features extracted for edge e from h to n",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H01-1001/parts/0-Figure-c1.png",
        "Caption": "Figure 6: Derivation with soft syntax model",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W13-2501/parts/0-Figure-c1.png",
        "Caption": "Table 17 WSD using predominant senses, training, and testing on all domain combinations (automatically classified corpora). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1001/parts/0-Figure-c5.png",
        "Caption": "Figure 12: The Malay fst After the Application of Compile-Replace to the Lower-Side Language",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4011/parts/0-Figure-c4.png",
        "Caption": "Figure 5: oscillating states in matrix CW for an unweighted graph ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1060/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Illustration on temporality",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1063/parts/0-Table-c8.png",
        "Caption": "Table 5: Segmentation performance presented in previous work and of our combination model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2605/parts/0-Figure-c1.png",
        "Caption": "Table 5: Results of ablation experiments.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1016/parts/0-Table-c9.png",
        "Caption": "Figure 1: Network after pair-wise TER alignment.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6212/parts/0-Table-c1.png",
        "Caption": "Table 3: Accuracy scores for WSJ-PTB converted with head rules of Yamada and Matsumoto (2003) and labeling rules of Nivre (2006). Best dev setting: k = 3, \u03b1 = 0.4. Results marked with \u2020 use additional information sources and are not directly comparable to the others. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1059/parts/0-Figure-c2.png",
        "Caption": "Table 2: TercomTERs of invWER-oracles and (in paren- theses) oracle BLEU scores of confusion networks gen- erated with tercom and ITG alignments. The best results per row are shown in bold. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00-1025/parts/0-Figure-c4.png",
        "Caption": "Table 13 Evaluating predominant sense information for polysemous nouns on the Senseval-2 all-words task data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1084/parts/0-Figure-c6.png",
        "Caption": "Table 4: Gross statistics for several different treebanks. Test set OOV rate is computed using the following splits: ATB (Chiang et al., 2006); CTB6 (Huang and Harper, 2009); Ne- gra (Dubey and Keller, 2003); English, sections 2-21 (train) and section 23 (test). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3812/parts/0-Table-c1.png",
        "Caption": "Figure 4: Learning Curve on RIBES: comparing single- objective optimization and PMO. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-3114/parts/0-Figure-c3.png",
        "Caption": "Figure 4: Two dendrograms for the graph in Figure 3.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1081/parts/0-Table-c2.png",
        "Caption": "Table 3: Results for the non-pronoun resolution",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1072/parts/0-Table-c4.png",
        "Caption": "Fig. 1. The polarity classification (positive and negative) based on product aspect framework",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2230/parts/0-Table-c3.png",
        "Caption": "Table 3: Translation results for English-French",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1127/parts/0-Figure-c7.png",
        "Caption": "Table 3: Results for different user simulations. Numbers give % reductions in keystrokes. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1009/parts/0-Table-c2.png",
        "Caption": "Figure 2: The response of the rhyme search engine.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2023/parts/0-Table-c2.png",
        "Caption": "Table 18 Results on English-to-Dutch translation employing homogeneous features",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C04-1131/parts/0-Table-c1.png",
        "Caption": "Table 4: Results of different systems for pronoun resolution on MUC-6 and MUC-7 (*Here we only list backward feature assigner for pronominal candidates. In RealResolve-1 to RealResolve-4, the backward features for non-pronominal candidates are all found by DTnon\u2212pron .) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1038/parts/0-Table-c3.png",
        "Caption": "Table 6 Evaluation of Feature and Their Combinations",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1068/parts/0-Table-c6.png",
        "Caption": "Table 2: Overall accuracy of maximum entropy sys- tem using different subsets of features for Penn Chi- nese Treebank words (manually segmented, part-of- speech-tagged, parsed). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1012/parts/0-Figure-c1.png",
        "Caption": "Figure 4: A narrative chain and its reverse order.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C02-1033/parts/0-Table-c3.png",
        "Caption": "Table 16: Arabic Order-Free Structure",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1757/parts/0-Table-c2.png",
        "Caption": "Table 2 Estimation of model parameters. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-1004/parts/0-Figure-c1.png",
        "Caption": "Table 3: Features for \u2018Astronomer Edwin Hubble was born in Marshfield, Missouri\u2019.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W12-2303/parts/0-Figure-c1.png",
        "Caption": "Figure 2: Word alignment based translation model P(J, A|E) (IBM Model 4) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-2055/parts/0-Figure-c1.png",
        "Caption": "Figure 7: Percent of query language documents for which",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P08-1090/parts/0-Figure-c2.png",
        "Caption": "Table 7 Dev-set results of using the agreement-filter on top of the lexicon-enhanced lattice parser (parser does both segmentation and parsing). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P08-1043/parts/0-Figure-c1.png",
        "Caption": "Table 2: Filters applied to candidate pair (H, S)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-3144/parts/0-Table-c3.png",
        "Caption": "Table 1 Contingency table for the children of  canine  in the subject position of run. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Figure-c8.png",
        "Caption": "Table 7: Vocabulary size of NIST task (40K)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/S12-1023/parts/0-Table-c6.png",
        "Caption": "Table 1: Symmetry of window size",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1074/parts/0-Table-c5.png",
        "Caption": "Table 3: Domain specific results",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Figure-c5.png",
        "Caption": "Table 2: Word length statistics on test sets.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-1017/parts/0-Table-c6.png",
        "Caption": "Table 1: Statistics on the Italian EVALITA 2009       and English CoNLL 2003 corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D07-1012/parts/0-Table-c4.png",
        "Caption": "Table 14 Top four worst-case statistics of features for NE boundary errors. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1067/parts/0-Table-c1.png",
        "Caption": "Table 3. System performance on the is-a relation on the CHEM dataset. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Table-c11.png",
        "Caption": "Table 4: Effect of Arabic stemming features on coref- erence resolution. The row marked with \u201cTruth\u201d represents the results with \u201ctrue\u201d mentions while the row marked with \u201cSystem\u201d represents that mentions are detected by the system. Numbers under \u201cECM- F\u201d are Entity-Constrained-Mention F-measure and numbers under \u201cACE-Val\u201d are ACE-values. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-0441/parts/0-Table-c4.png",
        "Caption": "Table 11 Training, development, and test data from CTB5 for joint word segmentation and POS-tagging. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-1003/parts/0-Figure-c1.png",
        "Caption": "Figure 2 Comparison between the acfr-ratio for MI and Bootstrapped LIN methods, when using varying numbers of common top-ranked features in the words\u2019 feature vectors. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0815/parts/0-Table-c2.png",
        "Caption": "Table 1: Kleene Regular-Expression Assignment Examples.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-4009/parts/0-Table-c2.png",
        "Caption": "Figure 1: Example of a long jump alignment grid. All possible deletion, insertion, identity and substitution op- erations are depicted. Only long jump edges from the best path are drawn. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09_csl2013/parts/0-Figure-c6.png",
        "Caption": "Table 3: Variation in precision with random gold seed sets ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-3012/parts/0-Table-c3.png",
        "Caption": "Table 5: Comparison of difference systems on the ACE RDC 2004 corpus over both 7 types (outside the parentheses) and 23 subtypes (inside the parentheses) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1010/parts/0-Figure-c2.png",
        "Caption": "Table 4: Performance of various clustering-based seed sampling strategies on the held-out test data with the optimal cluster number for each clustering algorithm ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S13-2038/parts/0-Table-c1.png",
        "Caption": "Figure 1: The Classification Process",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-3238/parts/0-Table-c1.png",
        "Caption": "Table 2: Chinese character usage in 3 corpora. The   numbers in brackets indicate the percentage of  characters that are shared by at least 2 corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-1045/parts/0-Table-c2.png",
        "Caption": "Table 7: Word segmentation results.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W05-0709/parts/0-Table-c4.png",
        "Caption": "Figure 2: MUC-7: Level Distribution of the Facts Combined",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pbulletin/parts/0-Table-c5.png",
        "Caption": "Table 4: MRRs on the augmented candidate list.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Table-c1.png",
        "Caption": "Table 3: Multi-threaded time and memory consumption of Moses translating 3003 sentences on eight cores. Our code supports lazy memory mapping (-L) and prefault- ing (-P) with MAP POPULATE, the default. IRST is not threadsafe. Time for Moses itself to load, including load- ing the language model and phrase table, is included. Along with locking and background kernel operations such as prefaulting, this explains why wall time is not one-eighth that of the single-threaded case. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-3012/parts/0-Figure-c1.png",
        "Caption": "Table 6: Performance of Japanese Word Segmentation",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0815/parts/0-Figure-c3.png",
        "Caption": "Table 2: Distribution of dialogue acts in our dataset.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P03-1028/parts/0-Table-c3.png",
        "Caption": "Table 1: Machine translation performance for the experiments listed in this paper. Scores are case-sensitive IBM B LEU. For every choice of system, language pair, and feature set, PRO performs comparably with the other methods. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Table-c2.png",
        "Caption": "Table 3: General + Contextual Role Knowledge Sources",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E12-2021/parts/0-Figure-c4.png",
        "Caption": "Figure 5: Biography Sentence Evaluations.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1009/parts/0-Table-c12.png",
        "Caption": "Figure 3. Comparison of two augmentation strategies over different sampling strategies in selecting the initial seed set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N10-1068/parts/0-Figure-c6.png",
        "Caption": "Table 1: Numbers of expressions of all the differ- ent types from the DISCO and Reddy datasets. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c25.png",
        "Caption": "Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag. Across all languages, high performance can be attained by selecting a single tag per word type. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09prod/parts/0-Table-c1.png",
        "Caption": "Table 6: Parameters used in our system.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PTP_n09/parts/0-Figure-c3.png",
        "Caption": "Table 8: Detailed DIFF results",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1004/parts/0-Table-c4.png",
        "Caption": "Table 4: Number of recall errors according to mention type (rows anaphor, columns antecedent). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-3909/parts/0-Table-c5.png",
        "Caption": "Fig. 8. Dependency parse and triples for the sentence \u2013 \u201cMr Burke said it was a textbook landing considering the circumstances\u201d.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W05-0612/parts/0-Table-c1.png",
        "Caption": "Figure 1: (a) An undirected graph G representing the similarity matrix; (b) The bipartite graph showing three clusters on G; (c) The induced clusters U ; (d) The new graph G1 over clusters U ; (e) The new bipartite graph over G1 ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-2007/parts/0-Table-c1.png",
        "Caption": "Table 7: Test set results. Maamouri et al. (2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length \u2264 40. The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them. We are unaware of prior results for the Stanford parser. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P1018/parts/0-Table-c3.png",
        "Caption": "Figure 1: The predicates of two sentences (white: \u201cThe company has said it plans to restate its earnings for 2000 through 2002.\u201d; grey: \u201cThe company had announced in January that it would have to restate earnings (. . . )\u201d) from the Microsoft Research Paragraph Corpus are aligned by computing clusters with minimum cuts. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PbaneaCSL/parts/0-Table-c2.png",
        "Caption": "Table 1: Task characteristics: #sentences in Train/Dev, # of features, and metrics used. Our MT models are trained with standard phrase-based Moses software (Koehn and others, 2007), with IBM M4 alignments, 4gram SRILM, lexical ordering for PubMed and distance ordering for the NIST system. The decoder generates 50-best lists each iteration. We use SVMRank (Joachims, 2006) as opti- mization subroutine for PRO, which efficiently handle all pairwise samples without the need for sampling. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2104/parts/0-Table-c2.png",
        "Caption": "Table 5: Results of the character-category association model: best 5 guesses ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1066/parts/0-Figure-c4.png",
        "Caption": "Figure 3: Density of signature caseframes (Study 2).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1039/parts/0-Table-c1.png",
        "Caption": "Table 3. Statistics of the datasets. The row \u201cSuper- senses\u201d lists the number of instances of supersense labels, partitioned, in the following two rows, between verb and noun supersense labels. The lowest four rows summarize average polysemy figures at the synset and supersense level for both nouns and verbs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-2207/parts/0-Table-c3.png",
        "Caption": "Figure 2: PubMed Results. The curve represents the Pareto Frontier of all results collected after multiple runs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D13-1031/parts/0-Table-c2.png",
        "Caption": "Table 7: Synonyms for home",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N03-1027/parts/0-Table-c3.png",
        "Caption": "Fig. 4. Performance of the training algorithms, supervised against semi-supervised techniques. The semi-supervised precision is evaluated indirectly by using the predicted dataset as train-set against the human-annotated manual small dataset. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-3402/parts/0-Table-c2.png",
        "Caption": "Table 13 Transition-based feature templates for the dependency parser. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-3031/parts/0-Table-c2.png",
        "Caption": "Figure 2: Word alignment based translation model P(J, A|E) (IBM Model 4) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1084/parts/0-Figure-c2.png",
        "Caption": "Table 1: Parallel topics extracted by the bLSA model. Top words on the Chinese side are translated into English for illustration purpose. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1059/parts/0-Figure-c1.png",
        "Caption": "Table 2: Selected document statistics for three JDPA Corpus document sources.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1063/parts/0-Table-c10.png",
        "Caption": "Figure 1: Dissimilarity of temporal distributions of \u2018WTO\u2019 in English and Chinese corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Figure-c1.png",
        "Caption": "Table 3: Lexical features. Top part: Adding each feature separately; difference from CORE 12 (predicted). Bottom part: Greedily adding best features from previous part. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1048/parts/0-Table-c1.png",
        "Caption": "Table 1: Classification results with XLE starredness, parser exceptions and zero parses (Method 1) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-3238/parts/0-Table-c2.png",
        "Caption": "Table 3: Experiment 2: Results by training set size, \u03b8 = 1.0 ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1407/parts/0-Table-c1.png",
        "Caption": "Table 5: Possessive pronoun resolution examples",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1009/parts/0-Figure-c1.png",
        "Caption": "Figure 1: Results on the Bitter Lemons corpus",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D07-1012/parts/0-Table-c1.png",
        "Caption": "Table 24: Splitting Compounds in Russian",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J12-1003/parts/0-Figure-c6.png",
        "Caption": "Table 2: Impact of role sequence information on the HMM and Maxent classifiers. The combination results of the HMM and Maxent are also provided. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1033/parts/0-Table-c2.png",
        "Caption": "Figure 2: A partially scaled and inverted identity matrix J\u00b5 . Such a matrix can be used to trans- form a vector storing a domain and value repre- sentation into one containing the same domain but a partially inverted value, such as W and \u00acW de- scribed in Figure 1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PEAAI_n09/parts/0-Figure-c3.png",
        "Caption": "Table 2: Mixed-case TER and BLEU, and lower- case METEOR scores on Arabic NIST MT05. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-2003/parts/0-Table-c11.png",
        "Caption": "Figure 4: Improvements in F-measure on MUC-7 plotted against amount of selected unlabeled data used ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Table-c9.png",
        "Caption": "Figure 1: Bilingual training size vs. BLEU score (mid- dle line, left axis) and phrase table composition (top line, right axis) on Arabic Development Set. The baseline BLEU score (bottom line) is included for comparison. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1062/parts/0-Figure-c5.png",
        "Caption": "Table 3: Accuracy (recall) of systems on the two bench- marks. The systems are divided into three groups. Group 1 uses 10-fold cross-validation; groups 2 and 3 use the in- dependent test set. Groups 1 and 2 measure accuracy of logical form; group 3 measures accuracy of the answer; but there is very small difference between the two as seen from the Kwiatkowski et al. (2010) numbers. Our best system improves substantially over past work, despite us- ing no logical forms as training data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-3236/parts/0-Figure-c4.png",
        "Caption": "Figure 3 The lattice for the Hebrew sequence !\u202b( \u05d1\u05e6\u200c\u05dc\u05dd \u05d4\u05e0\u200c\u05e2\u05d9\u05dd\u202csee footnote 19). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S0885/parts/0-Figure-c2.png",
        "Caption": "Table 4: Translation performance of baseline and bLSA-Adapted Chinese-English SMT systems on manual transcriptions and 1-best ASR hypotheses ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0119/parts/0-Table-c2.png",
        "Caption": "Table 3: Categories of multi-character words that  are considered \u2018strings with internal structures\u2019   (see Section 4.2). Each category is illustrated    with an example from our corpus. Both the   individual characters and the compound they              form receive a POS tag. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1001/parts/0-Table-c1.png",
        "Caption": "Table 3: Accuracy (%) of the extracted sets of rules.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Figure-c7.png",
        "Caption": "Figure 6: Are the single most probable words for a given",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E06-1030/parts/0-Table-c4.png",
        "Caption": "Table 2: Comparison of Moses and KIT phrase extraction systems ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1066/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Semantic role learning curve.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-2606/parts/0-Table-c2.png",
        "Caption": "Table 1: Results on Penn (English) Treebank, Wall Street Journal, sentences with 100 words or fewer.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PE2006_p00/parts/0-Figure-c12.png",
        "Caption": "Table 4: Bagging with 50 gold seed sets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N03-1027/parts/0-Table-c8.png",
        "Caption": "Table 5 The accuracies of various word segmentors over the second SIGHAN bakeoff data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S0885/parts/0-Figure-c4.png",
        "Caption": "Table 6 Most frequent semantic roles for each syntactic position. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-2104/parts/0-Table-c1.png",
        "Caption": "Table 5: Parser performance on Brown;E, supervised adaptation",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-3144/parts/0-Table-c1.png",
        "Caption": "Table 2 Examples of positive and negative words. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PTASL_n09/parts/0-Figure-c5.png",
        "Caption": "Table 2: Evaluation of the Russian n-gram model.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c19.png",
        "Caption": "Table 1 Contingency table for the children of  canine  in the subject position of run. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2145/parts/0-Table-c1.png",
        "Caption": "Table 2: Experiment 2: Results for label unknown sense, NN-based outlier detection, \u03b8 = 1.0. \u03c3: stan- dard deviation ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1008/parts/0-Table-c2.png",
        "Caption": "Table 5: Variation in performance, by number of sentence boundaries (n), and by training corpus size.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N13-1093/parts/0-Table-c1.png",
        "Caption": "Figure 1: One possible breakdown of spoken Arabic into dialect groups: Maghrebi, Egyptian, Levantine, Gulf and Iraqi. Habash (2010) gives a breakdown along mostly the same lines. We used this map as an illustration for annotators in our dialect classification task (Section 3.1), with Arabic names for the dialects instead of English. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1039/parts/0-Figure-c1.png",
        "Caption": "Table 11 Comparison between a ME framework and the derived model on the same test set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4917/parts/0-Table-c2.png",
        "Caption": "Table 4: Results of the gloss classifier.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-1003/parts/0-Table-c8.png",
        "Caption": "Table 3: 2 billion word corpus statistics",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W05-1518/parts/0-Table-c2.png",
        "Caption": "Table 18 Type-sensitive improvement for Chinese/English NER. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1032/parts/0-Table-c2.png",
        "Caption": "Figure 2. Stratefied Sampling for initial seeds",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1084/parts/0-Table-c4.png",
        "Caption": "Fig. 1. Fuzzy hierarchical clustering for Paraphrase Extraction.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1063/parts/0-Table-c6.png",
        "Caption": "Table 3. Performance comparison, the numbers in parentheses report the performance over the 24 ACE subtypes while the numbers outside paren- theses is for the 5 ACE major types ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-2012/parts/0-Table-c4.png",
        "Caption": "Figure 1: structured-features for the instance i{\u201chim\u201d, \u201cthe man\u201d}",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2139/parts/0-Table-c3.png",
        "Caption": "Table 2: The 10 best languages for R EDDY using LCS.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0206/parts/0-Table-c3.png",
        "Caption": "Figure 2: Word-to-word alignment.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1125/parts/0-Figure-c1.png",
        "Caption": "Table 5: Effect of the introduction of equivalence classes. For the baseline we used the original in- flected word forms. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P08-1043/parts/0-Table-c1.png",
        "Caption": "Table 4: Bagging with 50 gold seed sets",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00-1025/parts/0-Figure-c14.png",
        "Caption": "Table 7: Resolution accuracies for the ACE test set.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1007/parts/0-Table-c3.png",
        "Caption": "Table 2: Statistics about the training/tuning/test datasets used in our experiments. The token counts are calculated before MADA segmentation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1081/parts/0-Table-c1.png",
        "Caption": "Table 3. Performance comparison, the numbers in parentheses report the performance over the 24 ACE subtypes while the numbers outside paren- theses is for the 5 ACE major types ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-4002/parts/0-Figure-c5.png",
        "Caption": "Table 9 The speeds of joint word segmentation and POS-tagging by 10-fold cross validation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1634/parts/0-Figure-c4.png",
        "Caption": "Table 2: Priority Order for Second Person ADs",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P08-2062/parts/0-Figure-c1.png",
        "Caption": "Table 1 The best two performing systems of each type (according to fine-grained recall) in Senseval-2 and -3. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W11-2206/parts/0-Figure-c2.png",
        "Caption": "Table 1. The count of the types of anaphora per corpus.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-2060/parts/0-Figure-c3.png",
        "Caption": "Table 4: Training phase: systems outperforming the baseline in terms of TRDR score. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2147/parts/0-Table-c2.png",
        "Caption": "Table 2: Global features in the entity kernel for reranking. These features are anchored for each entity instance and adapted to entity categories. For example, the entity string (first feature) of the entity \u201cUnited Nations\u201d with entity type \u201cORG\u201d is \u201cORG United Nations\u201d. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1407/parts/0-Table-c4.png",
        "Caption": "Table 17 Comparison of performance of MSRSeg: The versions that are trained using (semi-)supervised iterative training with different initial training sets (Rows 1 to 8) versus the version that is trained on annotated corpus of 20 million words (Row 9). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1011/parts/0-Figure-c3.png",
        "Caption": "Figure 6: Relation extraction rules used by Gamallo et.al.[36]",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P10-1124/parts/0-Figure-c2.png",
        "Caption": "Table 4: Results of Uryupina\u2019s discourse new clas- sifier ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1056/parts/0-Figure-c3.png",
        "Caption": "Figure 2: Automatic evaluation with 50% of Freebase relation data held out and 50% used in training on the 102 largest relations we use. Precision for three different feature sets (lexical features, syntactic features, and both) is reported at recall levels from 10 to 100,000. At the 100,000 recall level, we classify most of the instances into three relations: 60% as location-contains, 13% as person-place-of-birth, and 10% as person-nationality. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1081/parts/0-Figure-c2.png",
        "Caption": "Fig. 4. F-measure for the objective and subjective classes for cross-lingual bootstrapping.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PTP_n09/parts/0-Figure-c2.png",
        "Caption": "Table 5: Possessive pronoun resolution examples",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Table-c17.png",
        "Caption": "Fig. 9 BLEU difference curves of four context-informed models using TRIBL",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1109/parts/0-Figure-c2.png",
        "Caption": "Table 3:  Recall for morphological hasXY() descriptions ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-1914/parts/0-Figure-c1.png",
        "Caption": "Table 2: Experiment results (as F1 scores) where IM is identification of mentions and S - Setting. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Q13-1015/parts/0-Table-c2.png",
        "Caption": "Table 4: MAP values on corresponding test set ob- tained by each method. Figures in parentheses in- dicate optimal number of LDA topics. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1063/parts/0-Figure-c1.png",
        "Caption": "Figure 1. Learning Curves for Confusion Set               Disambiguation ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1407/parts/0-Table-c2.png",
        "Caption": "Table 1: Intrinsic evaluation accuracy [%] (development set) for Arabic segmentation and tagging. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1073/parts/0-Figure-c7.png",
        "Caption": "Table 1: POS/morphological feature accuracies on the development sets.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W01-0502/parts/0-Table-c2.png",
        "Caption": "Table 5 Effect of pruning parameter tp and heuristic function on error rate for direct-translation model (Np = 50,000). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4916/parts/0-Table-c4.png",
        "Caption": "Table 2: Statistics for the ACE corpus.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1007/parts/0-Table-c7.png",
        "Caption": "Table 7 Effect of pruning parameter Np and heuristic function on error rate for direct-translation model (tp = 10\u221212 ). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D11-1095/parts/0-Table-c3.png",
        "Caption": "Table 3: Translation results for French\u2192English",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-3012/parts/0-Table-c4.png",
        "Caption": "Table 3: F-measure on MUC-6 and MUC-7 test data",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N03-1027/parts/0-Table-c5.png",
        "Caption": "Table 5 Training and test conditions for the German-to-English Verbmobil corpus (*number of words without punctuation). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W04-0813/parts/0-Table-c2.png",
        "Caption": "Figure 1: structured-features for the instance i{\u201chim\u201d, \u201cthe man\u201d}",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2605/parts/0-Table-c4.png",
        "Caption": "Table 1: a) An example of a document from Tu\u0308Ba-D/Z, b) an abbreviated entity grid representation of it, and c) the feature vector representation of the abbreviated entity grid for transitions of length two. Mentions of the entity Frauen are underlined. nom: nominative, acc: accusative, oth: dative, oblique, and other arguments ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1009/parts/0-Table-c10.png",
        "Caption": "Figure 14 Reduplication for n = 4. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N03-3004/parts/0-Figure-c2.png",
        "Caption": "Table 3: Average precision (AP) and coverage (Cov) results for our proposed system ES-all and the baselines. \u2021 indicates AP statistical signifi- cance at the 0.95 level wrt all baselines. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-2015/parts/0-Table-c2.png",
        "Caption": "Table 5: LO sentence configuration scores",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0711/parts/0-Table-c14.png",
        "Caption": "Table 2: Illustrative tableau for a simple constraint sys- tem not capturable as a regular relation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S0885/parts/0-Table-c8.png",
        "Caption": "Table 1: Regular expression notation in foma.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P101167/parts/0-Table-c1.png",
        "Caption": "Table 2 The similarity score features used to represent pairs of templates. The columns specify the corpus over which the similarity score was computed, the template representation, the similarity measure employed, and the feature representation (as described in Section 4.1). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J10-3003/parts/0-Figure-c9.png",
        "Caption": "Table 4: Validation Features for Crosslingual Slot Filling",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Figure-c4.png",
        "Caption": "Table 11 NWI results on PK and CTB corpora, NWI as post-processor versus unified approach. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N13-1095/parts/0-Figure-c1.png",
        "Caption": "Figure 3: Propagation: Core items",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06128/parts/0-Figure-c3.png",
        "Caption": "Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in bold show the best results per language and setting. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-3114/parts/0-Figure-c6.png",
        "Caption": "Figure 1: An underspecified discourse structure and its five configurations",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N13-1110/parts/0-Table-c6.png",
        "Caption": "Figure 1: Example sentence and extracted features from the SENSEVAL 2 word church ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-2124/parts/0-Table-c4.png",
        "Caption": "Table 12 The set of new features. The last two columns denote the number and percentage of examples for which the value of the feature is non-zero in examples generated from the 23 gold-standard graphs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Figure-c10.png",
        "Caption": "Table 3. Results on three query categories.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-3306/parts/0-Table-c3.png",
        "Caption": "Figure 1: Properties of the training and test sets used in the shared task. The training data is the Europarl cor- pus, from which also the in-domain test set is taken. There is twice as much language modelling data, since training data for the machine translation system is filtered against sentences of length larger than 40 words. Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1005/parts/0-Figure-c3.png",
        "Caption": "Table 16 Accuracy comparisons between various dependency parsers on English data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1005/parts/0-Figure-c1.png",
        "Caption": "Table 1: State classification by minimum input consumed for the Finnish dictionary ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1060/parts/0-Table-c1.png",
        "Caption": "Table 7 Web results for other-anaphora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C04-1061/parts/0-Figure-c4.png",
        "Caption": "Table 4: Segmentation results by the pure subword-based IOB tagging. The separator \u201c/\u201d divides the results by three lexicon sizes as illustrated in Table 3. The first is character-based (s1), while the other two are subword-based with different lexicons (s2/s3). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1138/parts/0-Figure-c3.png",
        "Caption": "Figure 2: Diffs in the course of iteration. All models were with back-off mixing (+BM). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Plex_p00/parts/0-Figure-c2.png",
        "Caption": "Table 7: Fact vs. Statistical Cross-Doc Features",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-2040/parts/0-Table-c7.png",
        "Caption": "Table 14 Summary of results for unknown-boundary condition. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c14.png",
        "Caption": "Table 3: Mixed-case TER and BLEU, and lower-case METEOR scores on Chinese NIST MT03+MT04. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1138/parts/0-Figure-c4.png",
        "Caption": "Figure 5: oscillating states in matrix CW for an unweighted graph ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W05-0709/parts/0-Table-c2.png",
        "Caption": "Table 4: Ordered List of Increased/Decreased Number of Correctly Tagged Words ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c9.png",
        "Caption": "Table 2: Number of training examples",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C04-1194/parts/0-Table-c4.png",
        "Caption": "Figure 3: A Parallel Fragment Extraction System",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H01-1052/parts/0-Figure-c2.png",
        "Caption": "Table 1: Number of entries in 3 corpora",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N12-1006/parts/0-Figure-c1.png",
        "Caption": "Figure 2: Automatic evaluation with 50% of Freebase relation data held out and 50% used in training on the 102 largest relations we use. Precision for three different feature sets (lexical features, syntactic features, and both) is reported at recall levels from 10 to 100,000. At the 100,000 recall level, we classify most of the instances into three relations: 60% as location-contains, 13% as person-place-of-birth, and 10% as person-nationality. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1115/parts/0-Table-c4.png",
        "Caption": "Table 9: German-to-English Final System Results.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W05-1518/parts/0-Figure-c1.png",
        "Caption": "Figure 1: Parse tree binarization",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-0441/parts/0-Table-c5.png",
        "Caption": "Figure 2: Shallow parsing: chunking (Extracted from: http://kontext.fraunhofer.de)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Table-c14.png",
        "Caption": "Figure 2: Beam search algorithm for joint tagging and de- pendency parsing of input sentence x with weight vector w and beam parameters b1 and b2 . The symbols h.c, h.s and h.f denote, respectively, the configuration, score and feature representation of a hypothesis h; h.c.A denotes the arc set of h.c. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c15.png",
        "Caption": "Figure 1: Architecture of the translation approach based on Bayes\u2019 decision rule. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09_csl2013/parts/0-Table-c1.png",
        "Caption": "Figure 4: Compilation of a Regular Expression into an fst that Maps between Two Regular Languages ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C02-1025/parts/0-Table-c1.png",
        "Caption": "Table 4: Sizes of bilingual dictionaries induced by differ- ent alignment methods. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1006/parts/0-Table-c6.png",
        "Caption": "Figure 3: Learning curves of bootstrapping meth- ods for semantic classification on TS1. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2708/parts/0-Table-c1.png",
        "Caption": "Figure 4: LLDA Fmeausres for 3 feature conditions",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1045/parts/0-Table-c1.png",
        "Caption": "Table 3: Average results for 35 verbs",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H05-1083/parts/0-Table-c2.png",
        "Caption": "Table 4: Translation results for German-English",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D12-1074/parts/0-Figure-c2.png",
        "Caption": "Table 5: The most frequent OOV\u2019s (with counts \u2265 10) of the dialectal test sets against the MSA training data.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c25.png",
        "Caption": "Table 1: The semantic roles of cases beside C-3 verb cluster ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0106/parts/0-Table-c4.png",
        "Caption": "Figure 9: Chunk - Length and count of glue rules used decoding test set ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C00-2123/parts/0-Table-c4.png",
        "Caption": "Table 1: Table showing the number of pairs of different occurrences of the same token sequence, where one occurrence is given a certain label and the other occurrence is given a certain label. We show these counts both within documents, as well as over the whole corpus. As we would expect, most pairs of the same entity sequence are labeled the same(i.e. the diagonal has most of the density) at both the document and corpus levels. These statistics are from the CoNLL 2003 English training set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2145/parts/0-Table-c5.png",
        "Caption": "Table 11: Results of the corpus-based model",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1063/parts/0-Table-c9.png",
        "Caption": "Table 1: Size of co-occurrence databases",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1046/parts/0-Table-c5.png",
        "Caption": "Table 2. MAP of different IR systems with differ- ent segmenters. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1125/parts/0-Table-c2.png",
        "Caption": "Table 2: Impact of role sequence information on the HMM and Maxent classifiers. The combination results of the HMM and Maxent are also provided. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1125/parts/0-Figure-c2.png",
        "Caption": "Figure 2: The conditioning structure of the hierarchical PYP with an embedded character language models. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D12-1094/parts/0-Table-c4.png",
        "Caption": "Table 2: False negative matches on the Riedel (Riedel et al., 2010) and KBP dataset (Surdeanu et al., 2012). All numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and 5% in Riedel and KBP dataset, respectively. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D07-1076/parts/0-Figure-c2.png",
        "Caption": "Table 8 Thesaurus coverage of polysemous words (excluding multiwords) in WordNet 1.6. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-2003/parts/0-Figure-c1.png",
        "Caption": "Table 4: Results on the unseen plausibility dataset.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1005/parts/0-Figure-c5.png",
        "Caption": "Figure 1: A pair of comparable, non-parallel documents",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P08-1102/parts/0-Table-c3.png",
        "Caption": "Table 6. SA strength scores.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1141/parts/0-Table-c2.png",
        "Caption": "Table 2: Entity type constraints.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-1104/parts/0-Figure-c2.png",
        "Caption": "Table 1: Confidence scores for diese in ex. (1)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-2002/parts/0-Figure-c1.png",
        "Caption": "Table 1: Total annotation time, portion spent se- lecting annotation type, and absolute improve- ment for rapid mode. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0815/parts/0-Table-c6.png",
        "Caption": "Table 3. The F-measure improvement between the BMM-based CWS and it with WSM in the MSR_C track (OOV is 0.034) using a, b, and c system dictionary. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1761/parts/0-Table-c4.png",
        "Caption": "Figure 2: Instructions for judging of unsharpened factoids.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1056/parts/0-Table-c2.png",
        "Caption": "Table 1: Results for development and test set for the two languages by ME1 ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P01-1027/parts/0-Figure-c1.png",
        "Caption": "Figure 6: An automatically learned Prosecution Chain. Arrows indicate the before relation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c16.png",
        "Caption": "Figure 1: Graphical representation of our model. Hyper- parameters, the stickiness factor, and the frame and event initial and transition distributions are not shown for clar- ity. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-2164/parts/0-Figure-c5.png",
        "Caption": "Table 5 Accuracy for three classes on a general purpose list of 2,000 words. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-3008/parts/0-Table-c5.png",
        "Caption": "Figure 2. Stratefied Sampling for initial seeds",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1073/parts/0-Table-c2.png",
        "Caption": "Table 1 Common grammatical relations of Minipar involving nouns. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/M98-1003/parts/0-Figure-c6.png",
        "Caption": "Figure 2: Smoothed graphs depicting the dependency of Precision upon Recall using the LSA and PMI-based models ordering the expressions in TrainValD (left) and TestD (right) according to their non-compositionality. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P22324_w09/parts/0-Figure-c1.png",
        "Caption": "Figure 2: Learning curve of BLC20 on SE3",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1056/parts/0-Table-c6.png",
        "Caption": "Figure 2: Example of the Character Tagging Method: Word boundaries are indicated by vertical lines (\u2018|\u2019).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1053/parts/0-Table-c8.png",
        "Caption": "Table 3 Performance measures of the training algorithms. Green indicates the best performance, while red the worst. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P03-1028/parts/0-Table-c6.png",
        "Caption": "Figure 6 The initial frequencies of character sequences. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C00-2123/parts/0-Table-c6.png",
        "Caption": "Figure 1: An underspecified discourse structure and its five configurations",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C04-1061/parts/0-Figure-c3.png",
        "Caption": "Figure 3: extracts from the Akkadian project",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2210/parts/0-Table-c4.png",
        "Caption": "Table 2: BLEU scores of English to Russian ma- chine translation system evaluated on tst2012 and tst2013 using baseline GIZA++ alignment and transliteration augmented-GIZA++ alignment and post-processed the output by transliterating OOVs. Human evaluation in WMT13 is performed on TA-GIZA++ tested on tst2013 (marked with *) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1079/parts/0-Figure-c1.png",
        "Caption": "Table 1: Monolingual and Crosslingual Baseline Slot Filling Pipelines ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-3114/parts/0-Figure-c2.png",
        "Caption": "Table 1: 50-document corpora averages",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C04-1131/parts/0-Figure-c1.png",
        "Caption": "Table 2: Distribution of reasons for false positives (incorrect argument extractions) by BInc at K=20. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2101/parts/0-Figure-c5.png",
        "Caption": "Table 3. Accuracy of the proposed system on a set which contains misspelled queries that the users had reformulated ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1081/parts/0-Figure-c3.png",
        "Caption": "Table 3: Values obtained for Precision, Recall and F- score with method 1 by changing the threshold frequency of the correspondences and applying a post-filter. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P101121_p07/parts/0-Table-c3.png",
        "Caption": "Figure 2: Screenshot of Annis Linguistic Database",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N06-1027/parts/0-Table-c7.png",
        "Caption": "Table 4: Overall AP results of the different feature configurations, compared to two baselines. \u2020 in- dicates statistical significance at the 0.95 level wrt B3. \u2021 indicates statistical significance at 0.95 level wrt both B3 and B4. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1023/parts/0-Table-c2.png",
        "Caption": "Figure 8 A hierarchical summary of propositions involving nausea as an argument, such as headache is related to nausea, acupuncture helps with nausea, and Lorazepam treats nausea. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-1062/parts/0-Table-c1.png",
        "Caption": "Figure 1. Inter-annotator agreement of ACE 2005 relation annotation. Numbers are the distinct relation mentions whose both arguments are in the list of adjudicated entity mentions. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W08-0335/parts/0-Table-c2.png",
        "Caption": "Table 10 Comparing selectional preference frame definitions. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Table-c11.png",
        "Caption": "Table 1 The best two performing systems of each type (according to fine-grained recall) in Senseval-2 and -3. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E12-1020/parts/0-Table-c3.png",
        "Caption": "Table 1: Task characteristics: #sentences in Train/Dev, # of features, and metrics used. Our MT models are trained with standard phrase-based Moses software (Koehn and others, 2007), with IBM M4 alignments, 4gram SRILM, lexical ordering for PubMed and distance ordering for the NIST system. The decoder generates 50-best lists each iteration. We use SVMRank (Joachims, 2006) as opti- mization subroutine for PRO, which efficiently handle all pairwise samples without the need for sampling. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1017/parts/0-Table-c1.png",
        "Caption": "Figure 2. Learning Curves for Confusable Disambiguation",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H01-1001/parts/0-Table-c5.png",
        "Caption": "Figure 1: Example of a word with internal structure.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1059/parts/0-Figure-c2.png",
        "Caption": "Table 7 Some of the possible Spanish translations of the English phrase make with their memory-based con- text-dependent translation probabilities (rightmost column) compared against context-independent transla- tion probabilities of the baseline system ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E12-1054/parts/0-Table-c4.png",
        "Caption": "Figure 4: Two dendrograms for the graph in Figure 3.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1104/parts/0-Table-c3.png",
        "Caption": "Table 1. Result for microblog classification",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J06-2001/parts/0-Table-c2.png",
        "Caption": "Figure 2: PubMed Results. The curve represents the Pareto Frontier of all results collected after multiple runs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1727/parts/0-Table-c6.png",
        "Caption": "Table 1. Result for microblog classification",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1630/parts/0-Table-c1.png",
        "Caption": "Table 1. Expletive \u201cit\u201d compared results",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0119/parts/0-Table-c4.png",
        "Caption": "Table 6: Effects of using CRF. The separator \u201c/\u201d divides the results of s1, and s3.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1407/parts/0-Table-c5.png",
        "Caption": "Figure 2: Information Bottleneck (IB) iterative clustering. D is the Kullback-Leibler distance. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1045/parts/0-Figure-c4.png",
        "Caption": "Figure 3: A Parallel Fragment Extraction System",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W10-4135/parts/0-Table-c1.png",
        "Caption": "Table 2: Contribution of different features over 43          relation subtypes in the test data ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Figure-c6.png",
        "Caption": "Table 3: Accuracy (recall) of systems on the two bench- marks. The systems are divided into three groups. Group 1 uses 10-fold cross-validation; groups 2 and 3 use the in- dependent test set. Groups 1 and 2 measure accuracy of logical form; group 3 measures accuracy of the answer; but there is very small difference between the two as seen from the Kwiatkowski et al. (2010) numbers. Our best system improves substantially over past work, despite us- ing no logical forms as training data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/I05-3013/parts/0-Table-c2.png",
        "Caption": "Figure 5: MTO is not sensitive to the number of random substitutes sampled per word token. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I08-1070/parts/0-Table-c7.png",
        "Caption": "Table 1: Sizes of our comparable corpora",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c12.png",
        "Caption": "Table 2. Input sentences.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W05-1518/parts/0-Table-c5.png",
        "Caption": "Table 1. Performance of seven relation feature spaces over the 5 ACE major types using parse tree information only ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2211/parts/0-Table-c4.png",
        "Caption": "Table 5: Impact of the topic cache size",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-2003/parts/0-Table-c10.png",
        "Caption": "Table 1: Machine translation performance for the experiments listed in this paper. Scores are case-sensitive IBM B LEU. For every choice of system, language pair, and feature set, PRO performs comparably with the other methods. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P1018/parts/0-Table-c6.png",
        "Caption": "Table 4: Topics are meaningful within languages but di-",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-3016/parts/0-Table-c2.png",
        "Caption": "Table 1: Examples of phrase \u201cmeaningfulness\u201d (Note that the comments are not presented to Turkers).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4917/parts/0-Figure-c1.png",
        "Caption": "Figure 3: Morphological Analysis/Generation as a Relation between Analyses and Words ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1087/parts/0-Table-c2.png",
        "Caption": "Figure 4: MTO is fairly stable as long as the Z\u0303 constant 5.4 Morphological and orthographic features is within an order of magnitude of the real Z value. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1009/parts/0-Table-c11.png",
        "Caption": "Table 9 Initial NE recognition type-insensitive (type-sensitive) performance across various domains. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-1004/parts/0-Table-c1.png",
        "Caption": "Figure 1: Stages of the proposed method.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W12-1011/parts/0-Table-c1.png",
        "Caption": "Table 2: Results for GigaPairs (all numbers in %); re- sults that significantly differ from Full are marked with asterisks (* p<0.05; ** p<0.01). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PS15684_w09/parts/0-Figure-c7.png",
        "Caption": "Table 6. Feature impact experiments",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N07-1024/parts/0-Table-c10.png",
        "Caption": "Figure 1: The Buckwalter Arabic Morphological Analyzer\u2019s lookup process exemplified for the word lilkitAbi.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1141/parts/0-Figure-c9.png",
        "Caption": "Figure 4: LLDA Fmeausres for 3 feature conditions",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1048/parts/0-Table-c2.png",
        "Caption": "Table 4. Performance comparison on the ACE 2004 data over both 7 major types (the numbers outside parentheses) and 23 subtypes (the num- bers in parentheses) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-1004/parts/0-Figure-c3.png",
        "Caption": "Figure 1 (a) A Chinese sentence. Slashes indicate word boundaries. (b) An output of our word segmentation system. Square brackets indicate word boundaries. + indicates a morpheme boundary. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1089/parts/0-Table-c2.png",
        "Caption": "Figure 4: POS tagging accuracy using one-at-a- time, character-based POS tagger ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-1014/parts/0-Table-c1.png",
        "Caption": "Table 3: Op. Target - Op. Word Pair Extraction",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmorph_p00/parts/0-Table-c13.png",
        "Caption": "Table 2: Translation results for English\u2192German",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0815/parts/0-Table-c3.png",
        "Caption": "Figure 6: Are the single most probable words for a given",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-1098/parts/0-Table-c3.png",
        "Caption": "Table 1: Statistics on a travel conversation corpus",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1003/parts/0-Figure-c1.png",
        "Caption": "Table 4: Comparison of different configurations.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4917/parts/0-Table-c10.png",
        "Caption": "Table 14 Sample of human-interpretable French TSG rules. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-0119/parts/0-Table-c3.png",
        "Caption": "Table 1: Performance of the statistical approach using a trigram model based on Google Web1T. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1066/parts/0-Table-c2.png",
        "Caption": "Table 1: Scores for CityU corpus",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-1087/parts/0-Figure-c4.png",
        "Caption": "Figure 1: Histogram of token movement size ver- sus its occurrences performed by the model Neu- big on the source english data. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1147/parts/0-Table-c2.png",
        "Caption": "Figure 1: All the parameters of WSMs described in Section 2 used in all our experiments. Semicolon denotes OR. All the examined combinations of parameters are implied from reading the diagram from left to right. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PTASL_n09/parts/0-Figure-c1.png",
        "Caption": "Figure 1: Illustration on temporality",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1032/parts/0-Table-c3.png",
        "Caption": "Table 1: This table shows the performance achieved by the different systems, shown in accuracy (%). The Number of cases denotes the number of instances in the testset. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C02-1033/parts/0-Table-c2.png",
        "Caption": "Figure 7 Algorithm for breadth-first search with pruning. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1011/parts/0-Figure-c4.png",
        "Caption": "Table 3 Unknown word model features for Arabic and French. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1133/parts/0-Table-c2.png",
        "Caption": "Figure 2: Learning curves using different sam- pling strategies. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-0304/parts/0-Table-c1.png",
        "Caption": "Table 4: Example translations from the different methods. Boldface indicates correct translations. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N07-1015/parts/0-Table-c1.png",
        "Caption": "Figure 12 Interdigitation FSRA \u2013 general. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1089/parts/0-Table-c1.png",
        "Caption": "Table 3: Activity detection: Activities are detected on the Santa Barbara Corpus (SBC) and the meet- ing database (meet) either without clustering the activities (all) or clustering them according to their interactivity (interactive) (see Sec. 2 for details). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C02-1025/parts/0-Table-c3.png",
        "Caption": "Table 1: Entries from the English-Slovene sense cluster inventory.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1161/parts/0-Figure-c1.png",
        "Caption": "Table 3: MT06 Dev. Optimization & Test Set Spearman Correlation Results",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W02-1004/parts/0-Figure-c1.png",
        "Caption": "Figure 10 The correlation between the scales and the proper exponents. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1133/parts/0-Table-c4.png",
        "Caption": "Table 10. Speci\ufb01c Subject and Object Agreement Rules",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E09-1072/parts/0-Figure-c2.png",
        "Caption": "Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form    an (Table 1). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W09-0441/parts/0-Table-c1.png",
        "Caption": "Table 3: Distribution of Spurious Errors",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C02-1080/parts/0-Figure-c1.png",
        "Caption": "Table 7: Results for ensemble classifier.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1032/parts/0-Table-c7.png",
        "Caption": "Table 11 Model accuracy using unequal distribution of verb frequencies for the estimation of P(c). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J09-3004/parts/0-Table-c3.png",
        "Caption": "Figure 1: Baseline results for human word lists. Data: 700 positive and 700 negative reviews.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4916/parts/0-Table-c7.png",
        "Caption": "Table 2: The NP chunking results for six sys- tems associated with the project. The baseline results have been obtained by selecting the most frequent chunk tag associated with each part-of- speech tag. The best results for this task have been obtained with a combination of seven learn- ers, five of which were operated by project mem- bers. The combination of these five performances is not far off these best results. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Table-c11.png",
        "Caption": "Table 3. Results on three query categories.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1044/parts/0-Table-c6.png",
        "Caption": "Figure 2: Results for baseline using introspection and simple statistics of the data (including test data).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J07-4005/parts/0-Table-c14.png",
        "Caption": "Table 3: Analysis of results of segmentation on LDC training and test data for all CWS schemes",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2214/parts/0-Table-c1.png",
        "Caption": "Figure 1: Plate diagram representation of the model. ti - s, wi -s and si -s denote the tags, words and segmentations respectively. G-s are various DP-s in the model, Ej -s and \u03b2j -s are the tag-specific emission distributions and their respective Dirichlet prior parameters. H is Gamma base distribution. S is the base distribution over segments. Coupled DP concetrations parameters have been omitted for clarity. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C10-1070/parts/0-Figure-c5.png",
        "Caption": "Table 2: Corpus statistics of the MATR MT06 corpus that was used for experimental evaluation of the proposed measures. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-0329/parts/0-Figure-c2.png",
        "Caption": "Table 2: Effect of observation pruning on the translation quality (average over all test sets).",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1117/parts/0-Table-c1.png",
        "Caption": "Table 14 Top four worst-case statistics of features for NE boundary errors. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1059/parts/0-Figure-c2.png",
        "Caption": "Table 5 Dev-set results when using lattice parsing on top of an external lexicon/analyzer. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P13-1138/parts/0-Figure-c1.png",
        "Caption": "Table 3: Reordering accuracy (BLEU) results.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-2003/parts/0-Table-c3.png",
        "Caption": "Table 15 The training, development, and test data for English dependency parsing. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PEAAI_n09/parts/0-Table-c4.png",
        "Caption": "Figure 4 Illustration of the algorithm by Held and Karp for a traveling salesman problem with J = 5 cities. Not all permutations of cities have to be evaluated explicitly. For a given subset of cities the order in which the cities have been visited can be ignored. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pcoling_D10/parts/0-Figure-c1.png",
        "Caption": "Figure 3: Outcome of clustering procedure",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1115/parts/0-Table-c2.png",
        "Caption": "Table 6: POS tagging with deterministic constraints. The maximum in each column is bold. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P04-1036/parts/0-Figure-c2.png",
        "Caption": "Table 2. The result of pre-processing",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N04-1004/parts/0-Table-c1.png",
        "Caption": "Figure 7: Opinion HITS Performance with vary- ing parameter \u03b3 ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/H05-1115/parts/0-Table-c8.png",
        "Caption": "Table 2: Factoring of global feature collections g into f . xji denotes hxi , . . . xj i in sequence x = hx1 , . . .i. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1023/parts/0-Figure-c1.png",
        "Caption": "Table 2: TercomTERs of invWER-oracles and (in paren- theses) oracle BLEU scores of confusion networks gen- erated with tercom and ITG alignments. The best results per row are shown in bold. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N03-1010/parts/0-Figure-c4.png",
        "Caption": "Table 4: Development Sets Results.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/I05-1065/parts/0-Table-c4.png",
        "Caption": "Table 2: Results on the Arabic GALE Phase 2 evaluation set with one reference translation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-3016/parts/0-Table-c1.png",
        "Caption": "Table 1: BLEU scores achieved with different sets of parallel corpora. All systems are base- line n-code with POS factor models. The follow- ing shorthands are used to denote corpora, : \u201dN\u201d stands for News-Commentary, \u201dE\u201d for Europarl, \u201dC\u201d for CommonCrawl, \u201dU\u201d for UN and (nf) for non filtered corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1084/parts/0-Figure-c3.png",
        "Caption": "Table 1: BLEU-4 scores of different systems",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C10-1045/parts/0-Figure-c1.png",
        "Caption": "Table 15 Effect of adjacent contextual (non-NE) bigrams on the test set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P05-1021/parts/0-Table-c1.png",
        "Caption": "Figure 2: Tagging accuracy on development data depending on context size ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1008/parts/0-Figure-c2.png",
        "Caption": "Figure 8: Extracting sub-trees for S2 .",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P04-2007/parts/0-Table-c2.png",
        "Caption": "Table 2: Meaning of diacritics indicating statistical sig- nificance (\u03c72 tests) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1061/parts/0-Table-c3.png",
        "Caption": "Table 6: Functional features: gender, number, rationality.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H05-1001/parts/0-Table-c3.png",
        "Caption": "Figure 3: Parser projection with target trees. Using the true or 1-best parse trees in the source language is equivalent to having twice as much data in the target language. Note that the penalty for using automatic alignments instead of gold alignments is negligible; in fact, using Source text alone is often higher than +Gold alignments. Using gold source trees, however, significantly outperforms using 1-best source trees. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1155/parts/0-Table-c5.png",
        "Caption": "Table 2: False negative matches on the Riedel (Riedel et al., 2010) and KBP dataset (Surdeanu et al., 2012). All numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and 5% in Riedel and KBP dataset, respectively. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-2932/parts/0-Table-c2.png",
        "Caption": "Figure 6: Tune and test curves of five repetitions of the same Urdu-English PBMT baseline feature experiment. PRO is more stable than MERT. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Q13-1015/parts/0-Table-c4.png",
        "Caption": "Table 2: Effect of our method comparing with MERT and perceptron in terms of B LEU. We also compare our fast generation method with different data (only reachable or full data). #Data is the size of data for training the feature weights. * means significantly (Koehn, 2004) better than MERT (p < 0.01). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6201/parts/0-Table-c3.png",
        "Caption": "Table 3. The F-measure improvement between the BMM-based CWS and it with WSM in the MSR_C track (OOV is 0.034) using a, b, and c system dictionary. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0707/parts/0-Table-c8.png",
        "Caption": "Table 13 Macro-average recall, precision, and F1 on the development set and test set using the parameters that maximize F1 of the learned edges over the development set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6201/parts/0-Table-c2.png",
        "Caption": "Table 6: Counts for the POS tags mentioned in Table 5.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P09-1068/parts/0-Figure-c2.png",
        "Caption": "Table 1: Kendall\u2019s (\u03c4 ) correlation over WMT 2013 (all- en), for the full dataset and also the subset of the data containing a noun compound in both the reference and the MT output ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1016/parts/0-Table-c4.png",
        "Caption": "Table 2: Misspellings of receive",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W07-0909/parts/0-Table-c1.png",
        "Caption": "Table 1: Syntactic Seeding Heuristics",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P01-1005/parts/0-Table-c2.png",
        "Caption": "Table 4: Weights learned for employing rules whose En- glish sides are rooted at particular syntactic categories. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-2207/parts/0-Table-c8.png",
        "Caption": "Figure 1: Outline of word segmentation process",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H01-1062/parts/0-Table-c1.png",
        "Caption": "Table 2 Parsing performance with each POS tag set, on gold and predicted input. L AS = labeled attachment accuracy (dependency + relation). U AS = unlabeled attachment accuracy (dependency only). L S = relation label prediction accuracy. L AS diff = difference between labeled attachment accuracy on gold and predicted input. POS acc = POS tag prediction accuracy. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2204/parts/0-Table-c3.png",
        "Caption": "Figure 1. Removal and reduction of constituents using dependencies",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W05-0612/parts/0-Table-c4.png",
        "Caption": "Table 3: Impact of Joint Bilingual Name Tagging on Word Alignment (%). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-2207/parts/0-Table-c6.png",
        "Caption": "Table 1. F-scores of UML-DOP compared to      previous models on the same data ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0707/parts/0-Table-c4.png",
        "Caption": "Table 4: Hypegraph size measured by the average number of hyperedges (h = 1 for CF). \u201clattice\u201d is the average number of edges in the original CN. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1131/parts/0-Table-c4.png",
        "Caption": "Figure 2: Word prediction from a partial parse",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/W06-3114/parts/0-Figure-c15.png",
        "Caption": "Table 2: Features used in predicting the next parser action",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N09-1048/parts/0-Table-c3.png",
        "Caption": "Table 3: Average polysemy on SE2 and SE3",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pjournal/parts/0-Table-c1.png",
        "Caption": "Table 4 The results of setting 3 (Punctuation is used; the maximum length is 30). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W06-3909/parts/0-Table-c3.png",
        "Caption": "Table 2: Filters applied to candidate pair (H, S)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Figure-c4.png",
        "Caption": "Table 3: Single systems (Basque) in cross- validation, sorted by recall. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/M98-1003/parts/0-Figure-c13.png",
        "Caption": "Figure 3: Word type coverage by normalized frequency: words are grouped by word count / highest word count ratio: low [0, 0.01), medium [0.01, 0.1), high [0.1, 1]. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N09-1025/parts/0-Figure-c2.png",
        "Caption": "Table 1: Statistics on the Italian EVALITA 2009       and English CoNLL 2003 corpora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0815/parts/0-Table-c7.png",
        "Caption": "Table 3. English Name Tagger",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4047/parts/0-Figure-c2.png",
        "Caption": "Fig. 1. (a) An example in which an English sentence is parsed into a tree structure with 12 PCFG rules; (b) an instance in which a Chinese sentence (both Chinese characters and Chinese Pinyin are provided, and note that we will use Chinese Pinyin throughout the paper) is converted into an English tree using 6 STSG rules. The symbol to the upper right of a node indicates that this node is constructed using rule . ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W10-3212/parts/0-Table-c1.png",
        "Caption": "Table 4: Tagging accuracies on test data.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P04-1018/parts/0-Table-c5.png",
        "Caption": "Table 10. Speci\ufb01c Subject and Object Agreement Rules",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/ICDAR99/parts/0-Figure-c1.png",
        "Caption": "Figure 1: An excerpt from the graph for Italian. Three of the Italian vertices are connected to an automatically la- beled English vertex. Label propagation is used to propa- gate these tags inwards and results in tag distributions for the middle word of each Italian trigram. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J06-1004/parts/0-Figure-c10.png",
        "Caption": "Table 1: Frequency of Relation SubTypes in the ACE training and devtest corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2145/parts/0-Table-c4.png",
        "Caption": "Table 3: Recall on different types of empty categories. YX = (Yang and Xue, 2010), Ours = split 6\u00d7. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Q13-1015/parts/0-Figure-c1.png",
        "Caption": "Table 3 NEA type-insensitive (type-sensitive) performance on the test set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Table-c4.png",
        "Caption": "Table 7: Translation results for French-English",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-4917/parts/0-Table-c11.png",
        "Caption": "Figure 6: MUC-7: Level Distribution of Each of the Facts",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2204/parts/0-Table-c4.png",
        "Caption": "Figure 6: Multiple Bayesian learning runs (using averag- ing) for POS tagging. Each point represents one run; the y-axis is tagging accuracy and the x-axis is the average \u2212 log P(derivation) over all samples after burn-in. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P02-1061/parts/0-Table-c2.png",
        "Caption": "Figure 3: Example of the Hybrid Method",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PE2006_p00/parts/0-Table-c1.png",
        "Caption": "Table 2: Comparison of three statistical translation approaches (test on text input: 251 sentences = 2197 words + 430 punctuation marks). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-3209/parts/0-Figure-c2.png",
        "Caption": "Figure 1: Outline of word segmentation process",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-2012/parts/0-Table-c2.png",
        "Caption": "Figure 4: Sizes of the automata.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1040/parts/0-Table-c4.png",
        "Caption": "Table 2: Classification results with 5-gram and fre- quency threshold 4 (Method 2) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P06-1126/parts/0-Figure-c1.png",
        "Caption": "Figure 6: F-measure for the objective and subjective classes for multilingual bootstrapping (versus cross-lingual framework) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S12-1040/parts/0-Table-c2.png",
        "Caption": "Table 1: Parallel topics extracted by the bLSA model. Top words on the Chinese side are translated into English for illustration purpose. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Figure-c13.png",
        "Caption": "Figure 1: Dependency graph for Czech sentence from the Prague Dependency Treebank1",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PbaneaCSL/parts/0-Figure-c1.png",
        "Caption": "Figure 2: Precision-recall curve for the algorithms.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1061/parts/0-Table-c1.png",
        "Caption": "Table 1: POS/morphological feature accuracies on the development sets.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P04-1036/parts/0-Figure-c1.png",
        "Caption": "Table 4: Memory-based learner results",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/E12-1020/parts/0-Figure-c4.png",
        "Caption": "Table 7 The official vocabularies in Verbmobil. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P01-1017/parts/0-Table-c4.png",
        "Caption": "Figure 1: Overview of the system.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1106/parts/0-Table-c2.png",
        "Caption": "Table 4: Language detection accuracies (%) using a 4-gram language model for the letter sequence of          the source name in Latin script. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1038/parts/0-Figure-c4.png",
        "Caption": "Figure 1: Average Precision, Recall and F1 at dif- ferent top K rule cutoff points. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W04-0711/parts/0-Table-c15.png",
        "Caption": "Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and the score threshold \u03b1. Beam parameters fixed at b1 = 40, b2 = 4. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-0815/parts/0-Table-c5.png",
        "Caption": "Table 6: Language Model Results.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-2124/parts/0-Figure-c1.png",
        "Caption": "Table 11 Demonstration of the combination of the two pruning thresholds tC = 5.0 and tc = 12.5 to speed up the search process for the two reordering constraints GE and S3 (no = 50). The translation performance is shown in terms of mWER on the TEST-331 test set. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1084/parts/0-Table-c1.png",
        "Caption": "Figure 1: Proposed discourse structures for Ex. 4: (a) In terms of informational relations; (b) in terms of inten- tional relations ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/H05-1083/parts/0-Figure-c1.png",
        "Caption": "Table 1: Training, tuning, and test conditions",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-1005/parts/0-Figure-c10.png",
        "Caption": "Table 3: Most frequent monosemic words in BG",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N09-3012/parts/0-Table-c1.png",
        "Caption": "Table 6: Distribution of errors",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P07-1061/parts/0-Table-c2.png",
        "Caption": "Table 5 Results with Coref Rules Alone",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1083/parts/0-Table-c1.png",
        "Caption": "Table 2: B3 results for baselines and lexicalized feature sets across four domains.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1761/parts/0-Table-c5.png",
        "Caption": "Table 4: Experimental results of CityU corpus measured in F-measure. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-1004/parts/0-Table-c11.png",
        "Caption": "Table 8 %BLEU on tune and test sets for UR\u2192EN translation, using our unsupervised Urdu parser to incorporate source syntactic features. The two QPD rows are statistically indistinguishable on both test sets. Both are significantly better than all Moses results, but Hiero is significantly better than all others. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D10-1083/parts/0-Figure-c2.png",
        "Caption": "Table 3: Results of negated event/property detection on gold standard cue and scope annotation ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N04-1016/parts/0-Table-c10.png",
        "Caption": "Table 1: Cross-domain B3 (Bagga and Baldwin, 1998) results for Reconcile with its general feature set. The Paired Permutation test (Pesarin, 2001) was used for statistical significance testing and gray cells represent results that are not significantly different from the best result. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmendt_w11/parts/0-Table-c20.png",
        "Caption": "Figure 3: Learning curves in terms of word predic- tion accuracy on deciding between the confusable pair there, their, and they\u2019re, by IGT REE trained on TRAIN - REUTERS, and tested on REUTERS, AL - ICE, and BROWN . The top graphs are accuracies at- tained by the confusable expert; the bottom graphs are attained by the all-words predictor trained on TRAIN - REUTERS until 130 million examples, and on TRAIN - NYT beyond (marked by the vertical bar). ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/S13-1039/parts/0-Table-c1.png",
        "Caption": "Fig. 2. Cell phone experiment result (17 aspects)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-6202/parts/0-Figure-c2.png",
        "Caption": "Table 3: POS Tagging of known words using con- textual features (accuracy in percent). one-vs-all denotes training where example ` serves as positive example to the true tag and as negative example to all the other tags. SM| \u00a8R\u00a9 denotes training where                            2  example ` serves as positive example to the true tag ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J13-2001/parts/0-Table-c2.png",
        "Caption": "Figure 4: Compilation of a Regular Expression into an fst that Maps between Two Regular Languages ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/D09-1092/parts/0-Figure-c3.png",
        "Caption": "Table 2: Translation accuracy (BLEU) results.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J11-3001/parts/0-Figure-c1.png",
        "Caption": "Table 2: Meaning of different feature categories where s represents a specific target word and t repre- sents a specific source word. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-3004/parts/0-Table-c10.png",
        "Caption": "Figure 1: Example of semantic trees",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D09-1025/parts/0-Table-c3.png",
        "Caption": "Figure 1: Comparison of a confusion network and a lat- tice. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-1109/parts/0-Figure-c1.png",
        "Caption": "Table 1: Filters to improve the dictionary precision. Un- less otherwise noted, the filter was applied if either men- tion in the relation satisfied the condition. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-2210/parts/0-Table-c2.png",
        "Caption": "Figure 4: Expanding a partial hypothesis via a matching n-gram. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P11-1061/parts/0-Figure-c1.png",
        "Caption": "Figure 2: User Interface with Arabic Script Dis- play in Java. Mouse clicks on the virtual keyboard or key presses on the physical keyboard are inter- cepted, converted to Arabic Unicode characters, and stored in a buffer, which has a start and an end but no inherent ordering. The Arabic Canvas Object observes the buffer and contains an Ara- bic Scribe object that renders the string of Uni- code characters right-to-left as connected Arabic glyphs. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J03-1005/parts/0-Table-c5.png",
        "Caption": "Table 6: Type-level Results: Each cell report the type- level accuracy computed against the most frequent tag of each word type. The state-to-tag mapping is obtained from the best hyperparameter setting for 1-1 mapping shown in Table 3. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Figure-c1.png",
        "Caption": "Table 4: Effect of Arabic stemming features on coref- erence resolution. The row marked with \u201cTruth\u201d represents the results with \u201ctrue\u201d mentions while the row marked with \u201cSystem\u201d represents that mentions are detected by the system. Numbers under \u201cECM- F\u201d are Entity-Constrained-Mention F-measure and numbers under \u201cACE-Val\u201d are ACE-values. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/N06-2049/parts/0-Table-c4.png",
        "Caption": "Figure 3 Using Chinese translations as the distributional elements to extract a set of English paraphrastic patterns from a large English corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P07-1068/parts/0-Table-c3.png",
        "Caption": "Figure 3: A Parallel Fragment Extraction System",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1081/parts/0-Figure-c2.png",
        "Caption": "Figure 2. Percentage of examples of major syntactic classes.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-1004/parts/0-Table-c14.png",
        "Caption": "Figure 1: Graphical model of HM-BiTAM",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P13-1147/parts/0-Table-c5.png",
        "Caption": "Figure 2: A multiword expression in HeiST",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P01-1005/parts/0-Figure-c1.png",
        "Caption": "Table 3: Accuracy scores for WSJ-PTB converted with head rules of Yamada and Matsumoto (2003) and labeling rules of Nivre (2006). Best dev setting: k = 3, \u03b1 = 0.4. Results marked with \u2020 use additional information sources and are not directly comparable to the others. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D11-1044/parts/0-Figure-c1.png",
        "Caption": "Figure 5: Macro-accuracy for multilingual bootstrapping (versus cross-lingual framework) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmert_n09/parts/0-Table-c3.png",
        "Caption": "Figure 1. The Lattice of the 8 Patterns.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W12-3311/parts/0-Table-c3.png",
        "Caption": "Figure 1: The Classification Process",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-1115/parts/0-Table-c2.png",
        "Caption": "Table 7: Average feature values across best translations of sentences in the MT03 tuning set, both before MERT (column 2) and after (column 3). \u201cSame\u201d versions of tree- to-tree configuration features are shown; the rarer \u201cswap\u201d features showed a similar trend. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2018/P11-1061/parts/0-Table-c1.png",
        "Caption": "Table 10 Translation results on the Hansards task. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J10-3003/parts/0-Figure-c7.png",
        "Caption": "                 sentence length Figure 6: Time consumption of the various change types in ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-1020/parts/0-Table-c4.png",
        "Caption": "Table 1: Comparison of SWSD systems",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D12-1016/parts/0-Table-c3.png",
        "Caption": "Table 3: Results of the baseline model: best 5 guesses",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W01-1408/parts/0-Table-c5.png",
        "Caption": "Table 4: A subjective pronoun resolution example",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P12-1027/parts/0-Figure-c2.png",
        "Caption": "Table 5: Test results for POS tagging. Best baseline results are underlined and the overall best results bold. * indicates a significant difference between the best baseline and a PCRF model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/Pmorph_p00/parts/0-Table-c8.png",
        "Caption": "Table 5. Another example of some discovered paraphrases.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P05-1045/parts/0-Table-c5.png",
        "Caption": "Table 5: Results for cascading minority-preference DAC system \u2014 DACCMP (consult classifiers in reverse order of frequency of class); \u201cER\u201d refers to error reduction in percent over standard multiclass SVM (Table 2) ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W11-2605/parts/0-Table-c1.png",
        "Caption": "Table 7: Syncretism Example 1",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/N06-1027/parts/0-Table-c3.png",
        "Caption": "Figure 2: The error frequency distributions for confusing the correct sense with another sense of the given similarity when using a 5-word co-occurrence window as context. Dashed lines indicate the null models. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P03-1009/parts/0-Table-c1.png",
        "Caption": "Table 2: Overall performance of the 3 systems",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W05-1518/parts/0-Table-c6.png",
        "Caption": "Figure 2: Translation Model (IBM Model 4)",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W08-0335/parts/0-Table-c5.png",
        "Caption": "Table 2: Results \u2014 Evaluation A.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P10-2025/parts/0-Figure-c2.png",
        "Caption": "Table 1 Semi-fixed MWEs in French and English. The French adverb \u00e0 terme (\u2018in the end\u2019) can be modified by a small set of adjectives, and in turn some of these adjectives can be modified by an adverb such as tr\u00e8s (\u2018very\u2019). Similar restrictions appear in English. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-1014/parts/0-Figure-c5.png",
        "Caption": "Table 8 BNC results for other-anaphora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W13-3208/parts/0-Table-c1.png",
        "Caption": "Figure 2: Information Bottleneck (IB) iterative clustering. D is the Kullback-Leibler distance. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W10-1761/parts/0-Figure-c2.png",
        "Caption": "Table 2: Rhetorical pattern of C-Question",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/D09-1138/parts/0-Table-c5.png",
        "Caption": "Table 4: NIST08 Chinese-English translation BLEU",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W06-1630/parts/0-Table-c4.png",
        "Caption": "Figure 2: Architecture of NILER system.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/C04-1067/parts/0-Table-c4.png",
        "Caption": "Figure 3: Comparison of paraphrase generators. Top: the MOSES baseline; middle and bold: the \u201ctrue-score\u201d MCPG; down: the \u201ctranslator\u201d MCPG. The use of \u201ctrue-score\u201d improves the MCPG per- formances. MCPG reaches MOSES performance level. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P00175/parts/0-Table-c11.png",
        "Caption": "Figure 6: An non-regular OT approximation.",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J06-2001/parts/0-Table-c7.png",
        "Caption": "Table 2: Distribution of Error Types",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/E12-2021/parts/0-Figure-c2.png",
        "Caption": "Table 1: Meta-evaluation results at document level",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J10-3003/parts/0-Figure-c5.png",
        "Caption": "Figure 8 Order in which the English source positions are covered for the English-to-German reordering example given in Figure 7. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J04-2003/parts/0-Table-c6.png",
        "Caption": "Table 3. Data used for training SMT models (all counts in millions). Parallel data sets refer to the bitexts aligned to English and their token counts include both languages. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P12-1016/parts/0-Figure-c1.png",
        "Caption": "Table 4: Example input and best output found",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-4005/parts/0-Table-c22.png",
        "Caption": "Figure 4 Impact of corpus size (measured in number of running words in the corpus) on vocabulary size (measured in number of different full-form words found in the corpus) for the German part of the Verbmobil corpus. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/J13-1007/parts/0-Figure-c5.png",
        "Caption": "Figure 1: String-to-tree configurations; each is associated with a feature that counts its occurrences in a derivation. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P11-2037/parts/0-Figure-c1.png",
        "Caption": "Figure 4: Relation clusters and a few individual relations. Edge labels show the size of the inter- section. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-1056/parts/0-Table-c5.png",
        "Caption": "Table 15 Size of training data set and the adaptation results on AS open. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/C08-1014/parts/0-Figure-c2.png",
        "Caption": "Figure 4 The path of Selection. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/W08-0703/parts/0-Figure-c1.png",
        "Caption": "Table 1: Recall (R), Precision (P) and Mean Average Pre- cision (MAP) when only matching template hypotheses directly. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J06-2001/parts/0-Table-c5.png",
        "Caption": "Table 1: Approximate times in seconds to generate predictions of maximum word sequence length M , on a 1.2GHz processor, for the MEMD model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/P11-3012/parts/0-Table-c1.png",
        "Caption": "Fig. 1. System architecture overview",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/P06-2124/parts/0-Table-c1.png",
        "Caption": "Table 2: English Eve corpus results. Standard deviations are in parentheses; \u2217 denotes a significant difference from the M ORTAG model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/D13-1060/parts/0-Table-c4.png",
        "Caption": "Table 8 BNC results for other-anaphora. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Training-Set-2018/PTASL_n09/parts/0-Figure-c4.png",
        "Caption": "Figure 1: CoreMRR scores with different \u03b1 values using score combination. A higher \u03b1 puts more weight on the phonetic model. ",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/W03-1728/parts/0-Table-c3.png",
        "Caption": "Table 3. Results changing beam width k of the tree",
        "Matched": false
    },
    {
        "Figure_path": "./rawdata/Test-Set-2016/J05-1004/parts/0-Table-c6.png",
        "Caption": "Table 2: Entity type constraints.",
        "Matched": false
    }
]